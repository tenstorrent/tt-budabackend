# git checkout 531929601
# pytest yolov5

devices:
  arch: wormhole_b0

queues:

  # input
  ims_1:                                                                                     {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 800], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x0]]}

  # output
  yolov5_s_Ribbon.output_concatenate_259:                                                    {input: concatenate_259.dc.sparse_matmul.4.lc2_output_nop_0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [197, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0xcb20020]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4139e80], [5, 0x4003bf20], [0, 0x291e0c0], [0, 0x40062e40]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x401bbd20], [5, 0x428f6a0], [5, 0x400ad3e0], [0, 0x29c1760]]}
  input_1_conv2d_0_fork_clone1328:                                                           {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400e1b60], [1, 0x41c9240], [1, 0x40097b20]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x29ab7c0], [2, 0x4007e280], [3, 0x6973880], [3, 0x401a6160]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x98a6b60], [4, 0x401bcd60], [5, 0x42906e0], [5, 0x400ae420]]}
  input_1_conv2d_0:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29c27a0], [0, 0x400e23a0], [1, 0x41c9a80]]}
  input_2_conv2d_0:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40098360]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x29ac020], [2, 0x4007eae0], [3, 0x69740e0], [3, 0x401a69c0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x98a7ba0], [4, 0x401bdda0], [5, 0x4291720], [5, 0x400af460]]}
  input_1_conv2d_0_fork_clone1326:                                                           {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29c2fe0], [0, 0x400e2be0], [1, 0x41ca2c0]]}
  lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x40098ba0], [2, 0x29ac880], [2, 0x4007f340], [3, 0x6974940]]}
  lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x401a7220], [4, 0x98a8be0], [4, 0x401bede0], [5, 0x4292760]]}
  input_1_conv2d_3_fork_clone424:                                                            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400b04a0], [0, 0x29c3820], [0, 0x400e3420]]}
  lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x41cab00], [1, 0x40099820], [2, 0x29ad500], [2, 0x4007ffc0]]}
  lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x4007d240], [3, 0x6972840], [3, 0x401a5120], [4, 0x98a5b20]]}
  input_1_conv2d_3:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401a8260], [4, 0x98a9c20], [4, 0x401bfe20]]}
  input_2_conv2d_3:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42937a0]]}
  lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x400b1500], [0, 0x29c4880], [0, 0x400e4480], [1, 0x41cb780]]}
  lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:              {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4009a4a0], [2, 0x29ae180], [2, 0x40080c40], [3, 0x69b65e0]]}
  input_1_conv2d_3_fork_clone422:                                                            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401a92c0], [4, 0x98aac80], [4, 0x401c0e80]]}
  input_1_conv2d_6:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4294800], [5, 0x400b2180]]}
  input_2_conv2d_6:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29c5500]]}
  input_1_conv2d_9:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400e5100]]}
  input_2_conv2d_9:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41cc400]]}
  lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4009b4e0]]}
  lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x29af1c0]]}
  input_1_conv2d_12_fork_clone460:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40081c80]]}
  lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x69b7620]]}
  lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x401b75c0]]}
  input_1_conv2d_12:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29b2520]]}
  input_2_conv2d_12:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400cb740]]}
  lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x41b5220]]}
  lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4008b7e0]]}
  input_1_conv2d_12_fork_clone458:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x299c380]]}
  input_1_conv2d_16:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40072f60], [3, 0x6970760]]}
  input_2_conv2d_16:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401a2000]]}
  input_1_conv2d_20:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x989e900]]}
  input_2_conv2d_20:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401b6560]]}
  lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 39], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4286920]]}
  lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x400aa300]]}
  input_1_conv2d_23:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [3, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29b3da0], [0, 0x400cbf80], [1, 0x41b5d40], [1, 0x4008c820]]}
  input_2_conv2d_23:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x299dc00], [2, 0x400737a0], [3, 0x6970fa0], [3, 0x401a2840]]}
  input_1_conv2d_26:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98a09a0]]}
  input_2_conv2d_26:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400a92a0]]}
  input_1_conv2d_29:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4289ee0]]}
  input_2_conv2d_29:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400ab340]]}
  lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 40], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x29bd000], [0, 0x400d51e0]]}
  lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x41befa0], [1, 0x40095a80]]}
  input_1_conv2d_32:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [3, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x299e440], [2, 0x40073fe0]]}
  input_2_conv2d_32:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69717e0]]}
  input_1_conv2d_36:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401a3080]]}
  input_2_conv2d_36:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98a4ac0]]}
  lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 40], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x401b8600], [5, 0x428bf80]]}
  lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x400ac3a0], [0, 0x29c0720]]}
  input_1_conv2d_39:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [3, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400d8900], [1, 0x41bffe0]]}
  input_2_conv2d_39:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40096ac0]]}
  input_1_conv2d_43:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x29a76a0]]}
  input_2_conv2d_43:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x432a460]]}
  input_1_conv2d_47:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400a3d20]]}
  input_2_conv2d_47:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4016a740]]}
  lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 137], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2a7dac0]]}
  lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x4017ae60]]}
  input_1_conv2d_50:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4262160], [1, 0x400b2a00], [2, 0x29f5360], [2, 0x400b0060]]}
  input_2_conv2d_50:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a427c0], [3, 0x4021fa00], [4, 0x98b7000], [4, 0x40263ba0]]}
  input_1_conv2d_53:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x432b4c0]]}
  input_2_conv2d_53:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4016c7e0]]}
  input_1_conv2d_56:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a89740]]}
  input_2_conv2d_56:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4017bea0]]}
  lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 63], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4286a80]]}
  lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x400d7320]]}
  input_1_conv2d_59:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a19c80], [2, 0x400d4980]]}
  input_2_conv2d_59:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a43820], [3, 0x40220a60]]}
  input_1_conv2d_63:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98b8060]]}
  input_2_conv2d_63:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40264c00]]}
  lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 63], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x433b8e0]]}
  lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4016e880]]}
  input_1_conv2d_66:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x428c140], [1, 0x400d8360]]}
  input_2_conv2d_66:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a91960], [0, 0x4017df40]]}
  input_1_conv2d_70:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4025b980]]}
  input_2_conv2d_70:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98abce0]]}
  lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 63], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x401c1ee0]]}
  lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4295040]]}
  input_1_conv2d_73:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400b29c0], [0, 0x29c5d40]]}
  input_2_conv2d_73:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400e5940], [1, 0x41ccc40]]}
  input_1_conv2d_77:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4009c420]]}
  input_2_conv2d_77:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x29b0200]]}
  input_1_conv2d_81:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40083500]]}
  input_2_conv2d_81:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69b8560]]}
  lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 137], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x401eb340]]}
  lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x98add80]]}
  input_1_conv2d_84:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [18, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401c75a0], [5, 0x4296080], [5, 0x400d72e0], [0, 0x29ea660]]}
  input_2_conv2d_84:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400e69a0], [1, 0x41cdca0], [1, 0x400ac840], [2, 0x29b22a0]]}
  input_1_conv2d_87:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401aa320]]}
  input_2_conv2d_87:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69bc680]]}
  input_1_conv2d_90:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401f6fc0]]}
  input_2_conv2d_90:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98aedc0]]}
  lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 23], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x402599c0], [5, 0x43284a0]]}
  lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40169700], [0, 0x2a7ca80]]}
  input_1_conv2d_93:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [9, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400e8a40], [1, 0x41cfd40]]}
  input_2_conv2d_93:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400ae8e0]]}
  input_1_conv2d_97:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x29b4340]]}
  input_2_conv2d_97:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400abf40]]}
  input_1_conv2d_101:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69c07a0]]}
  input_2_conv2d_101:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402177e0]]}
  input_1_conv2d_104:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69755c0]]}
  input_2_conv2d_104:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98b2ee0]]}
  lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 35], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x67fd380], [3, 0x4006ce40], [4, 0x9810000], [4, 0x4006ff00]]}
  lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4138e40], [5, 0x4003aee0], [0, 0x291d080], [0, 0x40061e00]]}
  lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 35], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4131f20], [1, 0x4003e560], [2, 0x2920160], [2, 0x4003f020]]}
  lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x68003c0], [3, 0x4006fe80], [4, 0x9813040], [4, 0x40072f40]]}
  lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 35], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x41b14c0], [5, 0x40066260], [0, 0x2992da0], [0, 0x400ac900]]}
  lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4134f60], [1, 0x400415a0], [2, 0x29231a0], [2, 0x40042060]]}
  input_1_conv2d_111:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6801400], [3, 0x40070ec0]]}
  input_2_conv2d_111:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9814080], [4, 0x40073f80]]}
  input_1_conv2d_114:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x413a6e0]]}
  input_2_conv2d_114:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4003c780]]}
  lc.input_tensor.resize2d_117.dc.sparse_matmul.3.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 20], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x291e920]]}
  lc.input_tensor.resize2d_117.dc.sparse_matmul.3.1:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x400636a0]]}
  input_1_conv2d_119:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4135fa0]]}
  input_2_conv2d_119:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4003cf80]]}
  input_1_conv2d_122:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x29241e0]]}
  input_2_conv2d_122:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400430a0]]}
  lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 63], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x6883420]]}
  lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x400f2ee0]]}
  input_1_conv2d_125:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98181a0], [4, 0x400780a0]]}
  input_2_conv2d_125:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x417b700], [5, 0x400408a0]]}
  input_1_conv2d_128:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29204c0]]}
  input_2_conv2d_128:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400646e0]]}
  input_1_conv2d_132:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41567c0]]}
  input_2_conv2d_132:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40044680]]}
  input_1_conv2d_135:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x292c400]]}
  input_2_conv2d_135:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40045140]]}
  lc.input_tensor.resize2d_138.dc.sparse_matmul.3.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x6888ae0]]}
  lc.input_tensor.resize2d_138.dc.sparse_matmul.3.1:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x40009260]]}
  input_1_conv2d_140:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40000000]]}
  input_2_conv2d_140:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40d0100]]}
  input_1_conv2d_143:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40000000], [2, 0x28e0100]]}
  input_2_conv2d_143:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40000000]]}
  lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x67c0100], [3, 0x40000000], [4, 0x97a0100], [4, 0x40000000], [5, 0x40d0100]]}
  lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40000000], [0, 0x28e21a0], [0, 0x40008220], [1, 0x40d1160], [1, 0x40001060]]}
  input_1_conv2d_146:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [6, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x28e1160], [2, 0x40001060], [3, 0x67c1460]]}
  input_2_conv2d_146:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40001360]]}
  input_1_conv2d_149:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97a1460], [4, 0x40001360]]}
  input_2_conv2d_149:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d1460]]}
  input_1_conv2d_153:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 4], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40001040], [0, 0x28e31e0]]}
  input_2_conv2d_153:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x28e0100]]}
  input_1_conv2d_156:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40d21a0], [1, 0x400020a0]]}
  input_2_conv2d_156:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x28e7300]]}
  input_1_multiply_158:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40007200], [3, 0x67c7600]]}
  input_2_concatenate_160:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [2, 25], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400023c0], [4, 0x97a5580], [4, 0x40005480], [5, 0x40d24c0]]}
  input_1_multiply_164:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40005160], [0, 0x28e7300]]}
  input_1_concatenate_165:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [2, 25], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4000a2a0], [1, 0x40da3c0], [1, 0x4000a2c0], [2, 0x28eb420]]}
  input_1_multiply_168:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [2, 25], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400092a0], [3, 0x67c96a0], [3, 0x40035060], [4, 0x97d8220]]}
  input_1_multiply_171:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [2, 25], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40038120], [5, 0x4105160], [5, 0x40007200], [0, 0x28e93a0]]}
  lc.input_tensor.reshape_174.dc.sparse_matmul.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x4003cf40], [1, 0x410d060], [1, 0x4003cf60]]}
  lc.input_tensor.reshape_174.dc.sparse_matmul.3.1:                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x291e0c0], [2, 0x4003bf40], [3, 0x67fc340]]}
  lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 59], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40067d00], [4, 0x980aec0], [4, 0x4006adc0]]}
  lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4137e00], [5, 0x40039ea0], [0, 0x291c040]]}
  input_1_conv2d_177:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4003d4e0], [1, 0x410d600]]}
  input_2_conv2d_177:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4003d500], [2, 0x291f100]]}
  input_1_conv2d_181:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x988cf00]]}
  input_2_conv2d_181:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400425e0]]}
  input_1_conv2d_184:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4181e40]]}
  input_2_conv2d_184:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4006bfc0]]}
  lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x295e360], [2, 0x4004c620], [3, 0x692d720]]}
  lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x4019b8c0], [4, 0x988bec0], [4, 0x401084c0]]}
  input_1_conv2d_187:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41b4500], [5, 0x400692a0]]}
  input_2_conv2d_187:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2995de0], [0, 0x400af940]]}
  input_1_conv2d_190:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x418a060]]}
  input_2_conv2d_190:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4006e060]]}
  input_1_conv2d_194:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2960480]]}
  input_2_conv2d_194:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4004e740]]}
  input_1_conv2d_197:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x692f840]]}
  input_2_conv2d_197:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4019c900]]}
  input_1_multiply_199:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9871820], [4, 0x400ede20]]}
  input_2_concatenate_201:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40109500], [5, 0x41d8e20]]}
  input_1_multiply_205:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4008dbc0], [0, 0x2996e40]]}
  input_1_concatenate_206:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400b09a0], [1, 0x419a480]]}
  input_1_multiply_209:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40070100], [2, 0x2980ca0]]}
  input_1_multiply_212:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 13], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40052860], [3, 0x6950060]]}
  lc.input_tensor.reshape_215.dc.sparse_matmul.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x401a0a20], [4, 0x989d320], [4, 0x40123ba0]]}
  lc.input_tensor.reshape_215.dc.sparse_matmul.3.1:                                          {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41f34c0], [5, 0x400a8260], [0, 0x29b14e0]]}
  lc.input_tensor.reshape_217.dc.sparse_matmul.4.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x400cb040], [1, 0x41b4b20]]}
  lc.input_tensor.reshape_217.dc.sparse_matmul.4.1:                                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4008a7a0], [2, 0x299b340]]}
  lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 70], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x4006cf00], [3, 0x696a700]]}
  lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x401a0fc0], [4, 0x989d8c0]]}
  input_1_conv2d_218:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [9, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40124140], [5, 0x41f4500]]}
  input_2_conv2d_218:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40048220]]}
  input_1_conv2d_222:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x983cac0], [4, 0x4009c9c0]]}
  input_2_conv2d_222:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x417c760]]}
  input_1_conv2d_225:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40041900], [0, 0x2940ce0]]}
  input_2_conv2d_225:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40066780]]}
  lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 23], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4176fe0], [1, 0x400487a0]]}
  lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x293c820], [2, 0x400471e0]]}
  input_1_conv2d_228:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [9, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x688ac00], [3, 0x400f8040]]}
  input_2_conv2d_228:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x985d2e0]]}
  input_1_conv2d_231:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x400bd1e0], [5, 0x4180880]]}
  input_2_conv2d_231:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40051d20]]}
  input_1_conv2d_235:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2951100], [0, 0x4006a8a0]]}
  input_2_conv2d_235:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4178fa0]]}
  input_1_conv2d_238:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4004a760], [2, 0x293d860]]}
  input_2_conv2d_238:                                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400f3f20]]}
  input_1_multiply_240:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x691d020]]}
  input_2_concatenate_242:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4018a460]]}
  input_1_multiply_246:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9861400]]}
  input_1_concatenate_247:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x400dda00]]}
  input_1_multiply_250:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41a10a0]]}
  input_1_multiply_253:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40055e40]]}
  lc.input_tensor.reshape_256.dc.sparse_matmul.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2992120]]}
  lc.input_tensor.reshape_256.dc.sparse_matmul.3.1:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x400ab8c0]]}
  lc.input_tensor.reshape_258.dc.sparse_matmul.4.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x41811c0]]}
  lc.input_tensor.reshape_258.dc.sparse_matmul.4.1:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4006af80]]}
  lc.input_tensor.concatenate_259.dc.sparse_matmul.4.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x295e080], [2, 0x4004c340], [3, 0x692d440]]}
  lc.input_tensor.concatenate_259.dc.sparse_matmul.4.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x4019a880]]}

  # epoch_to_epoch
  e2e__fused_op_3_0:                                                                         {input: _fused_op_3, type: queue, entries: 64, grid_size: [2, 1], t: 5, mblock: [10, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a3e5a0], [2, 0x400f92a0]]}
  e2e__fused_op_2_0:                                                                         {input: _fused_op_2, type: queue, entries: 64, grid_size: [2, 1], t: 5, mblock: [10, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a44880], [3, 0x40221ac0]]}
  e2e__fused_op_5_0:                                                                         {input: _fused_op_5, type: queue, entries: 64, grid_size: [2, 1], t: 5, mblock: [10, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c0280], [4, 0x40266ca0]]}
  e2e__fused_op_21_0:                                                                        {input: _fused_op_21, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [13, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a929c0]]}
  e2e__fused_op_15_0:                                                                        {input: _fused_op_15, type: queue, entries: 64, grid_size: [1, 1], t: 13, mblock: [1, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4016f8c0]]}
  e2e__fused_op_20_0:                                                                        {input: _fused_op_20, type: queue, entries: 64, grid_size: [1, 1], t: 13, mblock: [1, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4340fa0]]}
  e2e__fused_op_32_0:                                                                        {input: _fused_op_32, type: queue, entries: 64, grid_size: [1, 1], t: 4, mblock: [1, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400fcc80]]}
  e2e__fused_op_24_0:                                                                        {input: _fused_op_24, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [13, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b0a60]]}
  e2e__fused_op_14_0:                                                                        {input: _fused_op_14, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4017efa0]]}
  e2e__fused_op_40_0:                                                                        {input: _fused_op_40, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [25, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a3e5a0]]}
  e2e_concatenate_139.dc.concatenate.0_0:                                                    {input: concatenate_139.dc.concatenate.0, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [25, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a44880], [3, 0x40221ac0]]}
  e2e__fused_op_44_0:                                                                        {input: _fused_op_44, type: queue, entries: 64, grid_size: [2, 4], t: 1, mblock: [25, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a929c0], [0, 0x41ae2fc0], [1, 0x4fe4a80], [1, 0x4091cca0], [2, 0x36f05c0], [2, 0x407932c0], [3, 0x83a88a0], [3, 0x41b85ae0]]}
  e2e__fused_op_39_0:                                                                        {input: _fused_op_39, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [13, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400f92a0]]}
  e2e_conv2d_194.dc.matmul.8_0:                                                              {input: conv2d_194.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 1], t: 8, mblock: [13, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x99c42a0]]}
  e2e__fused_op_33_0:                                                                        {input: _fused_op_33, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c0280], [4, 0x40266ca0], [5, 0x4340fa0], [5, 0x4016f8c0]]}
  e2e_conv2d_238.dc.matmul.8_0:                                                              {input: conv2d_238.dc.matmul.8, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4036acc0], [5, 0x4444fc0]]}
  e2e_concatenate_259.dc.concatenate.2_transpose_nop_2_7347_0:                               {input: concatenate_259.dc.concatenate.2_transpose_nop_2_7347, type: queue, entries: 64, grid_size: [2, 1], t: 3, mblock: [25, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x402738e0], [0, 0x2dbf1e0]]}
  e2e_reshape_217.dc.sparse_matmul.4.lc2_0:                                                  {input: reshape_217.dc.sparse_matmul.4.lc2, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [19, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4017efa0], [1, 0x42b0a60]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 64
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, ims_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 160}}
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [4, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_0_fork_clone1328],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 240, hstack: 3, vstack: 80],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, ims_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 160}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [4, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_0, input_2_conv2d_0],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 800}, vslice: 10], input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 240, hstack: 3, vstack: 80],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, ims_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 160}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [4, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_0_fork_clone1326],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 240, hstack: 3, vstack: 80],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 6], grid_size: [2, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11],
         t: 10, mblock: [20, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 0}}
    conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 10],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 10, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 80}}
    conv2d_3.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [4, 1], inputs: [conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_3_fork_clone424],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 120, hstack: 3, vstack: 40],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 5, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 10],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 10, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 80}}
    conv2d_3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 0], grid_size: [4, 1], inputs: [conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_3, input_2_conv2d_3], grid_transpose: true,
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 120, hstack: 3, vstack: 40],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 10],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 10, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 80}}
    conv2d_3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [4, 1], inputs: [conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_3_fork_clone422],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 120, hstack: 3, vstack: 40],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_3.dc.conv2d.1.dc.matmul.11, conv2d_3.dc.conv2d.3.dc.matmul.11, conv2d_3.dc.conv2d.5.dc.matmul.11],
         t: 5, mblock: [10, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 1}}
    conv2d_6.dc.matmul.8: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [_fused_op_1, input_1_conv2d_6, input_2_conv2d_6], grid_transpose: true,
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [conv2d_6.dc.matmul.8, conv2d_6.dc.matmul.8],
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 2}}
    conv2d_9.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [2, 1], inputs: [_fused_op_2, input_1_conv2d_9, input_2_conv2d_9], grid_transpose: true,
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 1, input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [7, 2], grid_size: [2, 1], inputs: [conv2d_9.dc.matmul.8, conv2d_9.dc.matmul.8], grid_transpose: true,
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 2}}
    conv2d_16.dc.matmul.8: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_1, input_1_conv2d_16, input_2_conv2d_16], grid_transpose: true,
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_5: {type: fused_op, grid_loc: [6, 7], grid_size: [2, 1], inputs: [conv2d_16.dc.matmul.8, conv2d_16.dc.matmul.8],
         t: 5, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 2}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 64
    conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_3_0, lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 100}}
    conv2d_12.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_12_fork_clone460],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 60, hstack: 3, vstack: 20],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_3_0, lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 100}}
    conv2d_12.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_12, input_2_conv2d_12],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 10], input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 60, hstack: 3, vstack: 20],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_3_0, lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 10, mblock: [30, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 100}}
    conv2d_12.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_12_fork_clone458],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 10}, hslice: 10], input_0_tms: [vslice: 60, hstack: 3, vstack: 20],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_4: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_12.dc.conv2d.1.dc.matmul.11, conv2d_12.dc.conv2d.3.dc.matmul.11, conv2d_12.dc.conv2d.5.dc.matmul.11, e2e__fused_op_2_0],
         t: 10, mblock: [10, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 48], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [vslice: 2],
         attributes: {fused_op_id: 4}}
    concatenate_19.dc.concatenate.0: {type: splice, grid_loc: [0, 7], grid_size: [1, 1], inputs: [_fused_op_4, e2e__fused_op_5_0],
         t: 10, mblock: [10, 2], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 2],
         attributes: {input0: [0, 10, 10], input1: [0, 10, 10]}}
    conv2d_20.dc.matmul.8: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [concatenate_19.dc.concatenate.0, input_1_conv2d_20, input_2_conv2d_20],
         t: 10, mblock: [10, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 200}, vslice: 10], input_1_tms: [broadcast: {c: 10}, hslice: 10],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_20.dc.matmul.8, conv2d_20.dc.matmul.8],
         t: 10, mblock: [10, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 6}}
    conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_6, lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [45, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 10],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 10, num_index_tiles: 1, num_sparse_tiles: 39, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 20}}
    conv2d_23.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_23, input_2_conv2d_23],
         t: 5, mblock: [5, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 90, hstack: 9, vstack: 10],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    _fused_op_7: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_23.dc.matmul.11, conv2d_23.dc.matmul.11],
         t: 5, mblock: [5, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 10, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 7}}
    conv2d_26.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_7, input_1_conv2d_26, input_2_conv2d_26],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_26.dc.matmul.8, conv2d_26.dc.matmul.8],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 10, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    conv2d_29.dc.matmul.8: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_8, input_1_conv2d_29, input_2_conv2d_29],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_9: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_29.dc.matmul.8, conv2d_29.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 9}}
    conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_9, lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [45, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 40, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 50}}
    conv2d_32.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_32, input_2_conv2d_32],
         t: 5, mblock: [5, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 90, hstack: 9, vstack: 10],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_8__fused_op_10: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_8],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [396], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_10: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_32.dc.matmul.11, conv2d_32.dc.matmul.11, buffer_0__fused_op_8__fused_op_10],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 380], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    conv2d_36.dc.matmul.8: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_10, input_1_conv2d_36, input_2_conv2d_36],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_11: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [conv2d_36.dc.matmul.8, conv2d_36.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 9}}
    conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_11, lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [45, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 40, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 50}}
    conv2d_39.dc.matmul.11: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_39, input_2_conv2d_39],
         t: 5, mblock: [5, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 90, hstack: 9, vstack: 10],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_7_conv2d_43.dc.matmul.8: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [_fused_op_7],
         t: 5, mblock: [5, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [352], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    conv2d_43.dc.matmul.8: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0__fused_op_7_conv2d_43.dc.matmul.8, input_1_conv2d_43, input_2_conv2d_43],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_0__fused_op_10__fused_op_12: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_10],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [396], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_12: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_39.dc.matmul.11, conv2d_39.dc.matmul.11, buffer_0__fused_op_10__fused_op_12],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 380], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    _fused_op_13: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_43.dc.matmul.8, conv2d_43.dc.matmul.8],
         t: 5, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [388, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    concatenate_46.dc.concatenate.0: {type: splice, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_12, _fused_op_13],
         t: 5, mblock: [5, 2], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 348], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 5, 5]}}
    conv2d_47.dc.matmul.8: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [concatenate_46.dc.concatenate.0, input_1_conv2d_47, input_2_conv2d_47],
         t: 5, mblock: [5, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_14: {type: fused_op, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_47.dc.matmul.8, conv2d_47.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 14}}
    conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_14, lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 13, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 137, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 25}}
    conv2d_50.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 4], inputs: [conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_50, input_2_conv2d_50],
         t: 13, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    _fused_op_15: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_50.dc.matmul.11, conv2d_50.dc.matmul.11],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_53.dc.matmul.8: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_15, input_1_conv2d_53, input_2_conv2d_53],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_16: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_53.dc.matmul.8, conv2d_53.dc.matmul.8],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 26, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 16}}
    conv2d_56.dc.matmul.8: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_16, input_1_conv2d_56, input_2_conv2d_56],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_56.dc.matmul.8, conv2d_56.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 13], input_0_tms: [vstack: 13],
         attributes: {fused_op_id: 17}}
    conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_17, lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 13, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 63, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 13}}
    conv2d_59.dc.matmul.11: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_59, input_2_conv2d_59],
         t: 13, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    _fused_op_18: {type: fused_op, grid_loc: [6, 5], grid_size: [1, 1], inputs: [conv2d_59.dc.matmul.11, conv2d_59.dc.matmul.11, _fused_op_16],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 412], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 18}}
    conv2d_63.dc.matmul.8: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_18, input_1_conv2d_63, input_2_conv2d_63],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_19: {type: fused_op, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_63.dc.matmul.8, conv2d_63.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 13], input_0_tms: [vstack: 13],
         attributes: {fused_op_id: 17}}
    conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_19, lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 13, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 63, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 13}}
    conv2d_66.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 2], inputs: [conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_66, input_2_conv2d_66],
         t: 13, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    _fused_op_20: {type: fused_op, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_66.dc.matmul.11, conv2d_66.dc.matmul.11, _fused_op_18],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 412], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 18}}
    conv2d_70.dc.matmul.8: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [_fused_op_20, input_1_conv2d_70, input_2_conv2d_70],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_21: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_70.dc.matmul.8, conv2d_70.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 13], input_0_tms: [vstack: 13],
         attributes: {fused_op_id: 17}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 64
    conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_21_0, lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 13, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 63, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 13}}
    conv2d_73.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_73, input_2_conv2d_73],
         t: 13, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    conv2d_77.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_15_0, input_1_conv2d_77, input_2_conv2d_77],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [vstack: 13, broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_22: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_73.dc.matmul.11, conv2d_73.dc.matmul.11, e2e__fused_op_20_0],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 18}}
    _fused_op_23: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 4], inputs: [conv2d_77.dc.matmul.8, conv2d_77.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 4], input_0_tms: [hstack: 4],
         attributes: {fused_op_id: 23}}
    concatenate_80.dc.concatenate.0: {type: splice, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_22, _fused_op_23],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 13],
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1]}}
    conv2d_81.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [concatenate_80.dc.concatenate.0, input_1_conv2d_81, input_2_conv2d_81],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_24: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_81.dc.matmul.8, conv2d_81.dc.matmul.8],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 13], input_0_tms: [vstack: 13],
         attributes: {fused_op_id: 24}}
    conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 4, mblock: [9, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 137, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 13}}
    conv2d_84.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_84, input_2_conv2d_84],
         t: 4, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [conv2d_84.dc.matmul.11, conv2d_84.dc.matmul.11],
         t: 4, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 8, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 25}}
    conv2d_87.dc.matmul.8: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_25, input_1_conv2d_87, input_2_conv2d_87],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_26: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_87.dc.matmul.8, conv2d_87.dc.matmul.8],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 8, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_90.dc.matmul.8: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_26, input_1_conv2d_90, input_2_conv2d_90],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_27: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_90.dc.matmul.8, conv2d_90.dc.matmul.8],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_27, lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [9, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 4],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 23, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_93.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_93, input_2_conv2d_93],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    conv2d_97.dc.matmul.8: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_25, input_1_conv2d_97, input_2_conv2d_97],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [220, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_28: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_93.dc.matmul.11, conv2d_93.dc.matmul.11, _fused_op_26],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 404], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 4], input_0_tms: [vslice: 4],
         attributes: {fused_op_id: 28}}
    _fused_op_29: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_97.dc.matmul.8, conv2d_97.dc.matmul.8],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [412, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    concatenate_100.dc.concatenate.0: {type: splice, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_28, _fused_op_29],
         t: 4, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 396], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2]}}
    conv2d_101.dc.matmul.8: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [concatenate_100.dc.concatenate.0, input_1_conv2d_101, input_2_conv2d_101],
         t: 4, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_30: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_101.dc.matmul.8, conv2d_101.dc.matmul.8],
         t: 4, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 25}}
    conv2d_104.dc.matmul.8: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [_fused_op_30, input_1_conv2d_104, input_2_conv2d_104],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_31: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_104.dc.matmul.8, conv2d_104.dc.matmul.8],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 8, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [4, 1], inputs: [lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.0, _fused_op_31, lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [25, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 4],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 35, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 1}}
    max_pool2d_107.dc.reduce_max.6: {type: reduce, grid_loc: [5, 3], grid_size: [4, 1], inputs: [max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25],
         attributes: {dim: z, type: max, z: 25}}
    max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [4, 1], inputs: [lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.0, max_pool2d_107.dc.reduce_max.6, lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [25, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 35, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 2}}
    max_pool2d_108.dc.reduce_max.6: {type: reduce, grid_loc: [7, 0], grid_size: [4, 1], inputs: [max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25],
         attributes: {dim: z, type: max, z: 25}}
    max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 4], grid_size: [4, 1], inputs: [lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.0, max_pool2d_108.dc.reduce_max.6, lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [25, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 35, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 2}}
    max_pool2d_109.dc.reduce_max.6: {type: reduce, grid_loc: [7, 4], grid_size: [4, 1], inputs: [max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25],
         attributes: {dim: z, type: max, z: 25}}
    buffer_0__fused_op_31_concatenate_110.dc.concatenate.0: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [_fused_op_31],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [420], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_max_pool2d_107.dc.reduce_max.6_concatenate_110.dc.concatenate.0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [max_pool2d_107.dc.reduce_max.6],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [420], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 4]}
    concatenate_110.dc.concatenate.0: {type: splice, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_31_concatenate_110.dc.concatenate.0, buffer_0_max_pool2d_107.dc.reduce_max.6_concatenate_110.dc.concatenate.0, max_pool2d_108.dc.reduce_max.6, max_pool2d_109.dc.reduce_max.6],
         t: 4, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 348, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [vslice: 4], input_2_tms: [vslice: 4],
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 2, 2], input3: [0, 2, 2]}}
    conv2d_111.dc.matmul.8: {type: matmul, grid_loc: [5, 1], grid_size: [1, 2], inputs: [concatenate_110.dc.concatenate.0, input_1_conv2d_111, input_2_conv2d_111],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}
    _fused_op_32: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_111.dc.matmul.8, conv2d_111.dc.matmul.8],
         t: 4, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 25}}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 64
    conv2d_114.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_32_0, input_1_conv2d_114, input_2_conv2d_114],
         t: 4, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 4], input_1_tms: [broadcast: {c: 4}, hslice: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 1], grid_size: [4, 1], inputs: [conv2d_114.dc.matmul.8, conv2d_114.dc.matmul.8], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 4], input_0_tms: [vstack: 4],
         attributes: {fused_op_id: 15}}
    resize2d_117.dc.sparse_matmul.3.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.resize2d_117.dc.sparse_matmul.3.0, _fused_op_33, lc.input_tensor.resize2d_117.dc.sparse_matmul.3.1],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 20, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 4}}
    concatenate_118.dc.concatenate.0: {type: splice, grid_loc: [0, 6], grid_size: [1, 1], inputs: [resize2d_117.dc.sparse_matmul.3.lc2, e2e__fused_op_24_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 26, 26], input1: [0, 26, 26]}}
    conv2d_119.dc.matmul.8: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [concatenate_118.dc.concatenate.0, input_1_conv2d_119, input_2_conv2d_119],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_34: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_119.dc.matmul.8, conv2d_119.dc.matmul.8],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 23}}
    conv2d_122.dc.matmul.8: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [_fused_op_34, input_1_conv2d_122, input_2_conv2d_122],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_35: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [conv2d_122.dc.matmul.8, conv2d_122.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 17}}
    conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_35, lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 13, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 63, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 13}}
    conv2d_125.dc.matmul.11: {type: matmul, grid_loc: [1, 4], grid_size: [1, 2], inputs: [conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_125, input_2_conv2d_125],
         t: 13, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    conv2d_128.dc.matmul.8: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [concatenate_118.dc.concatenate.0, input_1_conv2d_128, input_2_conv2d_128],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [332, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_36: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_125.dc.matmul.11, conv2d_125.dc.matmul.11],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 16}}
    _fused_op_37: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 4], inputs: [conv2d_128.dc.matmul.8, conv2d_128.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [411, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 4], input_0_tms: [hstack: 4],
         attributes: {fused_op_id: 23}}
    concatenate_131.dc.concatenate.0: {type: splice, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_36, _fused_op_37],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 412], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 13],
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1]}}
    conv2d_132.dc.matmul.8: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [concatenate_131.dc.concatenate.0, input_1_conv2d_132, input_2_conv2d_132],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_38: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_132.dc.matmul.8, conv2d_132.dc.matmul.8],
         t: 13, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_135.dc.matmul.8: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_38, input_1_conv2d_135, input_2_conv2d_135],
         t: 13, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, vslice: 13], input_1_tms: [broadcast: {c: 13}, hslice: 13],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_39: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_135.dc.matmul.8, conv2d_135.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 13], input_0_tms: [vstack: 13],
         attributes: {fused_op_id: 17}}
    resize2d_138.dc.sparse_matmul.3.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.resize2d_138.dc.sparse_matmul.3.0, _fused_op_39, lc.input_tensor.resize2d_138.dc.sparse_matmul.3.1],
         t: 1, mblock: [25, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 13, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    concatenate_139.dc.concatenate.0: {type: splice, grid_loc: [3, 2], grid_size: [2, 1], inputs: [resize2d_138.dc.sparse_matmul.3.lc2, e2e__fused_op_14_0], grid_transpose: true,
         t: 1, mblock: [25, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 25, 25], input1: [0, 25, 25]}}
    conv2d_140.dc.matmul.8: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [concatenate_139.dc.concatenate.0, input_1_conv2d_140, input_2_conv2d_140],
         t: 2, mblock: [25, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_40: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_140.dc.matmul.8, conv2d_140.dc.matmul.8],
         t: 2, mblock: [25, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 40}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 64
    conv2d_143.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e__fused_op_40_0, input_1_conv2d_143, input_2_conv2d_143],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_41: {type: fused_op, grid_loc: [0, 1], grid_size: [2, 1], inputs: [conv2d_143.dc.matmul.8, conv2d_143.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 41}}
    conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_41, lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [45, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 10}}
    conv2d_146.dc.matmul.11: {type: matmul, grid_loc: [1, 2], grid_size: [5, 1], inputs: [conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_146, input_2_conv2d_146], grid_transpose: true,
         t: 1, mblock: [5, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_149.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_concatenate_139.dc.concatenate.0_0, input_1_conv2d_149, input_2_conv2d_149], grid_transpose: true,
         t: 2, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_42: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_146.dc.matmul.11, conv2d_146.dc.matmul.11],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 41}}
    _fused_op_43: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 2], inputs: [conv2d_149.dc.matmul.8, conv2d_149.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2], input_0_tms: [hstack: 2],
         attributes: {fused_op_id: 43}}
    concatenate_152.dc.concatenate.0: {type: splice, grid_loc: [2, 4], grid_size: [2, 1], inputs: [_fused_op_42, _fused_op_43],
         t: 1, mblock: [25, 2], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 25, 25], input1: [0, 25, 25]}}
    conv2d_153.dc.matmul.8: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [concatenate_152.dc.concatenate.0, input_1_conv2d_153, input_2_conv2d_153],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 4], inputs: [conv2d_153.dc.matmul.8, conv2d_153.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 4], input_0_tms: [hstack: 4],
         attributes: {fused_op_id: 43}}
    conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [3, 1], inputs: [lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_44, lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [39, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 59, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 10}}
    conv2d_177.dc.matmul.11: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_177, input_2_conv2d_177],
         t: 1, mblock: [13, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    _fused_op_46: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_177.dc.matmul.11, conv2d_177.dc.matmul.11],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 17}}
    concatenate_180.dc.concatenate.0: {type: splice, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_46, e2e__fused_op_39_0],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 13, 13], input1: [0, 13, 13]}}
    conv2d_181.dc.matmul.8: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [concatenate_180.dc.concatenate.0, input_1_conv2d_181, input_2_conv2d_181],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_47: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_181.dc.matmul.8, conv2d_181.dc.matmul.8],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 23}}
    conv2d_184.dc.matmul.8: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_47, input_1_conv2d_184, input_2_conv2d_184],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_48: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_184.dc.matmul.8, conv2d_184.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 17}}
    conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 5], grid_size: [3, 1], inputs: [lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_48, lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [39, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 13}}
    conv2d_187.dc.matmul.11: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_187, input_2_conv2d_187],
         t: 1, mblock: [13, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    conv2d_190.dc.matmul.8: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [concatenate_180.dc.concatenate.0, input_1_conv2d_190, input_2_conv2d_190],
         t: 4, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [364, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_49: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_187.dc.matmul.11, conv2d_187.dc.matmul.11],
         t: 1, mblock: [13, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 17}}
    _fused_op_50: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 4], inputs: [conv2d_190.dc.matmul.8, conv2d_190.dc.matmul.8],
         t: 1, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [411, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 4], input_0_tms: [hstack: 4],
         attributes: {fused_op_id: 23}}
    concatenate_193.dc.concatenate.0: {type: splice, grid_loc: [7, 0], grid_size: [1, 1], inputs: [_fused_op_49, _fused_op_50],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 220], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 13, 13], input1: [0, 13, 13]}}
    conv2d_194.dc.matmul.8: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [concatenate_193.dc.concatenate.0, input_1_conv2d_194, input_2_conv2d_194],
         t: 8, mblock: [13, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}, hslice: 8], input_1_tms: [hslice: 8], input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 64
    conv2d_156.dc.matmul.8: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [e2e__fused_op_44_0, input_1_conv2d_156, input_2_conv2d_156],
         t: 1, mblock: [25, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 50}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_45_transpose_nop_7605: {type: nop, grid_loc: [1, 6], grid_size: [2, 1], inputs: [conv2d_156.dc.matmul.8],
         t: 4, mblock: [1, 25], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 4]}
    _fused_op_45: {type: fused_op, grid_loc: [3, 0], grid_size: [2, 2], inputs: [_fused_op_45_transpose_nop_7605, input_1_multiply_158, input_2_concatenate_160, input_1_multiply_164, input_1_concatenate_165, input_1_multiply_168, input_1_multiply_171],
         t: 1, mblock: [2, 25], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 24, 0, 24, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {c: 50}], input_1_tms: [broadcast: {c: 50}], input_0_tms: [vstack: 4],
         attributes: {fused_op_id: 45}}
    reshape_174.dc.sparse_matmul.3.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [3, 1], inputs: [lc.input_tensor.reshape_174.dc.sparse_matmul.3.0, _fused_op_45, lc.input_tensor.reshape_174.dc.sparse_matmul.3.1], grid_transpose: true,
         t: 1, mblock: [3, 25], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 8, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    _fused_op_51: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_194.dc.matmul.8_0, e2e_conv2d_194.dc.matmul.8_0],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8], input_0_tms: [hstack: 8],
         attributes: {fused_op_id: 24}}
    conv2d_197.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_51, input_1_conv2d_197, input_2_conv2d_197],
         t: 1, mblock: [13, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 13}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_52_transpose_nop_7632: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [conv2d_197.dc.matmul.8], grid_transpose: true,
         t: 4, mblock: [1, 13], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 4]}
    _fused_op_52: {type: fused_op, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_52_transpose_nop_7632, input_1_multiply_199, input_2_concatenate_201, input_1_multiply_205, input_1_concatenate_206, input_1_multiply_209, input_1_multiply_212],
         t: 1, mblock: [2, 13], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 4],
         attributes: {fused_op_id: 52}}
    reshape_215.dc.sparse_matmul.3.lc2: {type: matmul, grid_loc: [5, 3], grid_size: [3, 1], inputs: [lc.input_tensor.reshape_215.dc.sparse_matmul.3.0, _fused_op_52, lc.input_tensor.reshape_215.dc.sparse_matmul.3.1],
         t: 1, mblock: [3, 13], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 4}}
    reshape_217.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [lc.input_tensor.reshape_217.dc.sparse_matmul.4.0, reshape_215.dc.sparse_matmul.3.lc2, lc.input_tensor.reshape_217.dc.sparse_matmul.4.1],
         t: 1, mblock: [19, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose, hslice: 3, vstack: 3],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 13}}
    conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_51, lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [9, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 13, num_index_tiles: 1, num_sparse_tiles: 70, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 1}}
    conv2d_218.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_218, input_2_conv2d_218],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [0, 3], grid_size: [2, 1], inputs: [conv2d_218.dc.matmul.11, conv2d_218.dc.matmul.11],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 53}}
    concatenate_221.dc.concatenate.0: {type: splice, grid_loc: [0, 4], grid_size: [2, 1], inputs: [_fused_op_53, e2e__fused_op_33_0],
         t: 1, mblock: [1, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2]}}
    conv2d_222.dc.matmul.8: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [concatenate_221.dc.concatenate.0, input_1_conv2d_222, input_2_conv2d_222],
         t: 2, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_54: {type: fused_op, grid_loc: [2, 3], grid_size: [2, 1], inputs: [conv2d_222.dc.matmul.8, conv2d_222.dc.matmul.8], grid_transpose: true,
         t: 2, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_225.dc.matmul.8: {type: matmul, grid_loc: [3, 2], grid_size: [2, 1], inputs: [_fused_op_54, input_1_conv2d_225, input_2_conv2d_225],
         t: 2, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    _fused_op_55: {type: fused_op, grid_loc: [3, 3], grid_size: [2, 1], inputs: [conv2d_225.dc.matmul.8, conv2d_225.dc.matmul.8],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2], input_0_tms: [vstack: 2],
         attributes: {fused_op_id: 53}}
    conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_55, lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [9, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 23, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 4}}
    conv2d_228.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, input_1_conv2d_228, input_2_conv2d_228],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [vslice: 9, hstack: 9],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    conv2d_231.dc.matmul.8: {type: matmul, grid_loc: [1, 0], grid_size: [2, 1], inputs: [concatenate_221.dc.concatenate.0, input_1_conv2d_231, input_2_conv2d_231],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_56: {type: fused_op, grid_loc: [3, 6], grid_size: [2, 1], inputs: [conv2d_228.dc.matmul.11, conv2d_228.dc.matmul.11],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 53}}
    _fused_op_57: {type: fused_op, grid_loc: [2, 5], grid_size: [2, 1], inputs: [conv2d_231.dc.matmul.8, conv2d_231.dc.matmul.8],
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [384, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 53}}
    concatenate_234.dc.concatenate.0: {type: splice, grid_loc: [4, 7], grid_size: [2, 1], inputs: [_fused_op_56, _fused_op_57],
         t: 1, mblock: [1, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 352], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2]}}
    conv2d_235.dc.matmul.8: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [concatenate_234.dc.concatenate.0, input_1_conv2d_235, input_2_conv2d_235],
         t: 1, mblock: [1, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_58: {type: fused_op, grid_loc: [6, 5], grid_size: [2, 1], inputs: [conv2d_235.dc.matmul.8, conv2d_235.dc.matmul.8],
         t: 1, mblock: [1, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 58}}
    conv2d_238.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [2, 1], inputs: [_fused_op_58, input_1_conv2d_238, input_2_conv2d_238], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    concatenate_259.dc.concatenate.2_transpose_nop_7347: {type: nop, grid_loc: [6, 0], grid_size: [3, 1], inputs: [reshape_174.dc.sparse_matmul.3.lc2], grid_transpose: true,
         t: 3, mblock: [1, 25], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3]}
    concatenate_259.dc.concatenate.2_transpose_nop_2_7347: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [concatenate_259.dc.concatenate.2_transpose_nop_7347],
         t: 3, mblock: [25, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 64
    _fused_op_59_transpose_nop_7658: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_238.dc.matmul.8_0],
         t: 1, mblock: [4, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    _fused_op_59: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_59_transpose_nop_7658, input_1_multiply_240, input_2_concatenate_242, input_1_multiply_246, input_1_concatenate_247, input_1_multiply_250, input_1_multiply_253],
         t: 1, mblock: [4, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 59}}
    reshape_256.dc.sparse_matmul.3.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.reshape_256.dc.sparse_matmul.3.0, _fused_op_59, lc.input_tensor.reshape_256.dc.sparse_matmul.3.1],
         t: 1, mblock: [9, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 4}}
    reshape_258.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.reshape_258.dc.sparse_matmul.4.0, reshape_256.dc.sparse_matmul.3.lc2, lc.input_tensor.reshape_258.dc.sparse_matmul.4.1],
         t: 1, mblock: [5, 3], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose, hslice: 3, vstack: 3],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 4}}
    concatenate_259.dc.concatenate.2: {type: splice, grid_loc: [0, 4], grid_size: [1, 3], inputs: [e2e_concatenate_259.dc.concatenate.2_transpose_nop_2_7347_0, e2e_reshape_217.dc.sparse_matmul.4.lc2_0, reshape_258.dc.sparse_matmul.4.lc2],
         t: 1, mblock: [99, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 3],
         attributes: {input0: [0, 75, 75], input1: [0, 19, 19], input2: [0, 5, 5]}}
    concatenate_259.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 3], inputs: [lc.input_tensor.concatenate_259.dc.sparse_matmul.4.0, concatenate_259.dc.concatenate.2, lc.input_tensor.concatenate_259.dc.sparse_matmul.4.1],
         t: 1, mblock: [197, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 9, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 22}}
    concatenate_259.dc.sparse_matmul.4.lc2_output_nop_0: {type: nop, grid_loc: [1, 3], grid_size: [1, 3], inputs: [concatenate_259.dc.sparse_matmul.4.lc2], untilize_output: true,
         t: 1, mblock: [197, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$gptr_q1: 0, $lptr_q1: 0, $c_zero: 0, $c_one: 1, $c_microbatch_size: 64, $gptr_q6: 0, $lptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0, $gptr_q3: 0, $gptr_q2: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_2_0, e2e__fused_op_3_0, e2e__fused_op_5_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               ims_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_0_fork_clone1328: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_0_fork_clone1326: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_3_fork_clone424: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_3: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_3: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_3_fork_clone422: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_9: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_9: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_15_0, e2e__fused_op_20_0, e2e__fused_op_21_0, e2e__fused_op_14_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_12.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_12_fork_clone460: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_12.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_12: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_12: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_12.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_12_fork_clone458: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_20: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_20: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_23.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_23: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_23: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_26: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_26: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_29: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_29: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_32.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_32: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_32: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_36: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_36: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_39.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_39: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_39: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_43: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_43: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_47: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_47: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_50.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_50: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_50: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_53: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_53: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_56: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_56: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_59.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_59: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_59: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_63: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_63: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_66.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_66: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_66: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_70: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_70: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_2_0, e2e__fused_op_3_0, e2e__fused_op_5_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_24_0, e2e__fused_op_32_0]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_20_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_73: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_73: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_77: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_77: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_81: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_81: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_84.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_84: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_84: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_87: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_87: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_90: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_90: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_93.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_93: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_93: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_97: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_97: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_101: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_101: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_104: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_104: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_107.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_108.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_109.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_111: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_111: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_15_0, e2e__fused_op_20_0, e2e__fused_op_21_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_39_0, e2e_concatenate_139.dc.concatenate.0_0, e2e__fused_op_40_0, e2e__fused_op_33_0]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_24_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_32_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               input_1_conv2d_114: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_114: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.resize2d_117.dc.sparse_matmul.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.resize2d_117.dc.sparse_matmul.3.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_119: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_119: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_125.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_125: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_125: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_128: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_128: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_132: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_132: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_135: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_135: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.resize2d_138.dc.sparse_matmul.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.resize2d_138.dc.sparse_matmul.3.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_140: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_140: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_14_0, e2e__fused_op_24_0, e2e__fused_op_32_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_44_0, e2e_conv2d_194.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_concatenate_139.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_40_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               input_1_conv2d_143: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_143: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_146: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_146: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_149: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_149: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_153: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_153: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_177.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_177: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_177: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_181: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_181: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_184: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_184: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_187.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_187: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_187: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_190: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_190: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_194: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_194: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_39_0, e2e_concatenate_139.dc.concatenate.0_0, e2e__fused_op_40_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_reshape_217.dc.sparse_matmul.4.lc2_0, e2e_conv2d_238.dc.matmul.8_0, e2e_concatenate_259.dc.concatenate.2_transpose_nop_2_7347_0]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_194.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               input_1_conv2d_156: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_156: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_158: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_concatenate_160: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_164: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_concatenate_165: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_168: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_171: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_174.dc.sparse_matmul.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_174.dc.sparse_matmul.3.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_197: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_197: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_199: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_concatenate_201: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_205: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_concatenate_206: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_209: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_212: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_215.dc.sparse_matmul.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_215.dc.sparse_matmul.3.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_217.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_217.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_218.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_218: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_218: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_222: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_222: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_225: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_225: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_228.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_231: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_231: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_235: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_235: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_conv2d_238: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_conv2d_238: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_33_0, e2e__fused_op_44_0, e2e_conv2d_194.dc.matmul.8_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_reshape_217.dc.sparse_matmul.4.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_238.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_concatenate_259.dc.concatenate.2_transpose_nop_2_7347_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_multiply_240: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_2_concatenate_242: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_246: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_concatenate_247: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_250: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_253: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_256.dc.sparse_matmul.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_256.dc.sparse_matmul.3.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_258.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_258.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_259.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_259.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_reshape_217.dc.sparse_matmul.4.lc2_0, e2e_conv2d_238.dc.matmul.8_0, e2e_concatenate_259.dc.concatenate.2_transpose_nop_2_7347_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [20, 1], ublock: [2, 1], output: dest}
        - conv2d_0.dc.conv2d.3.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [20, 1], ublock: [2, 1], output: intermed0}
        - sigmoid_1.0: { type: sigmoid, inputs: [intermed0], mblock: [20, 1], ublock: [2, 1], output: dest}
        - multiply_2.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [20, 1], ublock: [2, 1], output: output}
  1: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - conv2d_3.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [10, 1], ublock: [2, 2], output: dest}
        - conv2d_3.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [10, 1], ublock: [2, 2], output: intermed0}
        - sigmoid_4.0: { type: sigmoid, inputs: [intermed0], mblock: [10, 1], ublock: [2, 2], output: dest}
        - multiply_5.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [10, 1], ublock: [2, 2], output: output}
  2: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_7.0: { type: sigmoid, inputs: [input0], mblock: [10, 1], ublock: [2, 1], output: dest}
        - multiply_8.0: { type: multiply, inputs: [input1, dest], mblock: [10, 1], ublock: [2, 1], output: output}
  4: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - conv2d_12.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [10, 1], ublock: [2, 1], output: dest}
        - conv2d_12.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [10, 1], ublock: [2, 1], output: intermed0}
        - sigmoid_13.0: { type: sigmoid, inputs: [intermed0], mblock: [10, 1], ublock: [2, 1], output: dest}
        - multiply_14.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [10, 1], ublock: [2, 1], output: dest}
        - add_15.0: { type: add, inputs: [input3, dest], mblock: [10, 1], ublock: [2, 1], output: output}
  6: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_21.0: { type: sigmoid, inputs: [input0], mblock: [10, 1], ublock: [2, 2], output: dest}
        - multiply_22.0: { type: multiply, inputs: [input1, dest], mblock: [10, 1], ublock: [2, 2], output: output}
  7: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_24.0: { type: sigmoid, inputs: [input0], mblock: [5, 1], ublock: [2, 4], output: dest}
        - multiply_25.0: { type: multiply, inputs: [input1, dest], mblock: [5, 1], ublock: [2, 4], output: output}
  8: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_27.0: { type: sigmoid, inputs: [input0], mblock: [5, 1], ublock: [2, 2], output: dest}
        - multiply_28.0: { type: multiply, inputs: [input1, dest], mblock: [5, 1], ublock: [2, 2], output: output}
  9: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_30.0: { type: sigmoid, inputs: [input0], mblock: [25, 1], ublock: [2, 2], output: dest}
        - multiply_31.0: { type: multiply, inputs: [input1, dest], mblock: [25, 1], ublock: [2, 2], output: output}
  10: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - sigmoid_33.0: { type: sigmoid, inputs: [input0], mblock: [5, 1], ublock: [2, 2], output: dest}
        - multiply_34.0: { type: multiply, inputs: [input1, dest], mblock: [5, 1], ublock: [2, 2], output: dest}
        - add_35.0: { type: add, inputs: [input2, dest], mblock: [5, 1], ublock: [2, 2], output: output}
  14: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_48.0: { type: sigmoid, inputs: [input0], mblock: [25, 1], ublock: [2, 4], output: dest}
        - multiply_49.0: { type: multiply, inputs: [input1, dest], mblock: [25, 1], ublock: [2, 4], output: output}
  15: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_51.0: { type: sigmoid, inputs: [input0], mblock: [1, 2], ublock: [1, 4], output: dest}
        - multiply_52.0: { type: multiply, inputs: [input1, dest], mblock: [1, 2], ublock: [1, 4], output: output}
  16: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_54.0: { type: sigmoid, inputs: [input0], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_55.0: { type: multiply, inputs: [input1, dest], mblock: [1, 1], ublock: [1, 4], output: output}
  17: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_57.0: { type: sigmoid, inputs: [input0], mblock: [13, 1], ublock: [1, 4], output: dest}
        - multiply_58.0: { type: multiply, inputs: [input1, dest], mblock: [13, 1], ublock: [1, 4], output: output}
  18: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - sigmoid_60.0: { type: sigmoid, inputs: [input0], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_61.0: { type: multiply, inputs: [input1, dest], mblock: [1, 1], ublock: [1, 4], output: dest}
        - add_62.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 4], output: output}
  23: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_78.0: { type: sigmoid, inputs: [input0], mblock: [13, 1], ublock: [1, 1], output: dest}
        - multiply_79.0: { type: multiply, inputs: [input1, dest], mblock: [13, 1], ublock: [1, 1], output: output}
  24: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_82.0: { type: sigmoid, inputs: [input0], mblock: [13, 2], ublock: [1, 4], output: dest}
        - multiply_83.0: { type: multiply, inputs: [input1, dest], mblock: [13, 2], ublock: [1, 4], output: output}
  25: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_85.0: { type: sigmoid, inputs: [input0], mblock: [1, 4], ublock: [1, 4], output: dest}
        - multiply_86.0: { type: multiply, inputs: [input1, dest], mblock: [1, 4], ublock: [1, 4], output: output}
  28: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - sigmoid_94.0: { type: sigmoid, inputs: [input0], mblock: [1, 2], ublock: [1, 4], output: dest}
        - multiply_95.0: { type: multiply, inputs: [input1, dest], mblock: [1, 2], ublock: [1, 4], output: dest}
        - add_96.0: { type: add, inputs: [input2, dest], mblock: [1, 2], ublock: [1, 4], output: output}
  40: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_141.0: { type: sigmoid, inputs: [input0], mblock: [25, 1], ublock: [2, 1], output: dest}
        - multiply_142.0: { type: multiply, inputs: [input1, dest], mblock: [25, 1], ublock: [2, 1], output: output}
  41: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_144.0: { type: sigmoid, inputs: [input0], mblock: [25, 1], ublock: [1, 2], output: dest}
        - multiply_145.0: { type: multiply, inputs: [input1, dest], mblock: [25, 1], ublock: [1, 2], output: output}
  43: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_150.0: { type: sigmoid, inputs: [input0], mblock: [25, 1], ublock: [1, 1], output: dest}
        - multiply_151.0: { type: multiply, inputs: [input1, dest], mblock: [25, 1], ublock: [1, 1], output: output}
  45: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - sigmoid_157.0: { type: sigmoid, inputs: [input0], mblock: [2, 25], ublock: [2, 1], output: intermed0}
        - multiply_158.0: { type: multiply, inputs: [intermed0, input1], mblock: [2, 25], ublock: [2, 1], output: intermed1}
        - multiply_159.0: { type: multiply, inputs: [intermed1, intermed1], pop: [intermed1], mblock: [2, 25], ublock: [2, 1], output: dest}
        - multiply_162.0: { type: multiply, inputs: [dest, input2], mblock: [2, 25], ublock: [2, 1], output: intermed1}
        - multiply_164.0: { type: multiply, inputs: [intermed0, input3], mblock: [2, 25], ublock: [2, 1], output: dest}
        - add_167.0: { type: add, inputs: [dest, input4], mblock: [2, 25], ublock: [2, 1], output: dest}
        - multiply_168.0: { type: multiply, inputs: [dest, input5], mblock: [2, 25], ublock: [2, 1], output: dest}
        - add_170.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [2, 25], ublock: [2, 1], output: intermed1}
        - multiply_171.0: { type: multiply, inputs: [intermed0, input6], pop: [intermed0], mblock: [2, 25], ublock: [2, 1], output: dest}
        - add_172.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [2, 25], ublock: [2, 1], output: output}
  52: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - sigmoid_198.0: { type: sigmoid, inputs: [input0], mblock: [2, 13], ublock: [2, 1], output: intermed0}
        - multiply_199.0: { type: multiply, inputs: [intermed0, input1], mblock: [2, 13], ublock: [2, 1], output: intermed1}
        - multiply_200.0: { type: multiply, inputs: [intermed1, intermed1], pop: [intermed1], mblock: [2, 13], ublock: [2, 1], output: dest}
        - multiply_203.0: { type: multiply, inputs: [dest, input2], mblock: [2, 13], ublock: [2, 1], output: intermed1}
        - multiply_205.0: { type: multiply, inputs: [intermed0, input3], mblock: [2, 13], ublock: [2, 1], output: dest}
        - add_208.0: { type: add, inputs: [dest, input4], mblock: [2, 13], ublock: [2, 1], output: dest}
        - multiply_209.0: { type: multiply, inputs: [dest, input5], mblock: [2, 13], ublock: [2, 1], output: dest}
        - add_211.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [2, 13], ublock: [2, 1], output: intermed1}
        - multiply_212.0: { type: multiply, inputs: [intermed0, input6], pop: [intermed0], mblock: [2, 13], ublock: [2, 1], output: dest}
        - add_213.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [2, 13], ublock: [2, 1], output: output}
  53: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_219.0: { type: sigmoid, inputs: [input0], mblock: [1, 2], ublock: [2, 4], output: dest}
        - multiply_220.0: { type: multiply, inputs: [input1, dest], mblock: [1, 2], ublock: [2, 4], output: output}
  58: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - sigmoid_236.0: { type: sigmoid, inputs: [input0], mblock: [1, 4], ublock: [2, 4], output: dest}
        - multiply_237.0: { type: multiply, inputs: [input1, dest], mblock: [1, 4], ublock: [2, 4], output: output}
  59: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - sigmoid_239.0: { type: sigmoid, inputs: [input0], mblock: [4, 1], ublock: [2, 4], output: intermed0}
        - multiply_240.0: { type: multiply, inputs: [intermed0, input1], mblock: [4, 1], ublock: [2, 4], output: intermed1}
        - multiply_241.0: { type: multiply, inputs: [intermed1, intermed1], pop: [intermed1], mblock: [4, 1], ublock: [2, 4], output: dest}
        - multiply_244.0: { type: multiply, inputs: [dest, input2], mblock: [4, 1], ublock: [2, 4], output: intermed1}
        - multiply_246.0: { type: multiply, inputs: [intermed0, input3], mblock: [4, 1], ublock: [2, 4], output: dest}
        - add_249.0: { type: add, inputs: [dest, input4], mblock: [4, 1], ublock: [2, 4], output: dest}
        - multiply_250.0: { type: multiply, inputs: [dest, input5], mblock: [4, 1], ublock: [2, 4], output: dest}
        - add_252.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [4, 1], ublock: [2, 4], output: intermed1}
        - multiply_253.0: { type: multiply, inputs: [intermed0, input6], pop: [intermed0], mblock: [4, 1], ublock: [2, 4], output: dest}
        - add_254.0: { type: add, inputs: [intermed1, dest], pop: [intermed1], mblock: [4, 1], ublock: [2, 4], output: output}

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.08
    test-group: "perf_infra_wormhole_b0_silicon_nightly"
    test-name: "yolo_v5_hifi3_fp16b"