# git checkout 531929601
# pytest mobile_net_v3

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                                   {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 392], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x0]]}

  # output
  pt_mobilenet_v3_timm_large.output_add_795:                                                 {input: add_795, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x6388020]]}

  # parameter
  conv_stem.weight:                                                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40308220], [2, 0x2ab4320]]}
  conv_stem.weight_fork_clone2148:                                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401e0e60], [5, 0x431cce0]]}
  blocks.0.0.conv_dw.weight_fork_clone1361:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40270ea0]]}
  blocks.0.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69b09a0]]}
  blocks.0.0.conv_dw.weight_fork_clone1359:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4026f620]]}
  blocks.0.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x431b560]]}
  blocks.1.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c3660]]}
  blocks.1.0.conv_dw.weight_fork_clone1395:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2ab6420]]}
  blocks.1.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401e67c0]]}
  blocks.1.0.conv_dw.weight_fork_clone1393:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42f0620]]}
  blocks.1.0.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4322ea0], [5, 0x401f1240]]}
  blocks.1.1.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98cacc0]]}
  blocks.1.1.conv_dw.weight_fork_clone1429:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x431e560]]}
  blocks.1.1.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40272e20]]}
  blocks.1.1.conv_dw.weight_fork_clone1427:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c5f40]]}
  blocks.1.1.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401e47c0]]}
  blocks.2.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401cfaa0]]}
  blocks.2.0.conv_dw.weight_fork_clone1463:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2aa7dc0]]}
  blocks.2.0.conv_dw.weight_fork_clone1465:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b05420]]}
  blocks.2.0.conv_dw.weight_fork_clone1467:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401c80a0]]}
  blocks.2.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6997da0]]}
  blocks.2.0.conv_dw.weight_fork_clone1461:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40263ae0]]}
  blocks.2.0.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401c6820]]}
  blocks.2.0.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401e3f80]]}
  blocks.2.0.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2ab1040]]}
  blocks.2.0.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40304920]]}
  blocks.2.0.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401e9fa0]]}
  blocks.2.1.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401db5c0]]}
  blocks.2.1.conv_dw.weight_fork_clone1531:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402f9e40]]}
  blocks.2.1.conv_dw.weight_fork_clone1533:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b0d660]]}
  blocks.2.1.conv_dw.weight_fork_clone1535:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401d1320]]}
  blocks.2.1.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69a4780]]}
  blocks.2.1.conv_dw.weight_fork_clone1529:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40210a80]]}
  blocks.2.1.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4024a480]]}
  blocks.2.1.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69fb9c0]]}
  blocks.2.1.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4022ffc0]]}
  blocks.2.1.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2afc2a0]]}
  blocks.2.1.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b80bc0]]}
  blocks.2.2.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x43741a0]]}
  blocks.2.2.conv_dw.weight_fork_clone1599:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69f1720]]}
  blocks.2.2.conv_dw.weight_fork_clone1601:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40330820]]}
  blocks.2.2.conv_dw.weight_fork_clone1603:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b76920]]}
  blocks.2.2.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40240f80]]}
  blocks.2.2.conv_dw.weight_fork_clone1597:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4023cb40]]}
  blocks.2.2.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4024c520]]}
  blocks.2.2.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4023c300]]}
  blocks.2.2.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b8ef80]]}
  blocks.2.2.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40240d60]]}
  blocks.2.2.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x402bf8a0], [1, 0x4330e60], [1, 0x40346e00], [2, 0x2b085e0]]}
  blocks.3.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98f5a20], [4, 0x40226860], [5, 0x4372100], [5, 0x40230ac0]]}
  blocks.3.0.conv_dw.weight_fork_clone1667:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401f3ea0]]}
  blocks.3.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4300960]]}
  blocks.3.0.conv_dw.weight_fork_clone1665:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40206d60]]}
  blocks.3.0.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 3], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42f4640]]}
  blocks.3.1.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b2a5a0]]}
  blocks.3.1.conv_dw.weight_fork_clone1701:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69e6c60]]}
  blocks.3.1.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40325d60]]}
  blocks.3.1.conv_dw.weight_fork_clone1699:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b6be60]]}
  blocks.3.1.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2acef00]]}
  blocks.3.2.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x430ed20]]}
  blocks.3.2.conv_dw.weight_fork_clone1735:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98e1cc0]]}
  blocks.3.2.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x402001c0]]}
  blocks.3.2.conv_dw.weight_fork_clone1733:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x416b8c0]]}
  blocks.3.2.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400ae460]]}
  blocks.3.3.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4007bf40]]}
  blocks.3.3.conv_dw.weight_fork_clone1769:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x295a820]]}
  blocks.3.3.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400a5200]]}
  blocks.3.3.conv_dw.weight_fork_clone1767:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x415f580]]}
  blocks.3.3.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400a2100]]}
  blocks.4.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [3, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400be6c0]]}
  blocks.4.0.conv_dw.weight_fork_clone1803:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4008b340]]}
  blocks.4.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x292d540]]}
  blocks.4.0.conv_dw.weight_fork_clone1801:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400955e0]]}
  blocks.4.0.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [5, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4005d7a0]]}
  blocks.4.0.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x296e540]]}
  blocks.4.0.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40046a80]]}
  blocks.4.0.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4160ec0]]}
  blocks.4.0.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4108f80]]}
  blocks.4.1.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [4, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4105680], [5, 0x400165c0], [0, 0x29027e0]]}
  blocks.4.1.conv_dw.weight_fork_clone1865:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40e5680]]}
  blocks.4.1.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40015580]]}
  blocks.4.1.conv_dw.weight_fork_clone1863:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x28f5680]]}
  blocks.4.1.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [7, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40065220]]}
  blocks.4.1.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29271a0]]}
  blocks.4.1.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4003af20]]}
  blocks.4.1.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4154ac0]]}
  blocks.4.1.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6833800]]}
  blocks.5.0.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [4, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4133280], [1, 0x4007c780], [2, 0x29419c0]]}
  blocks.5.0.conv_dw.weight_fork_clone1927:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x67f3800]]}
  blocks.5.0.conv_dw.weight_fork_clone1929:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4003c780]]}
  blocks.5.0.conv_dw.weight_fork_clone1931:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4113a20]]}
  blocks.5.0.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402ab960]]}
  blocks.5.0.conv_dw.weight_fork_clone1925:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401784a0]]}
  blocks.5.0.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [7, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40104200]]}
  blocks.5.0.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98992a0]]}
  blocks.5.0.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40117c60]]}
  blocks.5.0.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6966300]]}
  blocks.5.0.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 5], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4289520]]}
  blocks.5.1.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [5, 5], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x429c500], [5, 0x40157c40], [0, 0x2aad000]]}
  blocks.5.1.conv_dw.weight_fork_clone1995:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401aaa00]]}
  blocks.5.1.conv_dw.weight_fork_clone1997:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a58680]]}
  blocks.5.1.conv_dw.weight_fork_clone1999:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40217800]]}
  blocks.5.1.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42bee20]]}
  blocks.5.1.conv_dw.weight_fork_clone1993:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4015e720]]}
  blocks.5.1.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [5, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4014b3e0]]}
  blocks.5.1.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98a3580]]}
  blocks.5.1.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400e4900]]}
  blocks.5.1.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6977fc0]]}
  blocks.5.1.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 5], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x68d6f20]]}
  blocks.5.2.conv_pw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2984940]]}
  blocks.5.2.conv_dw.weight_fork_clone2063:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x419e5e0]]}
  blocks.5.2.conv_dw.weight_fork_clone2065:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x688ac40]]}
  blocks.5.2.conv_dw.weight_fork_clone2067:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400d5480]]}
  blocks.5.2.conv_dw.weight:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x29663c0]]}
  blocks.5.2.conv_dw.weight_fork_clone2061:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 5], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4007da60]]}
  blocks.5.2.se.conv_reduce.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [5, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4011ee00]]}
  blocks.5.2.se.conv_reduce.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a230a0]]}
  blocks.5.2.se.conv_expand.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4021a500]]}
  blocks.5.2.se.conv_expand.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4279a40]]}
  blocks.5.2.conv_pwl.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 5], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4240e40]]}
  blocks.6.0.conv.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9838a80]]}
  conv_head.weight:                                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 5], ublock: [5, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a04f80], [0, 0x4014be20], [1, 0x41e14a0], [1, 0x40181f60]]}
  conv_head.bias:                                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 5], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9833920], [4, 0x400db780], [5, 0x423bce0], [5, 0x40111aa0]]}
  classifier.weight:                                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [20, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41ea8c0], [5, 0x400c0680], [0, 0x29b3b60], [0, 0x400faa00], [1, 0x4190080], [1, 0x40130b40], [2, 0x29d0c20], [2, 0x400af240]]}
  classifier.bias:                                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x400cb360]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x97e9460], [4, 0x40054620]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x401e2580], [3, 0x69b2a60]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x40272720], [1, 0x42eeaa0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401ee800], [0, 0x2b25bc0]]}
  input_1_add_1_fork_clone1216:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c5700]]}
  input_1_add_13:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402033a0]]}
  input_1_multiply_15:                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69b2220]]}
  lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x2ab33e0], [2, 0x401e1640]]}
  lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42eda60], [1, 0x403071e0]]}
  lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x401dfbe0], [3, 0x69afa60]]}
  lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x40202360], [4, 0x98c46c0]]}
  lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x2ab28c0], [2, 0x401e0b20]]}
  lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42eca20], [1, 0x403061a0]]}
  input_1_add_19:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b24440]]}
  input_1_add_19_fork_clone1240:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401ed080]]}
  input_1_add_33_fork_clone1200:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401df6e0]]}
  input_1_add_47_fork_clone1160:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40201300]]}
  lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x401dff20], [5, 0x431bda0], [5, 0x401ed8c0], [0, 0x2b24c80]]}
  lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x401e4ac0], [3, 0x69b4fa0]]}
  lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2b29660], [0, 0x40278c00], [1, 0x42f3700], [1, 0x4030af20]]}
  lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x43236e0], [5, 0x401f1a80]]}
  lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x401e3e40], [3, 0x69b4320], [3, 0x402060e0], [4, 0x98cc540]]}
  lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x40309ee0], [2, 0x2ab53e0]]}
  input_1_add_62:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40277ba0]]}
  input_1_add_62_fork_clone1115:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b28600]]}
  input_1_add_76_fork_clone1071:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40203be0]]}
  input_1_add_89_fork_clone1154:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40204860]]}
  lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 3], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x40277760], [1, 0x42f01e0], [1, 0x40309aa0], [2, 0x2ab4fa0], [2, 0x401e3a00], [3, 0x69b3ee0]]}
  lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401f0200], [0, 0x2b275c0]]}
  lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 3], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x2ab4b60], [2, 0x401e35c0], [3, 0x69b3aa0], [3, 0x40204420], [4, 0x98ca880], [4, 0x401e26e0]]}
  lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42ef1a0], [1, 0x40308a60]]}
  lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x401ef840], [0, 0x2b26c00]]}
  lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x401e16a0], [5, 0x431d520]]}
  input_1_add_104:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x403040e0], [2, 0x2ab0800], [2, 0x401df3a0]]}
  input_1_add_104_fork_clone1109:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b0ce20], [0, 0x4026c520], [1, 0x42e6cc0]]}
  input_1_add_118_fork_clone1066:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4318c80]]}
  input_1_add_132_fork_clone1019:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98bbde0]]}
  lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 58], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x699f7a0], [3, 0x401f7d20]]}
  lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x401dc920]]}
  lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 59], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42e1b80], [1, 0x402f4d00]]}
  lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x4026b4e0]]}
  lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 113], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x98a86e0]]}
  lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4317c40]]}
  lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 113], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x98b2260]]}
  lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x401f6ce0]]}
  lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 38], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x402f18a0], [2, 0x2aa4960], [2, 0x401d94c0]]}
  lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42e0b40]]}
  input_1_add_147:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 25], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2adf2a0]]}
  input_1_add_147_fork_clone940:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 25], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401bde00]]}
  lc.input_tensor.reduce_avg_161.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x430b100]]}
  input_1_add_165:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42eb1a0]]}
  input_1_multiply_167:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4026dda0]]}
  lc.input_tensor.multiply_167_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b17900]]}
  input_1_add_170_fork_clone846:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x431a500]]}
  input_1_add_183_fork_clone1013:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98c15c0]]}
  lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 39], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x401fdd40]]}
  lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x69aea20]]}
  lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 44], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42e7500]]}
  lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x4026cd60]]}
  lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 46], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x401e6040]]}
  lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x43194c0]]}
  lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 46], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x98bd660]]}
  lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x401fcd00]]}
  lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 44], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x401e2b20]]}
  lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2aaf7c0]]}
  input_1_add_198:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [2, 5], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4326bc0], [1, 0x4033cb60], [2, 0x2afe340], [2, 0x40232060], [3, 0x69fc200]]}
  input_1_add_198_fork_clone930:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [2, 5], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4022a9a0], [5, 0x43782c0], [5, 0x40236ac0], [0, 0x2b84ce0], [0, 0x402b5600]]}
  lc.input_tensor.reduce_avg_212.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98fb080]]}
  input_1_add_216:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4033aac0]]}
  input_1_multiply_218:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4324b20]]}
  lc.input_tensor.multiply_218_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x402a8ac0]]}
  input_1_add_221_fork_clone841:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402483e0]]}
  input_1_add_235_fork_clone993:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40228900]]}
  lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 39], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x98f7ac0]]}
  lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x40249440]]}
  lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 44], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x4022c320]]}
  lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2afb260]]}
  lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 46], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4320bc0]]}
  lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x402a7a80]]}
  lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 46], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x40232b60]]}
  lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x438e8a0]]}
  lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 44], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x991e9a0]]}
  lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x6a12fe0]]}
  input_1_add_250:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [2, 5], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b91020], [0, 0x402c0900], [1, 0x4331ec0], [1, 0x40347e60], [2, 0x2b09640]]}
  input_1_add_250_fork_clone902:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [2, 5], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4024e5c0], [4, 0x9914700], [4, 0x40236ce0], [5, 0x4384600], [5, 0x40242e00]]}
  lc.input_tensor.reduce_avg_264.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a064a0]]}
  input_1_add_268:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4382560]]}
  input_1_multiply_270:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40234c40]]}
  lc.input_tensor.multiply_270_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9907bc0]]}
  input_1_add_273_fork_clone822:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40258860]]}
  input_1_add_287_fork_clone769:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x430cc80], [1, 0x4031b240]]}
  input_1_add_299:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [25, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x434b900], [5, 0x401f7400], [0, 0x2b360a0], [0, 0x40283100]]}
  input_1_multiply_301:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [25, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2ad99c0], [2, 0x40209420], [3, 0x69c6400], [3, 0x40227b80]]}
  lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 47], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40221420], [4, 0x98ddc00]]}
  lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x69c4380]]}
  lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 86], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x2ac78a0]]}
  lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4031a200]]}
  lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 91], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x4027b3c0]]}
  lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2b35060]]}
  input_1_add_305:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x402026e0], [5, 0x433d560]]}
  input_1_add_305_fork_clone670:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40213080], [4, 0x98cf860]]}
  input_1_add_317:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401e5b00], [3, 0x69b5fe0]]}
  input_1_multiply_319:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4030be60], [2, 0x2ab9500]]}
  input_1_add_322_fork_clone566:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40279b40]]}
  input_1_add_335_fork_clone762:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401f2ac0]]}
  input_1_add_347:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4324720]]}
  input_1_multiply_349:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401e98a0]]}
  lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x98cd1c0]]}
  lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401f63c0]]}
  lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x40229c80]]}
  lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2afa220]]}
  lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x431f180]]}
  lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x402a6a40]]}
  input_1_add_353:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40226380], [3, 0x69e3360], [3, 0x40244ae0], [4, 0x98f2120], [4, 0x40222f60], [5, 0x436e800], [5, 0x4022d1c0]]}
  input_1_add_353_fork_clone663:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x436af00], [5, 0x402298c0], [0, 0x2b68560], [0, 0x402a3140], [1, 0x431b880], [1, 0x40322460], [2, 0x2af6920]]}
  input_1_add_365:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4031eb60], [2, 0x2af3020], [2, 0x40222a80], [3, 0x69dfa60], [3, 0x402411e0], [4, 0x98ee820], [4, 0x4021f660]]}
  input_1_multiply_367:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98eaf20], [4, 0x4021bd60], [5, 0x4367600], [5, 0x40225fc0], [0, 0x2b64c60], [0, 0x4029f840], [1, 0x4317f80]]}
  input_1_add_370_fork_clone561:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4031d2e0]]}
  input_1_add_384_fork_clone737:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4029c760]]}
  input_1_add_396:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2b4f700]]}
  input_1_multiply_398:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40210a60]]}
  lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4364f60]]}
  lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x4021ad20]]}
  lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x402254e0]]}
  lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x69c53c0]]}
  lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x401dd960]]}
  lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x400851a0]]}
  input_1_add_402:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x401fd960], [1, 0x42c5c60]]}
  input_1_add_402_fork_clone630:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x686b400], [3, 0x40061100]]}
  input_1_add_414:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2963a80], [2, 0x40052ce0]]}
  input_1_multiply_416:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4145f60], [1, 0x400a2720]]}
  input_1_add_419_fork_clone535:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x292bcc0]]}
  input_1_add_433_fork_clone705:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41687e0]]}
  input_1_add_445:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4003f0c0]]}
  input_1_multiply_447:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 3], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4008ab20]]}
  lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x6868d60]]}
  lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x40051ca0]]}
  lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x400a0080]]}
  lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4144f20]]}
  lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x292a280]]}
  lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4007af00]]}
  input_1_add_451:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97d3ee0], [4, 0x40034600]]}
  input_1_add_451_fork_clone594:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x685e2a0], [3, 0x4004bb80]]}
  input_1_add_463:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x294fd60], [2, 0x400471e0]]}
  input_1_multiply_465:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40056640], [4, 0x97de9a0]]}
  input_1_add_468_fork_clone515:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2982080]]}
  input_1_add_482_fork_clone474:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41688c0]]}
  input_1_add_494:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [7, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400ac3a0], [0, 0x2954740], [0, 0x400c95a0]]}
  input_1_multiply_496:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [7, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9800c20], [4, 0x4006bde0], [5, 0x418b920]]}
  lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x400829c0]]}
  lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x6888bc0]]}
  lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x297f9e0]]}
  lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x400bd680]]}
  lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x400c7b60]]}
  lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2953700]]}
  input_1_add_500:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [15, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2977fe0], [2, 0x40083940], [3, 0x68811c0], [3, 0x4007afc0], [4, 0x97f9220], [4, 0x400643e0], [5, 0x4183f20]]}
  input_1_add_500_fork_clone409:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [15, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4005c9e0], [5, 0x417c520], [5, 0x4008dbe0], [0, 0x294bd00], [0, 0x400c0160], [1, 0x41594c0], [1, 0x400b5c80]]}
  input_1_add_512:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [15, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4151ac0], [1, 0x400ae280], [2, 0x29705e0], [2, 0x4007bf40], [3, 0x68797c0], [3, 0x400735c0], [4, 0x97f1820]]}
  input_1_multiply_514:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [15, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4006bbc0], [4, 0x97e9e20], [4, 0x40054fe0], [5, 0x4174b20], [5, 0x400861e0], [0, 0x2944300], [0, 0x400b8760]]}
  lc.input_tensor.reduce_avg_517.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6875ec0]]}
  input_1_add_521:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4002f400]]}
  input_1_multiply_523:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x292e4c0]]}
  lc.input_tensor.multiply_523_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40038e80]]}
  input_1_add_526_fork_clone336:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40038e80]]}
  input_1_add_539_fork_clone467:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40035580], [4, 0x97b66c0], [4, 0x40017c20]]}
  input_1_add_551:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [7, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2915680], [2, 0x400165c0], [3, 0x67d70c0]]}
  input_1_multiply_553:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [7, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40020040], [1, 0x40f0140], [1, 0x40020040]]}
  lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2900140]]}
  lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40015580]]}
  lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x40015580]]}
  lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x97b5680]]}
  lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x67d5680]]}
  lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x40015580]]}
  input_1_add_557:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [21, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4000aac0], [5, 0x40dabc0], [5, 0x4000aac0], [0, 0x28f5680], [0, 0x40015580], [1, 0x40e5680], [1, 0x40015580]]}
  input_1_add_557_fork_clone398:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [21, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40dabc0], [1, 0x4000aac0], [2, 0x28eabc0], [2, 0x4000aac0], [3, 0x67cabc0], [3, 0x4000aac0], [4, 0x97aabc0]]}
  input_1_add_569:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [21, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x28e0100], [0, 0x40000000], [1, 0x40d0100], [1, 0x40000000], [2, 0x28e0100], [2, 0x40000000], [3, 0x67c0100]]}
  input_1_multiply_571:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [21, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40000000], [4, 0x97a0100], [4, 0x40000000], [5, 0x40d0100], [5, 0x40000000], [0, 0x28eabc0], [0, 0x4000aac0]]}
  lc.input_tensor.reduce_avg_574.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4141620]]}
  input_1_add_578:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40029b40]]}
  input_1_multiply_580:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97c9420]]}
  lc.input_tensor.multiply_580_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40048280]]}
  input_1_add_583_fork_clone331:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40045140]]}
  input_1_add_597_fork_clone283:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x67eff00], [3, 0x40038e80], [4, 0x97b9fc0]]}
  input_1_add_609:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6828d40], [3, 0x4003d7c0], [4, 0x97be960], [4, 0x4001f080], [5, 0x414a000], [5, 0x40030460], [0, 0x291c6e0]]}
  input_1_multiply_611:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 7], t: 1, mblock: [7, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400259a0], [0, 0x2911c20], [0, 0x4003bfc0], [1, 0x41287c0], [1, 0x40071cc0], [2, 0x2936f00], [2, 0x4003a680]]}
  lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x97bd8c0], [4, 0x4001dfe0], [5, 0x4148f60]]}
  lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x4003c780]]}
  lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 41], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x40036e00]]}
  lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2935ec0]]}
  lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2910b80], [0, 0x4003af20], [1, 0x4127720]]}
  lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40024960]]}
  lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 31], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x4001b520]]}
  lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x98a76a0]]}
  lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x400b76c0], [1, 0x4150a20], [1, 0x400ad1e0]]}
  lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2acd860]]}
  input_1_add_615:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [3, 2], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401b63c0], [3, 0x69746c0], [3, 0x400e1000], [4, 0x989fc80], [4, 0x40147ae0], [5, 0x42ba4c0]]}
  input_1_add_615_fork_clone190:                                                             {input: HOST, type: queue, entries: 1, grid_size: [3, 2], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40174ba0], [0, 0x2ac9f60], [0, 0x401fa060], [1, 0x42c2360], [1, 0x402a8060], [2, 0x2a44960]]}
  input_1_add_627:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [3, 2], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401b2ac0], [3, 0x6970dc0], [3, 0x400dd700], [4, 0x989c380], [4, 0x401441e0], [5, 0x42b6bc0]]}
  input_1_multiply_629:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 2], t: 1, mblock: [7, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401712a0], [0, 0x2ac6660], [0, 0x401f6760], [1, 0x42bea60], [1, 0x402a4760], [2, 0x2a41060]]}
  lc.input_tensor.reduce_avg_632.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42b5b60]]}
  input_1_add_636:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401a8000]]}
  input_1_multiply_638:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a365a0]]}
  lc.input_tensor.multiply_638_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402a3700]]}
  input_1_add_641_fork_clone81:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x401f3ea0]]}
  input_1_add_654_fork_clone276:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 5], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400d7560], [4, 0x9894140], [4, 0x400ff0a0]]}
  input_1_add_666:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40198c20], [3, 0x6956f20]]}
  input_1_multiply_668:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40294320], [2, 0x2a271c0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2aac900], [0, 0x401f37a0], [1, 0x4288e20]]}
  lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x400dc6c0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x6996780]]}
  lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x401d8480]]}
  lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x402f0280]]}
  lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42dfb00]]}
  lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2addc80]]}
  lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401bcdc0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x401c5200]]}
  lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2a48260]]}
  input_1_add_672:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401c90a0], [3, 0x69873a0]]}
  input_1_add_672_fork_clone179:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402e0ea0], [2, 0x2a492a0]]}
  input_1_add_684:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40208420], [1, 0x42d0720]]}
  input_1_multiply_686:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401ad9e0], [0, 0x2ace8a0]]}
  lc.input_tensor.reduce_avg_689.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42bddc0]]}
  input_1_add_693:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x401b9cc0]]}
  input_1_multiply_695:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x400e08e0]]}
  lc.input_tensor.multiply_695_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400a3820]]}
  input_1_add_698_fork_clone76:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400ac980]]}
  input_1_add_712_fork_clone253:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40121760]]}
  input_1_add_724:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41718e0]]}
  input_1_multiply_726:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400dc260]]}
  lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x29b26a0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x400bf640]]}
  lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x400c9d40]]}
  lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x6889c00]]}
  lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x400ab360]]}
  lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x2983900]]}
  lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x41702c0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x400db220]]}
  lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 16], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x400be020]]}
  lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x419d5a0]]}
  input_1_add_730:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40085060], [4, 0x98128a0]]}
  input_1_add_730_fork_clone147:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40094440], [4, 0x9821c80]]}
  input_1_add_742:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x400efcc0], [5, 0x428d120]]}
  input_1_multiply_744:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [15, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400c8180], [4, 0x9884d60]]}
  lc.input_tensor.reduce_avg_747.0:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6955ec0]]}
  input_1_add_751:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x401e43c0]]}
  input_1_multiply_753:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a9d520]]}
  lc.input_tensor.multiply_753_s_brcst_m2_0_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40116c00]]}
  input_1_add_756_fork_clone59:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9831060]]}
  input_1_add_770_fork_clone32:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400b8da0]]}
  input_1_add_782:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6937720]]}
  input_1_multiply_784:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 15], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40100660]]}
  lc.input_tensor.avg_pool2d_786.dc.reduce_avg.2.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2a22040]]}
  input_1_add_788:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400a4880]]}
  input_1_multiply_790:                                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6923200]]}

  # epoch_to_epoch
  e2e_conv2d_88.dc.matmul.8_0:                                                               {input: conv2d_88.dc.matmul.8, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [49, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40352100], [2, 0x2b138e0]]}
  e2e_conv2d_103.dc.conv2d.1.dc.depthwise.10_0:                                              {input: conv2d_103.dc.conv2d.1.dc.depthwise.10, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [7, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4024d0a0], [0, 0x2b9b2c0]]}
  e2e_conv2d_75.dc.matmul.8_0:                                                               {input: conv2d_75.dc.matmul.8, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [49, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x402caba0], [1, 0x433c160]]}
  e2e_conv2d_169.dc.matmul.8_0:                                                              {input: conv2d_169.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [25, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40246de0]]}
  e2e_multiply_271_0:                                                                        {input: multiply_271, type: queue, entries: 64, grid_size: [1, 1], t: 4, mblock: [25, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402598c0]]}
  e2e_add_233_0:                                                                             {input: add_233, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a14020]]}
  e2e_conv2d_286.dc.matmul.8_0:                                                              {input: conv2d_286.dc.matmul.8, type: queue, entries: 64, grid_size: [5, 2], t: 1, mblock: [5, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9922640], [4, 0x4024b220], [5, 0x438f8e0], [5, 0x4024d0a0], [0, 0x2b9b2c0], [0, 0x402caba0], [1, 0x433c160], [1, 0x40352100], [2, 0x2b138e0], [2, 0x4089fe00]]}
  e2e__fused_op_18_0:                                                                        {input: _fused_op_18, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [7, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9bac660], [4, 0x404d5240], [5, 0x4619900]]}
  e2e_conv2d_499.dc.conv2d.1.dc.depthwise.10_0:                                              {input: conv2d_499.dc.conv2d.1.dc.depthwise.10, type: queue, entries: 64, grid_size: [1, 1], t: 7, mblock: [1, 3], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x706d040]]}
  e2e_conv2d_499.dc.conv2d.5.dc.depthwise.10_0:                                              {input: conv2d_499.dc.conv2d.5.dc.depthwise.10, type: queue, entries: 64, grid_size: [1, 1], t: 7, mblock: [1, 3], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402598c0]]}
  e2e_conv2d_596.dc.matmul.8_0:                                                              {input: conv2d_596.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [7, 7], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x404d70c0], [0, 0x2e252e0], [0, 0x40554bc0]]}
  e2e_conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0:                    {input: conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [10, 2], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40246de0], [3, 0x6a14020], [3, 0x402598c0]]}
  e2e__fused_op_27_0:                                                                        {input: _fused_op_27, type: queue, entries: 64, grid_size: [1, 2], t: 2, mblock: [1, 15], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40352100], [2, 0x2b138e0]]}
  e2e_conv2d_640.dc.matmul.8_0:                                                              {input: conv2d_640.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x433c160]]}
  e2e__fused_op_34_0:                                                                        {input: _fused_op_34, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9922640]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 64
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [4, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv_stem.weight],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 56, hstack: 2, vstack: 28],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [4, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv_stem.weight_fork_clone2148],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 56, hstack: 2, vstack: 28],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, input_1_add_1_fork_clone1216, input_1_add_13, input_1_multiply_15],
         t: 1, mblock: [98, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [broadcast: {r: 392}], input_3_tms: [broadcast: {r: 392}], input_2_tms: [broadcast: {r: 392}], input_1_tms: [vstack: 14], input_0_tms: [vstack: 14],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_4: 1, input_3: 1, input_2: 1}}}
    conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 14, mblock: [6, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 98}}
    conv2d_18.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [2, 3], grid_size: [2, 1], inputs: [conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.0.0.conv_dw.weight_fork_clone1361],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 84, hstack: 3, vstack: 28],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 14, mblock: [6, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 98}}
    conv2d_18.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [0, 6], grid_size: [2, 1], inputs: [conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.0.0.conv_dw.weight],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 84, hstack: 3, vstack: 28],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 14, mblock: [6, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_18.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [2, 1], grid_size: [2, 1], inputs: [conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.0.0.conv_dw.weight_fork_clone1359],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 84, hstack: 3, vstack: 28],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_18.dc.conv2d.1.dc.depthwise.10, conv2d_18.dc.conv2d.3.dc.depthwise.10, conv2d_18.dc.conv2d.5.dc.depthwise.10, input_1_add_19, input_1_add_19_fork_clone1240],
         t: 14, mblock: [7, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [broadcast: {r: 392}, vslice: 14], input_3_tms: [broadcast: {r: 392}, vslice: 14],
         attributes: {fused_op_id: 1, kernel_broadcast: {input_4: 1, input_3: 1}}}
    conv2d_32.dc.matmul.8: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [_fused_op_1, blocks.0.0.conv_pw.weight, input_1_add_33_fork_clone1200],
         t: 14, mblock: [7, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 392}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 1, input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_45: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [conv2d_32.dc.matmul.8, _fused_op_0],
         t: 14, mblock: [7, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 406], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vslice: 14]}
    conv2d_46.dc.matmul.8: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_45, blocks.1.0.conv_pw.weight, input_1_add_47_fork_clone1160],
         t: 14, mblock: [7, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 392}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 4, input_1: 28}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [2, 2], inputs: [lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_46.dc.matmul.8, lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 14], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 22, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 28}}
    conv2d_61.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [4, 2], grid_size: [2, 1], inputs: [conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.0.conv_dw.weight_fork_clone1395],
         t: 1, mblock: [7, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [2, 2], inputs: [lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_46.dc.matmul.8, lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 14], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 22, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 28}}
    conv2d_61.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [4, 5], grid_size: [2, 1], inputs: [conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.0.conv_dw.weight],
         t: 1, mblock: [7, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [2, 2], inputs: [lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_46.dc.matmul.8, lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 14], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 18, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 28}}
    conv2d_61.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [6, 2], grid_size: [2, 1], inputs: [conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.0.conv_dw.weight_fork_clone1393],
         t: 1, mblock: [7, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [2, 7], grid_size: [2, 1], inputs: [conv2d_61.dc.conv2d.1.dc.depthwise.10, conv2d_61.dc.conv2d.3.dc.depthwise.10, conv2d_61.dc.conv2d.5.dc.depthwise.10, input_1_add_62, input_1_add_62_fork_clone1115],
         t: 1, mblock: [49, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [broadcast: {r: 98}], input_3_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 2}}
    conv2d_75.dc.matmul.8: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [_fused_op_2, blocks.1.0.conv_pwl.weight, input_1_add_76_fork_clone1071],
         t: 1, mblock: [49, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 98}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_88.dc.matmul.8: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [conv2d_75.dc.matmul.8, blocks.1.1.conv_pw.weight, input_1_add_89_fork_clone1154],
         t: 1, mblock: [49, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 98}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 3], grid_size: [2, 3], inputs: [lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_88.dc.matmul.8, lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_103.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [6, 6], grid_size: [2, 1], inputs: [conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.1.conv_dw.weight],
         t: 1, mblock: [7, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 64
    conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [2, 3], inputs: [lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_88.dc.matmul.8_0, lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_103.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.1.conv_dw.weight_fork_clone1429],
         t: 1, mblock: [7, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 3], inputs: [lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_88.dc.matmul.8_0, lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_103.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [0, 3], grid_size: [2, 1], inputs: [conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.1.conv_dw.weight_fork_clone1427],
         t: 1, mblock: [7, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 3], inputs: [e2e_conv2d_103.dc.conv2d.1.dc.depthwise.10_0, conv2d_103.dc.conv2d.3.dc.depthwise.10, conv2d_103.dc.conv2d.5.dc.depthwise.10, input_1_add_104, input_1_add_104_fork_clone1109],
         t: 1, mblock: [49, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [broadcast: {r: 98}], input_3_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 3}}
    conv2d_117.dc.matmul.8: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_3, blocks.1.1.conv_pwl.weight, input_1_add_118_fork_clone1066],
         t: 1, mblock: [49, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 98}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    add_130: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [conv2d_117.dc.matmul.8, e2e_conv2d_75.dc.matmul.8_0],
         t: 1, mblock: [49, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_131.dc.matmul.8: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_130, blocks.2.0.conv_pw.weight, input_1_add_132_fork_clone1019],
         t: 1, mblock: [49, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 98}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_131.dc.matmul.8, lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 116, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 7}}
    conv2d_146.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0.conv_dw.weight_fork_clone1463],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_131.dc.matmul.8, lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 118, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 7}}
    conv2d_146.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0.conv_dw.weight_fork_clone1465],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_131.dc.matmul.8, lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 113, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 7}}
    conv2d_146.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0.conv_dw.weight_fork_clone1467],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 4], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_131.dc.matmul.8, lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 113, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 7}}
    conv2d_146.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0.conv_dw.weight],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_131.dc.matmul.8, lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 114, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 7}}
    conv2d_146.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0.conv_dw.weight_fork_clone1461],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_4_transpose_nop_19078: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.5.dc.depthwise.10],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_4_transpose_nop_19079: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.7.dc.depthwise.10],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_4_transpose_nop_19080: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.1.dc.depthwise.10],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_4_transpose_nop_19081: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.3.dc.depthwise.10],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_4_transpose_nop_19082: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_146.dc.conv2d.9.dc.depthwise.10],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_4: {type: fused_op, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_4_transpose_nop_19078, _fused_op_4_transpose_nop_19079, _fused_op_4_transpose_nop_19080, _fused_op_4_transpose_nop_19081, _fused_op_4_transpose_nop_19082, input_1_add_147, input_1_add_147_fork_clone940],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 4}}
    reduce_avg_161.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_4, lc.input_tensor.reduce_avg_161.0],
         t: 1, mblock: [3, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 5}}
    conv2d_162.dc.matmul.8_transpose_nop_18573: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [reduce_avg_161.lc1],
         t: 1, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_162.dc.matmul.8: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [conv2d_162.dc.matmul.8_transpose_nop_18573, blocks.2.0.se.conv_reduce.weight, blocks.2.0.se.conv_reduce.bias],
         t: 1, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_164.dc.matmul.8: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_162.dc.matmul.8, blocks.2.0.se.conv_expand.weight, blocks.2.0.se.conv_expand.bias],
         t: 1, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    _fused_op_5: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [conv2d_164.dc.matmul.8, input_1_add_165, input_1_multiply_167],
         t: 1, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 5, kernel_broadcast: {input_1: 3}}}
    multiply_167_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_167_s_brcst_m2_0_0.0, _fused_op_5],
         t: 1, mblock: [25, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_4_multiply_168_transpose_nop_19085: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [_fused_op_4],
         t: 1, mblock: [3, 25], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [289], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_168_transpose_nop_19085: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_4_multiply_168_transpose_nop_19085],
         t: 1, mblock: [25, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [289], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    multiply_168: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [multiply_168_transpose_nop_19085, multiply_167_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [25, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [287, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_169.dc.matmul.8: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [multiply_168, blocks.2.0.conv_pwl.weight, input_1_add_170_fork_clone846],
         t: 2, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 64
    conv2d_182.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_169.dc.matmul.8_0, blocks.2.1.conv_pw.weight, input_1_add_183_fork_clone1013],
         t: 1, mblock: [25, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_182.dc.matmul.8, lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 39, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_197.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1.conv_dw.weight_fork_clone1531],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_182.dc.matmul.8, lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 44, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_197.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1.conv_dw.weight_fork_clone1533],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_182.dc.matmul.8, lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 46, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_197.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1.conv_dw.weight_fork_clone1535],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_182.dc.matmul.8, lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 46, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_197.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1.conv_dw.weight],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_182.dc.matmul.8, lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 44, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_197.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1.conv_dw.weight_fork_clone1529],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6_transpose_nop_19091: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.5.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_6_transpose_nop_19092: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.7.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_6_transpose_nop_19093: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.1.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_6_transpose_nop_19094: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.3.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_6_transpose_nop_19095: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_197.dc.conv2d.9.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_6: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 5], inputs: [_fused_op_6_transpose_nop_19091, _fused_op_6_transpose_nop_19092, _fused_op_6_transpose_nop_19093, _fused_op_6_transpose_nop_19094, _fused_op_6_transpose_nop_19095, input_1_add_198, input_1_add_198_fork_clone930],
         t: 1, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [hstack: 5], input_3_tms: [hstack: 5], input_2_tms: [hstack: 5], input_1_tms: [hstack: 5], input_0_tms: [hstack: 5],
         attributes: {fused_op_id: 6}}
    reduce_avg_212.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_6, lc.input_tensor.reduce_avg_212.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 5}}
    conv2d_213.dc.matmul.8_transpose_nop_18633: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [reduce_avg_212.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_213.dc.matmul.8: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_213.dc.matmul.8_transpose_nop_18633, blocks.2.1.se.conv_reduce.weight, blocks.2.1.se.conv_reduce.bias],
         t: 1, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_215.dc.matmul.8: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_213.dc.matmul.8, blocks.2.1.se.conv_expand.weight, blocks.2.1.se.conv_expand.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_215.dc.matmul.8, input_1_add_216, input_1_multiply_218],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 7, kernel_broadcast: {input_1: 4}}}
    multiply_218_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_218_s_brcst_m2_0_0.0, _fused_op_7],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {kernel_broadcast: {input_0: 100}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_6_multiply_219_transpose_nop_19098: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [2, 25], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [238], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_219_transpose_nop_19098: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [buffer_0__fused_op_6_multiply_219_transpose_nop_19098],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [389], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 4]}
    multiply_219: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [multiply_219_transpose_nop_19098, multiply_218_s_brcst_m2_0_0.lc1],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [387, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_220.dc.matmul.8: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [multiply_219, blocks.2.1.conv_pwl.weight, input_1_add_221_fork_clone841],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    add_233: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_220.dc.matmul.8, e2e_conv2d_169.dc.matmul.8_0],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [hstack: 2]}
    conv2d_234.dc.matmul.8: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_233, blocks.2.2.conv_pw.weight, input_1_add_235_fork_clone993],
         t: 1, mblock: [25, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_234.dc.matmul.8, lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 39, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_249.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.2.conv_dw.weight_fork_clone1599],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_234.dc.matmul.8, lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 44, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_249.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.2.conv_dw.weight_fork_clone1601],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_234.dc.matmul.8, lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 46, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_249.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [5, 6], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.2.conv_dw.weight_fork_clone1603],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_234.dc.matmul.8, lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 46, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_249.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.2.conv_dw.weight],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_234.dc.matmul.8, lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 44, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 25}}
    conv2d_249.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.2.conv_dw.weight_fork_clone1597],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 25, hstack: 5, vstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_8_transpose_nop_19104: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.5.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_8_transpose_nop_19105: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.7.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_8_transpose_nop_19106: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.1.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_8_transpose_nop_19107: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.3.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_8_transpose_nop_19108: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_249.dc.conv2d.9.dc.depthwise.10],
         t: 5, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_8: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 5], inputs: [_fused_op_8_transpose_nop_19104, _fused_op_8_transpose_nop_19105, _fused_op_8_transpose_nop_19106, _fused_op_8_transpose_nop_19107, _fused_op_8_transpose_nop_19108, input_1_add_250, input_1_add_250_fork_clone902],
         t: 1, mblock: [2, 5], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [hstack: 5], input_3_tms: [hstack: 5], input_2_tms: [hstack: 5], input_1_tms: [hstack: 5], input_0_tms: [hstack: 5],
         attributes: {fused_op_id: 6}}
    reduce_avg_264.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [_fused_op_8, lc.input_tensor.reduce_avg_264.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 5}}
    conv2d_265.dc.matmul.8_transpose_nop_18692: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [reduce_avg_264.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_265.dc.matmul.8: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_265.dc.matmul.8_transpose_nop_18692, blocks.2.2.se.conv_reduce.weight, blocks.2.2.se.conv_reduce.bias],
         t: 1, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_267.dc.matmul.8: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [conv2d_265.dc.matmul.8, blocks.2.2.se.conv_expand.weight, blocks.2.2.se.conv_expand.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    _fused_op_9: {type: fused_op, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_267.dc.matmul.8, input_1_add_268, input_1_multiply_270],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 7, kernel_broadcast: {input_1: 4}}}
    multiply_270_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_270_s_brcst_m2_0_0.0, _fused_op_9],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {kernel_broadcast: {input_0: 100}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_8_multiply_271_transpose_nop_19111: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_8],
         t: 1, mblock: [2, 25], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [238], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_271_transpose_nop_19111: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [buffer_0__fused_op_8_multiply_271_transpose_nop_19111],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [389], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 4]}
    multiply_271: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [multiply_271_transpose_nop_19111, multiply_270_s_brcst_m2_0_0.lc1],
         t: 4, mblock: [25, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [387, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 64
    conv2d_272.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [5, 1], inputs: [e2e_multiply_271_0, blocks.2.2.conv_pwl.weight, input_1_add_273_fork_clone822],
         t: 1, mblock: [5, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    add_285: {type: add, grid_loc: [0, 1], grid_size: [5, 1], inputs: [conv2d_272.dc.matmul.8, e2e_add_233_0],
         t: 1, mblock: [5, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_286.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [5, 2], inputs: [add_285, blocks.3.0.conv_pw.weight, input_1_add_287_fork_clone769],
         t: 1, mblock: [5, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 25}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, u_kt: 2}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 64
    _fused_op_10: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_conv2d_286.dc.matmul.8_0, input_1_add_299, input_1_multiply_301, e2e_conv2d_286.dc.matmul.8_0],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 10}}
    conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_10, lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 94, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_304.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.0.conv_dw.weight_fork_clone1667],
         t: 7, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_10, lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 86, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 5}}
    conv2d_304.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.0.conv_dw.weight],
         t: 7, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_10, lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 91, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_304.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [1, 4], grid_size: [1, 1], inputs: [conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.0.conv_dw.weight_fork_clone1665],
         t: 7, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_11: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 2], inputs: [conv2d_304.dc.conv2d.1.dc.depthwise.10, conv2d_304.dc.conv2d.3.dc.depthwise.10, conv2d_304.dc.conv2d.5.dc.depthwise.10, input_1_add_305, input_1_add_305_fork_clone670, input_1_add_317, input_1_multiply_319],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_6_tms: [vslice: 7], input_5_tms: [vslice: 7], input_4_tms: [vslice: 7], input_3_tms: [vslice: 7],
         attributes: {fused_op_id: 11}}
    conv2d_321.dc.matmul.8: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [_fused_op_11, blocks.3.0.conv_pwl.weight, input_1_add_322_fork_clone566],
         t: 7, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 14, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    conv2d_334.dc.matmul.8: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_321.dc.matmul.8, blocks.3.1.conv_pw.weight, input_1_add_335_fork_clone762],
         t: 7, mblock: [1, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 7}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_334.dc.matmul.8, input_1_add_347, input_1_multiply_349, conv2d_334.dc.matmul.8],
         t: 1, mblock: [7, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_3_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 12}}
    conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_12, lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_352.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.1.conv_dw.weight_fork_clone1701],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_12, lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_352.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.1.conv_dw.weight],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_12, lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_352.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.1.conv_dw.weight_fork_clone1699],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_13: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 7], inputs: [conv2d_352.dc.conv2d.1.dc.depthwise.10, conv2d_352.dc.conv2d.3.dc.depthwise.10, conv2d_352.dc.conv2d.5.dc.depthwise.10, input_1_add_353, input_1_add_353_fork_clone663, input_1_add_365, input_1_multiply_367],
         t: 1, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 13}}
    conv2d_369.dc.matmul.8: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_13, blocks.3.1.conv_pwl.weight, input_1_add_370_fork_clone561],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, u_kt: 7}}
    add_382: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_369.dc.matmul.8, conv2d_321.dc.matmul.8],
         t: 7, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 431], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 7]}
    conv2d_383.dc.matmul.8: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_382, blocks.3.2.conv_pw.weight, input_1_add_384_fork_clone737],
         t: 7, mblock: [1, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_14: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_383.dc.matmul.8, input_1_add_396, input_1_multiply_398, conv2d_383.dc.matmul.8],
         t: 1, mblock: [7, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_3_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 14}}
    conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_14, lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_401.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.2.conv_dw.weight_fork_clone1735],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_14, lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_401.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.2.conv_dw.weight],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_14, lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_401.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.2.conv_dw.weight_fork_clone1733],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_15: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [conv2d_401.dc.conv2d.1.dc.depthwise.10, conv2d_401.dc.conv2d.3.dc.depthwise.10, conv2d_401.dc.conv2d.5.dc.depthwise.10, input_1_add_402, input_1_add_402_fork_clone630, input_1_add_414, input_1_multiply_416],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 15}}
    conv2d_418.dc.matmul.8: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [_fused_op_15, blocks.3.2.conv_pwl.weight, input_1_add_419_fork_clone535],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    add_431: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_418.dc.matmul.8, add_382],
         t: 7, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 431], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 7]}
    conv2d_432.dc.matmul.8: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_431, blocks.3.3.conv_pw.weight, input_1_add_433_fork_clone705],
         t: 7, mblock: [1, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_432.dc.matmul.8, input_1_add_445, input_1_multiply_447, conv2d_432.dc.matmul.8],
         t: 1, mblock: [7, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_3_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 14}}
    conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_16, lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_450.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.3.conv_dw.weight_fork_clone1769],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_16, lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_450.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.3.conv_dw.weight],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_16, lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_450.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.3.conv_dw.weight_fork_clone1767],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_17: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 2], inputs: [conv2d_450.dc.conv2d.1.dc.depthwise.10, conv2d_450.dc.conv2d.3.dc.depthwise.10, conv2d_450.dc.conv2d.5.dc.depthwise.10, input_1_add_451, input_1_add_451_fork_clone594, input_1_add_463, input_1_multiply_465],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 15}}
    conv2d_467.dc.matmul.8: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_17, blocks.3.3.conv_pwl.weight, input_1_add_468_fork_clone515],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 3}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    add_480: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_467.dc.matmul.8, add_431],
         t: 7, mblock: [1, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 431], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 7]}
    conv2d_481.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_480, blocks.4.0.conv_pw.weight, input_1_add_482_fork_clone474],
         t: 7, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 15}, m_k: 1, min_buffer_input: 0, u_kt: 3}}
    _fused_op_18: {type: fused_op, grid_loc: [7, 1], grid_size: [1, 3], inputs: [conv2d_481.dc.matmul.8, input_1_add_494, input_1_multiply_496, conv2d_481.dc.matmul.8],
         t: 1, mblock: [7, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_3_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 18}}
    conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_18, lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_499.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.0.conv_dw.weight_fork_clone1803],
         t: 7, mblock: [1, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_18, lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_499.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [7, 7], grid_size: [1, 1], inputs: [conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.0.conv_dw.weight],
         t: 7, mblock: [1, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 64
    conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_18_0, lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_499.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.0.conv_dw.weight_fork_clone1801],
         t: 7, mblock: [1, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_19_transpose_nop_19184: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_conv2d_499.dc.conv2d.1.dc.depthwise.10_0],
         t: 1, mblock: [15, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hstack: 7]}
    _fused_op_19_transpose_nop_19185: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_499.dc.conv2d.3.dc.depthwise.10],
         t: 7, mblock: [15, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_19_transpose_nop_19186: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_conv2d_499.dc.conv2d.5.dc.depthwise.10_0],
         t: 1, mblock: [15, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hstack: 7]}
    _fused_op_19: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 7], inputs: [_fused_op_19_transpose_nop_19184, _fused_op_19_transpose_nop_19185, _fused_op_19_transpose_nop_19186, input_1_add_500, input_1_add_500_fork_clone409, input_1_add_512, input_1_multiply_514],
         t: 1, mblock: [15, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [hstack: 7],
         attributes: {fused_op_id: 19}}
    reduce_avg_517.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_19, lc.input_tensor.reduce_avg_517.0],
         t: 1, mblock: [15, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 7}}
    conv2d_518.dc.matmul.8_transpose_nop_18839: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [reduce_avg_517.lc1],
         t: 1, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_518.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_518.dc.matmul.8_transpose_nop_18839, blocks.4.0.se.conv_reduce.weight, blocks.4.0.se.conv_reduce.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 5}}
    conv2d_520.dc.matmul.8: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_518.dc.matmul.8, blocks.4.0.se.conv_expand.weight, blocks.4.0.se.conv_expand.bias],
         t: 1, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 15}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_20: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_520.dc.matmul.8, input_1_add_521, input_1_multiply_523],
         t: 1, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 20, kernel_broadcast: {input_1: 15}}}
    multiply_523_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_523_s_brcst_m2_0_0.0, _fused_op_20],
         t: 1, mblock: [7, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_19_buffer_0__fused_op_19_multiply_524_transpose_nop_19191: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_19],
         t: 1, mblock: [15, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [229], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_19_multiply_524_transpose_nop_19191: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_0__fused_op_19_buffer_0__fused_op_19_multiply_524_transpose_nop_19191],
         t: 1, mblock: [15, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [229], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_524_transpose_nop_19191: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_19_multiply_524_transpose_nop_19191],
         t: 1, mblock: [7, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [229], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    multiply_524: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [multiply_524_transpose_nop_19191, multiply_523_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [7, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [227, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_525.dc.matmul.8: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [multiply_524, blocks.4.0.conv_pwl.weight, input_1_add_526_fork_clone336],
         t: 2, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 4, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_538.dc.matmul.8: {type: matmul, grid_loc: [3, 0], grid_size: [1, 3], inputs: [conv2d_525.dc.matmul.8, blocks.4.1.conv_pw.weight, input_1_add_539_fork_clone467],
         t: 1, mblock: [7, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 7}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 3], inputs: [conv2d_538.dc.matmul.8, input_1_add_551, input_1_multiply_553, conv2d_538.dc.matmul.8],
         t: 1, mblock: [7, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 12}}
    conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_556.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.1.conv_dw.weight_fork_clone1865],
         t: 7, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_556.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.1.conv_dw.weight],
         t: 7, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_556.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.1.conv_dw.weight_fork_clone1863],
         t: 7, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_22_transpose_nop_19204: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.1.dc.depthwise.10],
         t: 7, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_22_transpose_nop_19205: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.3.dc.depthwise.10],
         t: 7, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_22_transpose_nop_19206: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_556.dc.conv2d.5.dc.depthwise.10],
         t: 7, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_22: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 7], inputs: [_fused_op_22_transpose_nop_19204, _fused_op_22_transpose_nop_19205, _fused_op_22_transpose_nop_19206, input_1_add_557, input_1_add_557_fork_clone398, input_1_add_569, input_1_multiply_571],
         t: 1, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [hstack: 7], input_1_tms: [hstack: 7], input_0_tms: [hstack: 7],
         attributes: {fused_op_id: 22}}
    reduce_avg_574.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [_fused_op_22, lc.input_tensor.reduce_avg_574.0],
         t: 1, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 7}}
    conv2d_575.dc.matmul.8_transpose_nop_18878: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [reduce_avg_574.lc1],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_575.dc.matmul.8: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [conv2d_575.dc.matmul.8_transpose_nop_18878, blocks.4.1.se.conv_reduce.weight, blocks.4.1.se.conv_reduce.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 7}}
    conv2d_577.dc.matmul.8: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [conv2d_575.dc.matmul.8, blocks.4.1.se.conv_expand.weight, blocks.4.1.se.conv_expand.bias],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 21}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_23: {type: fused_op, grid_loc: [6, 6], grid_size: [1, 1], inputs: [conv2d_577.dc.matmul.8, input_1_add_578, input_1_multiply_580],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 23, kernel_broadcast: {input_1: 21}}}
    multiply_580_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_580_s_brcst_m2_0_0.0, _fused_op_23],
         t: 1, mblock: [7, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_2__fused_op_22_multiply_581_transpose_nop_19211: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_22],
         t: 1, mblock: [21, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [145], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_22_multiply_581_transpose_nop_19211: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_2__fused_op_22_multiply_581_transpose_nop_19211],
         t: 1, mblock: [21, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [145], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_22_multiply_581_transpose_nop_19211: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_1__fused_op_22_multiply_581_transpose_nop_19211],
         t: 1, mblock: [21, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [145], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_581_transpose_nop_19211: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_22_multiply_581_transpose_nop_19211],
         t: 1, mblock: [7, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [145], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    multiply_581: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [multiply_581_transpose_nop_19211, multiply_580_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [7, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [143, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_582.dc.matmul.8: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [multiply_581, blocks.4.1.conv_pwl.weight, input_1_add_583_fork_clone331],
         t: 2, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    buffer_0_conv2d_525.dc.matmul.8_add_595: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_525.dc.matmul.8],
         t: 2, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [410], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_595: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [conv2d_582.dc.matmul.8, buffer_0_conv2d_525.dc.matmul.8_add_595],
         t: 2, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 406], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_596.dc.matmul.8: {type: matmul, grid_loc: [7, 3], grid_size: [1, 3], inputs: [add_595, blocks.5.0.conv_pw.weight, input_1_add_597_fork_clone283],
         t: 1, mblock: [7, 7], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 7}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 64
    _fused_op_24: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 7], inputs: [e2e_conv2d_596.dc.matmul.8_0, input_1_add_609, input_1_multiply_611, e2e_conv2d_596.dc.matmul.8_0],
         t: 1, mblock: [7, 3], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {fused_op_id: 24}}
    conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 36, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_614.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0.conv_dw.weight_fork_clone1927],
         t: 1, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 41, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_614.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [1, 6], grid_size: [2, 1], inputs: [conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0.conv_dw.weight_fork_clone1929],
         t: 1, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 36, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_614.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [2, 3], grid_size: [2, 1], inputs: [conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0.conv_dw.weight_fork_clone1931],
         t: 1, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 31, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_614.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [2, 4], grid_size: [2, 1], inputs: [conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0.conv_dw.weight],
         t: 1, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_24, lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 36, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_614.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [4, 0], grid_size: [2, 1], inputs: [conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0.conv_dw.weight_fork_clone1925],
         t: 1, mblock: [1, 3], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_25_transpose_nop_19226: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_614.dc.conv2d.5.dc.depthwise.10],
         t: 2, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_25_transpose_nop_19227: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_614.dc.conv2d.7.dc.depthwise.10],
         t: 2, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_25_transpose_nop_19228: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_614.dc.conv2d.1.dc.depthwise.10],
         t: 2, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_25_transpose_nop_19229: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_614.dc.conv2d.3.dc.depthwise.10],
         t: 2, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_25_transpose_nop_19230: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_614.dc.conv2d.9.dc.depthwise.10],
         t: 2, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_25: {type: fused_op, grid_loc: [4, 4], grid_size: [3, 2], inputs: [_fused_op_25_transpose_nop_19226, _fused_op_25_transpose_nop_19227, _fused_op_25_transpose_nop_19228, _fused_op_25_transpose_nop_19229, _fused_op_25_transpose_nop_19230, input_1_add_615, input_1_add_615_fork_clone190, input_1_add_627, input_1_multiply_629],
         t: 1, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [hstack: 2], input_3_tms: [hstack: 2], input_2_tms: [hstack: 2], input_1_tms: [hstack: 2], input_0_tms: [hstack: 2],
         attributes: {fused_op_id: 25}}
    reduce_avg_632.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_25, lc.input_tensor.reduce_avg_632.0],
         t: 1, mblock: [21, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_633.dc.matmul.8_transpose_nop_18928: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [reduce_avg_632.lc1],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_633.dc.matmul.8: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [conv2d_633.dc.matmul.8_transpose_nop_18928, blocks.5.0.se.conv_reduce.weight, blocks.5.0.se.conv_reduce.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 7}}
    conv2d_635.dc.matmul.8: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_633.dc.matmul.8, blocks.5.0.se.conv_expand.weight, blocks.5.0.se.conv_expand.bias],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 21}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_26: {type: fused_op, grid_loc: [5, 6], grid_size: [1, 1], inputs: [conv2d_635.dc.matmul.8, input_1_add_636, input_1_multiply_638],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 23, kernel_broadcast: {input_1: 21}}}
    multiply_638_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_638_s_brcst_m2_0_0.0, _fused_op_26],
         t: 2, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {kernel_broadcast: {input_1: 42}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_639_transpose_nop_19235: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_25],
         t: 2, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [397], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, vslice: 2]}
    multiply_639: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [multiply_639_transpose_nop_19235, multiply_638_s_brcst_m2_0_0.lc1],
         t: 2, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [395, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_640.dc.matmul.8: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_639, blocks.5.0.conv_pwl.weight, input_1_add_641_fork_clone81],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 5}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    conv2d_653.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [conv2d_640.dc.matmul.8, blocks.5.1.conv_pw.weight, input_1_add_654_fork_clone276],
         t: 2, mblock: [1, 5], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 10}, l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_27: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 2], inputs: [conv2d_653.dc.matmul.8, input_1_add_666, input_1_multiply_668, conv2d_653.dc.matmul.8],
         t: 2, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [vslice: 2], input_1_tms: [vslice: 2],
         attributes: {fused_op_id: 27}}
    conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 3], grid_size: [1, 3], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_27, lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 2], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 64
    conv2d_671.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0, blocks.5.1.conv_dw.weight_fork_clone1995],
         t: 1, mblock: [1, 15], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_27_0, lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 30, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_671.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.1.conv_dw.weight_fork_clone1997],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_27_0, lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 30, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_671.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [1, 4], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.1.conv_dw.weight_fork_clone1999],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_27_0, lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 30, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_671.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.1.conv_dw.weight],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_27_0, lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 30, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_671.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.1.conv_dw.weight_fork_clone1993],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_28_transpose_nop_19250: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.5.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_28_transpose_nop_19251: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.7.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_28_transpose_nop_19252: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.1.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_28_transpose_nop_19253: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.3.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_28_transpose_nop_19254: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.9.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_28: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 2], inputs: [_fused_op_28_transpose_nop_19250, _fused_op_28_transpose_nop_19251, _fused_op_28_transpose_nop_19252, _fused_op_28_transpose_nop_19253, _fused_op_28_transpose_nop_19254, input_1_add_672, input_1_add_672_fork_clone179, input_1_add_684, input_1_multiply_686],
         t: 1, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [hstack: 2], input_3_tms: [hstack: 2], input_2_tms: [hstack: 2], input_1_tms: [hstack: 2], input_0_tms: [hstack: 2],
         attributes: {fused_op_id: 28}}
    reduce_avg_689.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [_fused_op_28, lc.input_tensor.reduce_avg_689.0],
         t: 2, mblock: [15, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_690.dc.matmul.8_transpose_nop_18979: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [reduce_avg_689.lc1],
         t: 2, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_690.dc.matmul.8: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_690.dc.matmul.8_transpose_nop_18979, blocks.5.1.se.conv_reduce.weight, blocks.5.1.se.conv_reduce.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 5}}
    conv2d_692.dc.matmul.8: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_690.dc.matmul.8, blocks.5.1.se.conv_expand.weight, blocks.5.1.se.conv_expand.bias],
         t: 1, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 30}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 1}}
    _fused_op_29: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_692.dc.matmul.8, input_1_add_693, input_1_multiply_695],
         t: 1, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 29, kernel_broadcast: {input_1: 30}}}
    multiply_695_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_695_s_brcst_m2_0_0.0, _fused_op_29],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {kernel_broadcast: {input_1: 60}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_696_transpose_nop_19259: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_28],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, vslice: 2]}
    multiply_696: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [multiply_696_transpose_nop_19259, multiply_695_s_brcst_m2_0_0.lc1],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [374, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_697.dc.matmul.8: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [multiply_696, blocks.5.1.conv_pwl.weight, input_1_add_698_fork_clone76],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 5}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    add_710: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_697.dc.matmul.8, e2e_conv2d_640.dc.matmul.8_0],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 4, input_dram_io_buf_size_tiles: [0, 49], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_711.dc.matmul.8: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_710, blocks.5.2.conv_pw.weight, input_1_add_712_fork_clone253],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 30}, l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_30: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_711.dc.matmul.8, input_1_add_724, input_1_multiply_726, conv2d_711.dc.matmul.8],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 4, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [vslice: 2], input_1_tms: [vslice: 2],
         attributes: {fused_op_id: 30}}
    conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_30, lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_729.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2.conv_dw.weight_fork_clone2063],
         t: 1, mblock: [2, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_30, lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_729.dc.conv2d.7.dc.depthwise.10: {type: depthwise, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2.conv_dw.weight_fork_clone2065],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [168, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_30, lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_729.dc.conv2d.9.dc.depthwise.10: {type: depthwise, grid_loc: [5, 2], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2.conv_dw.weight_fork_clone2067],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [168, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_30, lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_729.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [4, 4], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2.conv_dw.weight],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [168, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_30, lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [5, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_729.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2.conv_dw.weight_fork_clone2061],
         t: 2, mblock: [1, 5], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [168, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 5, hstack: 5],
         attributes: {l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_31_transpose_nop_19274: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.5.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, hslice: 2]}
    _fused_op_31_transpose_nop_19275: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.7.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_31_transpose_nop_19276: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.1.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_31_transpose_nop_19277: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.3.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_31_transpose_nop_19278: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.9.dc.depthwise.10],
         t: 2, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    _fused_op_31: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 2], inputs: [_fused_op_31_transpose_nop_19274, _fused_op_31_transpose_nop_19275, _fused_op_31_transpose_nop_19276, _fused_op_31_transpose_nop_19277, _fused_op_31_transpose_nop_19278, input_1_add_730, input_1_add_730_fork_clone147, input_1_add_742, input_1_multiply_744],
         t: 1, mblock: [15, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 222, 0, 0, 0, 0, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_4_tms: [hstack: 2], input_3_tms: [hstack: 2], input_2_tms: [hstack: 2], input_1_tms: [hstack: 2], input_0_tms: [hstack: 2],
         attributes: {fused_op_id: 28}}
    reduce_avg_747.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_31, lc.input_tensor.reduce_avg_747.0],
         t: 2, mblock: [15, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_748.dc.matmul.8_transpose_nop_19029: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [reduce_avg_747.lc1],
         t: 2, mblock: [1, 15], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose]}
    conv2d_748.dc.matmul.8: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_748.dc.matmul.8_transpose_nop_19029, blocks.5.2.se.conv_reduce.weight, blocks.5.2.se.conv_reduce.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 5}}
    conv2d_750.dc.matmul.8: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_748.dc.matmul.8, blocks.5.2.se.conv_expand.weight, blocks.5.2.se.conv_expand.bias],
         t: 1, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 30}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 1}}
    _fused_op_32: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_750.dc.matmul.8, input_1_add_751, input_1_multiply_753],
         t: 1, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {r: 1}],
         attributes: {fused_op_id: 29, kernel_broadcast: {input_1: 30}}}
    multiply_753_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_753_s_brcst_m2_0_0.0, _fused_op_32],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {kernel_broadcast: {input_1: 60}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_754_transpose_nop_19283: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_31],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [378], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_0_tms: [transpose, vslice: 2]}
    multiply_754: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [multiply_754_transpose_nop_19283, multiply_753_s_brcst_m2_0_0.lc1],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [374, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_755.dc.matmul.8: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [multiply_754, blocks.5.2.conv_pwl.weight, input_1_add_756_fork_clone59],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 5}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    add_768: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_755.dc.matmul.8, add_710],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 427], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_769.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_768, blocks.6.0.conv.weight, input_1_add_770_fork_clone32],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 30}, l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    _fused_op_33: {type: fused_op, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_769.dc.matmul.8, input_1_add_782, input_1_multiply_784, conv2d_769.dc.matmul.8],
         t: 2, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [vslice: 2], input_1_tms: [vslice: 2],
         attributes: {fused_op_id: 30}}
    avg_pool2d_786.dc.reduce_avg.2.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.avg_pool2d_786.dc.reduce_avg.2.0, _fused_op_33],
         t: 1, mblock: [1, 15], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 2],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_787.dc.matmul.8: {type: matmul, grid_loc: [7, 3], grid_size: [1, 4], inputs: [avg_pool2d_786.dc.reduce_avg.2.lc1, conv_head.weight, conv_head.bias],
         t: 1, mblock: [1, 5], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {r: 1}],
         attributes: {bias: true, kernel_broadcast: {input_2: 10}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    _fused_op_34: {type: fused_op, grid_loc: [7, 7], grid_size: [1, 1], inputs: [conv2d_787.dc.matmul.8, input_1_add_788, input_1_multiply_790, conv2d_787.dc.matmul.8],
         t: 1, mblock: [1, 10], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {z: 1}], input_1_tms: [broadcast: {z: 1}],
         attributes: {fused_op_id: 34, kernel_broadcast: {input_2: 40, input_1: 40}}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 64
    matmul_794: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e__fused_op_34_0, classifier.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [40, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 20}}
    add_795: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_794, classifier.bias], untilize_output: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0, $lptr_q1: 0, $c_zero: 0, $c_one: 1, $lptr_q7: 0, $c_microbatch_size: 64, $gptr_q6: 0, $gptr_q1: 0, $lptr_q8: 0, $gptr_q8: 0, $lptr_q5: 0, $lptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $gptr_q7: 0, $gptr_q4: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_conv2d_75.dc.matmul.8_0, e2e_conv2d_88.dc.matmul.8_0, e2e_conv2d_103.dc.conv2d.1.dc.depthwise.10_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv_stem.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv_stem.weight_fork_clone2148: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1_fork_clone1216: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_13: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_15: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0.conv_dw.weight_fork_clone1361: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_18.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0.conv_dw.weight_fork_clone1359: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_19: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_19_fork_clone1240: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_33_fork_clone1200: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_47_fork_clone1160: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0.conv_dw.weight_fork_clone1395: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_61.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0.conv_dw.weight_fork_clone1393: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_62: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_62_fork_clone1115: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_76_fork_clone1071: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_89_fork_clone1154: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_103.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_conv2d_169.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e_conv2d_75.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_88.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_103.dc.conv2d.1.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_103.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1.conv_dw.weight_fork_clone1429: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_103.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1.conv_dw.weight_fork_clone1427: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_104: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_104_fork_clone1109: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_118_fork_clone1066: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_132_fork_clone1019: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_dw.weight_fork_clone1463: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_dw.weight_fork_clone1465: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_dw.weight_fork_clone1467: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_146.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_dw.weight_fork_clone1461: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_147: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_147_fork_clone940: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_161.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.0.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.0.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.0.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_165: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_167: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_167_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_170_fork_clone846: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_75.dc.matmul.8_0, e2e_conv2d_88.dc.matmul.8_0, e2e_conv2d_103.dc.conv2d.1.dc.depthwise.10_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_233_0, e2e_multiply_271_0]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e_conv2d_169.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               blocks.2.1.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_183_fork_clone1013: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_dw.weight_fork_clone1531: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_dw.weight_fork_clone1533: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_dw.weight_fork_clone1535: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_197.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_dw.weight_fork_clone1529: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_198: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_198_fork_clone930: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_212.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.1.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.1.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.1.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_216: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_218: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_218_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_221_fork_clone841: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_235_fork_clone993: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_dw.weight_fork_clone1599: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_dw.weight_fork_clone1601: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_dw.weight_fork_clone1603: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_249.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.conv_dw.weight_fork_clone1597: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_250: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_250_fork_clone902: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_264.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.2.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.2.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.2.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.2.2.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_268: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_270: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_270_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_169.dc.matmul.8_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_conv2d_286.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_add_233_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_multiply_271_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               blocks.2.2.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_273_fork_clone822: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_287_fork_clone769: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_233_0, e2e_multiply_271_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_18_0, e2e_conv2d_499.dc.conv2d.5.dc.depthwise.10_0, e2e_conv2d_499.dc.conv2d.1.dc.depthwise.10_0]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e_conv2d_286.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               input_1_add_299: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_301: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0.conv_dw.weight_fork_clone1667: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_304.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0.conv_dw.weight_fork_clone1665: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_305: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_305_fork_clone670: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_317: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_319: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_322_fork_clone566: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_335_fork_clone762: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_347: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_349: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1.conv_dw.weight_fork_clone1701: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_352.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1.conv_dw.weight_fork_clone1699: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_353: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_353_fork_clone663: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_365: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_367: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_370_fork_clone561: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_384_fork_clone737: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_396: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_398: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2.conv_dw.weight_fork_clone1735: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_401.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2.conv_dw.weight_fork_clone1733: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_402: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_402_fork_clone630: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_414: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_416: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_419_fork_clone535: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.3.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_433_fork_clone705: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_445: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_447: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.3.conv_dw.weight_fork_clone1769: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.3.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_450.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.3.conv_dw.weight_fork_clone1767: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_451: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_451_fork_clone594: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_463: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_465: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.3.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_468_fork_clone515: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_482_fork_clone474: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_494: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_496: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_499.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.conv_dw.weight_fork_clone1803: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_499.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_286.dc.matmul.8_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_conv2d_596.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_499.dc.conv2d.5.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_499.dc.conv2d.1.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_499.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.conv_dw.weight_fork_clone1801: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_500: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_500_fork_clone409: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_512: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_514: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_517.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.0.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.0.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.0.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_521: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_523: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_523_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_526_fork_clone336: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_539_fork_clone467: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_551: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_553: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.conv_dw.weight_fork_clone1865: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_556.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.conv_dw.weight_fork_clone1863: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_557: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_557_fork_clone398: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_569: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_571: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_574.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.1.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.1.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.4.1.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_578: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_580: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_580_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_583_fork_clone331: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_597_fork_clone283: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_18_0, e2e_conv2d_499.dc.conv2d.5.dc.depthwise.10_0, e2e_conv2d_499.dc.conv2d.1.dc.depthwise.10_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_conv2d_640.dc.matmul.8_0, e2e__fused_op_27_0, e2e_conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_conv2d_596.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_add_609: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_611: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_dw.weight_fork_clone1927: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_dw.weight_fork_clone1929: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_dw.weight_fork_clone1931: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_614.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_dw.weight_fork_clone1925: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_615: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_615_fork_clone190: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_627: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_629: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_632.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.0.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.0.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.0.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_636: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_638: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_638_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_641_fork_clone81: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_654_fork_clone276: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_666: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_668: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_596.dc.matmul.8_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_34_0]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_conv2d_640.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e__fused_op_27_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               blocks.5.1.conv_dw.weight_fork_clone1995: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_dw.weight_fork_clone1997: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_dw.weight_fork_clone1999: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_dw.weight_fork_clone1993: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_672: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_672_fork_clone179: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_684: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_686: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_689.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.1.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.1.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.1.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_693: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_695: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_695_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_698_fork_clone76: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_pw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_712_fork_clone253: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_724: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_726: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_dw.weight_fork_clone2063: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.7.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_dw.weight_fork_clone2065: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.9.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_dw.weight_fork_clone2067: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_dw.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_dw.weight_fork_clone2061: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_730: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_730_fork_clone147: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_742: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_744: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reduce_avg_747.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.se.conv_reduce.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.2.se.conv_reduce.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.2.se.conv_expand.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               blocks.5.2.se.conv_expand.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_751: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_753: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.multiply_753_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2.conv_pwl.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_756_fork_clone59: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.6.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_770_fork_clone32: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_782: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_784: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.avg_pool2d_786.dc.reduce_avg.2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv_head.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               conv_head.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_788: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_790: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_640.dc.matmul.8_0, e2e__fused_op_27_0, e2e_conv2d_671.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               classifier.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               classifier.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_34_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 128]
    - endloop


fused_ops:
  0: 
    inputs: 5
    intermediates: 2
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.4.0: { type: add, inputs: [input0, input1], mblock: [98, 1], ublock: [2, 1], output: dest}
        - add_12.0: { type: add, inputs: [dest, input2], mblock: [98, 1], ublock: [2, 1], output: intermed0}
        - add_13.0: { type: add, inputs: [intermed0, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [98, 1], ublock: [2, 1], output: intermed1}
        - multiply_15.0: { type: multiply, inputs: [intermed1, input4], pop: [intermed1], mblock: [98, 1], ublock: [2, 1], output: dest}
        - multiply_16.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [98, 1], ublock: [2, 1], output: output}
  1: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_18.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [2, 1], output: dest}
        - conv2d_18.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [2, 1], output: dest}
        - multiply_25.0: { type: multiply, inputs: [dest, input3], mblock: [7, 1], ublock: [2, 1], output: dest}
        - add_30.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [7, 1], ublock: [2, 1], output: output}
  2: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_61.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [49, 1], ublock: [1, 2], output: dest}
        - conv2d_61.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [49, 1], ublock: [1, 2], output: dest}
        - multiply_68.0: { type: multiply, inputs: [dest, input3], mblock: [49, 1], ublock: [1, 2], output: dest}
        - add_73.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [49, 1], ublock: [1, 2], output: output}
  3: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_103.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [49, 1], ublock: [1, 1], output: dest}
        - conv2d_103.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [49, 1], ublock: [1, 1], output: dest}
        - multiply_110.0: { type: multiply, inputs: [dest, input3], mblock: [49, 1], ublock: [1, 1], output: dest}
        - add_115.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [49, 1], ublock: [1, 1], output: output}
  4: 
    inputs: 7
    intermediates: 1
    schedules: 
      -
        - conv2d_146.dc.add.11.0: { type: add, inputs: [input0, input1], mblock: [3, 25], ublock: [1, 1], output: intermed0}
        - conv2d_146.dc.add.10.0: { type: add, inputs: [input2, input3], mblock: [3, 25], ublock: [1, 1], output: dest}
        - conv2d_146.dc.add.12.0: { type: add, inputs: [input4, dest], mblock: [3, 25], ublock: [1, 1], output: dest}
        - conv2d_146.dc.add.13.0: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [3, 25], ublock: [1, 1], output: dest}
        - multiply_153.0: { type: multiply, inputs: [dest, input5], mblock: [3, 25], ublock: [1, 1], output: dest}
        - add_158.0: { type: add, inputs: [dest, input6], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [3, 25], ublock: [1, 1], output: output}
  5: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - add_165.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 3], ublock: [1, 1], output: intermed0}
        - multiply_167.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 3], ublock: [1, 1], output: output}
  6: 
    inputs: 7
    intermediates: 1
    schedules: 
      -
        - conv2d_197.dc.add.11.0: { type: add, inputs: [input0, input1], mblock: [2, 5], ublock: [2, 1], output: intermed0}
        - conv2d_197.dc.add.10.0: { type: add, inputs: [input2, input3], mblock: [2, 5], ublock: [2, 1], output: dest}
        - conv2d_197.dc.add.12.0: { type: add, inputs: [input4, dest], mblock: [2, 5], ublock: [2, 1], output: dest}
        - conv2d_197.dc.add.13.0: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [2, 5], ublock: [2, 1], output: dest}
        - multiply_204.0: { type: multiply, inputs: [dest, input5], mblock: [2, 5], ublock: [2, 1], output: dest}
        - add_209.0: { type: add, inputs: [dest, input6], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [2, 5], ublock: [2, 1], output: output}
  7: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - add_216.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 4], output: intermed0}
        - multiply_218.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 1], ublock: [1, 4], output: output}
  10: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_299.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [25, 1], ublock: [1, 2], output: intermed0}
        - multiply_301.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [25, 1], ublock: [1, 2], output: dest}
        - multiply_302.0: { type: multiply, inputs: [input3, dest], mblock: [25, 1], ublock: [1, 2], output: output}
  11: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - conv2d_304.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 4], output: dest}
        - conv2d_304.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_311.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 4], output: dest}
        - add_316.0: { type: add, inputs: [dest, input4], mblock: [1, 1], ublock: [1, 4], output: intermed0}
        - add_317.0: { type: add, inputs: [intermed0, input5], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 4], output: intermed1}
        - multiply_319.0: { type: multiply, inputs: [intermed1, input6], pop: [intermed1], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_320.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [1, 1], ublock: [1, 4], output: output}
  12: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_347.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 7], ublock: [1, 1], output: intermed0}
        - multiply_349.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [7, 7], ublock: [1, 1], output: dest}
        - multiply_350.0: { type: multiply, inputs: [input3, dest], mblock: [7, 7], ublock: [1, 1], output: output}
  13: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - conv2d_352.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [1, 1], output: dest}
        - conv2d_352.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [1, 1], output: dest}
        - multiply_359.0: { type: multiply, inputs: [dest, input3], mblock: [7, 1], ublock: [1, 1], output: dest}
        - add_364.0: { type: add, inputs: [dest, input4], mblock: [7, 1], ublock: [1, 1], output: intermed0}
        - add_365.0: { type: add, inputs: [intermed0, input5], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 1], ublock: [1, 1], output: intermed1}
        - multiply_367.0: { type: multiply, inputs: [intermed1, input6], pop: [intermed1], mblock: [7, 1], ublock: [1, 1], output: dest}
        - multiply_368.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [7, 1], ublock: [1, 1], output: output}
  14: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_396.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 3], ublock: [1, 2], output: intermed0}
        - multiply_398.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [7, 3], ublock: [1, 2], output: dest}
        - multiply_399.0: { type: multiply, inputs: [input3, dest], mblock: [7, 3], ublock: [1, 2], output: output}
  15: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - conv2d_401.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 3], ublock: [1, 1], output: dest}
        - conv2d_401.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 3], ublock: [1, 1], output: dest}
        - multiply_408.0: { type: multiply, inputs: [dest, input3], mblock: [7, 3], ublock: [1, 1], output: dest}
        - add_413.0: { type: add, inputs: [dest, input4], mblock: [7, 3], ublock: [1, 1], output: intermed0}
        - add_414.0: { type: add, inputs: [intermed0, input5], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 3], ublock: [1, 1], output: intermed1}
        - multiply_416.0: { type: multiply, inputs: [intermed1, input6], pop: [intermed1], mblock: [7, 3], ublock: [1, 1], output: dest}
        - multiply_417.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [7, 3], ublock: [1, 1], output: output}
  18: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_494.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 5], ublock: [1, 1], output: intermed0}
        - multiply_496.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [7, 5], ublock: [1, 1], output: dest}
        - multiply_497.0: { type: multiply, inputs: [input3, dest], mblock: [7, 5], ublock: [1, 1], output: output}
  19: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - conv2d_499.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [15, 1], ublock: [1, 1], output: dest}
        - conv2d_499.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [15, 1], ublock: [1, 1], output: dest}
        - multiply_506.0: { type: multiply, inputs: [dest, input3], mblock: [15, 1], ublock: [1, 1], output: dest}
        - add_511.0: { type: add, inputs: [dest, input4], mblock: [15, 1], ublock: [1, 1], output: intermed0}
        - add_512.0: { type: add, inputs: [intermed0, input5], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [15, 1], ublock: [1, 1], output: intermed1}
        - multiply_514.0: { type: multiply, inputs: [intermed1, input6], pop: [intermed1], mblock: [15, 1], ublock: [1, 1], output: dest}
        - multiply_515.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [15, 1], ublock: [1, 1], output: output}
  20: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - add_521.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 15], ublock: [1, 1], output: intermed0}
        - multiply_523.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 15], ublock: [1, 1], output: output}
  22: 
    inputs: 7
    intermediates: 2
    schedules: 
      -
        - conv2d_556.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [21, 1], ublock: [1, 1], output: dest}
        - conv2d_556.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [21, 1], ublock: [1, 1], output: dest}
        - multiply_563.0: { type: multiply, inputs: [dest, input3], mblock: [21, 1], ublock: [1, 1], output: dest}
        - add_568.0: { type: add, inputs: [dest, input4], mblock: [21, 1], ublock: [1, 1], output: intermed0}
        - add_569.0: { type: add, inputs: [intermed0, input5], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [21, 1], ublock: [1, 1], output: intermed1}
        - multiply_571.0: { type: multiply, inputs: [intermed1, input6], pop: [intermed1], mblock: [21, 1], ublock: [1, 1], output: dest}
        - multiply_572.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [21, 1], ublock: [1, 1], output: output}
  23: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - add_578.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 21], ublock: [1, 1], output: intermed0}
        - multiply_580.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 21], ublock: [1, 1], output: output}
  24: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_609.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 3], ublock: [1, 1], output: intermed0}
        - multiply_611.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [7, 3], ublock: [1, 1], output: dest}
        - multiply_612.0: { type: multiply, inputs: [input3, dest], mblock: [7, 3], ublock: [1, 1], output: output}
  25: 
    inputs: 9
    intermediates: 2
    schedules: 
      -
        - conv2d_614.dc.add.11.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [1, 1], output: intermed0}
        - conv2d_614.dc.add.10.0: { type: add, inputs: [input2, input3], mblock: [7, 1], ublock: [1, 1], output: dest}
        - conv2d_614.dc.add.12.0: { type: add, inputs: [input4, dest], mblock: [7, 1], ublock: [1, 1], output: dest}
        - conv2d_614.dc.add.13.0: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [7, 1], ublock: [1, 1], output: dest}
        - multiply_621.0: { type: multiply, inputs: [dest, input5], mblock: [7, 1], ublock: [1, 1], output: dest}
        - add_626.0: { type: add, inputs: [dest, input6], mblock: [7, 1], ublock: [1, 1], output: intermed0}
        - add_627.0: { type: add, inputs: [intermed0, input7], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 1], ublock: [1, 1], output: intermed1}
        - multiply_629.0: { type: multiply, inputs: [intermed1, input8], pop: [intermed1], mblock: [7, 1], ublock: [1, 1], output: dest}
        - multiply_630.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [7, 1], ublock: [1, 1], output: output}
  27: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_666.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 15], ublock: [1, 1], output: intermed0}
        - multiply_668.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 15], ublock: [1, 1], output: dest}
        - multiply_669.0: { type: multiply, inputs: [input3, dest], mblock: [1, 15], ublock: [1, 1], output: output}
  28: 
    inputs: 9
    intermediates: 2
    schedules: 
      -
        - conv2d_671.dc.add.11.0: { type: add, inputs: [input0, input1], mblock: [15, 1], ublock: [2, 1], output: intermed0}
        - conv2d_671.dc.add.10.0: { type: add, inputs: [input2, input3], mblock: [15, 1], ublock: [2, 1], output: dest}
        - conv2d_671.dc.add.12.0: { type: add, inputs: [input4, dest], mblock: [15, 1], ublock: [2, 1], output: dest}
        - conv2d_671.dc.add.13.0: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [15, 1], ublock: [2, 1], output: dest}
        - multiply_678.0: { type: multiply, inputs: [dest, input5], mblock: [15, 1], ublock: [2, 1], output: dest}
        - add_683.0: { type: add, inputs: [dest, input6], mblock: [15, 1], ublock: [2, 1], output: intermed0}
        - add_684.0: { type: add, inputs: [intermed0, input7], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [15, 1], ublock: [2, 1], output: intermed1}
        - multiply_686.0: { type: multiply, inputs: [intermed1, input8], pop: [intermed1], mblock: [15, 1], ublock: [2, 1], output: dest}
        - multiply_687.0: { type: multiply, inputs: [intermed0, dest], pop: [intermed0], mblock: [15, 1], ublock: [2, 1], output: output}
  29: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - add_693.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 15], ublock: [1, 2], output: intermed0}
        - multiply_695.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 15], ublock: [1, 2], output: output}
  30: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_724.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 15], ublock: [1, 2], output: intermed0}
        - multiply_726.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 15], ublock: [1, 2], output: dest}
        - multiply_727.0: { type: multiply, inputs: [input3, dest], mblock: [1, 15], ublock: [1, 2], output: output}
  34: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_788.0: { type: add, inputs: [input0, input1], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 10], ublock: [1, 4], output: intermed0}
        - multiply_790.0: { type: multiply, inputs: [intermed0, input2], pop: [intermed0], mblock: [1, 10], ublock: [1, 4], output: dest}
        - multiply_791.0: { type: multiply, inputs: [input3, dest], mblock: [1, 10], ublock: [1, 4], output: output}

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.08
    test-group: "perf_infra_wormhole_b0_silicon_nightly"
    test-name: "mobilenet_v3_hifi2_fp16b"