# git checkout 4a62b4c36
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large_tc -opt 4 --loop_count 32 -mb 64 -bp NLP -df Fp16_b -mf HiFi3 -o perf.json --auto_transpose

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                           {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x0]]}
  attention_mask:                                    {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x6180020]]}

  # output
  bert_encoders.output_layernorm_1271:               {input: _fused_op_167_output_nop_0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x648c040]]}

  # parameter
  layer.0.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x423fa280], [4, 0xbb0f140], [4, 0x423b9280], [5, 0x63a8420], [5, 0x423c1480], [0, 0x4d80ca0], [0, 0x42887980], [1, 0x6a62420]]}
  layer.0.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4158e4a0], [4, 0xaca7460], [4, 0x41506320], [5, 0x55ab9a0]]}
  layer.0.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4276ab60], [2, 0x502f5a0], [2, 0x425ce5e0], [3, 0x8cdaaa0], [3, 0x4243b2a0], [4, 0xbb50160], [4, 0x423fa2a0], [5, 0x63e9440]]}
  layer.0.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x424024a0], [0, 0x4dc1cc0], [0, 0x428c89a0], [1, 0x6aa3440]]}
  layer.0.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbb91180], [4, 0x4243b2c0], [5, 0x642a460], [5, 0x424065c0], [0, 0x4dc5de0], [0, 0x428ccac0], [1, 0x6aa7560], [1, 0x427ac3c0]]}
  layer.0.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5070e00], [2, 0x4262fe20], [3, 0x8d3c2e0], [3, 0x4249cae0]]}
  layer.0.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbbd21a0], [4, 0x4247c2e0], [5, 0x646b480], [5, 0x424475e0], [0, 0x4e06e00], [0, 0x4290dae0], [1, 0x6ae8580], [1, 0x427ed3e0]]}
  layer.0.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5074f20], [2, 0x42633f40], [3, 0x8d40400], [3, 0x424a0c00]]}
  layer.0.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x656f500]]}
  layer.0.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4254b660]]}
  layer.0.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4f0ae80], [0, 0x42a11b60], [1, 0x6bec600], [1, 0x428f1460], [2, 0x513c0a0], [2, 0x426ba8e0], [3, 0x8dc9640], [3, 0x42529e40], [4, 0xbc98b20], [4, 0x42583440], [5, 0x657f920], [5, 0x4255ba80], [0, 0x4f4bea0], [0, 0x42a52b80], [1, 0x6c2d620], [1, 0x42932480], [2, 0x517d0c0], [2, 0x426fb900], [3, 0x8e0a660], [3, 0x4256ae60], [4, 0xbcd9b40], [4, 0x425c4460], [5, 0x65c0940], [5, 0x4259caa0], [0, 0x4f8cec0], [0, 0x42a93ba0], [1, 0x6c6e640], [1, 0x429734a0], [2, 0x51be0e0], [2, 0x4273c920], [3, 0x8e4b680], [3, 0x425abe80]]}
  layer.0.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbd1ab60], [4, 0x42605480], [5, 0x6601960], [5, 0x425ddac0], [0, 0x4fcdee0], [0, 0x42ad4bc0], [1, 0x6caf660], [1, 0x429b44c0]]}
  layer.0.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x51ff100], [2, 0x4277d940], [3, 0x8e8c6a0], [3, 0x425ecea0], [4, 0xbd22d80], [4, 0x4260d6a0], [5, 0x6609b80], [5, 0x425e5ce0], [0, 0x4fd6100], [0, 0x42adcde0], [1, 0x6cb7880], [1, 0x429bc6e0], [2, 0x5281120], [2, 0x427ff960], [3, 0x8f0e6c0], [3, 0x4266eec0]]}
  layer.0.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbda4da0], [4, 0x4268f6c0], [5, 0x668bba0], [5, 0x42667d00], [0, 0x5058120], [0, 0x42b5ee00], [1, 0x6d398a0], [1, 0x42a3e700]]}
  layer.0.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x427ea200]]}
  layer.0.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x69c4ca0]]}
  layer.1.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x426cfc80], [2, 0x4f946c0], [2, 0x42481320], [3, 0x8b8d7e0], [3, 0x422f59c0], [4, 0xb9a98a0], [4, 0x422539e0], [5, 0x62a3b60]]}
  layer.1.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42334120], [0, 0x4cf3940], [0, 0x427fa620], [1, 0x69d50c0]]}
  layer.1.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42710ca0], [2, 0x4fd56e0], [2, 0x424c2340], [3, 0x8bce800], [3, 0x423369e0], [4, 0xb9ea8c0], [4, 0x42294a00], [5, 0x62e4b80]]}
  layer.1.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42338240], [0, 0x4cf7a60], [0, 0x427fe740], [1, 0x69d91e0]]}
  layer.1.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42377a00], [4, 0xba2b8e0], [4, 0x422d5a20], [5, 0x6325ba0], [5, 0x4233c360], [0, 0x4cfbb80], [0, 0x42802860], [1, 0x69dd300]]}
  layer.1.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42752500], [2, 0x5016f40], [2, 0x42564b80], [3, 0x8c71040]]}
  layer.1.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x423b8a20], [4, 0xba6c900], [4, 0x42316a40], [5, 0x6366bc0], [5, 0x4237d380], [0, 0x4d3cba0], [0, 0x42843880], [1, 0x6a1e320]]}
  layer.1.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42756620], [2, 0x501b060], [2, 0x42568ca0], [3, 0x8c75160]]}
  layer.1.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4275a740]]}
  layer.1.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x501f180]]}
  layer.1.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x424bd300], [5, 0x64ac4a0], [5, 0x42488600], [0, 0x4e47e20], [0, 0x4294eb00], [1, 0x6b295a0], [1, 0x4282e400], [2, 0x5079040], [2, 0x42638060], [3, 0x8d44520], [3, 0x424a4d20], [4, 0xbc13a00], [4, 0x424fe320], [5, 0x64ed4c0], [5, 0x424c9620], [0, 0x4e88e40], [0, 0x4298fb20], [1, 0x6b6a5c0], [1, 0x4286f420], [2, 0x50ba060], [2, 0x42679080], [3, 0x8d85540], [3, 0x424e5d40], [4, 0xbc54a20], [4, 0x4253f340], [5, 0x652e4e0], [5, 0x4250a640], [0, 0x4ec9e60], [0, 0x429d0b40], [1, 0x6bab5e0], [1, 0x428b0440], [2, 0x50fb080]]}
  layer.1.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6824160], [5, 0x4280d600], [0, 0x5183c20], [0, 0x42cbb500], [1, 0x6ea32e0], [1, 0x42ba8140], [2, 0x54d5540], [2, 0x42acc320]]}
  layer.1.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x923f940], [3, 0x429a29e0], [4, 0xc058940], [4, 0x4288a4e0], [5, 0x682c380], [5, 0x42815820], [0, 0x518be40], [0, 0x42cc3720], [1, 0x6eab500], [1, 0x42bb0360], [2, 0x54dd760], [2, 0x42ad4540], [3, 0x92c1960], [3, 0x42a24a00], [4, 0xc0da960], [4, 0x4290c500]]}
  layer.1.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x68ae3a0], [5, 0x42897840], [0, 0x520de60], [0, 0x42d45740], [1, 0x6f2d520], [1, 0x42c32380], [2, 0x555f780], [2, 0x42b56560]]}
  layer.1.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f2f5c0]]}
  layer.1.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42c34420]]}
  layer.2.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5561820], [2, 0x42b58600], [3, 0x93441c0], [3, 0x42b08240], [4, 0xc1be1a0], [4, 0x4298ed60], [5, 0x68b3520], [5, 0x4289c9c0]]}
  layer.2.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x923b820], [3, 0x4299e8c0], [4, 0xc054820], [4, 0x428863c0]]}
  layer.2.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d4a8c0], [1, 0x6f3f9e0], [1, 0x42c44840], [2, 0x55a2840], [2, 0x42b99620], [3, 0x93851e0], [3, 0x42b49260], [4, 0xc1ff1c0]]}
  layer.2.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x429cfd80], [5, 0x68f4540], [5, 0x428dd9e0], [0, 0x5223400]]}
  layer.2.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x93c6200], [3, 0x42b8a280], [4, 0xc2401e0], [4, 0x429d3ea0], [5, 0x68f8660], [5, 0x428e1b00], [0, 0x5227520], [0, 0x42d8c120]]}
  layer.2.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f81240], [1, 0x42ca6080], [2, 0x5604080], [2, 0x42bfae60]]}
  layer.2.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9407220], [3, 0x42bcb2a0], [4, 0xc281200], [4, 0x42a14ec0], [5, 0x6939680], [5, 0x42922b20], [0, 0x5268540], [0, 0x42dcd140]]}
  layer.2.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f85360], [1, 0x42caa1a0], [2, 0x56081a0], [2, 0x42bfef80]]}
  layer.2.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42669da0]]}
  layer.2.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x505a1c0]]}
  layer.2.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42b60ea0], [1, 0x6d3b940], [1, 0x42a407a0], [2, 0x5303980], [2, 0x428821c0], [3, 0x8f90f20], [3, 0x426f3fc0], [4, 0xbda9f20], [4, 0x42694840], [5, 0x6690d20], [5, 0x4267a1c0], [0, 0x506a5e0], [0, 0x42ba1ec0], [1, 0x6d7c960], [1, 0x42a817c0], [2, 0x53449a0], [2, 0x428c31e0], [3, 0x8fd1f40], [3, 0x42734fe0], [4, 0xbdeaf40], [4, 0x426d5860], [5, 0x66d1d40], [5, 0x426bb1e0], [0, 0x50ab600], [0, 0x42be2ee0], [1, 0x6dbd980], [1, 0x42ac27e0], [2, 0x53859c0], [2, 0x42904200], [3, 0x9012f60], [3, 0x42776000], [4, 0xbe2bf60]]}
  layer.2.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42716880], [5, 0x6712d60], [5, 0x426fc200], [0, 0x50ec620], [0, 0x42c23f00], [1, 0x6dfe9a0], [1, 0x42b03800], [2, 0x53c69e0]]}
  layer.2.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42945220], [3, 0x9053f80], [3, 0x427b7020], [4, 0xbe6cf80], [4, 0x4271eaa0], [5, 0x671af80], [5, 0x42704420], [0, 0x50f4840], [0, 0x42c2c120], [1, 0x6e06bc0], [1, 0x42b0ba20], [2, 0x53cec00], [2, 0x429c7240], [3, 0x90d5fa0], [3, 0x42839040], [4, 0xbeeefa0]]}
  layer.2.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x427a0ac0], [5, 0x679cfa0], [5, 0x42786440], [0, 0x5176860], [0, 0x42cae140], [1, 0x6e88be0], [1, 0x42b8da40], [2, 0x5450c20]]}
  layer.2.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e8ac80]]}
  layer.2.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b8fae0]]}
  layer.3.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5452cc0], [2, 0x42a49aa0], [3, 0x91b97e0], [3, 0x4291c880], [4, 0xbfd27e0], [4, 0x42804380], [5, 0x67a2120], [5, 0x4278b5c0]]}
  layer.3.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x517b9e0], [0, 0x42cb32c0], [1, 0x6e9b0a0], [1, 0x42b9ff00]]}
  layer.3.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5493ce0], [2, 0x42a8aac0], [3, 0x91fa800], [3, 0x4295d8a0], [4, 0xc013800], [4, 0x428453a0], [5, 0x67e3140], [5, 0x427cc5e0]]}
  layer.3.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x517fb00], [0, 0x42cb73e0], [1, 0x6e9f1c0], [1, 0x42ba4020]]}
  layer.3.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41ec3e60], [4, 0xb58eae0], [4, 0x41e05780], [5, 0x5ec8d60], [5, 0x41f56a80], [0, 0x48067c0], [0, 0x4223e1a0], [1, 0x6383fa0]]}
  layer.3.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x421d6820], [2, 0x4af1fc0], [2, 0x4206ffe0], [3, 0x88300e0]]}
  layer.3.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41f04e80], [4, 0xb5cfb00], [4, 0x41e467a0], [5, 0x5f09d80], [5, 0x41f97aa0], [0, 0x48477e0], [0, 0x4227f1c0], [1, 0x63c4fc0]]}
  layer.3.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x421da940], [2, 0x4af60e0], [2, 0x42074100], [3, 0x8834200]]}
  layer.3.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x421dea60]]}
  layer.3.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4afa200]]}
  layer.3.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41e93a20], [0, 0x4743760], [0, 0x4217b140], [1, 0x62c0f40], [1, 0x421137c0], [2, 0x4a2ef60], [2, 0x41facf80], [3, 0x876d080], [3, 0x41e41e20], [4, 0xb50caa0], [4, 0x41d83740], [5, 0x5e46d20], [5, 0x41ed4a40], [0, 0x4784780], [0, 0x421bc160], [1, 0x6301f60], [1, 0x421547e0], [2, 0x4a6ff80], [2, 0x41fedfa0], [3, 0x87ae0a0], [3, 0x41e82e40], [4, 0xb54dac0], [4, 0x41dc4760], [5, 0x5e87d40], [5, 0x41f15a60], [0, 0x47c57a0], [0, 0x421fd180], [1, 0x6342f80], [1, 0x42195800], [2, 0x4ab0fa0], [2, 0x4202efc0], [3, 0x87ef0c0]]}
  layer.3.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8838320], [3, 0x41f466e0], [4, 0xb672340], [4, 0x41ee8fe0], [5, 0x5f4b5e0], [5, 0x41fdbba0], [0, 0x488b8e0], [0, 0x422c32c0]]}
  layer.3.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64090c0], [1, 0x421eee80], [2, 0x4b0a620], [2, 0x42078a60], [3, 0x8840540], [3, 0x41f4e900], [4, 0xb67a560], [4, 0x41ef1200], [5, 0x5f53800], [5, 0x41fe3dc0], [0, 0x4893b00], [0, 0x422cb4e0], [1, 0x648b0e0], [1, 0x42270ea0], [2, 0x4b8c640], [2, 0x420faa80]]}
  layer.3.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88c2560], [3, 0x41fd0920], [4, 0xb6fc580], [4, 0x41f73220], [5, 0x5fd5820], [5, 0x42065de0], [0, 0x4915b20], [0, 0x4234d500]]}
  layer.3.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fd78c0]]}
  layer.3.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42067e80]]}
  layer.4.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42028d20], [1, 0x61617e0], [1, 0x41fb4060], [2, 0x489ec00], [2, 0x41e1cc20], [3, 0x85dcd20], [3, 0x41cb1ac0], [4, 0xb3041a0]]}
  layer.4.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41f29de0], [2, 0x4814980], [2, 0x41d929a0], [3, 0x8552aa0]]}
  layer.4.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41bee220], [4, 0xb2ba700], [4, 0x41af0bc0], [5, 0x5bb1900], [5, 0x41c5fe20], [0, 0x456eac0], [0, 0x41f65cc0], [1, 0x609e780]]}
  layer.4.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41f2df00], [2, 0x4818aa0], [2, 0x41d96ac0], [3, 0x8556bc0]]}
  layer.4.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x45afae0], [0, 0x41fa6ce0], [1, 0x60df7a0], [1, 0x41f32020], [2, 0x481cbc0], [2, 0x41d9abe0], [3, 0x855ace0], [3, 0x41c2fa80]]}
  layer.4.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb2fbf60], [4, 0x41b52400], [5, 0x5c13140], [5, 0x41cc1660]]}
  layer.4.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x45f0b00], [0, 0x41fe7d00], [1, 0x61207c0], [1, 0x41f73040], [2, 0x485dbe0], [2, 0x41ddbc00], [3, 0x859bd00], [3, 0x41c70aa0]]}
  layer.4.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb300080], [4, 0x41b56520], [5, 0x5c17260], [5, 0x41cc5780]]}
  layer.4.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x61a2800]]}
  layer.4.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41ff5080]]}
  layer.4.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x48dfc20], [2, 0x41e5dc40], [3, 0x861dd40], [3, 0x41cf2ae0], [4, 0xb3451c0], [4, 0x41b5ae80], [5, 0x5c1e460], [5, 0x41ccc980], [0, 0x4635440], [0, 0x4206ce20], [1, 0x61b2c20], [1, 0x420054a0], [2, 0x4920c40], [2, 0x41e9ec60], [3, 0x865ed60], [3, 0x41d33b00], [4, 0xb3861e0], [4, 0x41b9bea0], [5, 0x5c5f480], [5, 0x41d0d9a0], [0, 0x4676460], [0, 0x420ade40], [1, 0x61f3c40], [1, 0x420464c0], [2, 0x4961c60], [2, 0x41edfc80], [3, 0x869fd80], [3, 0x41d74b20], [4, 0xb3c7200], [4, 0x41bdcec0], [5, 0x5ca04a0], [5, 0x41d4e9c0]]}
  layer.4.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x46b7480], [0, 0x420eee60], [1, 0x6234c60], [1, 0x420874e0], [2, 0x49a2c80], [2, 0x41f20ca0], [3, 0x86e0da0], [3, 0x41db5b40]]}
  layer.4.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb408220], [4, 0x41c1dee0], [5, 0x5ce14c0], [5, 0x41d8f9e0], [0, 0x46bf6a0], [0, 0x420f7080], [1, 0x623ce80], [1, 0x4208f700], [2, 0x49aaea0], [2, 0x41f28ec0], [3, 0x86e8fc0], [3, 0x41dbdd60], [4, 0xb48a240], [4, 0x41c9ff00], [5, 0x5d634e0], [5, 0x41e11a00]]}
  layer.4.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x47416c0], [0, 0x421790a0], [1, 0x62beea0], [1, 0x42111720], [2, 0x4a2cec0], [2, 0x41faaee0], [3, 0x876afe0], [3, 0x41e3fd80]]}
  layer.4.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x623f1c0]]}
  layer.4.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x422cf780]]}
  layer.5.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4beb760], [0, 0x42684120], [1, 0x683ebe0], [1, 0x425aaba0], [2, 0x4e6cd40], [2, 0x4237a1a0], [3, 0x8a86660], [3, 0x421ee840]]}
  layer.5.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb9887c0], [4, 0x421ff460], [5, 0x624f5e0], [5, 0x422dfba0]]}
  layer.5.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4c2c780], [0, 0x426c5140], [1, 0x687fc00], [1, 0x425ebbc0], [2, 0x4eadd60], [2, 0x423bb1c0], [3, 0x8ac7680], [3, 0x4222f860]]}
  layer.5.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb98c8e0], [4, 0x42203580], [5, 0x6253700], [5, 0x422e3cc0]]}
  layer.5.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61fe1a0], [5, 0x4228e760], [0, 0x4baa740], [0, 0x42643100], [1, 0x67fdbc0], [1, 0x42569b80], [2, 0x4e2bd20], [2, 0x42339180]]}
  layer.5.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb990a00], [4, 0x422076a0], [5, 0x6257820], [5, 0x422e7de0]]}
  layer.5.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4c6dfe0], [0, 0x427069a0], [1, 0x68e1440], [1, 0x4264d400], [2, 0x4f0f5a0], [2, 0x4241ca00], [3, 0x8b28ec0], [3, 0x422910a0]]}
  layer.5.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb994b20], [4, 0x4220b7c0], [5, 0x625b940], [5, 0x422ebf00]]}
  layer.5.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb998c40]]}
  layer.5.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5212fe0]]}
  layer.5.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8956aa0], [3, 0x42064e60], [4, 0xb79de00], [4, 0x42014aa0], [5, 0x60b4fe0], [5, 0x421455a0], [0, 0x4a61580], [0, 0x424f9f40], [1, 0x66b82e0], [1, 0x4249e0a0], [2, 0x4d40260], [2, 0x4224d6c0], [3, 0x8997ac0], [3, 0x420a5e80], [4, 0xb7dee20], [4, 0x42055ac0], [5, 0x60f6000], [5, 0x421865c0], [0, 0x4aa25a0], [0, 0x4253af60], [1, 0x66f9300], [1, 0x424df0c0], [2, 0x4d81280], [2, 0x4228e6e0], [3, 0x89d8ae0], [3, 0x420e6ea0], [4, 0xb81fe40], [4, 0x42096ae0], [5, 0x6137020], [5, 0x421c75e0], [0, 0x4ae35c0], [0, 0x4257bf80]]}
  layer.5.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4c6fe80], [2, 0x4217d2e0], [3, 0x88c76e0], [3, 0x41fd5aa0], [4, 0xb701700], [4, 0x41f783a0], [5, 0x5fe7ce0], [5, 0x420782a0]]}
  layer.5.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x491bce0], [0, 0x423536c0], [1, 0x6511a60], [1, 0x42358800], [2, 0x4c780a0], [2, 0x42185500], [3, 0x88cf900], [3, 0x41fddcc0], [4, 0xb709920], [4, 0x41f805c0], [5, 0x5feff00], [5, 0x420804c0], [0, 0x499dd00], [0, 0x423d56e0], [1, 0x6593a80], [1, 0x423da820]]}
  layer.5.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4cfa0c0], [2, 0x42207520], [3, 0x8951920], [3, 0x4205fce0], [4, 0xb78b940], [4, 0x420025e0], [5, 0x6071f20], [5, 0x421024e0]]}
  layer.5.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb78d9e0]]}
  layer.5.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42004680]]}
  layer.6.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6073fc0], [5, 0x42104580], [0, 0x4a20560], [0, 0x424b8f20], [1, 0x66772c0], [1, 0x4245d080], [2, 0x4cff240], [2, 0x4220c6a0]]}
  layer.6.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4917bc0], [0, 0x4234f5a0], [1, 0x650d940], [1, 0x423546e0]]}
  layer.6.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x673a320], [1, 0x425200e0], [2, 0x4dc22a0], [2, 0x422cf700], [3, 0x8a19b00], [3, 0x42127ec0], [4, 0xb860e60], [4, 0x420d7b00]]}
  layer.6.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6178040], [5, 0x42208600], [0, 0x4b245e0], [0, 0x425bcfa0]]}
  layer.6.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42168ee0], [4, 0xb8a1e80], [4, 0x42118b20], [5, 0x617c160], [5, 0x4220c720], [0, 0x4b28700], [0, 0x425c10c0], [1, 0x677bb80]]}
  layer.6.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42561940], [2, 0x4e23ae0], [2, 0x42330f40], [3, 0x8a7b340]]}
  layer.6.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x421a9f00], [4, 0xb8e2ea0], [4, 0x42159b40], [5, 0x61bd180], [5, 0x4224d740], [0, 0x4b69720], [0, 0x426020e0], [1, 0x67bcba0]]}
  layer.6.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42565a60], [2, 0x4e27c00], [2, 0x42335060], [3, 0x8a7f460]]}
  layer.6.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7742060]]}
  layer.6.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43519280]]}
  layer.6.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f1c320], [2, 0x434dec20], [3, 0x9c08820], [3, 0x433dc480], [4, 0xca8dac0], [4, 0x431fbe40], [5, 0x7241520], [5, 0x431271e0], [0, 0x5a7e860], [0, 0x4367eaa0], [1, 0x7752480], [1, 0x435296a0], [2, 0x5f5d340], [2, 0x4351fc40], [3, 0x9c49840], [3, 0x4341d4a0], [4, 0xcaceae0], [4, 0x4323ce60], [5, 0x7282540], [5, 0x43168200], [0, 0x5abf880], [0, 0x436bfac0], [1, 0x77934a0], [1, 0x4356a6c0], [2, 0x5f9e360], [2, 0x43560c60], [3, 0x9c8a860], [3, 0x4345e4c0], [4, 0xcb0fb00], [4, 0x4327de80], [5, 0x72c3560], [5, 0x431a9220]]}
  layer.6.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5b008a0], [0, 0x43700ae0], [1, 0x77d44c0], [1, 0x435ab6e0], [2, 0x5fdf380], [2, 0x435a1c80], [3, 0x9ccb880], [3, 0x4349f4e0]]}
  layer.6.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcb50b20], [4, 0x432beea0], [5, 0x7304580], [5, 0x431ea240], [0, 0x5b08ac0], [0, 0x43708d00], [1, 0x77dc6e0], [1, 0x435b3900], [2, 0x5fe75a0], [2, 0x435a9ea0], [3, 0x9cd3aa0], [3, 0x434a7700], [4, 0xcbd2b40], [4, 0x43340ec0], [5, 0x73865a0], [5, 0x4326c260]]}
  layer.6.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x43679920], [1, 0x773ffc0], [1, 0x435171e0], [2, 0x5f1a280], [2, 0x434dcb80], [3, 0x9c06780], [3, 0x433da3e0], [4, 0xca8ba20]]}
  layer.6.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60aa5e0]]}
  layer.6.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4366cee0]]}
  layer.7.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d96ae0], [3, 0x4356a740], [4, 0xcc553a0], [4, 0x43424700], [5, 0x7469de0], [5, 0x432eeac0], [0, 0x5bcebe0], [0, 0x437cee20]]}
  layer.7.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc9ed280], [4, 0x4312c260], [5, 0x716f0a0], [5, 0x43054d60]]}
  layer.7.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4338d800], [2, 0x5e0a6a0], [2, 0x433e5dc0], [3, 0x9b0f9c0], [3, 0x432d62e0], [4, 0xc92a220], [4, 0x43069200], [5, 0x70ac040]]}
  layer.7.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42fcec00], [0, 0x58c8b80], [0, 0x434cef40], [1, 0x75f65c0]]}
  layer.7.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc96b240], [4, 0x430aa220], [5, 0x70ed060], [5, 0x42fd2d20], [0, 0x58ccca0], [0, 0x434d3060], [1, 0x75fa6e0], [1, 0x433cf060]]}
  layer.7.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e4bf00], [2, 0x43447600], [3, 0x9b71200], [3, 0x43337b20]]}
  layer.7.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc9ac260], [4, 0x430eb240], [5, 0x712e080], [5, 0x43013d40], [0, 0x590dcc0], [0, 0x43514080], [1, 0x763b700], [1, 0x43410080]]}
  layer.7.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e50020], [2, 0x4344b720], [3, 0x9b75320], [3, 0x4333bc40]]}
  layer.7.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4333fd60]]}
  layer.7.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc9f13a0]]}
  layer.7.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x43130380], [5, 0x71731c0], [5, 0x43058e80], [0, 0x59b0500], [0, 0x435b68c0], [1, 0x767cf60], [1, 0x43454180], [2, 0x5e57220], [2, 0x43452920], [3, 0x9b7c520], [3, 0x43350180], [4, 0xca017c0], [4, 0x431713a0], [5, 0x71b41e0], [5, 0x43099ea0], [0, 0x59f1520], [0, 0x435f78e0], [1, 0x76bdf80], [1, 0x434951a0], [2, 0x5e98240], [2, 0x43493940], [3, 0x9bbd540], [3, 0x433911a0], [4, 0xca427e0], [4, 0x431b23c0], [5, 0x71f5200], [5, 0x430daec0], [0, 0x5a32540], [0, 0x43638900], [1, 0x76fefa0], [1, 0x434d61c0], [2, 0x5ed9260]]}
  layer.7.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x434d4960], [3, 0x9bfe560], [3, 0x433d21c0], [4, 0xca83800], [4, 0x431f33e0], [5, 0x7236220], [5, 0x4311bee0], [0, 0x5a73560]]}
  layer.7.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x74aae00], [5, 0x4332fae0], [0, 0x5c0fc00], [0, 0x4380fe40], [1, 0x78a3040], [1, 0x4367a260], [2, 0x611c220], [2, 0x436deb20], [3, 0x9ddabe0], [3, 0x435ae840], [4, 0xcc994a0], [4, 0x43468800], [5, 0x752ce20], [5, 0x433b1b00], [0, 0x5c91c20], [0, 0x43891e60]]}
  layer.7.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7925060], [1, 0x436fc280], [2, 0x619e240], [2, 0x43760b40], [3, 0x9e5cc00], [3, 0x43630860], [4, 0xcd1b4c0], [4, 0x434ea820]]}
  layer.7.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75aee40]]}
  layer.7.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x43433b20]]}
  layer.8.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5b8aae0], [0, 0x4378ad20], [1, 0x785e700], [1, 0x43635920], [2, 0x60695c0], [2, 0x4362bec0], [3, 0x9d55ac0], [3, 0x43529720]]}
  layer.8.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x94f3520], [3, 0x42cb75a0], [4, 0xc36d500], [4, 0x42b5d880]]}
  layer.8.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6ae3020], [5, 0x42a526c0], [0, 0x5337100], [0, 0x42eab8e0], [1, 0x7026c00], [1, 0x42db5c60], [2, 0x5774c40], [2, 0x42cef380]]}
  layer.8.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x94f7640], [3, 0x42cbb6c0], [4, 0xc371620], [4, 0x42b619a0]]}
  layer.8.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42df6c80], [2, 0x57b5c60], [2, 0x42d303a0], [3, 0x94fb760], [3, 0x42cbf7e0], [4, 0xc375740], [4, 0x42b65ac0], [5, 0x6b24880]]}
  layer.8.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42a93f20], [0, 0x5398940], [0, 0x42f0d120], [1, 0x7088440]]}
  layer.8.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42e37ca0], [2, 0x57f6c80], [2, 0x42d713c0], [3, 0x953c780], [3, 0x42d00800], [4, 0xc3b6760], [4, 0x42ba6ae0], [5, 0x6b658a0]]}
  layer.8.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42a98040], [0, 0x539ca60], [0, 0x42f11240], [1, 0x708c560]]}
  layer.8.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42a9c160]]}
  layer.8.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53a0b80]]}
  layer.8.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42f15360], [1, 0x7090680], [1, 0x42e890e0], [2, 0x58994c0], [2, 0x42e13c00], [3, 0x957dfe0], [3, 0x42d44900], [4, 0xc3fa860], [4, 0x42beabe0], [5, 0x6ba99a0], [5, 0x42aac580], [0, 0x53b0fa0], [0, 0x42f56380], [1, 0x70d16a0], [1, 0x42eca100], [2, 0x58da4e0], [2, 0x42e54c20], [3, 0x95bf000], [3, 0x42d85920], [4, 0xc43b880], [4, 0x42c2bc00], [5, 0x6bea9c0], [5, 0x42aed5a0], [0, 0x53f1fc0], [0, 0x42f973a0], [1, 0x71126c0], [1, 0x42f0b120], [2, 0x591b500], [2, 0x42e95c40], [3, 0x9600020], [3, 0x42dc6940], [4, 0xc47c8a0]]}
  layer.8.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42c6cc20], [5, 0x6c2b9e0], [5, 0x42b2e5c0], [0, 0x5432fe0], [0, 0x42fd83c0], [1, 0x71536e0], [1, 0x42f4c140], [2, 0x595c520]]}
  layer.8.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42ed6c60], [3, 0x9641040], [3, 0x42e07960], [4, 0xc4bd8c0], [4, 0x42c74e40], [5, 0x6c33c00], [5, 0x42b367e0], [0, 0x543b200], [0, 0x42fe05e0], [1, 0x715b900], [1, 0x42f54360], [2, 0x5964740], [2, 0x42f58c80], [3, 0x96c3060], [3, 0x42e89980], [4, 0xc53f8e0]]}
  layer.8.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5820160], [0, 0x43426520], [1, 0x7530c80], [1, 0x432c8700], [2, 0x5d455a0], [2, 0x4339aac0], [3, 0x9ac46c0], [3, 0x4328afe0]]}
  layer.8.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42e0e160]]}
  layer.8.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f89480]]}
  layer.9.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42cae2c0], [2, 0x560c2c0], [2, 0x42c030a0], [3, 0x9448a80], [3, 0x42c0cb00], [4, 0xc2c2a60], [4, 0x42a58fc0], [5, 0x697d780]]}
  layer.9.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42966c20], [0, 0x52ac640], [0, 0x42e1e580], [1, 0x6f998a0]]}
  layer.9.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42cef2e0], [2, 0x564d2e0], [2, 0x42c440c0], [3, 0x9489aa0], [3, 0x42c4db20], [4, 0xc303a80], [4, 0x42a99fe0], [5, 0x69be7a0]]}
  layer.9.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4296ad40], [0, 0x52b0760], [0, 0x42e226a0], [1, 0x6f9d9c0]]}
  layer.9.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42adb000], [5, 0x69ff7c0], [5, 0x4296ee60], [0, 0x52b4880], [0, 0x42e267c0], [1, 0x6fa1ae0], [1, 0x42d30b40], [2, 0x56efb20]]}
  layer.9.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42ce6900], [3, 0x94eb2e0], [3, 0x42caf360], [4, 0xc3652c0]]}
  layer.9.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42b1c020], [5, 0x6a407e0], [5, 0x429afe80], [0, 0x52f58a0], [0, 0x42e677e0], [1, 0x6fe2b00], [1, 0x42d71b60], [2, 0x5730b40]]}
  layer.9.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42ceaa20], [3, 0x94ef400], [3, 0x42cb3480], [4, 0xc3693e0]]}
  layer.9.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42e78cc0]]}
  layer.9.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42e06a60]]}
  layer.9.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x56d0e20], [0, 0x432d71e0], [1, 0x73e1940], [1, 0x431793c0], [2, 0x5b7dcc0], [2, 0x43172200], [3, 0x989be00], [3, 0x430c3700], [4, 0xc7bf7a0], [4, 0x42e94560], [5, 0x6f142a0], [5, 0x42e16e80], [0, 0x5711e40], [0, 0x43318200], [1, 0x7422960], [1, 0x431ba3e0], [2, 0x5bbece0], [2, 0x431b3220], [3, 0x98dce20], [3, 0x43104720], [4, 0xc8007c0], [4, 0x42ed5580], [5, 0x6f552c0], [5, 0x42e57ea0], [0, 0x5752e60], [0, 0x43359220], [1, 0x7463980], [1, 0x431fb400], [2, 0x5bffd00], [2, 0x431f4240], [3, 0x991de40], [3, 0x43145740]]}
  layer.9.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc8417e0], [4, 0x42f165a0], [5, 0x6f962e0], [5, 0x42e98ec0], [0, 0x5793e80], [0, 0x4339a240], [1, 0x74a49a0], [1, 0x4323c420]]}
  layer.9.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c40d20], [2, 0x43235260], [3, 0x995ee60], [3, 0x43186760], [4, 0xc849a00], [4, 0x42f1e7c0], [5, 0x6f9e500], [5, 0x42ea10e0], [0, 0x579c0a0], [0, 0x433a2460], [1, 0x74acbc0], [1, 0x43244640], [2, 0x5cc2d40], [2, 0x432b7280], [3, 0x99e0e80], [3, 0x43208780]]}
  layer.9.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc8cba20], [4, 0x42fa07e0], [5, 0x7020520], [5, 0x42f23100], [0, 0x581e0c0], [0, 0x43424480], [1, 0x752ebe0], [1, 0x432c6660]]}
  layer.9.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f03e80]]}
  layer.9.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc8d0ba0]]}
  layer.10.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42fa5960], [5, 0x70256a0], [5, 0x42f28280], [0, 0x5822200], [0, 0x434285c0], [1, 0x7532d20], [1, 0x432ca7a0], [2, 0x5d47640]]}
  layer.10.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4339cb60], [3, 0x9ac6760], [3, 0x4328d080], [4, 0xc8e0fc0]]}
  layer.10.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42fe6980], [5, 0x70666c0], [5, 0x42f692a0], [0, 0x5863220], [0, 0x434695e0], [1, 0x7573d40], [1, 0x4330b7c0], [2, 0x5d88660]]}
  layer.10.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x433a0c80], [3, 0x9aca880], [3, 0x432911a0], [4, 0xc8e50e0]]}
  layer.10.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75b4d60], [1, 0x4334c7e0], [2, 0x5dc9680], [2, 0x433a4da0], [3, 0x9ace9a0], [3, 0x432952c0], [4, 0xc8e9200], [4, 0x430281e0]]}
  layer.10.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a7f20], [5, 0x42fcaae0], [0, 0x58c4a60], [0, 0x434cae20]]}
  layer.10.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc7681a0], [4, 0x42e3cf60], [5, 0x6e7fda0], [5, 0x42d82980], [0, 0x564e5a0], [0, 0x431f3980], [1, 0x72fe0e0], [1, 0x430f6b40]]}
  layer.10.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x725f940], [1, 0x430583a0], [2, 0x5a68780], [2, 0x4305ccc0]]}
  layer.10.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7263a60]]}
  layer.10.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4305c4c0]]}
  layer.10.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a6c8a0], [2, 0x43060de0], [3, 0x97c78e0], [3, 0x42fef1e0], [4, 0xc6a5140], [4, 0x42d79f00], [5, 0x6dbcd40], [5, 0x42cbf920], [0, 0x55c4340], [0, 0x43169720], [1, 0x7273e80], [1, 0x4306c8e0], [2, 0x5aad8c0], [2, 0x430a1e00], [3, 0x9808900], [3, 0x43030200], [4, 0xc6e6160], [4, 0x42dbaf20], [5, 0x6dfdd60], [5, 0x42d00940], [0, 0x5605360], [0, 0x431aa740], [1, 0x72b4ea0], [1, 0x430ad900], [2, 0x5aee8e0], [2, 0x430e2e20], [3, 0x9849920], [3, 0x43071220], [4, 0xc727180], [4, 0x42dfbf40], [5, 0x6e3ed80], [5, 0x42d41960]]}
  layer.10.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5646380], [0, 0x431eb760], [1, 0x72f5ec0], [1, 0x430ee920], [2, 0x5b2f900], [2, 0x43123e40], [3, 0x988a940], [3, 0x430b2240]]}
  layer.10.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6cb5c20], [5, 0x42bb8800], [0, 0x54bd220], [0, 0x43062600], [1, 0x71dd920], [1, 0x42fd6380], [2, 0x59e6760], [2, 0x42fdaca0], [3, 0x9745080], [3, 0x42f0b9a0], [4, 0xc5c1900], [4, 0x42cf76a0], [5, 0x6d37c40], [5, 0x42c3a820], [0, 0x553f240], [0, 0x430e4620]]}
  layer.10.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5b37b20], [2, 0x4312c060], [3, 0x9892b60], [3, 0x430ba460], [4, 0xc7a91c0], [4, 0x42e7df80], [5, 0x6ec0dc0], [5, 0x42dc39a0]]}
  layer.10.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc7ab260]]}
  layer.10.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42e80020]]}
  layer.11.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6ec2e60], [5, 0x42dc5a40], [0, 0x568fe00], [0, 0x432961c0], [1, 0x73a0920], [1, 0x431383a0], [2, 0x5b3cca0], [2, 0x431311e0]]}
  layer.11.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9897ce0], [3, 0x430bf5e0], [4, 0xc7bb680], [4, 0x42e90440]]}
  layer.11.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4220f8e0], [5, 0x625fa60], [5, 0x422f0020], [0, 0x4caf840], [0, 0x427a91e0], [1, 0x6983c80], [1, 0x4268ec60], [2, 0x4f536a0]]}
  layer.11.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x48eee20], [5, 0x40879b80], [0, 0x3052be0], [0, 0x408a9760]]}
  layer.11.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40805700], [4, 0xa0122c0], [4, 0x4079e5a0], [5, 0x48f2f40], [5, 0x4087dca0], [0, 0x3056d00], [0, 0x408ad880], [1, 0x49ab4c0]]}
  layer.11.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40908f00], [2, 0x32183a0], [2, 0x408c0d40], [3, 0x705a4c0]]}
  layer.11.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40846720], [4, 0xa0532e0], [4, 0x407df5c0], [5, 0x4933f60], [5, 0x408becc0], [0, 0x3097d20], [0, 0x408ee8a0], [1, 0x49ec4e0]]}
  layer.11.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4090d020], [2, 0x321c4c0], [2, 0x408c4e60], [3, 0x705e5e0]]}
  layer.11.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4094afa0]]}
  layer.11.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x70e4720]]}
  layer.11.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40909fa0], [4, 0xa177b40], [4, 0x40903e20], [5, 0x49f77e0], [5, 0x40a03d20], [0, 0x31dcd80], [0, 0x40a33900], [1, 0x4b31540], [1, 0x40996240], [2, 0x32a56e0], [2, 0x4095b3c0], [3, 0x70f4b40], [3, 0x4094afc0], [4, 0xa1b8b60], [4, 0x40944e40], [5, 0x4a38800], [5, 0x40a44d40], [0, 0x321dda0], [0, 0x40a74920], [1, 0x4b72560], [1, 0x409d7260], [2, 0x32e6700], [2, 0x4099c3e0], [3, 0x7135b60], [3, 0x4098bfe0], [4, 0xa1f9b80], [4, 0x40985e60], [5, 0x4a79820], [5, 0x40a85d60], [0, 0x325edc0], [0, 0x40ab5940], [1, 0x4bb3580]]}
  layer.11.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40a18280], [2, 0x3327720], [2, 0x409dd400], [3, 0x7176b80], [3, 0x409cd000], [4, 0xa23aba0], [4, 0x409c6e80], [5, 0x4aba840]]}
  layer.11.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40ac6d80], [0, 0x329fde0], [0, 0x40af6960], [1, 0x4bf45a0], [1, 0x40a204a0], [2, 0x332f940], [2, 0x409e5620], [3, 0x717eda0], [3, 0x409d5220], [4, 0xa242dc0], [4, 0x409cf0a0], [5, 0x4ac2a60], [5, 0x40b48da0], [0, 0x3321e00], [0, 0x40b78980], [1, 0x4c765c0]]}
  layer.11.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40aa24c0], [2, 0x33b1960], [2, 0x40a67640], [3, 0x7200dc0], [3, 0x40a57240], [4, 0xa2c4de0], [4, 0x40a510c0], [5, 0x4b44a80]]}
  layer.11.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40781d80]]}
  layer.11.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x48b2e60]]}
  layer.12.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x407a6680], [2, 0x30a2e80], [2, 0x4072b840], [3, 0x6e8a960], [3, 0x405f53e0], [4, 0x9de68e0], [4, 0x405d3ba0], [5, 0x4724c60]]}
  layer.12.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x407297c0], [0, 0x2f3b620], [0, 0x407921a0], [1, 0x48c3280]]}
  layer.12.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x407e76a0], [2, 0x30e3ea0], [2, 0x4076c860], [3, 0x6ecb980], [3, 0x40636400], [4, 0x9e27900], [4, 0x40614bc0], [5, 0x4765c80]]}
  layer.12.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4072d8e0], [0, 0x2f3f740], [0, 0x407962c0], [1, 0x48c73a0]]}
  layer.12.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9e68920], [4, 0x40655be0], [5, 0x47a6ca0], [5, 0x40731a00], [0, 0x2f43860], [0, 0x4079a3e0], [1, 0x48cb4c0], [1, 0x40828f00]]}
  layer.12.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x405f12c0], [4, 0x9de27c0], [4, 0x405cfa80], [5, 0x4720b40]]}
  layer.12.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6f2d1c0], [3, 0x40697c40], [4, 0x9ea9940], [4, 0x40696c00], [5, 0x47e7cc0], [5, 0x40772a20], [0, 0x2f84880], [0, 0x407db400]]}
  layer.12.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x490c4e0], [1, 0x40869f20], [2, 0x31287e0], [2, 0x407d1180]]}
  layer.12.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4910600]]}
  layer.12.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4086e040]]}
  layer.12.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x312c900], [2, 0x407d52a0], [3, 0x6f6ea20], [3, 0x4073a480], [4, 0x9f4c180], [4, 0x406d8460], [5, 0x482bdc0], [5, 0x407b6b20], [0, 0x2fc8980], [0, 0x4081f500], [1, 0x4920a20], [1, 0x4087e460], [2, 0x316d920], [2, 0x408162c0], [3, 0x6fafa40], [3, 0x4077b4a0], [4, 0x9f8d1a0], [4, 0x40719480], [5, 0x486cde0], [5, 0x407f7b40], [0, 0x30099a0], [0, 0x40860520], [1, 0x4961a40], [1, 0x408bf480], [2, 0x31ae940], [2, 0x408572e0], [3, 0x6ff0a60], [3, 0x407bc4c0], [4, 0x9fce1c0], [4, 0x4075a4a0], [5, 0x48ade00], [5, 0x40838b60]]}
  layer.12.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304a9c0], [0, 0x408a1540], [1, 0x49a2a60], [1, 0x409004a0], [2, 0x31ef960], [2, 0x40898300], [3, 0x7031a80], [3, 0x407fd4e0]]}
  layer.12.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x408ffce0], [0, 0x30d8d40], [0, 0x4092f8c0], [1, 0x4a2d500], [1, 0x40911140], [2, 0x32205e0], [2, 0x408c8f80], [3, 0x7062700], [3, 0x40887f80], [4, 0xa0f5b20], [4, 0x40881e00], [5, 0x49757c0], [5, 0x40981d00], [0, 0x315ad60], [0, 0x409b18e0], [1, 0x4aaf520]]}
  layer.12.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40c40c20], [3, 0x73da3a0], [3, 0x40bc3d60], [4, 0xa431900], [4, 0x40bfbb20], [5, 0x4cef4e0], [5, 0x40dde1e0], [0, 0x35e7620]]}
  layer.12.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40bfdbc0]]}
  layer.12.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4cf1580]]}
  layer.13.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40de0280], [0, 0x35e96c0], [0, 0x40ecac60], [1, 0x5029880], [1, 0x40dd80a0], [2, 0x3645d80], [2, 0x40c45da0], [3, 0x73df520]]}
  layer.13.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40bc8ee0], [4, 0xa436a80], [4, 0x40c0dfe0], [5, 0x4d019a0]]}
  layer.13.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40e212a0], [0, 0x362a6e0], [0, 0x40f0bc80], [1, 0x506a8a0], [1, 0x40e190c0], [2, 0x3686da0], [2, 0x40c86dc0], [3, 0x7420540]]}
  layer.13.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40ec6300], [1, 0x4fc3f40], [1, 0x40d72760], [2, 0x3641420]]}
  layer.13.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa47bbc0], [4, 0x40c53120], [5, 0x4d46ae0], [5, 0x40ea32e0], [0, 0x36ac720], [0, 0x40f8dcc0], [1, 0x50ec8e0], [1, 0x40e5a920]]}
  layer.13.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x36c8600], [2, 0x40ce8600], [3, 0x7481d80], [3, 0x40c2e840]]}
  layer.13.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa4bcbe0], [4, 0x40c94140], [5, 0x4d87b00], [5, 0x40ee4300], [0, 0x36ed740], [0, 0x40fcece0], [1, 0x512d900], [1, 0x40e9b940]]}
  layer.13.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x36cc720], [2, 0x40cec720], [3, 0x7485ea0], [3, 0x40c32960]]}
  layer.13.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40a592e0]]}
  layer.13.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2c6e80]]}
  layer.13.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40a53160], [5, 0x4b46b20], [5, 0x40bcb600], [0, 0x3405640], [0, 0x40c5c1c0], [1, 0x4cf8e20], [1, 0x40aa7640], [2, 0x33b6ae0], [2, 0x40a6c7c0], [3, 0x7205f40], [3, 0x40a69700], [4, 0xa2d72a0], [4, 0x40a94180], [5, 0x4b87b40], [5, 0x40c0c620], [0, 0x3446660], [0, 0x40c9d1e0], [1, 0x4d39e40], [1, 0x40ae8660], [2, 0x33f7b00], [2, 0x40aad7e0], [3, 0x7246f60], [3, 0x40aaa720], [4, 0xa3182c0], [4, 0x40ad51a0], [5, 0x4bc8b60], [5, 0x40c4d640], [0, 0x3487680], [0, 0x40cde200], [1, 0x4d7ae60], [1, 0x40b29680], [2, 0x3438b20]]}
  layer.13.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40aee800], [3, 0x7287f80], [3, 0x40aeb740], [4, 0xa3592e0], [4, 0x40b161c0], [5, 0x4c09b80], [5, 0x40c8e660], [0, 0x34c86a0]]}
  layer.13.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40d1f220], [1, 0x4dbbe80], [1, 0x40b6a6a0], [2, 0x3479b40], [2, 0x40af6a20], [3, 0x72901a0], [3, 0x40af3960], [4, 0xa361500], [4, 0x40b1e3e0], [5, 0x4c11da0], [5, 0x40c96880], [0, 0x34d08c0], [0, 0x40da1240], [1, 0x4e3dea0], [1, 0x40bec6c0], [2, 0x34fbb60]]}
  layer.13.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40b78a40], [3, 0x73121c0], [3, 0x40b75980], [4, 0xa3e3520], [4, 0x40ba0400], [5, 0x4c93dc0], [5, 0x40d188a0], [0, 0x35528e0]]}
  layer.13.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40ba24a0]]}
  layer.13.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4c95e60]]}
  layer.14.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d1a940], [0, 0x3554980], [0, 0x40e23aa0], [1, 0x4f216e0], [1, 0x40ccff00], [2, 0x357e3c0], [2, 0x40b7dbc0], [3, 0x7317340]]}
  layer.14.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40b7ab00], [4, 0xa3e86a0], [4, 0x40bb28c0], [5, 0x4ca6280]]}
  layer.14.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d5b960], [0, 0x35959a0], [0, 0x40e64ac0], [1, 0x4f62700], [1, 0x40d10f20], [2, 0x35bf3e0], [2, 0x40bbebe0], [3, 0x7358360]]}
  layer.14.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40b7ec20], [4, 0xa3ec7c0], [4, 0x40bb69e0], [5, 0x4caa3a0]]}
  layer.14.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3600400], [2, 0x40bffc00], [3, 0x7399380], [3, 0x40b82d40], [4, 0xa3f08e0], [4, 0x40bbab00], [5, 0x4cae4c0], [5, 0x40d9d1c0]]}
  layer.14.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40453bc0], [4, 0x9c450c0], [4, 0x40425040], [5, 0x4576100]]}
  layer.14.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40bcd000], [4, 0xa43aba0], [4, 0x40c12100], [5, 0x4d05ac0], [5, 0x40e622c0], [0, 0x366b700], [0, 0x40f4cca0], [1, 0x50ab8c0]]}
  layer.14.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401d7540], [4, 0x9977640], [4, 0x4012a2a0], [5, 0x421a380]]}
  layer.14.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401db660]]}
  layer.14.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x997b760]]}
  layer.14.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4012e3c0], [5, 0x421e4a0], [5, 0x4017a680], [0, 0x2abbfa0], [0, 0x402764a0], [1, 0x43465a0], [1, 0x402f12e0], [2, 0x2bd13e0], [2, 0x402f3b80], [3, 0x6a52ca0], [3, 0x401eba80], [4, 0x998bb80], [4, 0x4016f3e0], [5, 0x425f4c0], [5, 0x401bb6a0], [0, 0x2afcfc0], [0, 0x402b74c0], [1, 0x43875c0], [1, 0x40332300], [2, 0x2c12400], [2, 0x40334ba0], [3, 0x6a93cc0], [3, 0x4022caa0], [4, 0x99ccba0], [4, 0x401b0400], [5, 0x42a04e0], [5, 0x401fc6c0], [0, 0x2b3dfe0], [0, 0x402f84e0], [1, 0x43c85e0], [1, 0x40373320], [2, 0x2c53420]]}
  layer.14.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40375bc0], [3, 0x6ad4ce0], [3, 0x4026dac0], [4, 0x9a0dbc0], [4, 0x401f1420], [5, 0x42e1500], [5, 0x4023d6e0], [0, 0x2b7f000]]}
  layer.14.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40339500], [1, 0x4409600], [1, 0x403b4340], [2, 0x2c94440], [2, 0x4037dde0], [3, 0x6adcf00], [3, 0x40275ce0], [4, 0x9a15de0], [4, 0x401f9640], [5, 0x42e9720], [5, 0x40245900], [0, 0x2b87220], [0, 0x403bb520], [1, 0x448b620], [1, 0x40436360], [2, 0x2d16460]]}
  layer.14.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40177da0], [0, 0x2a586e0], [0, 0x40212be0], [1, 0x4343cc0], [1, 0x402ec160], [2, 0x2bcc260], [2, 0x402eea00], [3, 0x6a4db20]]}
  layer.14.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40403f20]]}
  layer.14.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6b63040]]}
  layer.15.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x402fbe20], [4, 0x9a9bf20], [4, 0x4027bea0], [5, 0x43ccf60], [5, 0x40329140], [0, 0x2c09a80], [0, 0x40440620], [1, 0x4510720]]}
  layer.15.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404bb460], [2, 0x2d9b560], [2, 0x40414340], [3, 0x6b73460]]}
  layer.15.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40000000], [1, 0x40d0100], [1, 0x40000000], [2, 0x28e0100], [2, 0x40000000], [3, 0x67c0100], [3, 0x40000000], [4, 0x97a0100]]}
  layer.15.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x403ffe00], [3, 0x6b5ef20], [3, 0x402f7d00], [4, 0x9a97e00]]}
  layer.15.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40041020], [1, 0x4111120], [1, 0x40041020], [2, 0x2921120], [2, 0x40041020], [3, 0x6801120], [3, 0x40041020], [4, 0x97e1120]]}
  layer.15.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40000840], [5, 0x40f0920], [5, 0x40020820], [0, 0x2901160]]}
  layer.15.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40082040], [1, 0x4152140], [1, 0x40082040], [2, 0x2962140], [2, 0x40082040], [3, 0x6842140], [3, 0x40082040], [4, 0x9822140]]}
  layer.15.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40004960], [5, 0x40f4a40], [5, 0x40024940], [0, 0x2905280]]}
  layer.15.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40008a80]]}
  layer.15.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40f8b60]]}
  layer.15.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40028a60], [0, 0x29093a0], [0, 0x400c38a0], [1, 0x41f4980], [1, 0x40124880], [2, 0x29a39a0], [2, 0x400c6140], [3, 0x6886240], [3, 0x400c6140], [4, 0x9866240], [4, 0x40018ea0], [5, 0x4108f80], [5, 0x40069a80], [0, 0x294a3c0], [0, 0x401048c0], [1, 0x42359a0], [1, 0x401658a0], [2, 0x29e49c0], [2, 0x40107160], [3, 0x68c7260], [3, 0x40107160], [4, 0x98a7260], [4, 0x40059ec0], [5, 0x4149fa0], [5, 0x400aaaa0], [0, 0x298b3e0], [0, 0x401458e0], [1, 0x42769c0], [1, 0x401a68c0], [2, 0x2a259e0], [2, 0x40148180], [3, 0x6908280]]}
  layer.15.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40148180], [4, 0x98e8280], [4, 0x4009aee0], [5, 0x418afc0], [5, 0x400ebac0], [0, 0x29cc400], [0, 0x40186900], [1, 0x42b79e0]]}
  layer.15.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x401e78e0], [2, 0x2a66a00], [2, 0x401891a0], [3, 0x69492a0], [3, 0x401503a0], [4, 0x98f04a0], [4, 0x400a3100], [5, 0x41931e0], [5, 0x400f3ce0], [0, 0x29d4620], [0, 0x4018eb20], [1, 0x42bfc00], [1, 0x40269900], [2, 0x2ae8a20], [2, 0x4020b1c0], [3, 0x69cb2c0]]}
  layer.15.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401d23c0], [4, 0x99724c0], [4, 0x40125120], [5, 0x4215200], [5, 0x40175d00], [0, 0x2a56640], [0, 0x40210b40], [1, 0x4341c20]]}
  layer.15.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2ddae80]]}
  layer.15.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4033ce40]]}
  layer.16.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x404d4b80], [0, 0x2d544e0], [0, 0x4058d920], [1, 0x465da20], [1, 0x404d7440], [2, 0x2dd7520], [2, 0x4045d640], [3, 0x6bbc760]]}
  layer.16.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40457ce0], [4, 0x9c491e0], [4, 0x40429160], [5, 0x457a220]]}
  layer.16.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40515ba0], [0, 0x2d95500], [0, 0x405ce940], [1, 0x469ea40], [1, 0x40518460], [2, 0x2e18540], [2, 0x4049e660], [3, 0x6bfd780]]}
  layer.16.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4045be00], [4, 0x9c4d300], [4, 0x4042d280], [5, 0x457e340]]}
  layer.16.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2e59560], [2, 0x404df680], [3, 0x6c3e7a0], [3, 0x4045ff20], [4, 0x9c51420], [4, 0x404313a0], [5, 0x4582460], [5, 0x40557400]]}
  layer.16.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2dd6d60], [0, 0x40630180], [1, 0x4700280], [1, 0x40579ca0]]}
  layer.16.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2e9a580], [2, 0x405206a0], [3, 0x6c7f7c0], [3, 0x404a0f40], [4, 0x9c92440], [4, 0x404723c0], [5, 0x45c3480], [5, 0x40598420]]}
  layer.16.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404d3320], [2, 0x2dd3400], [2, 0x40459520], [3, 0x6bb8640]]}
  layer.16.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x404b33e0]]}
  layer.16.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46044a0]]}
  layer.16.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x405d9440], [0, 0x2deb2a0], [0, 0x40634ae0], [1, 0x4765bc0], [1, 0x405df5e0], [2, 0x2edbde0], [2, 0x405647a0], [3, 0x6cc38c0], [3, 0x404e5040], [4, 0x9cd6540], [4, 0x404c3800], [5, 0x46148c0], [5, 0x4061a460], [0, 0x2e2c2c0], [0, 0x40675b00], [1, 0x47a6be0], [1, 0x40620600], [2, 0x2f1ce00], [2, 0x405a57c0], [3, 0x6d048e0], [3, 0x40526060], [4, 0x9d17560], [4, 0x40504820], [5, 0x46558e0], [5, 0x4065b480], [0, 0x2e6d2e0], [0, 0x406b6b20], [1, 0x47e7c00], [1, 0x40661620], [2, 0x2f5de20], [2, 0x405e67e0], [3, 0x6d45900]]}
  layer.16.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40567080], [4, 0x9d58580], [4, 0x40545840], [5, 0x4696900], [5, 0x4069c4a0], [0, 0x2eae300], [0, 0x406f7b40], [1, 0x4828c20]]}
  layer.16.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x406a2640], [2, 0x2f9ee40], [2, 0x40627800], [3, 0x6d86920], [3, 0x4056f2a0], [4, 0x9d607a0], [4, 0x4054da60], [5, 0x469eb20], [5, 0x406a46c0], [0, 0x2eb6520], [0, 0x406ffd60], [1, 0x4830e40], [1, 0x40724660], [2, 0x3020e60], [2, 0x406a9820], [3, 0x6e08940]]}
  layer.16.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x403cf2a0], [4, 0x9b5f7c0], [4, 0x4033f740], [5, 0x44f17e0], [5, 0x4044d9c0], [0, 0x2ccd320], [0, 0x40506760], [1, 0x45d6860]]}
  layer.16.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40418460]]}
  layer.16.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6b77580]]}
  layer.17.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4034d260], [4, 0x9add780], [4, 0x402bd700], [5, 0x446f7a0], [5, 0x403cb980], [0, 0x2c4b2e0], [0, 0x40484720], [1, 0x4554820]]}
  layer.17.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404c2660], [2, 0x2da2760], [2, 0x40428880], [3, 0x6b879a0]]}
  layer.17.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4038e280], [4, 0x9b1e7a0], [4, 0x402fe720], [5, 0x44b07c0], [5, 0x4040c9a0], [0, 0x2c8c300], [0, 0x404c5740], [1, 0x4595840]]}
  layer.17.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404c6780], [2, 0x2da6880], [2, 0x4042c9a0], [3, 0x6b8bac0]]}
  layer.17.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x403d1340], [4, 0x9b61860], [4, 0x403417e0], [5, 0x44f3880], [5, 0x4044fa60], [0, 0x2ccf3c0], [0, 0x40508800], [1, 0x45d8900]]}
  layer.17.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404cb0e0], [2, 0x2dcb1c0], [2, 0x404512e0], [3, 0x6bb0400]]}
  layer.17.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40412360], [4, 0x9ba2880], [4, 0x40382800], [5, 0x45348a0], [5, 0x40490a80], [0, 0x2d103e0], [0, 0x40549820], [1, 0x4619920]]}
  layer.17.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404cf200], [2, 0x2dcf2e0], [2, 0x40455400], [3, 0x6bb4520]]}
  layer.17.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41b3e600]]}
  layer.17.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x35d7200]]}
  layer.17.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x416da660], [0, 0x3fb8700], [0, 0x418e4700], [1, 0x5af9800], [1, 0x41873320], [2, 0x40f1400], [2, 0x4164f440], [3, 0x7ddf980], [3, 0x415925c0], [4, 0xacab580], [4, 0x4150a440], [5, 0x55afac0], [5, 0x4171b680], [0, 0x3ff9720], [0, 0x41925720], [1, 0x5b3a820], [1, 0x418b4340], [2, 0x4132420], [2, 0x41690460], [3, 0x7e209a0], [3, 0x415d35e0], [4, 0xacec5a0], [4, 0x4154b460], [5, 0x55f0ae0], [5, 0x4175c6a0], [0, 0x403a740], [0, 0x41966740], [1, 0x5b7b840], [1, 0x418f5360], [2, 0x4173440], [2, 0x416d1480], [3, 0x7e619c0]]}
  layer.17.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41614600], [4, 0xad2d5c0], [4, 0x4158c480], [5, 0x5631b00], [5, 0x4179d6c0], [0, 0x407b760], [0, 0x419a7760], [1, 0x5bbc860]]}
  layer.17.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41936380], [2, 0x41b4460], [2, 0x417124a0], [3, 0x7ea29e0], [3, 0x4161c820], [4, 0xad357e0], [4, 0x415946a0], [5, 0x5639d20], [5, 0x417a58e0], [0, 0x4083980], [0, 0x419af980], [1, 0x5bc4a80], [1, 0x419b83a0], [2, 0x4236480], [2, 0x417944c0], [3, 0x7f24a00]]}
  layer.17.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4169e840], [4, 0xadb7800], [4, 0x416166c0], [5, 0x56bbd40], [5, 0x41827900], [0, 0x41059a0], [0, 0x41a319a0], [1, 0x5c46aa0]]}
  layer.17.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x418299a0]]}
  layer.17.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7dcf560]]}
  layer.18.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41a33a40], [1, 0x5c48b40], [1, 0x41a3ac00], [2, 0x4319cc0], [2, 0x41877d00], [3, 0x7fa7260], [3, 0x416a39c0], [4, 0xadbc980]]}
  layer.18.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4161b840], [5, 0x56c0ec0], [5, 0x41839dc0], [0, 0x4108280]]}
  layer.18.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41a74a60], [1, 0x5c89b60], [1, 0x41a7bc20], [2, 0x435ace0], [2, 0x418b8d20], [3, 0x7fe8280], [3, 0x416e49e0], [4, 0xadfd9a0]]}
  layer.18.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4161f960], [5, 0x56c4fe0], [5, 0x4183dee0], [0, 0x410c3a0]]}
  layer.18.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80292a0], [3, 0x41725a00], [4, 0xae3e9c0], [4, 0x41623a80], [5, 0x56c9100], [5, 0x41842000], [0, 0x41104c0], [0, 0x41ab62c0]]}
  layer.18.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ccb3c0], [1, 0x41add460], [2, 0x43bc520], [2, 0x4191a560]]}
  layer.18.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x806a2c0], [3, 0x41766a20], [4, 0xae7f9e0], [4, 0x41664aa0], [5, 0x570a120], [5, 0x41883020], [0, 0x41514e0], [0, 0x41af72e0]]}
  layer.18.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40d4c80], [2, 0x415c8aa0], [3, 0x7d49400], [3, 0x41508340]]}
  layer.18.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x540a9e0]]}
  layer.18.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41536e00]]}
  layer.18.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3de42a0], [0, 0x4178a0a0], [1, 0x599f1a0], [1, 0x4170b980], [2, 0x3f0dbe0], [2, 0x41401a00], [3, 0x7b82360], [3, 0x413412a0], [4, 0xaab58e0], [4, 0x413147a0], [5, 0x541ae00], [5, 0x41547220], [0, 0x3e252c0], [0, 0x417cb0c0], [1, 0x59e01c0], [1, 0x4174c9a0], [2, 0x3f4ec00], [2, 0x41442a20], [3, 0x7bc3380], [3, 0x413822c0], [4, 0xaaf6900], [4, 0x413557c0], [5, 0x545be20], [5, 0x41588240], [0, 0x3e662e0], [0, 0x4180c0e0], [1, 0x5a211e0], [1, 0x4178d9c0], [2, 0x3f8fc20], [2, 0x41483a40], [3, 0x7c043a0], [3, 0x413c32e0]]}
  layer.18.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xab37920], [4, 0x413967e0], [5, 0x549ce40], [5, 0x415c9260], [0, 0x3ea7300], [0, 0x4184d100], [1, 0x5a62200], [1, 0x417ce9e0]]}
  layer.18.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fd0c40], [2, 0x414c4a60], [3, 0x7c453c0], [3, 0x41404300], [4, 0xab3fb40], [4, 0x4139ea00], [5, 0x54a5060], [5, 0x415d1480], [0, 0x3eaf520], [0, 0x41855320], [1, 0x5a6a420], [1, 0x417d6c00], [2, 0x4052c60], [2, 0x41546a80], [3, 0x7cc73e0], [3, 0x41486320]]}
  layer.18.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xabc1b60], [4, 0x41420a20], [5, 0x5527080], [5, 0x416534a0], [0, 0x3f31540], [0, 0x418d7340], [1, 0x5aec440], [1, 0x41858c20]]}
  layer.18.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4185acc0]]}
  layer.18.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40d8da0]]}
  layer.19.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x415ccbc0], [3, 0x7d4d520], [3, 0x4150c460], [4, 0xac25420], [4, 0x414842e0], [5, 0x5529960], [5, 0x41658620], [0, 0x3f366c0]]}
  layer.19.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x418dc4c0], [1, 0x5af15c0], [1, 0x4186b0e0], [2, 0x40e91c0]]}
  layer.19.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4160dbe0], [3, 0x7d8e540], [3, 0x4154d480], [4, 0xac66440], [4, 0x414c5300], [5, 0x556a980], [5, 0x41699640], [0, 0x3f776e0]]}
  layer.19.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x418e05e0], [1, 0x5af56e0], [1, 0x4186f200], [2, 0x40ed2e0]]}
  layer.19.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e6b3a0], [1, 0x41cf7240], [2, 0x4569840], [2, 0x41ae7860], [3, 0x82a50c0], [3, 0x419a1820], [4, 0xb1241e0], [4, 0x4195a6a0]]}
  layer.19.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a87ea0], [5, 0x41b363c0], [0, 0x4407120], [0, 0x41dacf20]]}
  layer.19.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5eac3c0], [1, 0x41d38260], [2, 0x45aa860], [2, 0x41b28880], [3, 0x82e60e0], [3, 0x419e2840], [4, 0xb165200], [4, 0x4199b6c0]]}
  layer.19.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a8bfc0], [5, 0x41b3a4e0], [0, 0x440b240], [0, 0x41db1040]]}
  layer.19.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a900e0]]}
  layer.19.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a77a80]]}
  layer.19.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x440f360], [0, 0x41db5160], [1, 0x5eedc20], [1, 0x41ddaaa0], [2, 0x464d0a0], [2, 0x41b6a0e0], [3, 0x832a1e0], [3, 0x41a26940], [4, 0xb1a9300], [4, 0x419df7c0], [5, 0x5aa0500], [5, 0x41b4ea20], [0, 0x4450380], [0, 0x41df6180], [1, 0x5f2ec40], [1, 0x41e1bac0], [2, 0x468e0c0], [2, 0x41bab100], [3, 0x836b200], [3, 0x41a67960], [4, 0xb1ea320], [4, 0x41a207e0], [5, 0x5ae1520], [5, 0x41b8fa40], [0, 0x44913a0], [0, 0x41e371a0], [1, 0x5f6fc60], [1, 0x41e5cae0], [2, 0x46cf0e0], [2, 0x41bec120], [3, 0x83ac220], [3, 0x41aa8980]]}
  layer.19.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb22b340], [4, 0x41a61800], [5, 0x5b22540], [5, 0x41bd0a60], [0, 0x44d23c0], [0, 0x41e781c0], [1, 0x5fb0c80], [1, 0x41e9db00]]}
  layer.19.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4710100], [2, 0x41c2d140], [3, 0x83ed240], [3, 0x41ae99a0], [4, 0xb233560], [4, 0x41a69a20], [5, 0x5b2a760], [5, 0x41bd8c80], [0, 0x44da5e0], [0, 0x41e803e0], [1, 0x5fb8ea0], [1, 0x41ea5d20], [2, 0x4792120], [2, 0x41caf160], [3, 0x846f260], [3, 0x41b6b9c0]]}
  layer.19.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb2b5580], [4, 0x41aeba40], [5, 0x5bac780], [5, 0x41c5aca0], [0, 0x455c600], [0, 0x41f02400], [1, 0x603aec0], [1, 0x41f27d40]]}
  layer.19.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x455e6a0]]}
  layer.19.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xafc4a80]]}
  layer.20.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41ae1580], [2, 0x43c0640], [2, 0x4191e680], [3, 0x80ab2e0], [3, 0x417a7a40], [4, 0xaec0a00], [4, 0x416a5ac0], [5, 0x574b140]]}
  layer.20.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x418c4040], [0, 0x4192500], [0, 0x41b38300], [1, 0x5ccfd20]]}
  layer.20.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41b225a0], [2, 0x4401660], [2, 0x4195f6a0], [3, 0x80ec300], [3, 0x417e8a60], [4, 0xaf01a20], [4, 0x416e6ae0], [5, 0x578c160]]}
  layer.20.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x418c8160], [0, 0x4196620], [0, 0x41b3c420], [1, 0x5cd3e40]]}
  layer.20.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaf42a40], [4, 0x41727b00], [5, 0x57cd180], [5, 0x418cc280], [0, 0x419a740], [0, 0x41b40540], [1, 0x5cd7f60], [1, 0x41b63e00]]}
  layer.20.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4442ec0], [2, 0x419c0ee0], [3, 0x814db40], [3, 0x4184a2a0]]}
  layer.20.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaf83a60], [4, 0x41768b20], [5, 0x580e1a0], [5, 0x4190d2a0], [0, 0x41db760], [0, 0x41b81560], [1, 0x5d18f80], [1, 0x41ba4e20]]}
  layer.20.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4446fe0], [2, 0x419c5000], [3, 0x8151c60], [3, 0x4184e3c0]]}
  layer.20.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x444b100]]}
  layer.20.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x419c9120]]}
  layer.20.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8155d80], [3, 0x418524e0], [4, 0xafd4ea0], [4, 0x4180b360], [5, 0x58b09e0], [5, 0x4194eb00], [0, 0x421f860], [0, 0x41bc5660], [1, 0x5d5d080], [1, 0x41be8f20], [2, 0x445b520], [2, 0x419d9540], [3, 0x8196da0], [3, 0x41893500], [4, 0xb015ec0], [4, 0x4184c380], [5, 0x58f1a00], [5, 0x4198fb20], [0, 0x4260880], [0, 0x41c06680], [1, 0x5d9e0a0], [1, 0x41c29f40], [2, 0x449c540], [2, 0x41a1a560], [3, 0x81d7dc0], [3, 0x418d4520], [4, 0xb056ee0], [4, 0x4188d3a0], [5, 0x5932a20], [5, 0x419d0b40], [0, 0x42a18a0], [0, 0x41c476a0]]}
  layer.20.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ddf0c0], [1, 0x41c6af60], [2, 0x44dd560], [2, 0x41a5b580], [3, 0x8218de0], [3, 0x41915540], [4, 0xb097f00], [4, 0x418ce3c0]]}
  layer.20.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5973a40], [5, 0x41a11b60], [0, 0x42e28c0], [0, 0x41c886c0], [1, 0x5de72e0], [1, 0x41c73180], [2, 0x44e5780], [2, 0x41a637a0], [3, 0x8221000], [3, 0x4191d760], [4, 0xb0a0120], [4, 0x418d65e0], [5, 0x59f5a60], [5, 0x41a93b80], [0, 0x43648e0], [0, 0x41d0a6e0]]}
  layer.20.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e69300], [1, 0x41cf51a0], [2, 0x45677a0], [2, 0x41ae57c0], [3, 0x82a3020], [3, 0x4199f780], [4, 0xb122140], [4, 0x41958600]]}
  layer.20.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40eca640]]}
  layer.20.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76fc340]]}
  layer.21.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40f09de0], [4, 0xa7d17a0], [4, 0x41002b20], [5, 0x50a0fe0], [5, 0x4119c800], [0, 0x38daa40], [0, 0x411be880], [1, 0x535b3e0]]}
  layer.21.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x410c9420], [2, 0x3927520], [2, 0x40edaa60], [3, 0x770c760]]}
  layer.21.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40f4ae00], [4, 0xa8127c0], [4, 0x41043b40], [5, 0x50e2000], [5, 0x411dd820], [0, 0x391ba60], [0, 0x411ff8a0], [1, 0x539c400]]}
  layer.21.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x410cd540], [2, 0x392b640], [2, 0x40edeb80], [3, 0x7710880]]}
  layer.21.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x395ca80], [0, 0x412408c0], [1, 0x53dd420], [1, 0x410d1660], [2, 0x392f760], [2, 0x40ee2ca0], [3, 0x77149a0], [3, 0x40f8c660]]}
  layer.21.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa854020], [4, 0x410a5380], [5, 0x5143840], [5, 0x4123f060]]}
  layer.21.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x399daa0], [0, 0x412818e0], [1, 0x541e440], [1, 0x41112680], [2, 0x3970780], [2, 0x40f23cc0], [3, 0x77559c0], [3, 0x40fcd680]]}
  layer.21.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa858140], [4, 0x410a94a0], [5, 0x5147960], [5, 0x41243180]]}
  layer.21.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4102bca0]]}
  layer.21.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4100fd00]]}
  layer.21.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x516e920], [1, 0x40edc960], [2, 0x36d0840], [2, 0x40cf0840], [3, 0x7489fc0], [3, 0x40c36a80], [4, 0xa4fe440], [4, 0x40d36980], [5, 0x4e2a340], [5, 0x40f25b60], [0, 0x373eb80], [0, 0x41020120], [1, 0x51af940], [1, 0x40f1d980], [2, 0x3711860], [2, 0x40d31860], [3, 0x74cafe0], [3, 0x40c77aa0], [4, 0xa53f460], [4, 0x40d779a0], [5, 0x4e6b360], [5, 0x40f66b80], [0, 0x377fba0], [0, 0x41061140], [1, 0x51f0960], [1, 0x40f5e9a0], [2, 0x3752880], [2, 0x40d72880], [3, 0x750c000], [3, 0x40cb8ac0], [4, 0xa580480], [4, 0x40db89c0]]}
  layer.21.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4eac380], [5, 0x40fa7ba0], [0, 0x37c0bc0], [0, 0x410a2160], [1, 0x5231980], [1, 0x40f9f9c0], [2, 0x37938a0], [2, 0x40db38a0]]}
  layer.21.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x754d020], [3, 0x40cf9ae0], [4, 0xa5c14a0], [4, 0x40df99e0], [5, 0x4eb45a0], [5, 0x40fafdc0], [0, 0x37c8de0], [0, 0x410aa380], [1, 0x5239ba0], [1, 0x40fa7be0], [2, 0x379bac0], [2, 0x40dbbac0], [3, 0x75cf040], [3, 0x40d7bb00], [4, 0xa6434c0], [4, 0x40e7ba00]]}
  layer.21.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f365c0], [5, 0x41031de0], [0, 0x384ae00], [0, 0x4112c3a0], [1, 0x52bbbc0], [1, 0x41029c00], [2, 0x381dae0], [2, 0x40e3dae0]]}
  layer.21.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x52bdc60]]}
  layer.21.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x372e760]]}
  layer.22.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x381fb80], [2, 0x40e3fb80], [3, 0x76518a0], [3, 0x40e5f340], [4, 0xa726d00], [4, 0x40efe260], [5, 0x4f3b740], [5, 0x41036f60]]}
  layer.22.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x384ff80], [0, 0x41131520], [1, 0x52ce080], [1, 0x4103c0c0]]}
  layer.22.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3860ba0], [2, 0x40e80ba0], [3, 0x76928c0], [3, 0x40ea0360], [4, 0xa767d20], [4, 0x40f3f280], [5, 0x4f7c760], [5, 0x41077f80]]}
  layer.22.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38540a0], [0, 0x41135640], [1, 0x52d21a0], [1, 0x410401e0]]}
  layer.22.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40f802a0], [5, 0x4fbd780], [5, 0x410b8fa0], [0, 0x38581c0], [0, 0x41139760], [1, 0x52d62c0], [1, 0x41044300], [2, 0x38a2400]]}
  layer.22.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40ec2400], [3, 0x76f4100], [3, 0x40f01ba0], [4, 0xa7c9560]]}
  layer.22.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40fc12c0], [5, 0x4ffe7a0], [5, 0x410f9fc0], [0, 0x38991e0], [0, 0x4117a780], [1, 0x53172e0], [1, 0x41085320], [2, 0x38e3420]]}
  layer.22.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40ec6520], [3, 0x76f8220], [3, 0x40f05cc0], [4, 0xa7cd680]]}
  layer.22.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x411e7c60]]}
  layer.22.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52d3c40]]}
  layer.22.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x413cf460], [0, 0x3c126e0], [0, 0x415b84e0], [1, 0x57cd5e0], [1, 0x41460840], [2, 0x3c62aa0], [2, 0x411b78a0], [3, 0x79330c0], [3, 0x411aad80], [4, 0xa9991c0], [4, 0x411f8080], [5, 0x52e4060], [5, 0x41410480], [0, 0x3c53700], [0, 0x415f9500], [1, 0x580e600], [1, 0x414a1860], [2, 0x3ca3ac0], [2, 0x411f88c0], [3, 0x79740e0], [3, 0x411ebda0], [4, 0xa9da1e0], [4, 0x412390a0], [5, 0x5325080], [5, 0x414514a0], [0, 0x3c94720], [0, 0x4163a520], [1, 0x584f620], [1, 0x414e2880], [2, 0x3ce4ae0], [2, 0x412398e0], [3, 0x79b5100]]}
  layer.22.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4122cdc0], [4, 0xaa1b200], [4, 0x4127a0c0], [5, 0x53660a0], [5, 0x414924c0], [0, 0x3cd5740], [0, 0x4167b540], [1, 0x5890640]]}
  layer.22.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x415238a0], [2, 0x3d25b00], [2, 0x4127a900], [3, 0x79f6120], [3, 0x41234fe0], [4, 0xaa23420], [4, 0x412822e0], [5, 0x536e2c0], [5, 0x4149a6e0], [0, 0x3cdd960], [0, 0x41683760], [1, 0x5898860], [1, 0x415a58c0], [2, 0x3da7b20], [2, 0x412fc920], [3, 0x7a78140]]}
  layer.22.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x412b7000], [4, 0xaaa5440], [4, 0x41304300], [5, 0x53f02e0], [5, 0x4151c700], [0, 0x3d5f980], [0, 0x41705780], [1, 0x591a880]]}
  layer.22.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x53f2380]]}
  layer.22.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4151e7a0]]}
  layer.23.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3d61a20], [0, 0x41707820], [1, 0x591c920], [1, 0x41628120], [2, 0x3e2a380], [2, 0x4137f180], [3, 0x7afd240], [3, 0x412bc180]]}
  layer.23.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaaaa5c0], [4, 0x41309480], [5, 0x54027a0], [5, 0x4152ebc0]]}
  layer.23.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3da2a40], [0, 0x41748840], [1, 0x595d940], [1, 0x41669140], [2, 0x3e6b3a0], [2, 0x413c01a0], [3, 0x7b3e260], [3, 0x412fd1a0]]}
  layer.23.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaaae6e0], [4, 0x4130d5a0], [5, 0x54068c0], [5, 0x41532ce0]]}
  layer.23.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39df300], [0, 0x41324120], [1, 0x54c0c80], [1, 0x41153ee0], [2, 0x39b4880], [2, 0x40f67dc0], [3, 0x7799ac0], [3, 0x41011780]]}
  layer.23.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa86c680], [4, 0x410cdde0], [5, 0x516c2a0], [5, 0x41267ac0]]}
  layer.23.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a20320], [0, 0x41365140], [1, 0x5501ca0], [1, 0x41194f00], [2, 0x39f58a0], [2, 0x40fa8de0], [3, 0x77daae0], [3, 0x410527a0]]}
  layer.23.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8707a0], [4, 0x410d1f00], [5, 0x51703c0], [5, 0x4126bbe0]]}
  layer.23.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8748c0]]}
  layer.23.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa85c260]]}
  layer.23.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x51744e0], [5, 0x4126fd00], [0, 0x3a61b80], [0, 0x41407980], [1, 0x55a44e0], [1, 0x411d6760], [2, 0x3a399a0], [2, 0x40fecee0], [3, 0x781ebe0], [3, 0x410968a0], [4, 0xa884ce0], [4, 0x410d6860], [5, 0x51b5500], [5, 0x412b0d20], [0, 0x3aa2ba0], [0, 0x414489a0], [1, 0x55e5500], [1, 0x41217780], [2, 0x3a7a9c0], [2, 0x4102df00], [3, 0x785fc00], [3, 0x410d78c0], [4, 0xa8c5d00], [4, 0x41117880], [5, 0x51f6520], [5, 0x412f1d40], [0, 0x3ae3bc0], [0, 0x414899c0], [1, 0x5626520], [1, 0x412587a0], [2, 0x3abb9e0], [2, 0x4106ef20]]}
  layer.23.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x78a0c20], [3, 0x411188e0], [4, 0xa906d20], [4, 0x411588a0], [5, 0x5237540], [5, 0x41332d60], [0, 0x3b24be0], [0, 0x414ca9e0]]}
  layer.23.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5667540], [1, 0x412997c0], [2, 0x3afca00], [2, 0x410aff40], [3, 0x78a8e40], [3, 0x41120b00], [4, 0xa90ef40], [4, 0x41160ac0], [5, 0x523f760], [5, 0x4133af80], [0, 0x3b2ce00], [0, 0x414d2c00], [1, 0x56e9560], [1, 0x4131b7e0], [2, 0x3b7ea20], [2, 0x41131f60]]}
  layer.23.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x792ae60], [3, 0x411a2b20], [4, 0xa990f60], [4, 0x411e2ae0], [5, 0x52c1780], [5, 0x413bcfa0], [0, 0x3baee20], [0, 0x41554c20]]}
  layer.23.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52c3820]]}
  layer.23.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x413bf040]]}

  # constant
  input_1_multiply_16:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x427abb80]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50705c0]]}
  dc.input_tensor.softmax_18.4:                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4260f600], [3, 0x8d1bac0], [3, 0x4247c2c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbc131c0]]}
  dc.input_tensor.layernorm_38.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4256cdc0], [3, 0x8c79280]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x426ba0a0]]}
  dc.input_tensor.layernorm_38.6:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8dc6560], [3, 0x42526d60]]}
  dc.input_tensor.layernorm_38.8:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbc95a40], [4, 0x42580360]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5303140]]}
  dc.input_tensor.layernorm_52.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42503360], [3, 0x8c0f820]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb9a9060]]}
  dc.input_tensor.layernorm_52.6:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42250900], [5, 0x62a0a80]]}
  dc.input_tensor.layernorm_52.8:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42331040], [0, 0x4cf0860]]}
  input_1_multiply_69:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42751cc0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5016700]]}
  dc.input_tensor.softmax_71.4:                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42460b00], [3, 0x8b6cfc0], [3, 0x422d51a0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x423f9a40]]}
  dc.input_tensor.layernorm_91.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbaad920], [4, 0x42357a60]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x63a7be0]]}
  dc.input_tensor.layernorm_91.6:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x423be3a0], [0, 0x4d7dbc0]]}
  dc.input_tensor.layernorm_91.8:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x428848a0], [1, 0x6a5f340]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9343980]]}
  dc.input_tensor.layernorm_105.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42aa6a20], [4, 0xc15c980]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4298e520]]}
  dc.input_tensor.layernorm_105.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x68b0440], [5, 0x428998e0]]}
  dc.input_tensor.layernorm_105.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x520ff00], [0, 0x42d477e0]]}
  input_1_multiply_122:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d8b8e0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f80a00]]}
  dc.input_tensor.softmax_124.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42c85860], [2, 0x55e3860], [2, 0x42bda640]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9448240]]}
  dc.input_tensor.layernorm_144.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbf70fc0], [4, 0x427a2b60]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8f906e0]]}
  dc.input_tensor.layernorm_144.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x426f0ee0], [4, 0xbda6e40]]}
  dc.input_tensor.layernorm_144.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42691760], [5, 0x668dc40]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42a49260]]}
  dc.input_tensor.layernorm_158.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9157fc0], [3, 0x428bb060]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42881980]]}
  dc.input_tensor.layernorm_158.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x679f040], [5, 0x427884e0]]}
  dc.input_tensor.layernorm_158.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5178900], [0, 0x42cb01e0]]}
  input_1_multiply_175:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x54d4d00]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42acbae0]]}
  dc.input_tensor.softmax_177.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x423fc1e0], [3, 0x8b086a0], [3, 0x42270880]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41f45ea0]]}
  dc.input_tensor.layernorm_197.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb610b20], [4, 0x41e877c0]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f4ada0]]}
  dc.input_tensor.layernorm_197.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41fd8ac0], [0, 0x4888800]]}
  dc.input_tensor.layernorm_197.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x422c01e0], [1, 0x6405fe0]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x650d100]]}
  dc.input_tensor.layernorm_211.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x422f2ec0], [2, 0x4c0e660]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4217caa0]]}
  dc.input_tensor.layernorm_211.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88c4600], [3, 0x41fd29c0]]}
  dc.input_tensor.layernorm_211.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb6fe620], [4, 0x41f752c0]]}
  input_1_multiply_228:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41c2f240]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb2fb720]]}
  dc.input_tensor.softmax_230.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41b31be0], [5, 0x5bf2920], [5, 0x41ca0e40]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4631b20]]}
  dc.input_tensor.layernorm_250.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41f044a0], [1, 0x603cf60]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41b5a640]]}
  dc.input_tensor.layernorm_250.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c1b380], [5, 0x41cc98a0]]}
  dc.input_tensor.layernorm_250.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4632360], [0, 0x42069d40]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb50c260]]}
  dc.input_tensor.layernorm_264.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41d21f20], [5, 0x5de5500]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42078220]]}
  dc.input_tensor.layernorm_264.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8a83580], [3, 0x421eb760]]}
  dc.input_tensor.layernorm_264.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb9856e0], [4, 0x421fc380]]}
  input_1_multiply_281:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4c6d7a0]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42706160]]}
  dc.input_tensor.softmax_283.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x68c0c20], [1, 0x4262cbe0], [2, 0x4eeed80]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4caf000]]}
  dc.input_tensor.layernorm_303.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x427479c0], [1, 0x6922460]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4268e420]]}
  dc.input_tensor.layernorm_303.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4f505c0], [2, 0x4245da20]]}
  dc.input_tensor.layernorm_303.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8b69ee0], [3, 0x422d20c0]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4a1fd20]]}
  dc.input_tensor.layernorm_317.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42457700], [1, 0x6615aa0]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4245c840]]}
  dc.input_tensor.layernorm_317.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4cfc160], [2, 0x422095c0]]}
  dc.input_tensor.layernorm_317.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x89539c0], [3, 0x42061d80]]}
  input_1_multiply_334:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x677b340]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42561100]]}
  dc.input_tensor.softmax_336.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4e032c0], [2, 0x42310720], [3, 0x8a5ab20]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x421eaf20]]}
  dc.input_tensor.layernorm_356.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb923ec0], [4, 0x4219ab60]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x431fb600]]}
  dc.input_tensor.layernorm_356.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x723e440], [5, 0x43124100]]}
  dc.input_tensor.layernorm_356.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5a7b780], [0, 0x4367b9c0]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcc54b60]]}
  dc.input_tensor.layernorm_370.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x433c2ee0], [5, 0x74085c0]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x432ee280]]}
  dc.input_tensor.layernorm_370.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5bcbb00], [0, 0x437cbd40]]}
  dc.input_tensor.layernorm_370.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x789f720], [1, 0x43676940]]}
  input_1_multiply_387:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x433ce820]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e4b6c0]]}
  dc.input_tensor.softmax_389.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x43426de0], [3, 0x9b509e0], [3, 0x43317300]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75f5d80]]}
  dc.input_tensor.layernorm_409.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x594ece0], [0, 0x435550a0]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x767c720]]}
  dc.input_tensor.layernorm_409.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x434510a0], [2, 0x5e54140]]}
  dc.input_tensor.layernorm_409.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4344f840], [3, 0x9b79440]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x78a2800]]}
  dc.input_tensor.layernorm_423.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60baa00], [2, 0x4367d300]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43679a20]]}
  dc.input_tensor.layernorm_423.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dd7b00], [3, 0x435ab760]]}
  dc.input_tensor.layernorm_423.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcc963c0], [4, 0x43465720]]}
  input_1_multiply_440:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6b24040]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42a936e0]]}
  dc.input_tensor.softmax_442.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5378120], [0, 0x42eec900], [1, 0x7067c20]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42ceeb40]]}
  dc.input_tensor.layernorm_462.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5837ca0], [2, 0x42db23e0]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x957d7a0]]}
  dc.input_tensor.layernorm_462.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42d41820], [4, 0xc3f7780]]}
  dc.input_tensor.layernorm_462.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42be7b00], [5, 0x6ba68c0]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42cf6e60]]}
  dc.input_tensor.layernorm_476.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x568e300], [2, 0x42c850e0]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc2c2220]]}
  dc.input_tensor.layernorm_476.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42a55ee0], [5, 0x697a6a0]]}
  dc.input_tensor.layernorm_476.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42963b40], [0, 0x52a9560]]}
  input_1_multiply_493:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42d30300]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42c0c2c0]]}
  dc.input_tensor.softmax_495.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x94caac0], [3, 0x42c8eb40], [4, 0xc344aa0]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42b5d040]]}
  dc.input_tensor.layernorm_515.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6a81800], [5, 0x429f0ea0]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53368c0]]}
  dc.input_tensor.layernorm_515.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42ea8800], [1, 0x7023b20]]}
  dc.input_tensor.layernorm_515.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42db2b80], [2, 0x5771b60]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5d44d60]]}
  dc.input_tensor.layernorm_529.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x433392a0], [3, 0x9a62ea0]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4328a7a0]]}
  dc.input_tensor.layernorm_529.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc8cdac0], [4, 0x42fa2880]]}
  dc.input_tensor.layernorm_529.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70225c0], [5, 0x42f251a0]]}
  input_1_multiply_546:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x430279a0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a76e0]]}
  dc.input_tensor.softmax_548.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42faa2c0], [0, 0x58a4240], [0, 0x434aa600]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x97c70a0]]}
  dc.input_tensor.layernorm_568.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x42f8d9c0], [4, 0xc643920]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42d796c0]]}
  dc.input_tensor.layernorm_568.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6db9c60], [5, 0x42cbc840]]}
  dc.input_tensor.layernorm_568.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x55c1260], [0, 0x43166640]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x568f5c0]]}
  dc.input_tensor.layernorm_582.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x432349a0], [1, 0x733f100]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43137b60]]}
  dc.input_tensor.layernorm_582.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5b39bc0], [2, 0x4312e100]]}
  dc.input_tensor.layernorm_582.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9894c00], [3, 0x430bc500]]}
  input_1_multiply_599:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x49aac80]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x409086c0]]}
  dc.input_tensor.softmax_601.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x31f7b80], [2, 0x408a0520], [3, 0x7039ca0]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40887740]]}
  dc.input_tensor.layernorm_621.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa094300], [4, 0x408205e0]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4974f80]]}
  dc.input_tensor.layernorm_621.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa00f1e0], [4, 0x4079b4c0]]}
  dc.input_tensor.layernorm_621.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40993160], [2, 0x32a2600]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40bcadc0]]}
  dc.input_tensor.layernorm_635.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x33a3e20], [0, 0x40bfa9a0]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4cf85e0]]}
  dc.input_tensor.layernorm_635.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3125700], [2, 0x407ce0a0]]}
  dc.input_tensor.layernorm_635.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x407266e0], [0, 0x2f38540]]}
  input_1_multiply_652:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x408286c0]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3124ec0]]}
  dc.input_tensor.softmax_654.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x407ad880], [3, 0x6f0c9a0], [3, 0x40677420]]}
  lc.input_tensor.layernorm_674.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6f6e1e0]]}
  dc.input_tensor.layernorm_674.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x406d8c60], [4, 0x9eea960]]}
  lc.input_tensor.layernorm_674.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x406d7c20]]}
  dc.input_tensor.layernorm_674.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4828ce0], [5, 0x407b3a40]]}
  dc.input_tensor.layernorm_674.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fc58a0], [0, 0x4081c420]]}
  lc.input_tensor.layernorm_688.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40eca420]]}
  dc.input_tensor.layernorm_688.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fc8060], [1, 0x40d76880]]}
  lc.input_tensor.layernorm_688.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3645540]]}
  dc.input_tensor.layernorm_688.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40c42cc0], [3, 0x73dc440]]}
  dc.input_tensor.layernorm_688.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40bc5e00], [4, 0xa4339a0]]}
  input_1_multiply_705:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40e5a0e0]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x36c7dc0]]}
  dc.input_tensor.softmax_707.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40cc7de0], [3, 0x7461560], [3, 0x40c0e020]]}
  lc.input_tensor.layernorm_727.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa4fdc00]]}
  dc.input_tensor.layernorm_727.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40cd5160], [5, 0x4dc8b20]]}
  lc.input_tensor.layernorm_727.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40f25320]]}
  dc.input_tensor.layernorm_727.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40b77a20], [4, 0xa3e55c0]]}
  dc.input_tensor.layernorm_727.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40a696e0], [3, 0x7202e60]]}
  lc.input_tensor.layernorm_741.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40e23260]]}
  dc.input_tensor.layernorm_741.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ebfec0], [1, 0x40c6e6e0]]}
  lc.input_tensor.layernorm_741.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x357db80]]}
  dc.input_tensor.layernorm_741.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40b7aae0], [3, 0x7314260]]}
  dc.input_tensor.layernorm_741.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40aa4560], [2, 0x33b3a00]]}
  input_1_multiply_758:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d9c980]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x35d69c0]]}
  dc.input_tensor.softmax_760.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40ea5ae0], [1, 0x4fa3720], [1, 0x40d51f40]]}
  lc.input_tensor.layernorm_780.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40179e40]]}
  dc.input_tensor.layernorm_780.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a5a780], [0, 0x40214c80]]}
  lc.input_tensor.layernorm_780.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4345d60]]}
  dc.input_tensor.layernorm_780.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402ee200], [2, 0x2bce300]]}
  dc.input_tensor.layernorm_780.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x402f0aa0], [3, 0x6a4fbc0]]}
  lc.input_tensor.layernorm_794.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4027b660]]}
  dc.input_tensor.layernorm_794.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x436b740], [5, 0x402c7920]]}
  lc.input_tensor.layernorm_794.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2c09240]]}
  dc.input_tensor.layernorm_794.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4043d540], [1, 0x450d640]]}
  dc.input_tensor.layernorm_794.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404b8380], [2, 0x2d98480]]}
  input_1_multiply_811:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x28e0100]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40000000]]}
  dc.input_tensor.softmax_813.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d0100], [5, 0x40000000], [0, 0x28e0940]]}
  lc.input_tensor.layernorm_833.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400c3060]]}
  dc.input_tensor.layernorm_833.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4193160], [1, 0x400c3060]]}
  lc.input_tensor.layernorm_833.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x29a3160]]}
  dc.input_tensor.layernorm_833.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x400c3060], [3, 0x6883160]]}
  dc.input_tensor.layernorm_833.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400c3060], [4, 0x9863160]]}
  lc.input_tensor.layernorm_847.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402eb920]]}
  dc.input_tensor.layernorm_847.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2b6aa40], [2, 0x4028d1e0]]}
  lc.input_tensor.layernorm_847.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6a4d2e0]]}
  dc.input_tensor.layernorm_847.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x401d4460], [4, 0x9974560]]}
  dc.input_tensor.layernorm_847.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x401271c0], [5, 0x42172a0]]}
  input_1_multiply_864:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40556bc0]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2dd6520]]}
  dc.input_tensor.softmax_866.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4060f960], [1, 0x46dfa60], [1, 0x40559480]]}
  lc.input_tensor.layernorm_886.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x406342a0]]}
  dc.input_tensor.layernorm_886.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47043a0], [1, 0x4057ddc0]]}
  lc.input_tensor.layernorm_886.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2edb5a0]]}
  dc.input_tensor.layernorm_886.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x405616c0], [3, 0x6cc07e0]]}
  dc.input_tensor.layernorm_886.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x404e1f60], [4, 0x9cd3460]]}
  lc.input_tensor.layernorm_900.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x402bcec0]]}
  dc.input_tensor.layernorm_900.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x440df80], [5, 0x4036a160]]}
  lc.input_tensor.layernorm_900.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2c4aaa0]]}
  dc.input_tensor.layernorm_900.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40481640], [1, 0x4551740]]}
  dc.input_tensor.layernorm_900.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404bf580], [2, 0x2d9f680]]}
  input_1_multiply_917:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9adcf40]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404ca8a0]]}
  dc.input_tensor.softmax_919.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x2daa9a0], [2, 0x40430ac0], [3, 0x6b8fbe0]]}
  lc.input_tensor.layernorm_939.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40453380]]}
  dc.input_tensor.layernorm_939.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9be38a0], [4, 0x403c3820]]}
  lc.input_tensor.layernorm_939.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x45758c0]]}
  dc.input_tensor.layernorm_939.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x404d1aa0], [0, 0x2d51400]]}
  dc.input_tensor.layernorm_939.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4058a840], [1, 0x465a940]]}
  lc.input_tensor.layernorm_953.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41a3a3c0]]}
  dc.input_tensor.layernorm_953.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42b84a0], [2, 0x418164e0]]}
  lc.input_tensor.layernorm_953.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fa6a20]]}
  dc.input_tensor.layernorm_953.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x416a08e0], [4, 0xadb98a0]]}
  dc.input_tensor.layernorm_953.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41618760], [5, 0x56bdde0]]}
  input_1_multiply_970:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41ab5a80]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ccab80]]}
  dc.input_tensor.softmax_972.4:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41abcc40], [2, 0x439bd00], [2, 0x418f9d40]]}
  lc.input_tensor.layernorm_992.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x599e960]]}
  dc.input_tensor.layernorm_992.1:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x416aa160], [2, 0x3eac3c0]]}
  lc.input_tensor.layernorm_992.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x414011c0]]}
  dc.input_tensor.layernorm_992.6:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b7f280], [3, 0x4133e1c0]]}
  dc.input_tensor.layernorm_992.8:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaab2800], [4, 0x413116c0]]}
  lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41789860]]}
  dc.input_tensor.layernorm_1006.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xabc3c00], [4, 0x41422ac0]]}
  lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5529120]]}
  dc.input_tensor.layernorm_1006.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41655540], [0, 0x3f335e0]]}
  dc.input_tensor.layernorm_1006.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x418d93e0], [1, 0x5aee4e0]]}
  input_1_multiply_1023:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4164ec00]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4107a40]]}
  dc.input_tensor.softmax_1025.4:                    {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41b15ba0], [0, 0x43e6900], [0, 0x41d8c700]]}
  lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5eed3e0]]}
  dc.input_tensor.layernorm_1045.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41d79280], [2, 0x45eb880]]}
  lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41b698a0]]}
  dc.input_tensor.layernorm_1045.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8327100], [3, 0x41a23860]]}
  dc.input_tensor.layernorm_1045.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb1a6220], [4, 0x419dc6e0]]}
  lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4814140]]}
  dc.input_tensor.layernorm_1059.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41d31180], [3, 0x84f1280]]}
  lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41bed9e0]]}
  dc.input_tensor.layernorm_1059.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb2b7620], [4, 0x41aedae0]]}
  dc.input_tensor.layernorm_1059.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bae820], [5, 0x41c5cd40]]}
  input_1_multiply_1076:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41b635c0]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4442680]]}
  dc.input_tensor.softmax_1078.4:                    {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x419a06c0], [3, 0x812d320], [3, 0x41829a80]]}
  lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ccf4e0]]}
  dc.input_tensor.layernorm_1098.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x417a9b40], [5, 0x584f1c0]]}
  lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4194e2c0]]}
  dc.input_tensor.layernorm_1098.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x421c780], [0, 0x41bc2580]]}
  dc.input_tensor.layernorm_1098.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d59fa0], [1, 0x41be5e40]]}
  lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e29b40]]}
  dc.input_tensor.layernorm_1112.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x503f7c0], [5, 0x4113afe0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da200]]}
  dc.input_tensor.layernorm_1112.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x411bb7a0], [1, 0x5358300]]}
  dc.input_tensor.layernorm_1112.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x410c6340], [2, 0x3924440]]}
  input_1_multiply_1129:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40f8be20]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x410022e0]]}
  dc.input_tensor.softmax_1131.4:                    {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41084b60], [5, 0x5123020], [5, 0x4121e840]]}
  lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39deac0]]}
  dc.input_tensor.layernorm_1151.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x412c2900], [1, 0x545f460]]}
  lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x411536a0]]}
  dc.input_tensor.layernorm_1151.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39b17a0], [2, 0x40f64ce0]]}
  dc.input_tensor.layernorm_1151.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x77969e0], [3, 0x4100e6a0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7651060]]}
  dc.input_tensor.layernorm_1165.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40dfdb20], [4, 0xa6c54e0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40efda20]]}
  dc.input_tensor.layernorm_1165.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f38660], [5, 0x41033e80]]}
  dc.input_tensor.layernorm_1165.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x384cea0], [0, 0x4112e440]]}
  input_1_multiply_1182:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38a1bc0]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40ec1bc0]]}
  dc.input_tensor.softmax_1184.4:                    {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76d38e0], [3, 0x40ee1380], [4, 0xa7a8d40]]}
  lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8537e0]]}
  dc.input_tensor.layernorm_1204.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x576bdc0], [1, 0x413ff020]]}
  lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c62260]]}
  dc.input_tensor.layernorm_1204.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x411b47c0], [3, 0x792ffe0]]}
  dc.input_tensor.layernorm_1204.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x411a7ca0], [4, 0xa9960e0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x416278e0]]}
  dc.input_tensor.layernorm_1218.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bb0ec0], [0, 0x41556cc0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4137e940]]}
  dc.input_tensor.layernorm_1218.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7afa160], [3, 0x412b90a0]]}
  dc.input_tensor.layernorm_1218.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaaa74e0], [4, 0x413063a0]]}
  input_1_multiply_1235:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3de3a60]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x410d6020]]}
  dc.input_tensor.softmax_1237.4:                    {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x410ad5c0], [5, 0x514ba80], [5, 0x412472a0]]}
  lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a61340]]}
  dc.input_tensor.layernorm_1257.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x413a6160], [1, 0x5542cc0]]}
  lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x411d5f20]]}
  dc.input_tensor.layernorm_1257.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a368c0], [2, 0x40fe9e00]]}
  dc.input_tensor.layernorm_1257.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x781bb00], [3, 0x410937c0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x576b580]]}
  dc.input_tensor.layernorm_1271.1:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4139d800], [2, 0x3c00a40]]}
  lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x411b3f80]]}
  dc.input_tensor.layernorm_1271.6:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x792cf00], [3, 0x411a4bc0]]}
  dc.input_tensor.layernorm_1271.8:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa993000], [4, 0x411e4b80]]}

  # epoch_to_epoch
  e2e_layernorm_38.dc.reduce_sum.0.lc1_0:            {input: layernorm_38.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_add_37_0:                                      {input: add_37, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}
  e2e_matmul_41_0:                                   {input: matmul_41, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x79ea120], [1, 0x437c1340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8002140], [1, 0x43dd9360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x81a3ca0], [0, 0x45da3ee0]]}
  e2e__fused_op_4_0:                                 {input: _fused_op_4, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_6_0:                                 {input: _fused_op_6, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x861a160], [1, 0x443f1380]]}
  e2e_layernorm_91.dc.reduce_sum.0.lc1_0:            {input: layernorm_91.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_add_90_0:                                      {input: add_90, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_matmul_94_0:                                   {input: matmul_94, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9f21cc0], [3, 0x436f5920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa539ce0], [3, 0x43d0d940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_11_0:                                {input: _fused_op_11, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_13_0:                                {input: _fused_op_13, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab51d00], [3, 0x44325960]]}
  e2e_layernorm_144.dc.reduce_sum.0.lc1_0:           {input: layernorm_144.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_143_0:                                     {input: add_143, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_147_0:                                  {input: matmul_147, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_18_0:                                {input: _fused_op_18, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}
  e2e__fused_op_20_0:                                {input: _fused_op_20, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_197.dc.reduce_sum.0.lc1_0:           {input: layernorm_197.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x81a3ca0], [0, 0x45da3ee0]]}
  e2e_add_196_0:                                     {input: add_196, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_matmul_200_0:                                  {input: matmul_200, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x9187120], [1, 0x44f5e340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x979f140], [1, 0x45576360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0]]}
  e2e__fused_op_25_0:                                {input: _fused_op_25, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_27_0:                                {input: _fused_op_27, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9db7160], [1, 0x45b8e380]]}
  e2e_layernorm_250.dc.reduce_sum.0.lc1_0:           {input: layernorm_250.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_add_249_0:                                     {input: add_249, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_matmul_253_0:                                  {input: matmul_253, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9f21cc0], [3, 0x436f5920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa539ce0], [3, 0x43d0d940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_32_0:                                {input: _fused_op_32, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_34_0:                                {input: _fused_op_34, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab51d00], [3, 0x44325960]]}
  e2e_layernorm_303.dc.reduce_sum.0.lc1_0:           {input: layernorm_303.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_302_0:                                     {input: add_302, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_306_0:                                  {input: matmul_306, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xc3b1d20], [3, 0x45b85980]]}
  e2e__fused_op_39_0:                                {input: _fused_op_39, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_41_0:                                {input: _fused_op_41, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_356.dc.reduce_sum.0.lc1_0:           {input: layernorm_356.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_add_355_0:                                     {input: add_355, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6f5bca0], [0, 0x44b5bee0]]}
  e2e_matmul_359_0:                                  {input: matmul_359, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x79ea120], [1, 0x437c1340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x8002140], [1, 0x43dd9360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x9b122e0], [5, 0x45996fc0]]}
  e2e__fused_op_46_0:                                {input: _fused_op_46, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x87bbcc0], [0, 0x463bbf00]]}
  e2e__fused_op_48_0:                                {input: _fused_op_48, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x861a160], [1, 0x443f1380]]}
  e2e_layernorm_409.dc.reduce_sum.0.lc1_0:           {input: layernorm_409.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_add_408_0:                                     {input: add_408, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x73e8340], [2, 0x449aac40]]}
  e2e_matmul_412_0:                                  {input: matmul_412, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9f21cc0], [3, 0x436f5920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa539ce0], [3, 0x43d0d940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x9e7a180], [1, 0x45c513a0]]}
  e2e__fused_op_53_0:                                {input: _fused_op_53, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8c48360], [2, 0x4620ac60]]}
  e2e__fused_op_55_0:                                {input: _fused_op_55, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab51d00], [3, 0x44325960]]}
  e2e_layernorm_462.dc.reduce_sum.0.lc1_0:           {input: layernorm_462.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_461_0:                                     {input: add_461, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdf655c0], [4, 0x44734920]]}
  e2e_matmul_465_0:                                  {input: matmul_465, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_60_0:                                {input: _fused_op_60, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}
  e2e__fused_op_62_0:                                {input: _fused_op_62, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_515.dc.reduce_sum.0.lc1_0:           {input: layernorm_515.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_add_514_0:                                     {input: add_514, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x81a3ca0], [0, 0x45da3ee0]]}
  e2e_matmul_518_0:                                  {input: matmul_518, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x79ea120], [1, 0x437c1340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x8002140], [1, 0x43dd9360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0]]}
  e2e__fused_op_67_0:                                {input: _fused_op_67, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_69_0:                                {input: _fused_op_69, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x861a160], [1, 0x443f1380]]}
  e2e_layernorm_568.dc.reduce_sum.0.lc1_0:           {input: layernorm_568.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_add_567_0:                                     {input: add_567, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_matmul_571_0:                                  {input: matmul_571, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0xb6becc0], [3, 0x44e92920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xbcd6ce0], [3, 0x454aa940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_74_0:                                {input: _fused_op_74, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_76_0:                                {input: _fused_op_76, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc2eed00], [3, 0x45ac2960]]}
  e2e_layernorm_621.dc.reduce_sum.0.lc1_0:           {input: layernorm_621.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_620_0:                                     {input: add_620, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_624_0:                                  {input: matmul_624, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_81_0:                                {input: _fused_op_81, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}
  e2e__fused_op_83_0:                                {input: _fused_op_83, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_674.dc.reduce_sum.0.lc1_0:           {input: layernorm_674.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_add_673_0:                                     {input: add_673, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x81a3ca0], [0, 0x45da3ee0]]}
  e2e_matmul_677_0:                                  {input: matmul_677, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x79ea120], [1, 0x437c1340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x8002140], [1, 0x43dd9360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0]]}
  e2e__fused_op_88_0:                                {input: _fused_op_88, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_90_0:                                {input: _fused_op_90, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x861a160], [1, 0x443f1380]]}
  e2e_layernorm_727.dc.reduce_sum.0.lc1_0:           {input: layernorm_727.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_add_726_0:                                     {input: add_726, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_matmul_730_0:                                  {input: matmul_730, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0xb6becc0], [3, 0x44e92920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xbcd6ce0], [3, 0x454aa940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_95_0:                                {input: _fused_op_95, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_97_0:                                {input: _fused_op_97, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc2eed00], [3, 0x45ac2960]]}
  e2e_layernorm_780.dc.reduce_sum.0.lc1_0:           {input: layernorm_780.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_779_0:                                     {input: add_779, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_783_0:                                  {input: matmul_783, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_102_0:                               {input: _fused_op_102, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}
  e2e__fused_op_104_0:                               {input: _fused_op_104, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_833.dc.reduce_sum.0.lc1_0:           {input: layernorm_833.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x81a3ca0], [0, 0x45da3ee0]]}
  e2e_add_832_0:                                     {input: add_832, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_matmul_836_0:                                  {input: matmul_836, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x9187120], [1, 0x44f5e340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x979f140], [1, 0x45576360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0]]}
  e2e__fused_op_109_0:                               {input: _fused_op_109, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_111_0:                               {input: _fused_op_111, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9db7160], [1, 0x45b8e380]]}
  e2e_layernorm_886.dc.reduce_sum.0.lc1_0:           {input: layernorm_886.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_add_885_0:                                     {input: add_885, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_matmul_889_0:                                  {input: matmul_889, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0xb6becc0], [3, 0x44e92920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xbcd6ce0], [3, 0x454aa940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360]]}
  e2e__fused_op_116_0:                               {input: _fused_op_116, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_118_0:                               {input: _fused_op_118, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc2eed00], [3, 0x45ac2960]]}
  e2e_layernorm_939.dc.reduce_sum.0.lc1_0:           {input: layernorm_939.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdf655c0], [4, 0x44734920]]}
  e2e_add_938_0:                                     {input: add_938, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_matmul_942_0:                                  {input: matmul_942, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x8e1f280], [5, 0x44ca3f60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x94372a0], [5, 0x452bbf80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940]]}
  e2e__fused_op_123_0:                               {input: _fused_op_123, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe0285e0], [4, 0x447f7940]]}
  e2e__fused_op_125_0:                               {input: _fused_op_125, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9a4f2c0], [5, 0x458d3fa0]]}
  e2e_layernorm_992.dc.reduce_sum.0.lc1_0:           {input: layernorm_992.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6f5bca0], [0, 0x44b5bee0]]}
  e2e_add_991_0:                                     {input: add_991, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_matmul_995_0:                                  {input: matmul_995, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x9187120], [1, 0x44f5e340], [2, 0x7a00300], [2, 0x44fc2c00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x979f140], [1, 0x45576360], [2, 0x8018320], [2, 0x455dac20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0]]}
  e2e__fused_op_130_0:                               {input: _fused_op_130, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0]]}
  e2e__fused_op_132_0:                               {input: _fused_op_132, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9db7160], [1, 0x45b8e380]]}
  e2e_layernorm_1045.dc.reduce_sum.0.lc1_0:          {input: layernorm_1045.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_add_1044_0:                                    {input: add_1044, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8630340], [2, 0x45bf2c40]]}
  e2e_matmul_1048_0:                                 {input: matmul_1048, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9f21cc0], [3, 0x436f5920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa539ce0], [3, 0x43d0d940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_137_0:                               {input: _fused_op_137, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_139_0:                               {input: _fused_op_139, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab51d00], [3, 0x44325960]]}
  e2e_layernorm_1098.dc.reduce_sum.0.lc1_0:          {input: layernorm_1098.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_1097_0:                                    {input: add_1097, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_1101_0:                                 {input: matmul_1101, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xc3b1d20], [3, 0x45b85980]]}
  e2e__fused_op_144_0:                               {input: _fused_op_144, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_146_0:                               {input: _fused_op_146, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82b22c0], [5, 0x44136fa0]]}
  e2e_layernorm_1151.dc.reduce_sum.0.lc1_0:          {input: layernorm_1151.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320]]}
  e2e_add_1150_0:                                    {input: add_1150, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6f5bca0], [0, 0x44b5bee0]]}
  e2e_matmul_1154_0:                                 {input: matmul_1154, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x79ea120], [1, 0x437c1340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x8002140], [1, 0x43dd9360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900], [5, 0x9b122e0], [5, 0x45996fc0]]}
  e2e__fused_op_151_0:                               {input: _fused_op_151, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x87bbcc0], [0, 0x463bbf00]]}
  e2e__fused_op_153_0:                               {input: _fused_op_153, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x861a160], [1, 0x443f1380]]}
  e2e_layernorm_1204.dc.reduce_sum.0.lc1_0:          {input: layernorm_1204.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x73e8340], [2, 0x449aac40]]}
  e2e_add_1203_0:                                    {input: add_1203, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e5eca0], [3, 0x43632900]]}
  e2e_matmul_1207_0:                                 {input: matmul_1207, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40], [0, 0x5d13c40], [0, 0x43913e80], [1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0xb6becc0], [3, 0x44e92920], [4, 0xe57d580], [4, 0x44d4c8e0], [5, 0x7bd7280], [5, 0x43a5bf60], [0, 0x632bc60], [0, 0x43f2bea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xbcd6ce0], [3, 0x454aa940], [4, 0xeb955a0], [4, 0x45364900], [5, 0x81ef2a0], [5, 0x44073f80], [0, 0x6943c80], [0, 0x44543ec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20]]}
  e2e__fused_op_158_0:                               {input: _fused_op_158, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcd1d560], [4, 0x434ec8c0]]}
  e2e__fused_op_160_0:                               {input: _fused_op_160, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc2eed00], [3, 0x45ac2960]]}
  e2e_layernorm_1257.dc.reduce_sum.0.lc1_0:          {input: layernorm_1257.dc.reduce_sum.0.lc1, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75bf260], [5, 0x43443f40]]}
  e2e_add_1256_0:                                    {input: add_1256, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf1ad5c0], [4, 0x4597c920]]}
  e2e_matmul_1260_0:                                 {input: matmul_1260, type: queue, entries: 64, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7927100], [1, 0x436fe320], [2, 0x61a02e0], [2, 0x43762be0], [3, 0x9e5eca0], [3, 0x43632900], [4, 0xcd1d560], [4, 0x434ec8c0], [5, 0x7682280], [5, 0x43506f60], [0, 0x7573c60], [0, 0x45173ea0], [1, 0x7f3f120], [1, 0x43d16340], [2, 0x67b8300], [2, 0x43d7ac00], [3, 0xa476cc0], [3, 0x43c4a920], [4, 0xd335580], [4, 0x43b048e0], [5, 0x7c9a2a0], [5, 0x43b1ef80], [0, 0x7b8bc80], [0, 0x4578bec0], [1, 0x8557140], [1, 0x4432e360], [2, 0x6dd0320], [2, 0x44392c20], [3, 0xaa8ece0], [3, 0x44262940], [4, 0xd94d5a0], [4, 0x4411c900]]}
  e2e__fused_op_165_0:                               {input: _fused_op_165, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d13c40], [0, 0x43913e80]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 64
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_14, input_1_multiply_16, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_1: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 3], inputs: [_fused_op_0, softmax_18.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_22: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_2: {type: fused_op, grid_loc: [4, 4], grid_size: [3, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4, _fused_op_1], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [5, 4], grid_size: [2, 2], inputs: [_fused_op_2, matmul_22],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_33: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 64
    _fused_op_3: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.1, e2e_layernorm_38.dc.reduce_sum.0.lc1_0, e2e_add_37_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_3__fused_op_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_3__fused_op_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_3__fused_op_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_3__fused_op_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_4: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8, buffer_0__fused_op_3__fused_op_4, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_41: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_4, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 64
    gelu_44: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_41_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_51: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_47, e2e__fused_op_4_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_51__fused_op_5: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_51__fused_op_5: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_51__fused_op_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_51__fused_op_5: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_51__fused_op_5], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_51__fused_op_5: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_51__fused_op_5], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_5: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, buffer_0_add_51__fused_op_5], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_5, _fused_op_5], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_5__fused_op_6: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_5], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_5__fused_op_6: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_5__fused_op_6], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_5__fused_op_6: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_5__fused_op_6], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_5__fused_op_6: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_5__fused_op_6], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_5__fused_op_6: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_5__fused_op_6], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_6: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8, buffer_0__fused_op_5__fused_op_6, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 64
    matmul_55: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_6_0, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_61: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_6_0, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_67: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_67, input_1_multiply_69, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_7],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_8: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_7, softmax_71.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_75: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_6_0, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_9: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4, _fused_op_8], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_82: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_9, matmul_75],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_86: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_90: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_86, e2e__fused_op_6_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 64
    _fused_op_10: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_91.1, e2e_layernorm_91.dc.reduce_sum.0.lc1_0, e2e_add_90_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_10, _fused_op_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_10__fused_op_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_10__fused_op_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_10__fused_op_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_10__fused_op_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_11: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8, buffer_0__fused_op_10__fused_op_11, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_94: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_11, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 64
    gelu_97: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_94_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_104: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_100, e2e__fused_op_11_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_104__fused_op_12: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_104],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_104__fused_op_12: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_104__fused_op_12],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_104__fused_op_12: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_104__fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_104__fused_op_12: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_104__fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_12: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, buffer_0_add_104__fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_12, _fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_12__fused_op_13: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_12__fused_op_13: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_12__fused_op_13], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_12__fused_op_13: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_12__fused_op_13], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_12__fused_op_13: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_12__fused_op_13], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_12__fused_op_13: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_12__fused_op_13], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_13: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8, buffer_0__fused_op_12__fused_op_13, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 64
    matmul_108: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_13_0, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_114: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_13_0, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_120: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_14: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_120, input_1_multiply_122, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_14],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_15: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_14, softmax_124.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_15, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_128: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_13_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_16: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4, _fused_op_15], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_135: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_16, matmul_128],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_139: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_143: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_139, e2e__fused_op_13_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 64
    _fused_op_17: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_144.1, e2e_layernorm_144.dc.reduce_sum.0.lc1_0, e2e_add_143_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_17, _fused_op_17],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_17__fused_op_18: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_17],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_17__fused_op_18: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_17__fused_op_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_17__fused_op_18: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_17__fused_op_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_17__fused_op_18: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_17__fused_op_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_17__fused_op_18: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_17__fused_op_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_18: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_144.8, buffer_0__fused_op_17__fused_op_18, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_147: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_18, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 64
    gelu_150: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_147_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_157: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_153, e2e__fused_op_18_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_157__fused_op_19: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_157__fused_op_19: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_157__fused_op_19],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_157__fused_op_19: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_157__fused_op_19], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_157__fused_op_19: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_157__fused_op_19], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_19: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, buffer_0_add_157__fused_op_19], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_19, _fused_op_19], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_19__fused_op_20: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_19], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_19__fused_op_20: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_19__fused_op_20], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_19__fused_op_20: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_19__fused_op_20], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_19__fused_op_20: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_19__fused_op_20], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_19__fused_op_20: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_19__fused_op_20], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_20: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8, buffer_0__fused_op_19__fused_op_20, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 64
    matmul_161: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_20_0, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_167: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_20_0, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_173: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_161, matmul_167],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_173, input_1_multiply_175, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_177.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_21],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_22: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_21, softmax_177.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_177.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_22, lc.input_tensor.softmax_177.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_181: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_20_0, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_23: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_177.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_177.4, _fused_op_22], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_188: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_23, matmul_181],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_192: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_196: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_192, e2e__fused_op_20_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_10_temporal_epoch_10:
    target_device: 0
    input_count: 64
    _fused_op_24: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_197.1, e2e_layernorm_197.dc.reduce_sum.0.lc1_0, e2e_add_196_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_24, _fused_op_24],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_197.dc.multiply.4, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_24__fused_op_25: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_24],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_24__fused_op_25: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_24__fused_op_25],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_24__fused_op_25: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_24__fused_op_25],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_24__fused_op_25: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_24__fused_op_25],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_24__fused_op_25: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_24__fused_op_25],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_25: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_197.8, buffer_0__fused_op_24__fused_op_25, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_200: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_25, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_11_temporal_epoch_11:
    target_device: 0
    input_count: 64
    gelu_203: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_200_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_210: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_206, e2e__fused_op_25_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_210__fused_op_26: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_210],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_210__fused_op_26: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_210__fused_op_26],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_210__fused_op_26: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_210__fused_op_26], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_210__fused_op_26: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_210__fused_op_26], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_26: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1, buffer_0_add_210__fused_op_26], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_26, _fused_op_26], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_26__fused_op_27: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_26], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_26__fused_op_27: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_26__fused_op_27], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_26__fused_op_27: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_26__fused_op_27], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_26__fused_op_27: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_26__fused_op_27], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_26__fused_op_27: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_26__fused_op_27], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_27: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_211.8, buffer_0__fused_op_26__fused_op_27, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_12_temporal_epoch_12:
    target_device: 0
    input_count: 64
    matmul_214: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_27_0, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_220: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_27_0, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_226: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_214, matmul_220],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_28: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_226, input_1_multiply_228, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_230.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_28],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_29: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_28, softmax_230.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_230.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_29, lc.input_tensor.softmax_230.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_234: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_27_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_30: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_230.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_230.4, _fused_op_29], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_241: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_30, matmul_234],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_245: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_249: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_245, e2e__fused_op_27_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_13_temporal_epoch_13:
    target_device: 0
    input_count: 64
    _fused_op_31: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_250.1, e2e_layernorm_250.dc.reduce_sum.0.lc1_0, e2e_add_249_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_250.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_31, _fused_op_31],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_250.dc.multiply.4, lc.input_tensor.layernorm_250.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_31__fused_op_32: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_31],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_31__fused_op_32: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_31__fused_op_32],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_31__fused_op_32: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_31__fused_op_32],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_31__fused_op_32: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_31__fused_op_32],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_31__fused_op_32: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_31__fused_op_32],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_32: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_250.6, layernorm_250.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_250.8, buffer_0__fused_op_31__fused_op_32, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_253: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_32, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_14_temporal_epoch_14:
    target_device: 0
    input_count: 64
    gelu_256: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_253_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_263: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_259, e2e__fused_op_32_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_263__fused_op_33: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_263],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_263__fused_op_33: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_263__fused_op_33],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_263__fused_op_33: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_263__fused_op_33], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_263__fused_op_33: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_263__fused_op_33], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_33: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_264.1, layernorm_264.dc.reduce_sum.0.lc1, buffer_0_add_263__fused_op_33], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_264.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_33, _fused_op_33], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_264.dc.multiply.4, lc.input_tensor.layernorm_264.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_33__fused_op_34: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_33], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_33__fused_op_34: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_33__fused_op_34], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_33__fused_op_34: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_33__fused_op_34], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_33__fused_op_34: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_33__fused_op_34], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_33__fused_op_34: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_33__fused_op_34], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_34: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_264.6, layernorm_264.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_264.8, buffer_0__fused_op_33__fused_op_34, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_15_temporal_epoch_15:
    target_device: 0
    input_count: 64
    matmul_267: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_34_0, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_273: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_34_0, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_279: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_267, matmul_273],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_35: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_279, input_1_multiply_281, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_283.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_35],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_36: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_35, softmax_283.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_283.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_283.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_287: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_34_0, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_37: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_283.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_283.4, _fused_op_36], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_294: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_37, matmul_287],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_298: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_302: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_298, e2e__fused_op_34_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_16_temporal_epoch_16:
    target_device: 0
    input_count: 64
    _fused_op_38: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_303.1, e2e_layernorm_303.dc.reduce_sum.0.lc1_0, e2e_add_302_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_303.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_38, _fused_op_38],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_303.dc.multiply.4, lc.input_tensor.layernorm_303.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_38__fused_op_39: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_38],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_38__fused_op_39: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_38__fused_op_39],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_38__fused_op_39: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_38__fused_op_39],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_38__fused_op_39: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_38__fused_op_39],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_38__fused_op_39: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_38__fused_op_39],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_39: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_303.6, layernorm_303.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_303.8, buffer_0__fused_op_38__fused_op_39, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_306: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_39, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_17_temporal_epoch_17:
    target_device: 0
    input_count: 64
    gelu_309: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_306_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_316: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_312, e2e__fused_op_39_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_316__fused_op_40: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_316],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_316__fused_op_40: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_316__fused_op_40],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_316__fused_op_40: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_316__fused_op_40], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_316__fused_op_40: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_316__fused_op_40], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_40: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_317.1, layernorm_317.dc.reduce_sum.0.lc1, buffer_0_add_316__fused_op_40], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_317.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_40, _fused_op_40], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_317.dc.multiply.4, lc.input_tensor.layernorm_317.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_40__fused_op_41: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_40], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_40__fused_op_41: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_40__fused_op_41], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_40__fused_op_41: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_40__fused_op_41], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_40__fused_op_41: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_40__fused_op_41], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_40__fused_op_41: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_40__fused_op_41], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_41: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_317.6, layernorm_317.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_317.8, buffer_0__fused_op_40__fused_op_41, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_18_temporal_epoch_18:
    target_device: 0
    input_count: 64
    matmul_320: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_41_0, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_326: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_41_0, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_332: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_320, matmul_326],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_42: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_332, input_1_multiply_334, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_336.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_42],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_43: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_42, softmax_336.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_336.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_43, lc.input_tensor.softmax_336.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_340: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_41_0, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_336.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_336.4, _fused_op_43], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_347: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_44, matmul_340],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_351: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_355: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_351, e2e__fused_op_41_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_19_temporal_epoch_19:
    target_device: 0
    input_count: 64
    _fused_op_45: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_356.1, e2e_layernorm_356.dc.reduce_sum.0.lc1_0, e2e_add_355_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_356.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_45, _fused_op_45],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_356.dc.multiply.4, lc.input_tensor.layernorm_356.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_45__fused_op_46: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_45],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_45__fused_op_46: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_45__fused_op_46],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_45__fused_op_46: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_45__fused_op_46],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_45__fused_op_46: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_45__fused_op_46],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_45__fused_op_46: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_45__fused_op_46],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_46: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_356.6, layernorm_356.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_356.8, buffer_0__fused_op_45__fused_op_46, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_359: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_46, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_20_temporal_epoch_20:
    target_device: 0
    input_count: 64
    gelu_362: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_359_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_369: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_365, e2e__fused_op_46_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_369__fused_op_47: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_369],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_369__fused_op_47: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_369__fused_op_47],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_369__fused_op_47: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_369__fused_op_47], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_369__fused_op_47: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_369__fused_op_47], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_47: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_370.1, layernorm_370.dc.reduce_sum.0.lc1, buffer_0_add_369__fused_op_47], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_370.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_47, _fused_op_47], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_370.dc.multiply.4, lc.input_tensor.layernorm_370.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_47__fused_op_48: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_47], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_47__fused_op_48: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_47__fused_op_48], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_47__fused_op_48: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_47__fused_op_48], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_47__fused_op_48: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_47__fused_op_48], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_47__fused_op_48: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_47__fused_op_48], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_48: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_370.6, layernorm_370.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_370.8, buffer_0__fused_op_47__fused_op_48, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_21_temporal_epoch_21:
    target_device: 0
    input_count: 64
    matmul_373: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_48_0, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_379: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_48_0, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_385: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_373, matmul_379],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_385, input_1_multiply_387, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_389.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_49],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_50: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_49, softmax_389.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_389.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_50, lc.input_tensor.softmax_389.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_393: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_48_0, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_51: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_389.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_389.4, _fused_op_50], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_400: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_51, matmul_393],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_404: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_408: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_404, e2e__fused_op_48_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_22_temporal_epoch_22:
    target_device: 0
    input_count: 64
    _fused_op_52: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_409.1, e2e_layernorm_409.dc.reduce_sum.0.lc1_0, e2e_add_408_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_409.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_52, _fused_op_52],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_409.dc.multiply.4, lc.input_tensor.layernorm_409.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_52__fused_op_53: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_52],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_52__fused_op_53: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_52__fused_op_53],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_52__fused_op_53: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_52__fused_op_53],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_52__fused_op_53: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_52__fused_op_53],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_52__fused_op_53: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_52__fused_op_53],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_53: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_409.6, layernorm_409.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_409.8, buffer_0__fused_op_52__fused_op_53, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_412: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_53, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_23_temporal_epoch_23:
    target_device: 0
    input_count: 64
    gelu_415: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_412_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_422: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_418, e2e__fused_op_53_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_422__fused_op_54: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_422],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_422__fused_op_54: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_422__fused_op_54],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_422__fused_op_54: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_422__fused_op_54], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_422__fused_op_54: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_422__fused_op_54], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_54: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_423.1, layernorm_423.dc.reduce_sum.0.lc1, buffer_0_add_422__fused_op_54], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_423.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_54, _fused_op_54], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_423.dc.multiply.4, lc.input_tensor.layernorm_423.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_54__fused_op_55: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_54], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_54__fused_op_55: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_54__fused_op_55], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_54__fused_op_55: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_54__fused_op_55], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_54__fused_op_55: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_54__fused_op_55], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_54__fused_op_55: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_54__fused_op_55], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_55: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_423.6, layernorm_423.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_423.8, buffer_0__fused_op_54__fused_op_55, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_24_temporal_epoch_24:
    target_device: 0
    input_count: 64
    matmul_426: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_55_0, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_432: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_55_0, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_438: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_426, matmul_432],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_56: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_438, input_1_multiply_440, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_442.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_56],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_57: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_56, softmax_442.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_442.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_57, lc.input_tensor.softmax_442.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_446: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_55_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_58: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_442.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_442.4, _fused_op_57], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_453: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_58, matmul_446],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_457: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_461: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_457, e2e__fused_op_55_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_25_temporal_epoch_25:
    target_device: 0
    input_count: 64
    _fused_op_59: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_462.1, e2e_layernorm_462.dc.reduce_sum.0.lc1_0, e2e_add_461_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_462.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_59, _fused_op_59],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_462.dc.multiply.4, lc.input_tensor.layernorm_462.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_59__fused_op_60: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_59],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_59__fused_op_60: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_59__fused_op_60],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_59__fused_op_60: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_59__fused_op_60],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_59__fused_op_60: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_59__fused_op_60],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_59__fused_op_60: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_59__fused_op_60],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_60: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_462.6, layernorm_462.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_462.8, buffer_0__fused_op_59__fused_op_60, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_465: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_60, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_26_temporal_epoch_26:
    target_device: 0
    input_count: 64
    gelu_468: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_465_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_471: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_468, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_475: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_471, e2e__fused_op_60_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_475__fused_op_61: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_475],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_475__fused_op_61: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_475__fused_op_61],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_475__fused_op_61: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_475__fused_op_61], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_475__fused_op_61: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_475__fused_op_61], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_61: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_476.1, layernorm_476.dc.reduce_sum.0.lc1, buffer_0_add_475__fused_op_61], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_476.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_61, _fused_op_61], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_476.dc.multiply.4, lc.input_tensor.layernorm_476.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_61__fused_op_62: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_61], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_61__fused_op_62: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_61__fused_op_62], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_61__fused_op_62: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_61__fused_op_62], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_61__fused_op_62: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_61__fused_op_62], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_61__fused_op_62: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_61__fused_op_62], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_62: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_476.6, layernorm_476.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_476.8, buffer_0__fused_op_61__fused_op_62, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_27_temporal_epoch_27:
    target_device: 0
    input_count: 64
    matmul_479: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_62_0, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_485: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_62_0, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_491: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_479, matmul_485],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_63: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_491, input_1_multiply_493, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_495.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_63],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_64: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_63, softmax_495.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_495.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_64, lc.input_tensor.softmax_495.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_499: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_62_0, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_65: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_495.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_495.4, _fused_op_64], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_506: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_65, matmul_499],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_510: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_514: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_510, e2e__fused_op_62_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_28_temporal_epoch_28:
    target_device: 0
    input_count: 64
    _fused_op_66: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_515.1, e2e_layernorm_515.dc.reduce_sum.0.lc1_0, e2e_add_514_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_515.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_66, _fused_op_66],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_515.dc.multiply.4, lc.input_tensor.layernorm_515.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_66__fused_op_67: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_66],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_66__fused_op_67: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_66__fused_op_67],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_66__fused_op_67: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_66__fused_op_67],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_66__fused_op_67: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_66__fused_op_67],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_66__fused_op_67: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_66__fused_op_67],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_67: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_515.6, layernorm_515.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_515.8, buffer_0__fused_op_66__fused_op_67, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_518: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_67, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_29_temporal_epoch_29:
    target_device: 0
    input_count: 64
    gelu_521: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_518_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_528: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_524, e2e__fused_op_67_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_528__fused_op_68: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_528],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_528__fused_op_68: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_528__fused_op_68],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_528__fused_op_68: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_528__fused_op_68], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_528__fused_op_68: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_528__fused_op_68], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_68: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_529.1, layernorm_529.dc.reduce_sum.0.lc1, buffer_0_add_528__fused_op_68], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_529.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_68, _fused_op_68], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_529.dc.multiply.4, lc.input_tensor.layernorm_529.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_68__fused_op_69: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_68], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_68__fused_op_69: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_68__fused_op_69], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_68__fused_op_69: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_68__fused_op_69], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_68__fused_op_69: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_68__fused_op_69], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_68__fused_op_69: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_68__fused_op_69], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_69: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_529.6, layernorm_529.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_529.8, buffer_0__fused_op_68__fused_op_69, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_30_temporal_epoch_30:
    target_device: 0
    input_count: 64
    matmul_532: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_69_0, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_538: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_69_0, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_544: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_532, matmul_538],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_70: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_544, input_1_multiply_546, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_548.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_70],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_71: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_70, softmax_548.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_548.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_71, lc.input_tensor.softmax_548.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_552: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_69_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_72: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_548.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_548.4, _fused_op_71], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_559: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_72, matmul_552],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_563: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_567: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_563, e2e__fused_op_69_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_31_temporal_epoch_31:
    target_device: 0
    input_count: 64
    _fused_op_73: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_568.1, e2e_layernorm_568.dc.reduce_sum.0.lc1_0, e2e_add_567_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_568.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_73, _fused_op_73],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_568.dc.multiply.4, lc.input_tensor.layernorm_568.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_73__fused_op_74: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_73],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_73__fused_op_74: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_73__fused_op_74],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_73__fused_op_74: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_73__fused_op_74],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_73__fused_op_74: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_73__fused_op_74],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_73__fused_op_74: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_73__fused_op_74],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_74: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_568.6, layernorm_568.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_568.8, buffer_0__fused_op_73__fused_op_74, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_571: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_74, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_32_temporal_epoch_32:
    target_device: 0
    input_count: 64
    gelu_574: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_571_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_581: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_577, e2e__fused_op_74_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_581__fused_op_75: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_581],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_581__fused_op_75: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_581__fused_op_75],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_581__fused_op_75: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_581__fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_581__fused_op_75: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_581__fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_75: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_582.1, layernorm_582.dc.reduce_sum.0.lc1, buffer_0_add_581__fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_582.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_75, _fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_582.dc.multiply.4, lc.input_tensor.layernorm_582.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_75__fused_op_76: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_75__fused_op_76: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_75__fused_op_76], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_75__fused_op_76: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_75__fused_op_76], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_75__fused_op_76: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_75__fused_op_76], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_75__fused_op_76: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_75__fused_op_76], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_76: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_582.6, layernorm_582.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_582.8, buffer_0__fused_op_75__fused_op_76, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_33_temporal_epoch_33:
    target_device: 0
    input_count: 64
    matmul_585: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_76_0, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_591: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_76_0, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_597: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_585, matmul_591],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_77: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_597, input_1_multiply_599, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_601.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_77],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_78: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_77, softmax_601.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_601.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_78, lc.input_tensor.softmax_601.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_605: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_76_0, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_79: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_601.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_601.4, _fused_op_78], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_612: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_79, matmul_605],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_616: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_620: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_616, e2e__fused_op_76_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_34_temporal_epoch_34:
    target_device: 0
    input_count: 64
    _fused_op_80: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_621.1, e2e_layernorm_621.dc.reduce_sum.0.lc1_0, e2e_add_620_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_621.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_80, _fused_op_80],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_621.dc.multiply.4, lc.input_tensor.layernorm_621.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_80__fused_op_81: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_80],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_80__fused_op_81: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_80__fused_op_81],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_80__fused_op_81: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_80__fused_op_81],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_80__fused_op_81: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_80__fused_op_81],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_80__fused_op_81: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_80__fused_op_81],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_81: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_621.6, layernorm_621.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_621.8, buffer_0__fused_op_80__fused_op_81, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_624: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_81, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_35_temporal_epoch_35:
    target_device: 0
    input_count: 64
    gelu_627: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_624_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_634: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_630, e2e__fused_op_81_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_634__fused_op_82: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_634],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_634__fused_op_82: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_634__fused_op_82],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_634__fused_op_82: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_634__fused_op_82], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_634__fused_op_82: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_634__fused_op_82], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_82: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_635.1, layernorm_635.dc.reduce_sum.0.lc1, buffer_0_add_634__fused_op_82], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_635.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_82, _fused_op_82], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_635.dc.multiply.4, lc.input_tensor.layernorm_635.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_82__fused_op_83: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_82], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_82__fused_op_83: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_82__fused_op_83], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_82__fused_op_83: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_82__fused_op_83], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_82__fused_op_83: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_82__fused_op_83], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_82__fused_op_83: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_82__fused_op_83], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_83: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_635.6, layernorm_635.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_635.8, buffer_0__fused_op_82__fused_op_83, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_36_temporal_epoch_36:
    target_device: 0
    input_count: 64
    matmul_638: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_83_0, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_644: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_83_0, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_650: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_638, matmul_644],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_84: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_650, input_1_multiply_652, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_654.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_84],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_85: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_84, softmax_654.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_654.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_85, lc.input_tensor.softmax_654.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_658: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_83_0, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_86: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_654.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_654.4, _fused_op_85], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_665: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_86, matmul_658],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_669: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_665, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_673: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_669, e2e__fused_op_83_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_673, lc.input_tensor.layernorm_674.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_37_temporal_epoch_37:
    target_device: 0
    input_count: 64
    _fused_op_87: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_674.1, e2e_layernorm_674.dc.reduce_sum.0.lc1_0, e2e_add_673_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_674.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_87, _fused_op_87],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_674.dc.multiply.4, lc.input_tensor.layernorm_674.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_87__fused_op_88: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_87],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_87__fused_op_88: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_87__fused_op_88],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_87__fused_op_88: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_87__fused_op_88],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_87__fused_op_88: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_87__fused_op_88],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_87__fused_op_88: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_87__fused_op_88],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_88: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_674.6, layernorm_674.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_674.8, buffer_0__fused_op_87__fused_op_88, layer.12.attention.output.LayerNorm.weight, layer.12.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_677: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_88, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_38_temporal_epoch_38:
    target_device: 0
    input_count: 64
    gelu_680: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_677_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_683: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_680, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_687: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_683, e2e__fused_op_88_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_687, lc.input_tensor.layernorm_688.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_687__fused_op_89: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_687],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_687__fused_op_89: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_687__fused_op_89],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_687__fused_op_89: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_687__fused_op_89], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_687__fused_op_89: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_687__fused_op_89], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_89: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_688.1, layernorm_688.dc.reduce_sum.0.lc1, buffer_0_add_687__fused_op_89], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_688.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_89, _fused_op_89], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_688.dc.multiply.4, lc.input_tensor.layernorm_688.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_89__fused_op_90: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_89], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_89__fused_op_90: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_89__fused_op_90], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_89__fused_op_90: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_89__fused_op_90], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_89__fused_op_90: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_89__fused_op_90], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_89__fused_op_90: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_89__fused_op_90], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_90: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_688.6, layernorm_688.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_688.8, buffer_0__fused_op_89__fused_op_90, layer.12.output.LayerNorm.weight, layer.12.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_39_temporal_epoch_39:
    target_device: 0
    input_count: 64
    matmul_691: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_90_0, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_697: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_90_0, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_703: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_691, matmul_697],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_91: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_703, input_1_multiply_705, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_707.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_91],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_92: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_91, softmax_707.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_707.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_92, lc.input_tensor.softmax_707.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_711: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_90_0, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_93: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_707.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_707.4, _fused_op_92], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_718: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_93, matmul_711],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_722: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_718, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_726: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_722, e2e__fused_op_90_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_726, lc.input_tensor.layernorm_727.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_40_temporal_epoch_40:
    target_device: 0
    input_count: 64
    _fused_op_94: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_727.1, e2e_layernorm_727.dc.reduce_sum.0.lc1_0, e2e_add_726_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_727.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_94, _fused_op_94],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_727.dc.multiply.4, lc.input_tensor.layernorm_727.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_94__fused_op_95: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_94],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_94__fused_op_95: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_94__fused_op_95],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_94__fused_op_95: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_94__fused_op_95],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_94__fused_op_95: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_94__fused_op_95],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_94__fused_op_95: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_94__fused_op_95],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_95: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_727.6, layernorm_727.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_727.8, buffer_0__fused_op_94__fused_op_95, layer.13.attention.output.LayerNorm.weight, layer.13.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_730: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_95, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_41_temporal_epoch_41:
    target_device: 0
    input_count: 64
    gelu_733: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_730_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_736: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_733, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_740: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_736, e2e__fused_op_95_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_740, lc.input_tensor.layernorm_741.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_740__fused_op_96: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_740],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_740__fused_op_96: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_740__fused_op_96],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_740__fused_op_96: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_740__fused_op_96], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_740__fused_op_96: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_740__fused_op_96], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_96: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_741.1, layernorm_741.dc.reduce_sum.0.lc1, buffer_0_add_740__fused_op_96], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_741.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_96, _fused_op_96], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_741.dc.multiply.4, lc.input_tensor.layernorm_741.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_96__fused_op_97: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_96], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_96__fused_op_97: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_96__fused_op_97], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_96__fused_op_97: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_96__fused_op_97], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_96__fused_op_97: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_96__fused_op_97], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_96__fused_op_97: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_96__fused_op_97], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_97: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_741.6, layernorm_741.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_741.8, buffer_0__fused_op_96__fused_op_97, layer.13.output.LayerNorm.weight, layer.13.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_42_temporal_epoch_42:
    target_device: 0
    input_count: 64
    matmul_744: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_97_0, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_750: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_97_0, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_756: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_744, matmul_750],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_98: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_756, input_1_multiply_758, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_760.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_98],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_99: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_98, softmax_760.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_760.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_99, lc.input_tensor.softmax_760.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_764: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_97_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_100: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_760.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_760.4, _fused_op_99], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_771: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_100, matmul_764],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_775: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_771, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_779: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_775, e2e__fused_op_97_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_779, lc.input_tensor.layernorm_780.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_43_temporal_epoch_43:
    target_device: 0
    input_count: 64
    _fused_op_101: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_780.1, e2e_layernorm_780.dc.reduce_sum.0.lc1_0, e2e_add_779_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_780.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_101, _fused_op_101],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_780.dc.multiply.4, lc.input_tensor.layernorm_780.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_101__fused_op_102: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_101],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_101__fused_op_102: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_101__fused_op_102],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_101__fused_op_102: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_101__fused_op_102],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_101__fused_op_102: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_101__fused_op_102],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_101__fused_op_102: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_101__fused_op_102],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_102: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_780.6, layernorm_780.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_780.8, buffer_0__fused_op_101__fused_op_102, layer.14.attention.output.LayerNorm.weight, layer.14.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_783: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_102, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_44_temporal_epoch_44:
    target_device: 0
    input_count: 64
    gelu_786: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_783_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_789: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_786, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_793: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_789, e2e__fused_op_102_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_793, lc.input_tensor.layernorm_794.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_793__fused_op_103: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_793],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_793__fused_op_103: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_793__fused_op_103],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_793__fused_op_103: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_793__fused_op_103], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_793__fused_op_103: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_793__fused_op_103], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_103: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_794.1, layernorm_794.dc.reduce_sum.0.lc1, buffer_0_add_793__fused_op_103], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_794.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_103, _fused_op_103], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_794.dc.multiply.4, lc.input_tensor.layernorm_794.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_103__fused_op_104: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_103], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_103__fused_op_104: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_103__fused_op_104], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_103__fused_op_104: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_103__fused_op_104], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_103__fused_op_104: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_103__fused_op_104], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_103__fused_op_104: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_103__fused_op_104], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_104: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_794.6, layernorm_794.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_794.8, buffer_0__fused_op_103__fused_op_104, layer.14.output.LayerNorm.weight, layer.14.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_45_temporal_epoch_45:
    target_device: 0
    input_count: 64
    matmul_797: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_104_0, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_803: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_104_0, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_809: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_797, matmul_803],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_105: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_809, input_1_multiply_811, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_813.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_105],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_106: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_105, softmax_813.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_813.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_106, lc.input_tensor.softmax_813.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_817: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_104_0, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_107: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_813.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_813.4, _fused_op_106], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_824: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_107, matmul_817],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_828: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_824, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_832: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_828, e2e__fused_op_104_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_832, lc.input_tensor.layernorm_833.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_46_temporal_epoch_46:
    target_device: 0
    input_count: 64
    _fused_op_108: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_833.1, e2e_layernorm_833.dc.reduce_sum.0.lc1_0, e2e_add_832_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_833.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_108, _fused_op_108],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_833.dc.multiply.4, lc.input_tensor.layernorm_833.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_108__fused_op_109: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_108],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_108__fused_op_109: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_108__fused_op_109],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_108__fused_op_109: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_108__fused_op_109],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_108__fused_op_109: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_108__fused_op_109],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_108__fused_op_109: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_108__fused_op_109],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_109: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_833.6, layernorm_833.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_833.8, buffer_0__fused_op_108__fused_op_109, layer.15.attention.output.LayerNorm.weight, layer.15.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_836: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_109, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_47_temporal_epoch_47:
    target_device: 0
    input_count: 64
    gelu_839: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_836_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_842: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_839, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_846: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_842, e2e__fused_op_109_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_846, lc.input_tensor.layernorm_847.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_846__fused_op_110: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_846],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_846__fused_op_110: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_846__fused_op_110],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_846__fused_op_110: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_846__fused_op_110], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_846__fused_op_110: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_846__fused_op_110], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_110: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_847.1, layernorm_847.dc.reduce_sum.0.lc1, buffer_0_add_846__fused_op_110], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_847.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_110, _fused_op_110], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_847.dc.multiply.4, lc.input_tensor.layernorm_847.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_110__fused_op_111: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_110], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_110__fused_op_111: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_110__fused_op_111], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_110__fused_op_111: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_110__fused_op_111], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_110__fused_op_111: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_110__fused_op_111], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_110__fused_op_111: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_110__fused_op_111], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_111: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_847.6, layernorm_847.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_847.8, buffer_0__fused_op_110__fused_op_111, layer.15.output.LayerNorm.weight, layer.15.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_48_temporal_epoch_48:
    target_device: 0
    input_count: 64
    matmul_850: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_111_0, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_856: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_111_0, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_862: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_850, matmul_856],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_112: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_862, input_1_multiply_864, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_866.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_112],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_113: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_112, softmax_866.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_866.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_113, lc.input_tensor.softmax_866.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_870: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_111_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_114: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_866.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_866.4, _fused_op_113], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_877: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_114, matmul_870],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_881: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_877, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_885: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_881, e2e__fused_op_111_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_885, lc.input_tensor.layernorm_886.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_49_temporal_epoch_49:
    target_device: 0
    input_count: 64
    _fused_op_115: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_886.1, e2e_layernorm_886.dc.reduce_sum.0.lc1_0, e2e_add_885_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_886.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_115, _fused_op_115],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_886.dc.multiply.4, lc.input_tensor.layernorm_886.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_115__fused_op_116: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_115],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_115__fused_op_116: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_115__fused_op_116],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_115__fused_op_116: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_115__fused_op_116],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_115__fused_op_116: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_115__fused_op_116],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_115__fused_op_116: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_115__fused_op_116],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_116: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_886.6, layernorm_886.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_886.8, buffer_0__fused_op_115__fused_op_116, layer.16.attention.output.LayerNorm.weight, layer.16.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_889: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_116, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_50_temporal_epoch_50:
    target_device: 0
    input_count: 64
    gelu_892: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_889_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_895: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_892, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_899: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_895, e2e__fused_op_116_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_899, lc.input_tensor.layernorm_900.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_899__fused_op_117: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_899],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_899__fused_op_117: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_899__fused_op_117],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_899__fused_op_117: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_899__fused_op_117], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_899__fused_op_117: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_899__fused_op_117], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_117: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_900.1, layernorm_900.dc.reduce_sum.0.lc1, buffer_0_add_899__fused_op_117], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_900.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_117, _fused_op_117], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_900.dc.multiply.4, lc.input_tensor.layernorm_900.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_117__fused_op_118: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_117], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_117__fused_op_118: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_117__fused_op_118], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_117__fused_op_118: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_117__fused_op_118], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_117__fused_op_118: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_117__fused_op_118], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_117__fused_op_118: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_117__fused_op_118], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_118: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_900.6, layernorm_900.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_900.8, buffer_0__fused_op_117__fused_op_118, layer.16.output.LayerNorm.weight, layer.16.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_51_temporal_epoch_51:
    target_device: 0
    input_count: 64
    matmul_903: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_118_0, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_909: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_118_0, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_915: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_903, matmul_909],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_119: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_915, input_1_multiply_917, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_919.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_119],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_120: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_119, softmax_919.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_919.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_120, lc.input_tensor.softmax_919.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_923: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_118_0, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_121: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_919.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_919.4, _fused_op_120], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_930: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_121, matmul_923],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_934: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_930, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_938: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_934, e2e__fused_op_118_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_938, lc.input_tensor.layernorm_939.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_52_temporal_epoch_52:
    target_device: 0
    input_count: 64
    _fused_op_122: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_939.1, e2e_layernorm_939.dc.reduce_sum.0.lc1_0, e2e_add_938_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_939.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_122, _fused_op_122],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_939.dc.multiply.4, lc.input_tensor.layernorm_939.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_122__fused_op_123: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_122],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_122__fused_op_123: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_122__fused_op_123],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_122__fused_op_123: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_122__fused_op_123],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_122__fused_op_123: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_122__fused_op_123],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_122__fused_op_123: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_122__fused_op_123],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_123: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_939.6, layernorm_939.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_939.8, buffer_0__fused_op_122__fused_op_123, layer.17.attention.output.LayerNorm.weight, layer.17.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_942: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_123, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_53_temporal_epoch_53:
    target_device: 0
    input_count: 64
    gelu_945: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_942_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_948: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_945, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_952: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_948, e2e__fused_op_123_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_952, lc.input_tensor.layernorm_953.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_952__fused_op_124: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_952],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_952__fused_op_124: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_952__fused_op_124],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_952__fused_op_124: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_952__fused_op_124], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_952__fused_op_124: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_952__fused_op_124], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_124: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_953.1, layernorm_953.dc.reduce_sum.0.lc1, buffer_0_add_952__fused_op_124], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_953.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_124, _fused_op_124], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_953.dc.multiply.4, lc.input_tensor.layernorm_953.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_124__fused_op_125: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_124], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_124__fused_op_125: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_124__fused_op_125], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_124__fused_op_125: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_124__fused_op_125], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_124__fused_op_125: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_124__fused_op_125], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_124__fused_op_125: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_124__fused_op_125], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_125: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_953.6, layernorm_953.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_953.8, buffer_0__fused_op_124__fused_op_125, layer.17.output.LayerNorm.weight, layer.17.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_54_temporal_epoch_54:
    target_device: 0
    input_count: 64
    matmul_956: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_125_0, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_962: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_125_0, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_968: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_956, matmul_962],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_126: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_968, input_1_multiply_970, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_972.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_126],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_127: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_126, softmax_972.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_972.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_127, lc.input_tensor.softmax_972.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_976: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_125_0, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_128: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_972.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_972.4, _fused_op_127], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_983: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_128, matmul_976],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_987: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_983, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_991: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_987, e2e__fused_op_125_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_991, lc.input_tensor.layernorm_992.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_55_temporal_epoch_55:
    target_device: 0
    input_count: 64
    _fused_op_129: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_992.1, e2e_layernorm_992.dc.reduce_sum.0.lc1_0, e2e_add_991_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_992.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_129, _fused_op_129],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_992.dc.multiply.4, lc.input_tensor.layernorm_992.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_129__fused_op_130: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_129],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_129__fused_op_130: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_129__fused_op_130],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_129__fused_op_130: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_129__fused_op_130],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_129__fused_op_130: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_129__fused_op_130],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_129__fused_op_130: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_129__fused_op_130],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_130: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_992.6, layernorm_992.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_992.8, buffer_0__fused_op_129__fused_op_130, layer.18.attention.output.LayerNorm.weight, layer.18.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_995: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_130, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_56_temporal_epoch_56:
    target_device: 0
    input_count: 64
    gelu_998: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_995_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1001: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_998, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1005: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1001, e2e__fused_op_130_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1005, lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1005__fused_op_131: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1005],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1005__fused_op_131: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1005__fused_op_131],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1005__fused_op_131: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1005__fused_op_131], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1005__fused_op_131: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1005__fused_op_131], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_131: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1006.1, layernorm_1006.dc.reduce_sum.0.lc1, buffer_0_add_1005__fused_op_131], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1006.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_131, _fused_op_131], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1006.dc.multiply.4, lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_131__fused_op_132: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_131], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_131__fused_op_132: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_131__fused_op_132], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_131__fused_op_132: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_131__fused_op_132], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_131__fused_op_132: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_131__fused_op_132], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_131__fused_op_132: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_131__fused_op_132], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_132: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1006.6, layernorm_1006.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1006.8, buffer_0__fused_op_131__fused_op_132, layer.18.output.LayerNorm.weight, layer.18.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_57_temporal_epoch_57:
    target_device: 0
    input_count: 64
    matmul_1009: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_132_0, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1015: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_132_0, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1021: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_1009, matmul_1015],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_133: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1021, input_1_multiply_1023, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_1025.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_133],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_134: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_133, softmax_1025.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_1025.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_134, lc.input_tensor.softmax_1025.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1029: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_132_0, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_135: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_1025.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1025.4, _fused_op_134], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_1036: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_135, matmul_1029],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1040: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_1036, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_1044: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1040, e2e__fused_op_132_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_1044, lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_58_temporal_epoch_58:
    target_device: 0
    input_count: 64
    _fused_op_136: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1045.1, e2e_layernorm_1045.dc.reduce_sum.0.lc1_0, e2e_add_1044_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1045.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_136, _fused_op_136],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1045.dc.multiply.4, lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_136__fused_op_137: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_136],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_136__fused_op_137: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_136__fused_op_137],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_136__fused_op_137: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_136__fused_op_137],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_136__fused_op_137: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_136__fused_op_137],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_136__fused_op_137: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_136__fused_op_137],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_137: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1045.6, layernorm_1045.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1045.8, buffer_0__fused_op_136__fused_op_137, layer.19.attention.output.LayerNorm.weight, layer.19.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_1048: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_137, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_59_temporal_epoch_59:
    target_device: 0
    input_count: 64
    gelu_1051: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_1048_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1054: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_1051, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1058: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1054, e2e__fused_op_137_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1058, lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1058__fused_op_138: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1058],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1058__fused_op_138: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1058__fused_op_138],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1058__fused_op_138: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1058__fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1058__fused_op_138: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1058__fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_138: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1059.1, layernorm_1059.dc.reduce_sum.0.lc1, buffer_0_add_1058__fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1059.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_138, _fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1059.dc.multiply.4, lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_138__fused_op_139: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_138__fused_op_139: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_138__fused_op_139], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_138__fused_op_139: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_138__fused_op_139], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_138__fused_op_139: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_138__fused_op_139], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_138__fused_op_139: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_138__fused_op_139], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_139: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1059.6, layernorm_1059.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1059.8, buffer_0__fused_op_138__fused_op_139, layer.19.output.LayerNorm.weight, layer.19.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_60_temporal_epoch_60:
    target_device: 0
    input_count: 64
    matmul_1062: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_139_0, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1068: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_139_0, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1074: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_1062, matmul_1068],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_140: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1074, input_1_multiply_1076, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_1078.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_140],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_141: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_140, softmax_1078.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_1078.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_141, lc.input_tensor.softmax_1078.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1082: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_139_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_142: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_1078.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1078.4, _fused_op_141], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_1089: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_142, matmul_1082],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1093: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_1089, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_1097: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1093, e2e__fused_op_139_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_1097, lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_61_temporal_epoch_61:
    target_device: 0
    input_count: 64
    _fused_op_143: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1098.1, e2e_layernorm_1098.dc.reduce_sum.0.lc1_0, e2e_add_1097_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1098.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_143, _fused_op_143],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1098.dc.multiply.4, lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_143__fused_op_144: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_143],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_143__fused_op_144: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_143__fused_op_144],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_143__fused_op_144: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_143__fused_op_144],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_143__fused_op_144: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_143__fused_op_144],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_143__fused_op_144: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_143__fused_op_144],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_144: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1098.6, layernorm_1098.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1098.8, buffer_0__fused_op_143__fused_op_144, layer.20.attention.output.LayerNorm.weight, layer.20.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_1101: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_144, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_62_temporal_epoch_62:
    target_device: 0
    input_count: 64
    gelu_1104: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_1101_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1107: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_1104, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1111: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1107, e2e__fused_op_144_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1111, lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1111__fused_op_145: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1111],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1111__fused_op_145: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1111__fused_op_145],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1111__fused_op_145: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1111__fused_op_145], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1111__fused_op_145: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1111__fused_op_145], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_145: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1112.1, layernorm_1112.dc.reduce_sum.0.lc1, buffer_0_add_1111__fused_op_145], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1112.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_145, _fused_op_145], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1112.dc.multiply.4, lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_145__fused_op_146: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_145], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_145__fused_op_146: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_145__fused_op_146], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_145__fused_op_146: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_145__fused_op_146], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_145__fused_op_146: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_145__fused_op_146], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_145__fused_op_146: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_145__fused_op_146], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_146: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1112.6, layernorm_1112.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1112.8, buffer_0__fused_op_145__fused_op_146, layer.20.output.LayerNorm.weight, layer.20.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_63_temporal_epoch_63:
    target_device: 0
    input_count: 64
    matmul_1115: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_146_0, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1121: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_146_0, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1127: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_1115, matmul_1121],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_147: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1127, input_1_multiply_1129, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_1131.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_147],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_148: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_147, softmax_1131.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_1131.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_148, lc.input_tensor.softmax_1131.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1135: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_146_0, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_149: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_1131.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1131.4, _fused_op_148], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_1142: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_149, matmul_1135],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1146: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_1142, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_1150: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1146, e2e__fused_op_146_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_1150, lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_64_temporal_epoch_64:
    target_device: 0
    input_count: 64
    _fused_op_150: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1151.1, e2e_layernorm_1151.dc.reduce_sum.0.lc1_0, e2e_add_1150_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1151.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_150, _fused_op_150],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1151.dc.multiply.4, lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_150__fused_op_151: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_150],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_150__fused_op_151: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_150__fused_op_151],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_150__fused_op_151: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_150__fused_op_151],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_150__fused_op_151: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_150__fused_op_151],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_150__fused_op_151: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_150__fused_op_151],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_151: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1151.6, layernorm_1151.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1151.8, buffer_0__fused_op_150__fused_op_151, layer.21.attention.output.LayerNorm.weight, layer.21.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_1154: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_151, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_65_temporal_epoch_65:
    target_device: 0
    input_count: 64
    gelu_1157: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_1154_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1160: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_1157, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1164: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1160, e2e__fused_op_151_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1164, lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1164__fused_op_152: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1164],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1164__fused_op_152: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1164__fused_op_152],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1164__fused_op_152: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1164__fused_op_152], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1164__fused_op_152: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1164__fused_op_152], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_152: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1165.1, layernorm_1165.dc.reduce_sum.0.lc1, buffer_0_add_1164__fused_op_152], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1165.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_152, _fused_op_152], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1165.dc.multiply.4, lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_152__fused_op_153: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_152], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_152__fused_op_153: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_152__fused_op_153], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_152__fused_op_153: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_152__fused_op_153], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_152__fused_op_153: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_152__fused_op_153], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_152__fused_op_153: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_152__fused_op_153], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_153: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1165.6, layernorm_1165.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1165.8, buffer_0__fused_op_152__fused_op_153, layer.21.output.LayerNorm.weight, layer.21.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_66_temporal_epoch_66:
    target_device: 0
    input_count: 64
    matmul_1168: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_153_0, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1174: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_153_0, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1180: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_1168, matmul_1174],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_154: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1180, input_1_multiply_1182, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_1184.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_154],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_155: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_154, softmax_1184.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_1184.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_155, lc.input_tensor.softmax_1184.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1188: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [e2e__fused_op_153_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_156: {type: fused_op, grid_loc: [4, 0], grid_size: [3, 1], inputs: [softmax_1184.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1184.4, _fused_op_155], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_1195: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_156, matmul_1188],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1199: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [matmul_1195, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_1203: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1199, e2e__fused_op_153_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_1203, lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_67_temporal_epoch_67:
    target_device: 0
    input_count: 64
    _fused_op_157: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1204.1, e2e_layernorm_1204.dc.reduce_sum.0.lc1_0, e2e_add_1203_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1204.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_157, _fused_op_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1204.dc.multiply.4, lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_157__fused_op_158: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_157__fused_op_158: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_157__fused_op_158],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_157__fused_op_158: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_157__fused_op_158],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_157__fused_op_158: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_157__fused_op_158],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_157__fused_op_158: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_157__fused_op_158],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_158: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1204.6, layernorm_1204.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1204.8, buffer_0__fused_op_157__fused_op_158, layer.22.attention.output.LayerNorm.weight, layer.22.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_1207: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_158, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_68_temporal_epoch_68:
    target_device: 0
    input_count: 64
    gelu_1210: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_1207_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1213: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_1210, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1217: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1213, e2e__fused_op_158_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1217, lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1217__fused_op_159: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1217],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1217__fused_op_159: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1217__fused_op_159],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1217__fused_op_159: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1217__fused_op_159], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1217__fused_op_159: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1217__fused_op_159], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_159: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1218.1, layernorm_1218.dc.reduce_sum.0.lc1, buffer_0_add_1217__fused_op_159], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1218.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_159, _fused_op_159], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1218.dc.multiply.4, lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_159__fused_op_160: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [_fused_op_159], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_159__fused_op_160: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_159__fused_op_160], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_159__fused_op_160: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_159__fused_op_160], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_159__fused_op_160: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_159__fused_op_160], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_159__fused_op_160: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_159__fused_op_160], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_160: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1218.6, layernorm_1218.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1218.8, buffer_0__fused_op_159__fused_op_160, layer.22.output.LayerNorm.weight, layer.22.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}

  fwd_0_69_temporal_epoch_69:
    target_device: 0
    input_count: 64
    matmul_1221: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e__fused_op_160_0, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1227: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e__fused_op_160_0, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_1233: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_1221, matmul_1227],
         t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_161: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1233, input_1_multiply_1235, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_1237.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_161],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_162: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 3], inputs: [_fused_op_161, softmax_1237.dc.reduce_max.0],
         t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_1237.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_162, lc.input_tensor.softmax_1237.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1241: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e__fused_op_160_0, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_163: {type: fused_op, grid_loc: [4, 4], grid_size: [3, 1], inputs: [softmax_1237.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1237.4, _fused_op_162], grid_transpose: true,
         t: 16, mblock: [2, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 264], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_1248: {type: matmul, grid_loc: [5, 4], grid_size: [2, 2], inputs: [_fused_op_163, matmul_1241],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_1252: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_1248, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_1256: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1252, e2e__fused_op_160_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_1256, lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_70_temporal_epoch_70:
    target_device: 0
    input_count: 64
    _fused_op_164: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1257.1, e2e_layernorm_1257.dc.reduce_sum.0.lc1_0, e2e_add_1256_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1257.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_164, _fused_op_164],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1257.dc.multiply.4, lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_164__fused_op_165: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [_fused_op_164],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_164__fused_op_165: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_164__fused_op_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_164__fused_op_165: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_164__fused_op_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_164__fused_op_165: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_164__fused_op_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_164__fused_op_165: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_164__fused_op_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_165: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1257.6, layernorm_1257.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1257.8, buffer_0__fused_op_164__fused_op_165, layer.23.attention.output.LayerNorm.weight, layer.23.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    matmul_1260: {type: matmul, grid_loc: [3, 0], grid_size: [4, 8], inputs: [_fused_op_165, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_71_temporal_epoch_71:
    target_device: 0
    input_count: 64
    gelu_1263: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_1260_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1266: {type: matmul, grid_loc: [2, 0], grid_size: [3, 8], inputs: [gelu_1263, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_1270: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_1266, e2e__fused_op_165_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_1270, lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_1270__fused_op_166: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1270],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_1270__fused_op_166: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_3_add_1270__fused_op_166],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_1270__fused_op_166: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [buffer_2_add_1270__fused_op_166], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_1270__fused_op_166: {type: nop, grid_loc: [5, 2], grid_size: [2, 1], inputs: [buffer_1_add_1270__fused_op_166], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_166: {type: fused_op, grid_loc: [5, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1271.1, layernorm_1271.dc.reduce_sum.0.lc1, buffer_0_add_1270__fused_op_166], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {fused_op_id: 3}}
    layernorm_1271.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [_fused_op_166, _fused_op_166], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1271.dc.multiply.4, lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_166__fused_op_167: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [_fused_op_166], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_166__fused_op_167: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_166__fused_op_167], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_166__fused_op_167: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_3__fused_op_166__fused_op_167], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_166__fused_op_167: {type: nop, grid_loc: [7, 0], grid_size: [2, 1], inputs: [buffer_2__fused_op_166__fused_op_167], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_166__fused_op_167: {type: nop, grid_loc: [7, 2], grid_size: [2, 1], inputs: [buffer_1__fused_op_166__fused_op_167], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_167: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1271.6, layernorm_1271.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1271.8, buffer_0__fused_op_166__fused_op_167, layer.23.output.LayerNorm.weight, layer.23.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 24, 24], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: false, fused_op_id: 4}}
    _fused_op_167_output_nop_0: {type: nop, grid_loc: [7, 6], grid_size: [2, 1], inputs: [_fused_op_167], untilize_output: true, grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$gptr_q78: 0, $lptr_q78: 0, $gptr_q77: 0, $lptr_q75: 0, $gptr_q74: 0, $gptr_q73: 0, $lptr_q73: 0, $lptr_q71: 0, $gptr_q70: 0, $gptr_q71: 0, $gptr_q69: 0, $gptr_q67: 0, $gptr_q21: 0, $lptr_q46: 0, $lptr_q77: 0, $gptr_q50: 0, $gptr_q75: 0, $gptr_q25: 0, $gptr_q30: 0, $gptr_q26: 0, $lptr_q9: 0, $lptr_q10: 0, $lptr_q26: 0, $lptr_q51: 0, $lptr_q11: 0, $lptr_q30: 0, $gptr_q93: 0, $lptr_q31: 0, $lptr_q65: 0, $gptr_q33: 0, $lptr_q33: 0, $lptr_q22: 0, $gptr_q46: 0, $gptr_q34: 0, $lptr_q35: 0, $gptr_q6: 0, $lptr_q38: 0, $gptr_q19: 0, $gptr_q91: 0, $gptr_q90: 0, $lptr_q95: 0, $lptr_q74: 0, $lptr_q93: 0, $lptr_q27: 0, $lptr_q87: 0, $gptr_q27: 0, $gptr_q7: 0, $gptr_q83: 0, $gptr_q85: 0, $lptr_q55: 0, $lptr_q59: 0, $gptr_q23: 0, $lptr_q15: 0, $gptr_q87: 0, $c_microbatch_size: 64, $lptr_q90: 0, $lptr_q82: 0, $c_one: 1, $gptr_q9: 0, $c_zero: 0, $gptr_q81: 0, $lptr_q89: 0, $lptr_q53: 0, $lptr_q83: 0, $gptr_q66: 0, $lptr_q81: 0, $gptr_q86: 0, $gptr_q35: 0, $lptr_q94: 0, $lptr_q85: 0, $lptr_q29: 0, $gptr_q13: 0, $lptr_q14: 0, $gptr_q38: 0, $lptr_q41: 0, $gptr_q42: 0, $lptr_q23: 0, $gptr_q39: 0, $lptr_q42: 0, $gptr_q79: 0, $gptr_q47: 0, $gptr_q53: 0, $gptr_q54: 0, $lptr_q18: 0, $gptr_q15: 0, $lptr_q2: 0, $gptr_q2: 0, $lptr_q58: 0, $lptr_q79: 0, $gptr_q59: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q69: 0, $lptr_q91: 0, $gptr_q82: 0, $lptr_q17: 0, $lptr_q6: 0, $gptr_q37: 0, $lptr_q7: 0, $gptr_q3: 0, $lptr_q61: 0, $lptr_q34: 0, $gptr_q10: 0, $gptr_q11: 0, $gptr_q14: 0, $lptr_q70: 0, $gptr_q62: 0, $lptr_q67: 0, $gptr_q17: 0, $lptr_q39: 0, $gptr_q94: 0, $gptr_q51: 0, $gptr_q29: 0, $gptr_q95: 0, $gptr_q41: 0, $gptr_q22: 0, $gptr_q65: 0, $lptr_q43: 0, $lptr_q13: 0, $gptr_q43: 0, $lptr_q45: 0, $lptr_q25: 0, $lptr_q86: 0, $gptr_q45: 0, $lptr_q47: 0, $lptr_q49: 0, $gptr_q49: 0, $lptr_q37: 0, $gptr_q18: 0, $lptr_q54: 0, $lptr_q3: 0, $lptr_q66: 0, $gptr_q55: 0, $lptr_q57: 0, $gptr_q57: 0, $gptr_q89: 0, $lptr_q21: 0, $lptr_q50: 0, $gptr_q58: 0, $gptr_q61: 0, $gptr_q31: 0, $gptr_q63: 0, $lptr_q62: 0, $lptr_q19: 0, $lptr_q63: 0}
    - staticvar: {$gptr_q64_shadow: 0, $gptr_q60_shadow: 0, $gptr_q56_shadow: 0, $gptr_q52_shadow: 0, $gptr_q48_shadow: 0, $gptr_q40_shadow: 0, $gptr_q12_shadow: 0, $lptr_q16: 0, $lptr_q92: 0, $gptr_q60: 0, $lptr_q4: 0, $gptr_q36: 0, $lptr_q8: 0, $gptr_q20: 0, $lptr_q20: 0, $gptr_q24: 0, $lptr_q24: 0, $gptr_q52: 0, $gptr_q1: 0, $gptr_q88: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q84_shadow: 0, $gptr_q24_shadow: 0, $gptr_q84: 0, $lptr_q84: 0, $gptr_q64: 0, $gptr_q80: 0, $lptr_q48: 0, $lptr_q52: 0, $gptr_q12: 0, $lptr_q80: 0, $gptr_q28: 0, $gptr_q8_shadow: 0, $gptr_q76: 0, $gptr_q28_shadow: 0, $lptr_q88: 0, $gptr_q72: 0, $gptr_q68: 0, $lptr_q72: 0, $gptr_q92: 0, $gptr_q4: 0, $lptr_q44: 0, $lptr_q36: 0, $lptr_q12: 0, $lptr_q32: 0, $gptr_q8: 0, $lptr_q40: 0, $gptr_q32: 0, $lptr_q28: 0, $gptr_q16: 0, $gptr_q68_shadow: 0, $gptr_q40: 0, $gptr_q72_shadow: 0, $gptr_q44: 0, $gptr_q48: 0, $lptr_q56: 0, $gptr_q76_shadow: 0, $gptr_q56: 0, $gptr_q0: 0, $lptr_q60: 0, $lptr_q64: 0, $gptr_q4_shadow: 0, $gptr_q80_shadow: 0, $lptr_q68: 0, $lptr_q76: 0, $gptr_q88_shadow: 0, $gptr_q16_shadow: 0, $gptr_q32_shadow: 0, $gptr_q44_shadow: 0, $gptr_q36_shadow: 0, $gptr_q20_shadow: 0, $gptr_q1_shadow: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q88, set, $gptr_q88_shadow]
    -   varinst: [$gptr_q84, set, $gptr_q84_shadow]
    -   varinst: [$gptr_q80, set, $gptr_q80_shadow]
    -   varinst: [$gptr_q36, set, $gptr_q36_shadow]
    -   varinst: [$gptr_q32, set, $gptr_q32_shadow]
    -   varinst: [$gptr_q28, set, $gptr_q28_shadow]
    -   varinst: [$gptr_q24, set, $gptr_q24_shadow]
    -   varinst: [$gptr_q20, set, $gptr_q20_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -   varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -   varinst: [$gptr_q12, set, $gptr_q12_shadow]
    -   varinst: [$gptr_q16, set, $gptr_q16_shadow]
    -   varinst: [$gptr_q40, set, $gptr_q40_shadow]
    -   varinst: [$gptr_q44, set, $gptr_q44_shadow]
    -   varinst: [$gptr_q48, set, $gptr_q48_shadow]
    -   varinst: [$gptr_q52, set, $gptr_q52_shadow]
    -   varinst: [$gptr_q56, set, $gptr_q56_shadow]
    -   varinst: [$gptr_q60, set, $gptr_q60_shadow]
    -   varinst: [$gptr_q64, set, $gptr_q64_shadow]
    -   varinst: [$gptr_q68, set, $gptr_q68_shadow]
    -   varinst: [$gptr_q72, set, $gptr_q72_shadow]
    -   varinst: [$gptr_q76, set, $gptr_q76_shadow]
    -   allocate_queue: [e2e_add_37_0, e2e_layernorm_38.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_4_0, e2e_matmul_41_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e_add_37_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_38.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               dc.input_tensor.layernorm_38.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_37_0, e2e_layernorm_38.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_6_0]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_4_0, e2e_matmul_41_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_90_0, e2e_layernorm_91.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_6_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_11_0, e2e_matmul_94_0]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e_add_90_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_91.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               dc.input_tensor.layernorm_91.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_90_0, e2e_layernorm_91.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_13_0]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e__fused_op_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_11_0, e2e_matmul_94_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_143_0, e2e_layernorm_144.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_13_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_13_0]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_18_0, e2e_matmul_147_0]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_add_143_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_layernorm_144.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               dc.input_tensor.layernorm_144.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_143_0, e2e_layernorm_144.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_20_0]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_matmul_147_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_18_0, e2e_matmul_147_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_196_0, e2e_layernorm_197.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_20_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_20_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_25_0, e2e_matmul_200_0]
    -   execute: {graph_name: fwd_0_10_temporal_epoch_10, queue_settings: {
               e2e_add_196_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_layernorm_197.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               dc.input_tensor.layernorm_197.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_196_0, e2e_layernorm_197.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_27_0]
    -   execute: {graph_name: fwd_0_11_temporal_epoch_11, queue_settings: {
               e2e__fused_op_25_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_matmul_200_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_25_0, e2e_matmul_200_0]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_249_0, e2e_layernorm_250.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_12_temporal_epoch_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_27_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_230.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_27_0]
    -   varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_32_0, e2e_matmul_253_0]
    -   execute: {graph_name: fwd_0_13_temporal_epoch_13, queue_settings: {
               e2e_add_249_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_layernorm_250.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               dc.input_tensor.layernorm_250.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_249_0, e2e_layernorm_250.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_34_0]
    -   execute: {graph_name: fwd_0_14_temporal_epoch_14, queue_settings: {
               e2e__fused_op_32_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_matmul_253_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_32_0, e2e_matmul_253_0]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_302_0, e2e_layernorm_303.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_15_temporal_epoch_15, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_283.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_34_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_39_0, e2e_matmul_306_0]
    -   execute: {graph_name: fwd_0_16_temporal_epoch_16, queue_settings: {
               e2e_add_302_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_layernorm_303.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               dc.input_tensor.layernorm_303.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_302_0, e2e_layernorm_303.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_41_0]
    -   execute: {graph_name: fwd_0_17_temporal_epoch_17, queue_settings: {
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_matmul_306_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_39_0, e2e_matmul_306_0]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_355_0, e2e_layernorm_356.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_18_temporal_epoch_18, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_336.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_41_0]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_46_0, e2e_matmul_359_0]
    -   execute: {graph_name: fwd_0_19_temporal_epoch_19, queue_settings: {
               e2e_add_355_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_layernorm_356.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               dc.input_tensor.layernorm_356.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_355_0, e2e_layernorm_356.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q26, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_48_0]
    -   execute: {graph_name: fwd_0_20_temporal_epoch_20, queue_settings: {
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_matmul_359_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_46_0, e2e_matmul_359_0]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_408_0, e2e_layernorm_409.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_21_temporal_epoch_21, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_48_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_389.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_48_0]
    -   varinst: [$gptr_q28_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_53_0, e2e_matmul_412_0]
    -   execute: {graph_name: fwd_0_22_temporal_epoch_22, queue_settings: {
               e2e_add_408_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_layernorm_409.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               dc.input_tensor.layernorm_409.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_408_0, e2e_layernorm_409.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q30, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_55_0]
    -   execute: {graph_name: fwd_0_23_temporal_epoch_23, queue_settings: {
               e2e__fused_op_53_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_matmul_412_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_53_0, e2e_matmul_412_0]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_461_0, e2e_layernorm_462.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_24_temporal_epoch_24, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_442.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_55_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_60_0, e2e_matmul_465_0]
    -   execute: {graph_name: fwd_0_25_temporal_epoch_25, queue_settings: {
               e2e_add_461_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_layernorm_462.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               dc.input_tensor.layernorm_462.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_461_0, e2e_layernorm_462.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q34, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_62_0]
    -   execute: {graph_name: fwd_0_26_temporal_epoch_26, queue_settings: {
               e2e__fused_op_60_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_matmul_465_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_60_0, e2e_matmul_465_0]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_514_0, e2e_layernorm_515.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_27_temporal_epoch_27, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_495.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_62_0]
    -   varinst: [$gptr_q36_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_67_0, e2e_matmul_518_0]
    -   execute: {graph_name: fwd_0_28_temporal_epoch_28, queue_settings: {
               e2e_add_514_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_layernorm_515.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               dc.input_tensor.layernorm_515.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_514_0, e2e_layernorm_515.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q38, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_69_0]
    -   execute: {graph_name: fwd_0_29_temporal_epoch_29, queue_settings: {
               e2e__fused_op_67_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               e2e_matmul_518_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_67_0, e2e_matmul_518_0]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_567_0, e2e_layernorm_568.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_30_temporal_epoch_30, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e__fused_op_69_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_548.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_69_0]
    -   varinst: [$gptr_q40_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_74_0, e2e_matmul_571_0]
    -   execute: {graph_name: fwd_0_31_temporal_epoch_31, queue_settings: {
               e2e_add_567_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e_layernorm_568.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               dc.input_tensor.layernorm_568.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_567_0, e2e_layernorm_568.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q42, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_76_0]
    -   execute: {graph_name: fwd_0_32_temporal_epoch_32, queue_settings: {
               e2e__fused_op_74_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_matmul_571_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_74_0, e2e_matmul_571_0]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_620_0, e2e_layernorm_621.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_33_temporal_epoch_33, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_601.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_76_0]
    -   varinst: [$gptr_q44_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q45, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_81_0, e2e_matmul_624_0]
    -   execute: {graph_name: fwd_0_34_temporal_epoch_34, queue_settings: {
               e2e_add_620_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_layernorm_621.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               dc.input_tensor.layernorm_621.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_620_0, e2e_layernorm_621.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_83_0]
    -   execute: {graph_name: fwd_0_35_temporal_epoch_35, queue_settings: {
               e2e__fused_op_81_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               e2e_matmul_624_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_81_0, e2e_matmul_624_0]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_673_0, e2e_layernorm_674.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_36_temporal_epoch_36, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               e2e__fused_op_83_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_652: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_654.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_83_0]
    -   varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_88_0, e2e_matmul_677_0]
    -   execute: {graph_name: fwd_0_37_temporal_epoch_37, queue_settings: {
               e2e_add_673_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e_layernorm_674.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               dc.input_tensor.layernorm_674.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_673_0, e2e_layernorm_674.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q50, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_90_0]
    -   execute: {graph_name: fwd_0_38_temporal_epoch_38, queue_settings: {
               e2e__fused_op_88_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               e2e_matmul_677_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_88_0, e2e_matmul_677_0]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_726_0, e2e_layernorm_727.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_39_temporal_epoch_39, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               e2e__fused_op_90_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_707.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_90_0]
    -   varinst: [$gptr_q52_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q53, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q52, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q53, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_95_0, e2e_matmul_730_0]
    -   execute: {graph_name: fwd_0_40_temporal_epoch_40, queue_settings: {
               e2e_add_726_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               e2e_layernorm_727.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               dc.input_tensor.layernorm_727.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_726_0, e2e_layernorm_727.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q54, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q54, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_97_0]
    -   execute: {graph_name: fwd_0_41_temporal_epoch_41, queue_settings: {
               e2e__fused_op_95_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e_matmul_730_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_95_0, e2e_matmul_730_0]
    -   varinst: [$gptr_q55, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q55, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_779_0, e2e_layernorm_780.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_42_temporal_epoch_42, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               e2e__fused_op_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_758: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_760.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_97_0]
    -   varinst: [$gptr_q56_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q57, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q56, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q57, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_102_0, e2e_matmul_783_0]
    -   execute: {graph_name: fwd_0_43_temporal_epoch_43, queue_settings: {
               e2e_add_779_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               e2e_layernorm_780.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               dc.input_tensor.layernorm_780.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_779_0, e2e_layernorm_780.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q58, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q58, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_104_0]
    -   execute: {graph_name: fwd_0_44_temporal_epoch_44, queue_settings: {
               e2e__fused_op_102_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               e2e_matmul_783_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_102_0, e2e_matmul_783_0]
    -   varinst: [$gptr_q59, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q59, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_832_0, e2e_layernorm_833.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_45_temporal_epoch_45, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               e2e__fused_op_104_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_813.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_104_0]
    -   varinst: [$gptr_q60_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q61, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q60, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q61, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_109_0, e2e_matmul_836_0]
    -   execute: {graph_name: fwd_0_46_temporal_epoch_46, queue_settings: {
               e2e_add_832_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               e2e_layernorm_833.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               dc.input_tensor.layernorm_833.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_832_0, e2e_layernorm_833.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q62, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q62, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_111_0]
    -   execute: {graph_name: fwd_0_47_temporal_epoch_47, queue_settings: {
               e2e__fused_op_109_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               e2e_matmul_836_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_109_0, e2e_matmul_836_0]
    -   varinst: [$gptr_q63, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q63, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_885_0, e2e_layernorm_886.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_48_temporal_epoch_48, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               e2e__fused_op_111_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_864: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_866.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_111_0]
    -   varinst: [$gptr_q64_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q65, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q64, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q65, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_116_0, e2e_matmul_889_0]
    -   execute: {graph_name: fwd_0_49_temporal_epoch_49, queue_settings: {
               e2e_add_885_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               e2e_layernorm_886.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               dc.input_tensor.layernorm_886.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_885_0, e2e_layernorm_886.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q66, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q66, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_118_0]
    -   execute: {graph_name: fwd_0_50_temporal_epoch_50, queue_settings: {
               e2e__fused_op_116_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               e2e_matmul_889_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_116_0, e2e_matmul_889_0]
    -   varinst: [$gptr_q67, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q67, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_938_0, e2e_layernorm_939.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_51_temporal_epoch_51, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               e2e__fused_op_118_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_919.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_118_0]
    -   varinst: [$gptr_q68_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q69, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q68, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q69, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_123_0, e2e_matmul_942_0]
    -   execute: {graph_name: fwd_0_52_temporal_epoch_52, queue_settings: {
               e2e_add_938_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               e2e_layernorm_939.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               dc.input_tensor.layernorm_939.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_938_0, e2e_layernorm_939.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q70, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q70, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_125_0]
    -   execute: {graph_name: fwd_0_53_temporal_epoch_53, queue_settings: {
               e2e__fused_op_123_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               e2e_matmul_942_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_123_0, e2e_matmul_942_0]
    -   varinst: [$gptr_q71, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q71, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_991_0, e2e_layernorm_992.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_54_temporal_epoch_54, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72},
               e2e__fused_op_125_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q73, rd_ptr_global: $gptr_q73},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_970: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_972.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_125_0]
    -   varinst: [$gptr_q72_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q73, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q72, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q73, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_130_0, e2e_matmul_995_0]
    -   execute: {graph_name: fwd_0_55_temporal_epoch_55, queue_settings: {
               e2e_add_991_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74},
               e2e_layernorm_992.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74},
               dc.input_tensor.layernorm_992.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_991_0, e2e_layernorm_992.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q74, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q74, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_132_0]
    -   execute: {graph_name: fwd_0_56_temporal_epoch_56, queue_settings: {
               e2e__fused_op_130_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75},
               e2e_matmul_995_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_130_0, e2e_matmul_995_0]
    -   varinst: [$gptr_q75, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q75, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_1044_0, e2e_layernorm_1045.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_57_temporal_epoch_57, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76},
               e2e__fused_op_132_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1025.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_132_0]
    -   varinst: [$gptr_q76_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q77, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q76, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q77, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_137_0, e2e_matmul_1048_0]
    -   execute: {graph_name: fwd_0_58_temporal_epoch_58, queue_settings: {
               e2e_add_1044_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78},
               e2e_layernorm_1045.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78},
               dc.input_tensor.layernorm_1045.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_1044_0, e2e_layernorm_1045.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q78, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q78, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_139_0]
    -   execute: {graph_name: fwd_0_59_temporal_epoch_59, queue_settings: {
               e2e__fused_op_137_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q79, rd_ptr_global: $gptr_q79},
               e2e_matmul_1048_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q79, rd_ptr_global: $gptr_q79},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_137_0, e2e_matmul_1048_0]
    -   varinst: [$gptr_q79, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q79, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_1097_0, e2e_layernorm_1098.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_60_temporal_epoch_60, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q80, rd_ptr_global: $gptr_q80},
               e2e__fused_op_139_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q81, rd_ptr_global: $gptr_q81},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1076: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1078.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_139_0]
    -   varinst: [$gptr_q80_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q81, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q80, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q81, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_144_0, e2e_matmul_1101_0]
    -   execute: {graph_name: fwd_0_61_temporal_epoch_61, queue_settings: {
               e2e_add_1097_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82},
               e2e_layernorm_1098.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82},
               dc.input_tensor.layernorm_1098.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_1097_0, e2e_layernorm_1098.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q82, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q82, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_146_0]
    -   execute: {graph_name: fwd_0_62_temporal_epoch_62, queue_settings: {
               e2e__fused_op_144_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83},
               e2e_matmul_1101_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_144_0, e2e_matmul_1101_0]
    -   varinst: [$gptr_q83, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q83, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_1150_0, e2e_layernorm_1151.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_63_temporal_epoch_63, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q84, rd_ptr_global: $gptr_q84},
               e2e__fused_op_146_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q85, rd_ptr_global: $gptr_q85},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1131.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_146_0]
    -   varinst: [$gptr_q84_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q85, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q84, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q85, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_151_0, e2e_matmul_1154_0]
    -   execute: {graph_name: fwd_0_64_temporal_epoch_64, queue_settings: {
               e2e_add_1150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q86, rd_ptr_global: $gptr_q86},
               e2e_layernorm_1151.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q86, rd_ptr_global: $gptr_q86},
               dc.input_tensor.layernorm_1151.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_1150_0, e2e_layernorm_1151.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q86, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q86, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_153_0]
    -   execute: {graph_name: fwd_0_65_temporal_epoch_65, queue_settings: {
               e2e__fused_op_151_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q87, rd_ptr_global: $gptr_q87},
               e2e_matmul_1154_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q87, rd_ptr_global: $gptr_q87},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_151_0, e2e_matmul_1154_0]
    -   varinst: [$gptr_q87, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q87, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_1203_0, e2e_layernorm_1204.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_66_temporal_epoch_66, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q88, rd_ptr_global: $gptr_q88},
               e2e__fused_op_153_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q89, rd_ptr_global: $gptr_q89},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1182: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1184.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_153_0]
    -   varinst: [$gptr_q88_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q89, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q88, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q89, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_158_0, e2e_matmul_1207_0]
    -   execute: {graph_name: fwd_0_67_temporal_epoch_67, queue_settings: {
               e2e_add_1203_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q90, rd_ptr_global: $gptr_q90},
               e2e_layernorm_1204.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q90, rd_ptr_global: $gptr_q90},
               dc.input_tensor.layernorm_1204.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_1203_0, e2e_layernorm_1204.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q90, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q90, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_160_0]
    -   execute: {graph_name: fwd_0_68_temporal_epoch_68, queue_settings: {
               e2e__fused_op_158_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q91, rd_ptr_global: $gptr_q91},
               e2e_matmul_1207_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q91, rd_ptr_global: $gptr_q91},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_158_0, e2e_matmul_1207_0]
    -   varinst: [$gptr_q91, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q91, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_add_1256_0, e2e_layernorm_1257.dc.reduce_sum.0.lc1_0]
    -   execute: {graph_name: fwd_0_69_temporal_epoch_69, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q92, rd_ptr_global: $gptr_q92},
               e2e__fused_op_160_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q93, rd_ptr_global: $gptr_q93},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1237.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_160_0]
    -   varinst: [$gptr_q92, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q93, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q92, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q93, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_165_0, e2e_matmul_1260_0]
    -   execute: {graph_name: fwd_0_70_temporal_epoch_70, queue_settings: {
               e2e_add_1256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q94, rd_ptr_global: $gptr_q94},
               e2e_layernorm_1257.dc.reduce_sum.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q94, rd_ptr_global: $gptr_q94},
               dc.input_tensor.layernorm_1257.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_1256_0, e2e_layernorm_1257.dc.reduce_sum.0.lc1_0]
    -   varinst: [$gptr_q94, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q94, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_71_temporal_epoch_71, queue_settings: {
               e2e__fused_op_165_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q95, rd_ptr_global: $gptr_q95},
               e2e_matmul_1260_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q95, rd_ptr_global: $gptr_q95},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_165_0, e2e_matmul_1260_0]
    -   varinst: [$gptr_q95, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q95, incwrap, $c_microbatch_size, 128]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16.0: { type: multiply, inputs: [input0, input1], mblock: [3, 3], ublock: [2, 4], output: dest}
        - add_17.0: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [3, 3], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1.0: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.2.0: { type: exp, inputs: [dest], mblock: [3, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.add.5.0: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.6.0: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: intermed0}
        - softmax_18.dc.multiply.7.0: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 12}, tile_broadcast: c], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.2.0: { type: multiply, inputs: [input0, input1], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.subtract.3.0: { type: subtract, inputs: [input2, dest], mblock: [3, 8], ublock: [2, 4], output: output}
  4: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.7.0: { type: multiply, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9.0: { type: add, inputs: [dest, input2], mblock: [3, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10.0: { type: sqrt, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11.0: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: intermed0}
        - layernorm_38.dc.multiply.12.0: { type: multiply, inputs: [input3, intermed0], input_1_tms: [broadcast: {c: 32}, tile_broadcast: c], pop: [intermed0], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.13.0: { type: multiply, inputs: [dest, input4], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14.0: { type: add, inputs: [dest, input5], mblock: [3, 8], ublock: [2, 4], output: output}

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.08
    test-group: "perf_infra_wormhole_b0_silicon_push"
    test-name: "bert_large_inference_hifi3_fp16b"