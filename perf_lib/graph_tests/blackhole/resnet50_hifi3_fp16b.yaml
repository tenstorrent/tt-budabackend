# git checkout 531929601
# pytest resnet

devices:
  arch: wormhole_b0

queues:

  # input
  pixel_values:                                                                                            {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 392], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x0]]}

  # output
  resnet50.output_add_759:                                                                                 {input: add_759, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x6388020]]}

  # parameter
  resnet.embedder.embedder.convolution.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d52b60], [1, 0x70e8ba0], [1, 0x42b403c0], [2, 0x555b840]]}
  resnet.embedder.embedder.convolution.weight_fork_clone1919:                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42bdac00], [3, 0x8c2ed60], [3, 0x41dec320], [4, 0xc004c80]]}
  resnet.embedder.embedder.convolution.weight_fork_clone1921:                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41de6d20], [4, 0xbfff680], [4, 0x41dfdd40], [5, 0x61f09e0]]}
  resnet.embedder.embedder.convolution.weight_fork_clone1923:                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c2b800], [3, 0x41de8dc0], [4, 0xc001720], [4, 0x41dffde0]]}
  resnet.encoder.stages.0.layers.0.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d4bda0], [1, 0x70e1de0]]}
  resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1327:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41df38a0]]}
  resnet.encoder.stages.0.layers.0.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x555e940]]}
  resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1325:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5775ee0]]}
  resnet.encoder.stages.0.layers.0.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61f9ba0], [5, 0x42084ea0]]}
  resnet.encoder.stages.0.layers.0.shortcut.convolution.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c321c0], [3, 0x41def780]]}
  resnet.encoder.stages.0.layers.1.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [8, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b41420]]}
  resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1374:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d43b60]]}
  resnet.encoder.stages.0.layers.1.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61e3780]]}
  resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1372:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41dd9ee0]]}
  resnet.encoder.stages.0.layers.1.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5548fc0], [2, 0x42bb82a0]]}
  resnet.encoder.stages.0.layers.2.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [8, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d3b940]]}
  resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1408:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41df4a60]]}
  resnet.encoder.stages.0.layers.2.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c24620]]}
  resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1406:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b33900]]}
  resnet.encoder.stages.0.layers.2.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c235c0], [3, 0x41de5cc0], [4, 0xbffe620], [4, 0x41dfcce0], [5, 0x61ef980], [5, 0x4207b4a0], [0, 0x576d420], [0, 0x42d4ad40]]}
  resnet.encoder.stages.1.layers.0.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [8, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70d7b00], [1, 0x42b2a680]]}
  resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1442:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbfe6000]]}
  resnet.encoder.stages.1.layers.0.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42bbc3c0]]}
  resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1440:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b83780]]}
  resnet.encoder.stages.1.layers.0.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x57a4f20]]}
  resnet.encoder.stages.1.layers.0.shortcut.convolution.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x62319e0], [5, 0x4208e120]]}
  resnet.encoder.stages.1.layers.1.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42c4eba0]]}
  resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1491:                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7126b60], [1, 0x42b7c280], [2, 0x55c0c20], [2, 0x42c48a00]]}
  resnet.encoder.stages.1.layers.1.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41e7fac0]]}
  resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1489:                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55d1080], [2, 0x42c714e0], [3, 0x8c6d980], [3, 0x41e980e0]]}
  resnet.encoder.stages.1.layers.1.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc02eae0]]}
  resnet.encoder.stages.1.layers.2.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c4d160]]}
  resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1525:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x710d500]]}
  resnet.encoder.stages.1.layers.2.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x62162e0]]}
  resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1523:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41df9a40]]}
  resnet.encoder.stages.1.layers.2.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5564ae0]]}
  resnet.encoder.stages.1.layers.3.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70ebca0]]}
  resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1559:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61fdcc0]]}
  resnet.encoder.stages.1.layers.3.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc0133e0]]}
  resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1557:                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c40f20], [3, 0x41e73880], [4, 0xc00d240], [4, 0x41e45320]]}
  resnet.encoder.stages.1.layers.3.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b5ab20]]}
  resnet.encoder.stages.2.layers.0.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5781320], [0, 0x42db9980]]}
  resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1593:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41e12060]]}
  resnet.encoder.stages.2.layers.0.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42d55080]]}
  resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1591:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x427ef540]]}
  resnet.encoder.stages.2.layers.0.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 8], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41d18a20], [0, 0x53c60a0]]}
  resnet.encoder.stages.2.layers.0.shortcut.convolution.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x427ae520], [2, 0x51d0b00], [2, 0x42780a00], [3, 0x87a6860]]}
  resnet.encoder.stages.2.layers.1.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41abbf00], [5, 0x5f2c7e0]]}
  resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1642:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4277d900], [2, 0x519fee0]]}
  resnet.encoder.stages.2.layers.1.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5214c00], [2, 0x427cbce0]]}
  resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1640:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x429c3ee0]]}
  resnet.encoder.stages.2.layers.1.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41b25960], [5, 0x5f96240]]}
  resnet.encoder.stages.2.layers.2.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87efaa0], [3, 0x41a468c0]]}
  resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1676:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x429626c0]]}
  resnet.encoder.stages.2.layers.2.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x419e50a0]]}
  resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1674:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4271b0a0]]}
  resnet.encoder.stages.2.layers.2.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41c71080], [0, 0x5363800]]}
  resnet.encoder.stages.2.layers.3.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41943860], [4, 0xbc0a180]]}
  resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1710:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8722140]]}
  resnet.encoder.stages.2.layers.3.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5eb9140]]}
  resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1708:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x418e2040]]}
  resnet.encoder.stages.2.layers.3.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x428e1060], [1, 0x6cf1160]]}
  resnet.encoder.stages.2.layers.4.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x419a0fa0], [4, 0xbcac9c0]]}
  resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1744:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4271c100]]}
  resnet.encoder.stages.2.layers.4.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41cb20a0]]}
  resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1742:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [8, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbc4b1a0]]}
  resnet.encoder.stages.2.layers.4.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41d56e40], [4, 0xbf61ac0], [4, 0x41dd3200], [5, 0x61bfca0]]}
  resnet.encoder.stages.2.layers.5.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [8, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbcf2540], [4, 0x41b05140], [5, 0x5f75a20], [5, 0x41d59a40]]}
  resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1778:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x549e740], [2, 0x42addc40]]}
  resnet.encoder.stages.2.layers.5.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42aad020], [3, 0x8b45ea0]]}
  resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1776:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41f68b40], [0, 0x565aac0], [0, 0x42c2c5c0], [1, 0x6fc6c00]]}
  resnet.encoder.stages.2.layers.5.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42a8c800], [3, 0x8b25680], [3, 0x41cfb100], [4, 0xbf073c0]]}
  resnet.encoder.stages.3.layers.0.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41d1b920], [4, 0xbf27be0], [4, 0x41d78b20], [5, 0x61655c0], [5, 0x41f81160], [0, 0x56730e0], [0, 0x42c44be0], [1, 0x6fdf220]]}
  resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1812:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42b55a20], [3, 0x8bbecc0], [3, 0x41d77660], [4, 0xbf822e0]]}
  resnet.encoder.stages.3.layers.0.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x420156c0], [0, 0x5707620], [0, 0x42cd90c0], [1, 0x70731e0]]}
  resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1810:                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41da25e0], [5, 0x618f080], [5, 0x41fe4aa0], [0, 0x56d6a00], [0, 0x42ca84a0], [1, 0x70425c0], [1, 0x42af5920], [2, 0x55154e0]]}
  resnet.encoder.stages.3.layers.0.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [4, 8], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41fa3a80], [0, 0x56959e0], [0, 0x42c67480], [1, 0x70015a0], [1, 0x42ab4900], [2, 0x54d44c0], [2, 0x42b139c0], [3, 0x8b7cc60]]}
  resnet.encoder.stages.3.layers.0.shortcut.convolution.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41e84520], [0, 0x55d6a20], [0, 0x42ba56a0], [1, 0x6f0fea0], [1, 0x429f5360], [2, 0x53e5fa0], [2, 0x429d9bc0], [3, 0x8a72a40]]}
  resnet.encoder.stages.3.layers.1.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8939ca0], [3, 0x41b90ac0], [4, 0xbdbd1a0], [4, 0x41c0f120], [5, 0x602e880], [5, 0x41dedfa0], [0, 0x54ce8a0], [0, 0x42ace120]]}
  resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1861:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x52c88a0], [2, 0x4287f5c0], [3, 0x88d8480], [3, 0x41b2f2a0]]}
  resnet.encoder.stages.3.layers.1.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8830ac0], [3, 0x41a878e0], [4, 0xbd147a0], [4, 0x41b66980]]}
  resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1859:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x546afc0], [0, 0x42a6a840], [1, 0x6e11f60], [1, 0x428d6e80]]}
  resnet.encoder.stages.3.layers.1.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [4, 8], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6dd0f40], [1, 0x42895e60], [2, 0x5286840], [2, 0x4283d920], [3, 0x8896400], [3, 0x41aed220], [4, 0xbd7a0e0], [4, 0x41bcc2c0]]}
  resnet.encoder.stages.3.layers.2.layer.0.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fd7260], [5, 0x41d96980], [0, 0x5425e80], [0, 0x42a25700], [1, 0x6d8ff20], [1, 0x42854e40], [2, 0x5245820], [2, 0x427fc900]]}
  resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1895:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [6, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f91ec0], [1, 0x42a77380], [2, 0x5467fc0], [2, 0x42a5bbe0], [3, 0x8af4a60], [3, 0x41cca4e0], [4, 0xbed67a0], [4, 0x41d2b7e0]]}
  resnet.encoder.stages.3.layers.2.layer.1.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbe74f80], [4, 0x41cc9fc0], [5, 0x60e6640], [5, 0x41f06540]]}
  resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1893:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [6, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x429c4740], [2, 0x53b5380], [2, 0x429a8fa0], [3, 0x8a41e20], [3, 0x41c98c40], [4, 0xbe44360], [4, 0x41c993a0], [5, 0x60b5a20]]}
  resnet.encoder.stages.3.layers.2.layer.2.convolution.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 2], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42967f80], [3, 0x8a00e00], [3, 0x41c57c20], [4, 0xbe03340], [4, 0x41c58380], [5, 0x6074a00], [5, 0x41e43500], [0, 0x5595a00]]}
  classifier.1.weight:                                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [4, 1], ublock: [16, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x550f8c0], [0, 0x42b0f140], [1, 0x6e79940], [1, 0x4293e600], [2, 0x532f240], [2, 0x428e5f60], [3, 0x897ede0], [3, 0x41bd5c00]]}
  classifier.1.bias:                                                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41e2efc0]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x42bdd120], [3, 0x8c31280], [3, 0x41dee840], [4, 0xc0071a0], [4, 0x41e05840], [5, 0x61f8c60], [5, 0x42083f60]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x5693900], [0, 0x42c65400], [1, 0x6fffa40], [1, 0x42aaf7a0], [2, 0x54cf360], [2, 0x42b0e860], [3, 0x8b77b00]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x42bdbc60], [3, 0x8c2fdc0], [3, 0x41ded380], [4, 0xc005ce0], [4, 0x41e04380], [5, 0x61f77a0], [5, 0x42082aa0], [0, 0x5774a20]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41e03340], [5, 0x61f6760], [5, 0x42081a60], [0, 0x57739e0], [0, 0x42d51b20], [1, 0x70e7b60], [1, 0x42b3f380], [2, 0x555a800]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x61f5820], [5, 0x42080b20], [0, 0x5772aa0], [0, 0x42d50be0], [1, 0x70e6c20], [1, 0x42b3e440], [2, 0x55598c0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42b3d400], [2, 0x5558880], [2, 0x42bd9bc0], [3, 0x8c2dd20], [3, 0x41deb2e0], [4, 0xc003c40], [4, 0x41e02300]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x8c2c860], [3, 0x41de9e20], [4, 0xc002780], [4, 0x41e00e40], [5, 0x61f4360], [5, 0x4207f660], [0, 0x57715e0], [0, 0x42d4f720]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x61f3320], [5, 0x4207e620], [0, 0x57705a0], [0, 0x42d4e6e0], [1, 0x70e4720], [1, 0x42b3c3c0], [2, 0x5557840], [2, 0x42bd8b80]]}
  input_1_add_1_fork_clone1120:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42bd7b20]]}
  lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 18], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x61f1a40], [5, 0x4207cd40], [0, 0x576ecc0], [0, 0x42d4ce00], [1, 0x70e2e40], [1, 0x42b3aae0], [2, 0x5555f60]]}
  lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                                   {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42b39aa0], [2, 0x5554f20], [2, 0x42bd6ae0], [3, 0x8c2a7c0], [3, 0x41de7d80], [4, 0xc0006e0], [4, 0x41dfeda0]]}
  input_1_add_16_fork_clone1163:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4207c500], [0, 0x576e480]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x70e5760]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xc00a180]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x8c362e0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x42be0100]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42b49640]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x70eac60]]}
  input_1_add_30_fork_clone1095:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x577c080]]}
  input_1_add_44_fork_clone1005:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc0080e0], [4, 0x41e06780]]}
  input_1_add_57_fork_clone1010:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x555c8a0], [2, 0x42bde060]]}
  input_1_add_72_fork_clone1130:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70e9c00]]}
  lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x42d53bc0]]}
  lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x42d49d00]]}
  lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x576af00]]}
  lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x42079400]]}
  lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x41dfac00]]}
  lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbfe4fc0]]}
  input_1_add_86_fork_clone1047:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41dd8e80]]}
  input_1_add_100_fork_clone952:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70d5a60], [1, 0x42b285e0]]}
  input_1_add_115_fork_clone1083:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5769ea0]]}
  lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x42077f40]]}
  lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x61e2740]]}
  lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbfe3b00]]}
  lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x8c21540]]}
  lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x42bd5a40]]}
  lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x5553ee0]]}
  input_1_add_129_fork_clone993:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70e0d80]]}
  input_1_add_143_fork_clone899:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70dfd20], [1, 0x42b328a0], [2, 0x5552e80], [2, 0x42bd49e0]]}
  input_1_add_158_fork_clone965:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4207a440], [0, 0x576c3c0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 70], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x61e9920]]}
  lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41dfbca0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 67], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x41de0080]]}
  lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x8c22580]]}
  lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 68], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x554d0e0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x42db8940]]}
  input_1_add_172:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x712e060]]}
  input_1_add_172_fork_clone871:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42dde660]]}
  input_1_add_186_fork_clone769:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x420afca0]]}
  lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 23], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x6253240]]}
  lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41e4d540]]}
  input_1_add_199_fork_clone774:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c49040], [3, 0x41e7b9a0]]}
  input_1_add_214_fork_clone913:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55c6dc0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x420ae940], [0, 0x57a3bc0], [0, 0x42ddd300], [1, 0x712cd00], [1, 0x42b82420]]}
  lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x8c48000], [3, 0x41e7a960], [4, 0xc02ba00], [4, 0x41e4c500], [5, 0x6252200]]}
  lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x42ddb1e0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x57a2b80]]}
  lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x420b7ec0], [0, 0x57c5740], [0, 0x42e11300], [1, 0x7160d00], [1, 0x42b9bda0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xc04f300], [4, 0x41e81220], [5, 0x6287ea0], [5, 0x420b8e00], [0, 0x57c6680]]}
  input_1_add_228:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6255200]]}
  input_1_add_228_fork_clone815:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41e4e580]]}
  input_1_add_242_fork_clone714:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55c8e60]]}
  input_1_add_257_fork_clone859:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc02ca40]]}
  lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x42c6f3c0]]}
  lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x710c4c0]]}
  lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x577f200]]}
  lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4208a000]]}
  lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x41e3b4c0]]}
  lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xc00b1c0]]}
  input_1_add_271:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41e08820]]}
  input_1_add_271_fork_clone757:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42be1140]]}
  input_1_add_285_fork_clone663:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b4a6e0]]}
  input_1_add_300_fork_clone803:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42db68a0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x577d0e0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x42088fc0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x8c377a0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41e4b4c0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42b7b340], [2, 0x55bfce0], [2, 0x42c47ac0], [3, 0x8c470c0], [3, 0x41e79a20]]}
  lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x62309a0], [5, 0x4208d0e0], [0, 0x57a1b40], [0, 0x42dda1a0], [1, 0x7125b20]]}
  input_1_add_314:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42c14e20]]}
  input_1_add_314_fork_clone702:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x558d040]]}
  input_1_add_328_fork_clone610:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b52900]]}
  input_1_add_343_fork_clone675:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x622e900], [5, 0x4208b040]]}
  lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 94], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x41e3d1c0]]}
  lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xc00c200]]}
  lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 86], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x8c398c0]]}
  lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x42c13de0]]}
  lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 91], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5585300]]}
  lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x5211b20]]}
  input_1_add_357:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d71780]]}
  input_1_add_357_fork_clone580:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42945fa0]]}
  input_1_add_371_fork_clone466:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 8], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41afcf20], [5, 0x5f6d800]]}
  lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 31], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbcefa80]]}
  lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x42940e40]]}
  input_1_add_384_fork_clone471:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41d14900], [0, 0x53c1f80], [0, 0x42941e80], [1, 0x6d6d660]]}
  input_1_add_399_fork_clone624:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x419e1fc0], [4, 0xbced9e0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x87a41c0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x4277f9c0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x6d6afc0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x419e4060]]}
  lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42853400]]}
  lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x6d8eee0]]}
  input_1_add_413:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5409760]]}
  input_1_add_413_fork_clone520:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41d7a260]]}
  input_1_add_427_fork_clone401:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x427c1a20], [3, 0x87e7880]]}
  input_1_add_442_fork_clone568:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5212b60], [2, 0x427c9c40]]}
  lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42850d60]]}
  lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x6d8dea0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x54070c0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41d138c0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbd12d60]]}
  lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x519c800]]}
  input_1_add_456:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d32180]]}
  input_1_add_456_fork_clone454:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42922080]]}
  input_1_add_470_fork_clone344:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41aaaa80], [5, 0x5f1a960]]}
  input_1_add_485_fork_clone508:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4271a060], [3, 0x8783960]]}
  lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x519a160]]}
  lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4271a060]]}
  lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x5361160]]}
  lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41c70040]]}
  lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x41aa9040]]}
  lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbc09140]]}
  input_1_add_499:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d4e8a0]]}
  input_1_add_499_fork_clone389:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53a5860]]}
  input_1_add_513_fork_clone291:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41ab3ce0], [5, 0x5f245c0]]}
  input_1_add_528_fork_clone442:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4277d920], [3, 0x87a2120]]}
  lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x519d840]]}
  lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4277c8c0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x4293e7a0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x53a4820]]}
  lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x5f22b80]]}
  lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41ab2ca0]]}
  input_1_add_542:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41984880]]}
  input_1_add_542_fork_clone332:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8785a00]]}
  input_1_add_556_fork_clone240:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41d3e7e0], [4, 0xbf49460], [4, 0x41d9a3a0], [5, 0x6186e40]]}
  input_1_add_571_fork_clone377:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbf48400], [4, 0x41d99340], [5, 0x6185de0], [5, 0x41fa1980]]}
  lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x41d3c140]]}
  lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x8b76ac0]]}
  lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42aad100]]}
  lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x5659a80]]}
  lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x549cd00]]}
  lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42aac0c0]]}
  input_1_add_585:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6148ea0]]}
  input_1_add_585_fork_clone279:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41d5c400]]}
  input_1_add_599_fork_clone190:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42c284a0], [1, 0x6fc2ae0], [1, 0x42aa7fa0], [2, 0x5498be0]]}
  input_1_add_614_fork_clone252:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42076ee0], [0, 0x5768e40], [0, 0x42d3a8e0], [1, 0x70d4a00], [1, 0x42b27580], [2, 0x5547f60], [2, 0x42bb7240], [3, 0x8c204e0]]}
  lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 25], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x61e04c0]]}
  lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41df3a20]]}
  lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 22], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5546100]]}
  lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x42b26540]]}
  lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x41fa29e0], [0, 0x5694940]]}
  lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x42b549e0], [3, 0x8bbdc80]]}
  input_1_add_628:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbf516a0]]}
  input_1_add_628_fork_clone163:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41d46a20]]}
  input_1_add_642_fork_clone82:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42ab07e0], [2, 0x54d03a0], [2, 0x42b0f8a0], [3, 0x8b78b40], [3, 0x41d42900], [4, 0xbf4d580], [4, 0x41d9e4c0], [5, 0x618af60]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7000a80]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x42c66440]]}
  input_1_add_655_fork_clone87:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e75820], [1, 0x4293a4e0], [2, 0x532b120], [2, 0x428e1e40], [3, 0x897acc0], [3, 0x41bd1ae0], [4, 0xbdfe1c0], [4, 0x41c50140]]}
  input_1_add_670_fork_clone203:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x602d820], [5, 0x41decf40], [0, 0x54cd840], [0, 0x42acd0c0], [1, 0x6e747c0], [1, 0x42939480], [2, 0x532a0c0], [2, 0x428e0de0]]}
  lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x41c0e340]]}
  lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbdbc160]]}
  lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x429386a0]]}
  lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x6e73780]]}
  lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x4287e940]]}
  lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x52c7860]]}
  input_1_add_684:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41ddbac0]]}
  input_1_add_684_fork_clone116:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x601c3a0]]}
  input_1_add_698_fork_clone47:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88922e0], [3, 0x41ae9100], [4, 0xbd75fc0], [4, 0x41bc81a0], [5, 0x6018280], [5, 0x41dd79a0], [0, 0x5466ea0], [0, 0x42a66720]]}
  input_1_add_713_fork_clone151:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88d7420], [3, 0x41b2e240], [4, 0xbdbb100], [4, 0x41c0d2e0], [5, 0x602c7c0], [5, 0x41debee0], [0, 0x54cc7e0], [0, 0x42acc060]]}
  lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x41f67d60]]}
  lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x6147e60]]}
  lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x42c276c0]]}
  lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x5658a40]]}
  lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x41cc9860]]}
  lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x606f8a0]]}
  input_1_add_727:                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6effa80]]}
  input_1_add_727_fork_clone70:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42b95280]]}
  input_1_add_741_fork_clone24:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41c54260], [5, 0x60708e0], [5, 0x41e3f3e0], [0, 0x55918e0], [0, 0x42b91160], [1, 0x6efb960], [1, 0x429c0620], [2, 0x53b1260]]}
  lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0:                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbe022e0]]}

  # epoch_to_epoch
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0:                                                     {input: conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 28, mblock: [1, 1], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40e38020], [0, 0x4529140], [0, 0x41aa9040], [1, 0x5eb9140], [1, 0x418e2040], [2, 0x4362140], [2, 0x418e2040]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0:                                                     {input: conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, type: queue, entries: 64, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40c71020], [2, 0x36f1120], [2, 0x40c71020], [3, 0x7ab1120], [3, 0x40c71020], [4, 0xaf98120], [4, 0x40e38020], [5, 0x5248120]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0:                                                     {input: conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 28, mblock: [1, 1], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa160100], [4, 0x40000000], [5, 0x4410100], [5, 0x40000000], [0, 0x36f1120], [0, 0x40c71020], [1, 0x5081120]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0:                                                     {input: conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11, type: queue, entries: 64, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2a80100], [0, 0x40000000], [1, 0x4410100], [1, 0x40000000], [2, 0x2a80100], [2, 0x40000000], [3, 0x6e40100], [3, 0x40000000]]}
  e2e__fused_op_3_0:                                                                                       {input: _fused_op_3, type: queue, entries: 64, grid_size: [1, 1], t: 14, mblock: [7, 1], ublock: [1, 2], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42e12240]]}
  e2e_conv2d_142.dc.matmul.8_0:                                                                            {input: conv2d_142.dc.matmul.8, type: queue, entries: 64, grid_size: [7, 4], t: 2, mblock: [7, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x938fb40], [3, 0x425ba2a0], [4, 0xc76c360], [4, 0x4259e280], [5, 0x69a4f00], [5, 0x427d5e60], [0, 0x5ee36e0], [0, 0x44e10280], [1, 0xaa41c80], [1, 0x4647cd20], [2, 0x640f260], [2, 0x43aaf6c0], [3, 0x971db60], [3, 0x429482c0], [4, 0xcafa380], [4, 0x4292c2a0], [5, 0x6d32f20], [5, 0x42b63e80], [0, 0x6271700], [0, 0x4519e2a0], [1, 0xadcfca0], [1, 0x4680ad40], [2, 0x679d280], [2, 0x43e3d6e0], [3, 0x9aabb80], [3, 0x42cd62e0], [4, 0xce883a0], [4, 0x42cba2c0]]}
  e2e_buffer_1_add_112_add_155_0:                                                                          {input: buffer_1_add_112_add_155, type: queue, entries: 64, grid_size: [1, 2], t: 14, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7161c40], [1, 0x42b9cce0]]}
  e2e_buffer_0_add_112_add_155_0:                                                                          {input: buffer_0_add_112_add_155, type: queue, entries: 64, grid_size: [7, 2], t: 2, mblock: [7, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55d7220], [2, 0x42c77680], [3, 0x8c73b20], [3, 0x41e9e280], [4, 0xc050340], [4, 0x41e82260], [5, 0x6288ee0], [5, 0x420b9e40], [0, 0x57c76c0], [0, 0x446f4260], [1, 0xa325c60], [1, 0x45d60d00], [2, 0x5cf3240], [2, 0x433936a0]]}
  e2e_conv2d_256.dc.matmul.8_0:                                                                            {input: conv2d_256.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 1], t: 25, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65ff720]]}
  e2e_buffer_0_conv2d_256.dc.matmul.8_conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0:  {input: buffer_0_conv2d_256.dc.matmul.8_conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70c0f40]]}
  e2e_buffer_6_add_254_add_297_0:                                                                          {input: buffer_6_add_254_add_297, type: queue, entries: 64, grid_size: [1, 1], t: 25, mblock: [1, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42ef1ea0]]}
  e2e__fused_op_8_0:                                                                                       {input: _fused_op_8, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x441cb700]]}
  e2e_conv2d_383.dc.matmul.10_0:                                                                           {input: conv2d_383.dc.matmul.10, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4552c2c0], [1, 0xb15dcc0], [1, 0x46b98d60], [2, 0x6b2b2a0]]}
  e2e_conv2d_541.dc.conv2d.1.dc.matmul.11_0:                                                               {input: conv2d_541.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd2163c0]]}
  e2e_conv2d_541.dc.conv2d.3.dc.matmul.11_0:                                                               {input: conv2d_541.dc.conv2d.3.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x430482e0]]}
  e2e_conv2d_541.dc.conv2d.5.dc.matmul.11_0:                                                               {input: conv2d_541.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x43064300]]}
  e2e_add_525_0:                                                                                           {input: add_525, type: queue, entries: 64, grid_size: [1, 1], t: 7, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e39ba0]]}
  e2e_conv2d_613.dc.matmul.8_0:                                                                            {input: conv2d_613.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 8], t: 7, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d72f60], [5, 0x461b9ec0], [0, 0x72b1740], [0, 0x45c482e0], [1, 0xb879ce0], [1, 0x472b4d80], [2, 0x72472c0], [2, 0x448e7720]]}
  e2e_conv2d_627.dc.conv2d.1.dc.matmul.11_0:                                                               {input: conv2d_627.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbaa9bc0], [3, 0x43780320], [4, 0xd9323e0], [4, 0x43764300]]}
  e2e_conv2d_627.dc.conv2d.5.dc.matmul.11_0:                                                               {input: conv2d_627.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f39f80], [5, 0x46380ee0], [0, 0x7478760], [0, 0x45e0f300]]}
  e2e__fused_op_14_0:                                                                                      {input: _fused_op_14, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [1, 4], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x803dfa0]]}
  e2e_conv2d_654.dc.matmul.10_0:                                                                           {input: conv2d_654.dc.matmul.10, type: queue, entries: 64, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xba40d00], [1, 0x4747bda0], [2, 0x740e2e0], [2, 0x44aae740], [3, 0xbbadbe0], [3, 0x43884340], [4, 0xda36400], [4, 0x43868320]]}
  e2e_conv2d_712.dc.matmul.8_0:                                                                            {input: conv2d_712.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7616300], [2, 0x44cb6760], [3, 0xbdb5c00], [3, 0x43a8c360], [4, 0xdc3e420], [4, 0x43a70340], [5, 0x844dfc0], [5, 0x474c4f20]]}
  e2e_conv2d_726.dc.conv2d.1.dc.matmul.11_0:                                                               {input: conv2d_726.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x757c780], [0, 0x45f13320], [1, 0xbc48d20], [1, 0x47683dc0]]}
  e2e_add_710_0:                                                                                           {input: add_710, type: queue, entries: 64, grid_size: [1, 1], t: 8, mblock: [1, 2], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46484f00]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 64
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, pixel_values, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 28, mblock: [1, 1], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.embedder.embedder.convolution.weight],
         t: 28, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 28}, hslice: 28], input_0_tms: [vslice: 56, hstack: 4, vstack: 14],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, pixel_values, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [8, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.embedder.embedder.convolution.weight_fork_clone1919],
         t: 49, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 32, hstack: 4, vstack: 8],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, pixel_values, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 28, mblock: [1, 1], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.embedder.embedder.convolution.weight_fork_clone1921],
         t: 28, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 28}, hslice: 28], input_0_tms: [vslice: 56, hstack: 4, vstack: 14],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0, pixel_values, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [8, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.embedder.embedder.convolution.weight_fork_clone1923],
         t: 49, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 32, hstack: 4, vstack: 8],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 64
    _fused_op_0: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0, input_1_add_1_fork_clone1120],
         t: 4, mblock: [49, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [12, 12, 12, 12, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 392}, vslice: 4], input_3_tms: [vstack: 49, vslice: 4], input_2_tms: [vstack: 7], input_1_tms: [vstack: 49, vslice: 4], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_4: 4}}}
    max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 14, mblock: [3, 1], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 4],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 18, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 98}}
    max_pool2d_14.dc.reduce_max.6: {type: reduce, grid_loc: [1, 1], grid_size: [7, 1], inputs: [max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2], grid_transpose: true,
         t: 14, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 28, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9],
         attributes: {dim: z, type: max, z: 9}}
    conv2d_15.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [max_pool2d_14.dc.reduce_max.6, resnet.encoder.stages.0.layers.0.layer.0.convolution.weight, input_1_add_16_fork_clone1163],
         t: 14, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_29.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1327],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_29.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.0.layer.1.convolution.weight],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_29.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1325],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.matmul.11, conv2d_29.dc.conv2d.3.dc.matmul.11, conv2d_29.dc.conv2d.5.dc.matmul.11, input_1_add_30_fork_clone1095],
         t: 14, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 14],
         attributes: {fused_op_id: 1}}
    conv2d_43.dc.matmul.8: {type: matmul, grid_loc: [3, 4], grid_size: [1, 2], inputs: [_fused_op_1, resnet.encoder.stages.0.layers.0.layer.2.convolution.weight, input_1_add_44_fork_clone1005],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    buffer_0_max_pool2d_14.dc.reduce_max.6_conv2d_56.dc.matmul.8: {type: nop, grid_loc: [2, 2], grid_size: [1, 2], inputs: [max_pool2d_14.dc.reduce_max.6],
         t: 14, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [419], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    conv2d_56.dc.matmul.8: {type: matmul, grid_loc: [3, 2], grid_size: [1, 2], inputs: [buffer_0_max_pool2d_14.dc.reduce_max.6_conv2d_56.dc.matmul.8, resnet.encoder.stages.0.layers.0.shortcut.convolution.weight, input_1_add_57_fork_clone1010],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [357, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_69: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_43.dc.matmul.8, conv2d_56.dc.matmul.8],
         t: 14, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 312], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_71.dc.matmul.8: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_69, resnet.encoder.stages.0.layers.1.layer.0.convolution.weight, input_1_add_72_fork_clone1130],
         t: 14, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 8}}
    conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_85.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1374],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_85.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.1.layer.1.convolution.weight],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_85.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1372],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_2: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.1.dc.matmul.11, conv2d_85.dc.conv2d.3.dc.matmul.11, conv2d_85.dc.conv2d.5.dc.matmul.11, input_1_add_86_fork_clone1047],
         t: 14, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 14],
         attributes: {fused_op_id: 1}}
    conv2d_99.dc.matmul.8: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [_fused_op_2, resnet.encoder.stages.0.layers.1.layer.2.convolution.weight, input_1_add_100_fork_clone952],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_69_add_112: {type: nop, grid_loc: [4, 0], grid_size: [1, 2], inputs: [add_69],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [376], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_69_add_112: {type: nop, grid_loc: [5, 0], grid_size: [1, 2], inputs: [buffer_3_add_69_add_112],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [376], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_69_add_112: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [buffer_2_add_69_add_112],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [376], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_69_add_112: {type: nop, grid_loc: [5, 5], grid_size: [1, 2], inputs: [buffer_1_add_69_add_112],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [376], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_112: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_99.dc.matmul.8, buffer_0_add_69_add_112],
         t: 14, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 312], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_114.dc.matmul.8: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_112, resnet.encoder.stages.0.layers.2.layer.0.convolution.weight, input_1_add_115_fork_clone1083],
         t: 14, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 14], input_1_tms: [broadcast: {c: 14}, hslice: 14],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 8}}
    conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_128.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1408],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_128.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.2.layer.1.convolution.weight],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_128.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1406],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_3: {type: fused_op, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.1.dc.matmul.11, conv2d_128.dc.conv2d.3.dc.matmul.11, conv2d_128.dc.conv2d.5.dc.matmul.11, input_1_add_129_fork_clone993],
         t: 14, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 14],
         attributes: {fused_op_id: 1}}
    buffer_3_add_112_add_155: {type: nop, grid_loc: [6, 3], grid_size: [1, 2], inputs: [add_112],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_112_add_155: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [buffer_3_add_112_add_155],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_112_add_155: {type: nop, grid_loc: [7, 6], grid_size: [1, 2], inputs: [buffer_2_add_112_add_155],
         t: 14, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 64
    conv2d_142.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [7, 4], inputs: [e2e__fused_op_3_0, resnet.encoder.stages.0.layers.2.layer.2.convolution.weight, input_1_add_143_fork_clone899],
         t: 2, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [49, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vstack: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_112_add_155: {type: nop, grid_loc: [0, 0], grid_size: [7, 2], inputs: [e2e_buffer_1_add_112_add_155_0],
         t: 2, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 7]}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 64
    add_155: {type: add, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_conv2d_142.dc.matmul.8_0, e2e_buffer_0_add_112_add_155_0],
         t: 2, mblock: [49, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_157.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [add_155, resnet.encoder.stages.1.layers.0.layer.0.convolution.weight, input_1_add_158_fork_clone965],
         t: 2, mblock: [49, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 8}}
    conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 70, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 49}}
    conv2d_171.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1442],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 67, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 49}}
    conv2d_171.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.0.layer.1.convolution.weight],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 68, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 49}}
    conv2d_171.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1440],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_4: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.1.dc.matmul.11, conv2d_171.dc.conv2d.3.dc.matmul.11, conv2d_171.dc.conv2d.5.dc.matmul.11, input_1_add_172, input_1_add_172_fork_clone871],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 25], input_3_tms: [vslice: 25],
         attributes: {fused_op_id: 4}}
    conv2d_185.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_4, resnet.encoder.stages.1.layers.0.layer.2.convolution.weight, input_1_add_186_fork_clone769],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_0_add_155_conv2d_198.dc.sparse_matmul.9.lc2: {type: nop, grid_loc: [0, 4], grid_size: [1, 2], inputs: [add_155],
         t: 2, mblock: [49, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [40], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    conv2d_198.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0, buffer_0_add_155_conv2d_198.dc.sparse_matmul.9.lc2, lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1],
         t: 25, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 392, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 23, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_198.dc.matmul.10: {type: matmul, grid_loc: [1, 5], grid_size: [1, 2], inputs: [conv2d_198.dc.sparse_matmul.9.lc2, resnet.encoder.stages.1.layers.0.shortcut.convolution.weight, input_1_add_199_fork_clone774],
         t: 25, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [216, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    add_211: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [conv2d_185.dc.matmul.8, conv2d_198.dc.matmul.10],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 392], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_213.dc.matmul.8: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_211, resnet.encoder.stages.1.layers.1.layer.0.convolution.weight, input_1_add_214_fork_clone913],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 50, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_213.dc.matmul.8, lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 25],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_227.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 6], grid_size: [5, 1], inputs: [conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1491],
         t: 1, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    buffer_0_conv2d_213.dc.matmul.8_conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_213.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [232], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 25]}
    conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_conv2d_213.dc.matmul.8_conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 400, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_227.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.1.layer.1.convolution.weight],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [344, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_213.dc.matmul.8, lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 25],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_227.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 0], grid_size: [5, 1], inputs: [conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1489], grid_transpose: true,
         t: 1, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.1.dc.matmul.11, conv2d_227.dc.conv2d.3.dc.matmul.11, conv2d_227.dc.conv2d.5.dc.matmul.11, input_1_add_228, input_1_add_228_fork_clone815],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 25], input_3_tms: [vslice: 25], input_2_tms: [vslice: 25], input_1_tms: [vslice: 25],
         attributes: {fused_op_id: 4}}
    conv2d_241.dc.matmul.8: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_5, resnet.encoder.stages.1.layers.1.layer.2.convolution.weight, input_1_add_242_fork_clone714],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_0_add_211_buffer_6_add_211_add_254: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_211],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_6_add_211_add_254: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_0_add_211_buffer_6_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_5_add_211_add_254: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [buffer_6_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_4_add_211_add_254: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_5_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_211_add_254: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_4_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_211_add_254: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_3_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_211_add_254: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_211_add_254: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [buffer_1_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_254: {type: add, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_241.dc.matmul.8, buffer_0_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 392], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_256.dc.matmul.8: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [add_254, resnet.encoder.stages.1.layers.2.layer.0.convolution.weight, input_1_add_257_fork_clone859],
         t: 25, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    buffer_0_conv2d_256.dc.matmul.8_conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_256.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 25]}
    buffer_6_add_254_add_297: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 64
    conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_256.dc.matmul.8_0, lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 40, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 25],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_270.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1525],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_buffer_0_conv2d_256.dc.matmul.8_conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0, lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 40, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_270.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.2.layer.1.convolution.weight],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_256.dc.matmul.8_0, lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 25],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 25}}
    conv2d_270.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1523],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_6: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.1.dc.matmul.11, conv2d_270.dc.conv2d.3.dc.matmul.11, conv2d_270.dc.conv2d.5.dc.matmul.11, input_1_add_271, input_1_add_271_fork_clone757],
         t: 5, mblock: [5, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 5], input_3_tms: [vslice: 5],
         attributes: {fused_op_id: 6}}
    conv2d_284.dc.matmul.8: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_6, resnet.encoder.stages.1.layers.2.layer.2.convolution.weight, input_1_add_285_fork_clone663],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_5_add_254_add_297: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_buffer_6_add_254_add_297_0],
         t: 8, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 25, hslice: 8]}
    buffer_4_add_254_add_297: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [buffer_5_add_254_add_297],
         t: 8, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_254_add_297: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [buffer_4_add_254_add_297],
         t: 8, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_254_add_297: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [buffer_3_add_254_add_297],
         t: 8, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_254_add_297: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [buffer_2_add_254_add_297],
         t: 8, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_254_add_297: {type: nop, grid_loc: [2, 0], grid_size: [1, 8], inputs: [buffer_1_add_254_add_297],
         t: 1, mblock: [25, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8]}
    add_297: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_284.dc.matmul.8, buffer_0_add_254_add_297],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 5],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_299.dc.matmul.8: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_297, resnet.encoder.stages.1.layers.3.layer.0.convolution.weight, input_1_add_300_fork_clone803],
         t: 5, mblock: [5, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 10, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_299.dc.matmul.8, lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 300, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_313.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1559],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [300, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    buffer_0_conv2d_299.dc.matmul.8_conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_299.dc.matmul.8],
         t: 1, mblock: [25, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [232], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 5]}
    conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_conv2d_299.dc.matmul.8_conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 300, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_313.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.3.layer.1.convolution.weight],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [300, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_299.dc.matmul.8, lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_313.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 3], grid_size: [5, 1], inputs: [conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1557], grid_transpose: true,
         t: 1, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_7: {type: fused_op, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.1.dc.matmul.11, conv2d_313.dc.conv2d.3.dc.matmul.11, conv2d_313.dc.conv2d.5.dc.matmul.11, input_1_add_314, input_1_add_314_fork_clone702],
         t: 5, mblock: [5, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 160, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 5], input_3_tms: [vslice: 5], input_1_tms: [vslice: 5],
         attributes: {fused_op_id: 6}}
    conv2d_327.dc.matmul.8: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_7, resnet.encoder.stages.1.layers.3.layer.2.convolution.weight, input_1_add_328_fork_clone610],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_0_add_297_buffer_6_add_297_add_340: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_297],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_6_add_297_add_340: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_add_297_buffer_6_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_5_add_297_add_340: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_6_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_4_add_297_add_340: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_5_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_297_add_340: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_4_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_297_add_340: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [buffer_3_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_297_add_340: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [buffer_2_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_297_add_340: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_1_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_340: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [conv2d_327.dc.matmul.8, buffer_0_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 264], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_342.dc.matmul.8: {type: matmul, grid_loc: [6, 1], grid_size: [1, 2], inputs: [add_340, resnet.encoder.stages.2.layers.0.layer.0.convolution.weight, input_1_add_343_fork_clone675],
         t: 5, mblock: [5, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 94, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 5}}
    conv2d_356.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1593],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 86, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 5}}
    conv2d_356.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.0.layer.1.convolution.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 91, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 5}}
    conv2d_356.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1591],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [7, 6], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.1.dc.matmul.11, conv2d_356.dc.conv2d.3.dc.matmul.11, conv2d_356.dc.conv2d.5.dc.matmul.11, input_1_add_357, input_1_add_357_fork_clone580],
         t: 1, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    conv2d_383.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0, add_340, lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1],
         t: 1, mblock: [7, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 31, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 5}}
    conv2d_383.dc.matmul.10: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [conv2d_383.dc.sparse_matmul.9.lc2, resnet.encoder.stages.2.layers.0.shortcut.convolution.weight, input_1_add_384_fork_clone471],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 64
    conv2d_370.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e__fused_op_8_0, resnet.encoder.stages.2.layers.0.layer.2.convolution.weight, input_1_add_371_fork_clone466],
         t: 8, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 8], input_1_tms: [hslice: 8], input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    add_396: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_370.dc.matmul.8, e2e_conv2d_383.dc.matmul.10_0],
         t: 8, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 8],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_398.dc.matmul.8: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [add_396, resnet.encoder.stages.2.layers.1.layer.0.convolution.weight, input_1_add_399_fork_clone624],
         t: 1, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 8],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 336, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_412.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1642],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [296, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 336, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_412.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 4], grid_size: [1, 2], inputs: [conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.1.layer.1.convolution.weight],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [296, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_412.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1640],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    _fused_op_9: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.1.dc.matmul.11, conv2d_412.dc.conv2d.3.dc.matmul.11, conv2d_412.dc.conv2d.5.dc.matmul.11, input_1_add_413, input_1_add_413_fork_clone520],
         t: 7, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 272, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 7], input_3_tms: [vslice: 7], input_1_tms: [vslice: 7],
         attributes: {fused_op_id: 9}}
    conv2d_426.dc.matmul.8: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [_fused_op_9, resnet.encoder.stages.2.layers.1.layer.2.convolution.weight, input_1_add_427_fork_clone401],
         t: 7, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    buffer_4_add_396_add_439: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [add_396],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8]}
    buffer_3_add_396_add_439: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_4_add_396_add_439],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_396_add_439: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_3_add_396_add_439],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_396_add_439: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_2_add_396_add_439],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_396_add_439: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1_add_396_add_439],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_439: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_426.dc.matmul.8, buffer_0_add_396_add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 7],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_441.dc.matmul.8: {type: matmul, grid_loc: [2, 6], grid_size: [1, 2], inputs: [add_439, resnet.encoder.stages.2.layers.2.layer.0.convolution.weight, input_1_add_442_fork_clone568],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_455.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1676],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_455.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.2.layer.1.convolution.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_455.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1674],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    _fused_op_10: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.1.dc.matmul.11, conv2d_455.dc.conv2d.3.dc.matmul.11, conv2d_455.dc.conv2d.5.dc.matmul.11, input_1_add_456, input_1_add_456_fork_clone454],
         t: 1, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    conv2d_469.dc.matmul.8: {type: matmul, grid_loc: [4, 3], grid_size: [1, 2], inputs: [_fused_op_10, resnet.encoder.stages.2.layers.2.layer.2.convolution.weight, input_1_add_470_fork_clone344],
         t: 7, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    buffer_4_add_439_add_482: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_439_add_482: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [buffer_4_add_439_add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_439_add_482: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_3_add_439_add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_439_add_482: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_2_add_439_add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_439_add_482: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_1_add_439_add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_482: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_469.dc.matmul.8, buffer_0_add_439_add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 360], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_484.dc.matmul.8: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [add_482, resnet.encoder.stages.2.layers.3.layer.0.convolution.weight, input_1_add_485_fork_clone508],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_484.dc.matmul.8, lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_498.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1710],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_484.dc.matmul.8, lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_498.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.3.layer.1.convolution.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_484.dc.matmul.8, lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_498.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1708],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    _fused_op_11: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.1.dc.matmul.11, conv2d_498.dc.conv2d.3.dc.matmul.11, conv2d_498.dc.conv2d.5.dc.matmul.11, input_1_add_499, input_1_add_499_fork_clone389],
         t: 1, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    conv2d_512.dc.matmul.8: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [_fused_op_11, resnet.encoder.stages.2.layers.3.layer.2.convolution.weight, input_1_add_513_fork_clone291],
         t: 7, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    buffer_4_add_482_add_525: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_482],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_482_add_525: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_4_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_482_add_525: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [buffer_3_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_482_add_525: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [buffer_2_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_482_add_525: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [buffer_1_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [368], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_525: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_512.dc.matmul.8, buffer_0_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 360], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_527.dc.matmul.8: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [add_525, resnet.encoder.stages.2.layers.4.layer.0.convolution.weight, input_1_add_528_fork_clone442],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1744],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.4.layer.1.convolution.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1742],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 64
    _fused_op_12: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_541.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_541.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_541.dc.conv2d.5.dc.matmul.11_0, input_1_add_542, input_1_add_542_fork_clone332],
         t: 1, mblock: [7, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 8}}
    conv2d_555.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [_fused_op_12, resnet.encoder.stages.2.layers.4.layer.2.convolution.weight, input_1_add_556_fork_clone240],
         t: 8, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 8], input_1_tms: [hslice: 8], input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {bias: true, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    add_568: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_555.dc.matmul.8, e2e_add_525_0],
         t: 8, mblock: [7, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7, hslice: 8],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_570.dc.matmul.8: {type: matmul, grid_loc: [1, 0], grid_size: [1, 4], inputs: [add_568, resnet.encoder.stages.2.layers.5.layer.0.convolution.weight, input_1_add_571_fork_clone377],
         t: 1, mblock: [7, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 8],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 336, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_584.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 2], inputs: [conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1778],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 336, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_584.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.5.layer.1.convolution.weight],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 8}}
    conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_584.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1776],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    _fused_op_13: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_584.dc.conv2d.1.dc.matmul.11, conv2d_584.dc.conv2d.3.dc.matmul.11, conv2d_584.dc.conv2d.5.dc.matmul.11, input_1_add_585, input_1_add_585_fork_clone279],
         t: 7, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 272, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 7], input_3_tms: [vslice: 7], input_1_tms: [vslice: 7],
         attributes: {fused_op_id: 9}}
    conv2d_598.dc.matmul.8: {type: matmul, grid_loc: [3, 2], grid_size: [1, 4], inputs: [_fused_op_13, resnet.encoder.stages.2.layers.5.layer.2.convolution.weight, input_1_add_599_fork_clone190],
         t: 7, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    buffer_4_add_568_add_611: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_568],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8]}
    buffer_3_add_568_add_611: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [buffer_4_add_568_add_611],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_568_add_611: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_3_add_568_add_611],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_568_add_611: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [buffer_2_add_568_add_611],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_568_add_611: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [buffer_1_add_568_add_611],
         t: 1, mblock: [7, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_611: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_598.dc.matmul.8, buffer_0_add_568_add_611],
         t: 7, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 7],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_613.dc.matmul.8: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [add_611, resnet.encoder.stages.3.layers.0.layer.0.convolution.weight, input_1_add_614_fork_clone252],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_613.dc.matmul.8, lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 25, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_627.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 2], grid_size: [1, 4], inputs: [conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1812],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 4}}
    conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_613.dc.matmul.8, lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 22, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_627.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 4], inputs: [conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.0.layer.1.convolution.weight],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 4}}
    conv2d_654.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0, add_611, lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1],
         t: 1, mblock: [2, 4], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_654.dc.matmul.10: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [conv2d_654.dc.sparse_matmul.9.lc2, resnet.encoder.stages.3.layers.0.shortcut.convolution.weight, input_1_add_655_fork_clone87],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 4}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 64
    conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_613.dc.matmul.8_0, lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 7}}
    conv2d_627.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1810],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 4}}
    _fused_op_14: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_conv2d_627.dc.conv2d.1.dc.matmul.11_0, conv2d_627.dc.conv2d.3.dc.matmul.11, e2e_conv2d_627.dc.conv2d.5.dc.matmul.11_0, input_1_add_628, input_1_add_628_fork_clone163],
         t: 2, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 24, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 2], input_3_tms: [vslice: 2], input_2_tms: [vslice: 2], input_1_tms: [vslice: 2], input_0_tms: [vslice: 2],
         attributes: {fused_op_id: 14}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 64
    conv2d_641.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e__fused_op_14_0, resnet.encoder.stages.3.layers.0.layer.2.convolution.weight, input_1_add_642_fork_clone82],
         t: 8, mblock: [1, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 8], input_1_tms: [hslice: 8], input_0_tms: [vstack: 2, broadcast: {r: 8}, vslice: 8],
         attributes: {bias: true, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    add_667: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_641.dc.matmul.8, e2e_conv2d_654.dc.matmul.10_0],
         t: 8, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 16, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 8],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_669.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [add_667, resnet.encoder.stages.3.layers.1.layer.0.convolution.weight, input_1_add_670_fork_clone203],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 8],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_669.dc.matmul.8, lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_683.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1861],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 8}}
    conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_669.dc.matmul.8, lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_683.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.1.layer.1.convolution.weight],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 8}}
    conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_669.dc.matmul.8, lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_683.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 4], inputs: [conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1859],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 8}}
    _fused_op_15: {type: fused_op, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_683.dc.conv2d.1.dc.matmul.11, conv2d_683.dc.conv2d.3.dc.matmul.11, conv2d_683.dc.conv2d.5.dc.matmul.11, input_1_add_684, input_1_add_684_fork_clone116],
         t: 1, mblock: [1, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 15}}
    conv2d_697.dc.matmul.8: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_15, resnet.encoder.stages.3.layers.1.layer.2.convolution.weight, input_1_add_698_fork_clone47],
         t: 8, mblock: [1, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 8], input_1_tms: [hslice: 8], input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {bias: true, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    buffer_2_add_667_add_710: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_667],
         t: 8, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_667_add_710: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_2_add_667_add_710],
         t: 8, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_667_add_710: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_1_add_667_add_710],
         t: 8, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [400], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_710: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_697.dc.matmul.8, buffer_0_add_667_add_710],
         t: 8, mblock: [1, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 384], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_712.dc.matmul.8: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [add_710, resnet.encoder.stages.3.layers.2.layer.0.convolution.weight, input_1_add_713_fork_clone151],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 8],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_712.dc.matmul.8, lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_726.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 4], inputs: [conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.2.layer.1.convolution.weight],
         t: 1, mblock: [1, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 8}}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 64
    conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_712.dc.matmul.8_0, lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 32, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 2}}
    conv2d_726.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1895],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 6}}
    conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_712.dc.matmul.8_0, lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [3, 2], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 32, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 2}}
    conv2d_726.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1893],
         t: 2, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 6}}
    _fused_op_16: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_conv2d_726.dc.conv2d.1.dc.matmul.11_0, conv2d_726.dc.conv2d.3.dc.matmul.11, conv2d_726.dc.conv2d.5.dc.matmul.11, input_1_add_727, input_1_add_727_fork_clone70],
         t: 2, mblock: [1, 4], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 2], input_3_tms: [vslice: 2], input_2_tms: [vslice: 2], input_0_tms: [vslice: 2],
         attributes: {fused_op_id: 14}}
    conv2d_740.dc.matmul.8: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [_fused_op_16, resnet.encoder.stages.3.layers.2.layer.2.convolution.weight, input_1_add_741_fork_clone24],
         t: 2, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, u_kt: 16}}
    add_753: {type: add, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_740.dc.matmul.8, e2e_add_710_0],
         t: 2, mblock: [1, 16], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vslice: 2],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    avg_pool2d_755.dc.reduce_avg.2.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0, add_753],
         t: 1, mblock: [1, 16], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_758: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [avg_pool2d_755.dc.reduce_avg.2.lc1, classifier.1.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 16}}
    add_759: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_758, classifier.1.bias], untilize_output: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 64, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q9: 0, $lptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q7: 0, $gptr_q4: 0}
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               pixel_values: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.embedder.embedder.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.embedder.embedder.convolution.weight_fork_clone1919: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.embedder.embedder.convolution.weight_fork_clone1921: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.embedder.embedder.convolution.weight_fork_clone1923: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               input_1_add_1_fork_clone1120: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_16_fork_clone1163: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1327: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.layer.1.convolution.weight_fork_clone1325: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_30_fork_clone1095: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_44_fork_clone1005: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.0.shortcut.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_57_fork_clone1010: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.1.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_72_fork_clone1130: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1374: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.1.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.1.layer.1.convolution.weight_fork_clone1372: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_86_fork_clone1047: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.1.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_100_fork_clone952: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.2.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_115_fork_clone1083: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1408: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.2.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.0.layers.2.layer.1.convolution.weight_fork_clone1406: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_129_fork_clone993: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e__fused_op_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_1_add_112_add_155_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               resnet.encoder.stages.0.layers.2.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_143_fork_clone899: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_conv2d_142.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_add_112_add_155_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               resnet.encoder.stages.1.layers.0.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_158_fork_clone965: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1442: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.0.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.0.layer.1.convolution.weight_fork_clone1440: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_172: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_172_fork_clone871: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.0.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_186_fork_clone769: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.0.shortcut.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_199_fork_clone774: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.1.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_214_fork_clone913: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1491: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.1.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.1.layer.1.convolution.weight_fork_clone1489: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_228_fork_clone815: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.1.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_242_fork_clone714: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.2.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_257_fork_clone859: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e_conv2d_256.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_0_conv2d_256.dc.matmul.8_conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_6_add_254_add_297_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1525: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.2.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.2.layer.1.convolution.weight_fork_clone1523: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_271: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_271_fork_clone757: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.2.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_285_fork_clone663: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.3.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_300_fork_clone803: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1559: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.3.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.3.layer.1.convolution.weight_fork_clone1557: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_314: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_314_fork_clone702: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.1.layers.3.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_328_fork_clone610: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.0.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_343_fork_clone675: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1593: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.0.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.0.layer.1.convolution.weight_fork_clone1591: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_357: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_357_fork_clone580: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.0.shortcut.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_384_fork_clone471: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_383.dc.matmul.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               resnet.encoder.stages.2.layers.0.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_371_fork_clone466: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.1.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_399_fork_clone624: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1642: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.1.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.1.layer.1.convolution.weight_fork_clone1640: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_413: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_413_fork_clone520: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.1.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_427_fork_clone401: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.2.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_442_fork_clone568: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1676: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.2.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.2.layer.1.convolution.weight_fork_clone1674: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_456: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_456_fork_clone454: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.2.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_470_fork_clone344: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.3.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_485_fork_clone508: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1710: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.3.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.3.layer.1.convolution.weight_fork_clone1708: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_499: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_499_fork_clone389: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.3.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_513_fork_clone291: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.4.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_528_fork_clone442: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1744: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.4.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.4.layer.1.convolution.weight_fork_clone1742: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_add_525_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_541.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_541.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_541.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_add_542: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_542_fork_clone332: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.4.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_556_fork_clone240: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.5.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_571_fork_clone377: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1778: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.5.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.5.layer.1.convolution.weight_fork_clone1776: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_585: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_585_fork_clone279: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.2.layers.5.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_599_fork_clone190: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.0.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_614_fork_clone252: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1812: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.0.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.0.shortcut.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_655_fork_clone87: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_conv2d_613.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_conv2d_627.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_conv2d_627.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.0.layer.1.convolution.weight_fork_clone1810: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_628: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_628_fork_clone163: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_conv2d_654.dc.matmul.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               resnet.encoder.stages.3.layers.0.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_642_fork_clone82: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.1.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_670_fork_clone203: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1861: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.1.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.1.layer.1.convolution.weight_fork_clone1859: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_684: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_684_fork_clone116: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.1.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_698_fork_clone47: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.2.layer.0.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_713_fork_clone151: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.2.layer.1.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
               e2e_add_710_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_conv2d_712.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_conv2d_726.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1895: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.2.layer.1.convolution.weight_fork_clone1893: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_727: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_727_fork_clone70: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               resnet.encoder.stages.3.layers.2.layer.2.convolution.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_741_fork_clone24: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               classifier.1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               classifier.1.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 128]
    - endloop


fused_ops:
  0: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.8.0: { type: add, inputs: [input0, input1], mblock: [49, 1], ublock: [2, 2], output: intermed0}
        - conv2d_0.dc.conv2d.3.dc.add.9.0: { type: add, inputs: [input2, input3], mblock: [49, 1], ublock: [2, 2], output: dest}
        - conv2d_0.dc.conv2d.3.dc.add.10.0: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [49, 1], ublock: [2, 2], output: dest}
        - add_12.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [49, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_29.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [1, 2], output: dest}
        - conv2d_29.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [1, 2], output: dest}
        - add_41.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [7, 1], ublock: [1, 2], output: output}
  4: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_171.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 4], output: dest}
        - conv2d_171.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_178.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 4], output: dest}
        - add_183.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 1], ublock: [1, 4], output: output}
  6: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_270.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 1], ublock: [1, 4], output: dest}
        - conv2d_270.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 1], ublock: [1, 4], output: dest}
        - multiply_277.0: { type: multiply, inputs: [dest, input3], mblock: [5, 1], ublock: [1, 4], output: dest}
        - add_282.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [5, 1], ublock: [1, 4], output: output}
  8: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_356.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 2], ublock: [1, 4], output: dest}
        - conv2d_356.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 2], ublock: [1, 4], output: dest}
        - multiply_363.0: { type: multiply, inputs: [dest, input3], mblock: [7, 2], ublock: [1, 4], output: dest}
        - add_368.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [7, 2], ublock: [1, 4], output: output}
  9: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_412.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 2], ublock: [1, 4], output: dest}
        - conv2d_412.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 2], ublock: [1, 4], output: dest}
        - multiply_419.0: { type: multiply, inputs: [dest, input3], mblock: [1, 2], ublock: [1, 4], output: dest}
        - add_424.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 2], ublock: [1, 4], output: output}
  14: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_627.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 4], ublock: [1, 4], output: dest}
        - conv2d_627.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 4], ublock: [1, 4], output: dest}
        - multiply_634.0: { type: multiply, inputs: [dest, input3], mblock: [1, 4], ublock: [1, 4], output: dest}
        - add_639.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 4], ublock: [1, 4], output: output}
  15: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_683.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 4], ublock: [2, 4], output: dest}
        - conv2d_683.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 4], ublock: [2, 4], output: dest}
        - multiply_690.0: { type: multiply, inputs: [dest, input3], mblock: [1, 4], ublock: [2, 4], output: dest}
        - add_695.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 4], ublock: [2, 4], output: output}

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.08
    test-group: "perf_infra_wormhole_b0_silicon_nightly"
    test-name: "resnet50_hifi3_fp16b"