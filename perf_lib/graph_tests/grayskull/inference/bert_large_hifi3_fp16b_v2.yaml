# git checkout 5259b590
# pybuda/test/benchmark/benchmark.py -m bert -c large -opt 3 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c300020]]}

  # output
  bert_encoders.output_layernorm_1271:                                          {input: layernorm_1271.dc.add.10, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b91900], [6, 0x5b9c3a0], [7, 0x5b77aa0], [0, 0x5b97aa0], [1, 0x5b8a760], [2, 0x5b8d000], [3, 0x5b8d000], [4, 0x5b7fcc0], [5, 0x5bb2120], [6, 0x5bbcbc0], [7, 0x5b982c0], [0, 0x5bb82c0], [1, 0x5baaf80], [2, 0x5bad820], [3, 0x5bad820], [4, 0x5ba04e0]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x802c780], [3, 0x80665c0], [4, 0x80318c0], [5, 0x8019ae0], [6, 0x80414a0], [7, 0x7fe04c0], [0, 0x804c780], [1, 0x800cfc0]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x802e820], [3, 0x8068660], [4, 0x8033960], [5, 0x801bb80], [6, 0x8043540], [7, 0x7fe2560], [0, 0x804e820], [1, 0x800f060], [2, 0x804f040], [3, 0x8088e80], [4, 0x8054180], [5, 0x803c3a0], [6, 0x8063d60], [7, 0x8002d80], [0, 0x806f040], [1, 0x802f880]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x806f860], [3, 0x80a96a0], [4, 0x80749a0], [5, 0x805cbc0], [6, 0x8084580], [7, 0x80235a0], [0, 0x808f860], [1, 0x80500a0]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8086620], [7, 0x8025640], [0, 0x8091900], [1, 0x8052140], [2, 0x8072140], [3, 0x80abf80], [4, 0x8076a40], [5, 0x805f4a0], [6, 0x80a6e40], [7, 0x8045e60], [0, 0x80b2120], [1, 0x8072960], [2, 0x8092960], [3, 0x80cc7a0], [4, 0x8097260], [5, 0x807fcc0]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x80c7660], [7, 0x8066680], [0, 0x80d2940], [1, 0x8093180], [2, 0x80b3180], [3, 0x80ecfc0], [4, 0x80b7a80], [5, 0x80a04e0]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x80c9700], [7, 0x8068720], [0, 0x80d49e0], [1, 0x8095220], [2, 0x80b5220], [3, 0x80ef060], [4, 0x80b9b20], [5, 0x80a2580], [6, 0x80e9f20], [7, 0x8088f40], [0, 0x80f5200], [1, 0x80b5a40], [2, 0x80d5a40], [3, 0x810f880], [4, 0x80da340], [5, 0x80c2da0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x800af20], [2, 0x802a6e0], [3, 0x8064520], [4, 0x802f820], [5, 0x8017a40], [6, 0x803f400], [7, 0x7fde420], [0, 0x804a6e0]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x80e35c0]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80a9fa0]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8116260], [1, 0x80d9340], [2, 0x80f9340], [3, 0x81308e0], [4, 0x80fb3a0], [5, 0x80f39e0], [6, 0x810b7c0], [7, 0x80ba3c0], [0, 0x8157280], [1, 0x811a360], [2, 0x813a360], [3, 0x8171900], [4, 0x813c3c0], [5, 0x8134a00], [6, 0x814c7e0], [7, 0x80fb3e0], [0, 0x81982a0], [1, 0x815b380], [2, 0x817b380], [3, 0x81b2920], [4, 0x817d3e0], [5, 0x8175a20], [6, 0x818d800], [7, 0x813c400], [0, 0x81d92c0], [1, 0x819c3a0], [2, 0x81bc3a0], [3, 0x81f3940], [4, 0x81be400], [5, 0x81b6a40], [6, 0x81ce820], [7, 0x817d420]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x821a2e0], [1, 0x81dd3c0], [2, 0x81fd3c0], [3, 0x8234960], [4, 0x81ff420], [5, 0x81f7a60], [6, 0x820f840], [7, 0x81be440]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8222500], [1, 0x81e55e0], [2, 0x82055e0], [3, 0x823cb80], [4, 0x8207640], [5, 0x81ffc80], [6, 0x8217a60], [7, 0x81c6660], [0, 0x8263520], [1, 0x8226600], [2, 0x8246600], [3, 0x827dba0], [4, 0x8248660], [5, 0x8240ca0], [6, 0x8258a80], [7, 0x8207680], [0, 0x82a4540], [1, 0x8267620], [2, 0x8287620], [3, 0x82bebc0], [4, 0x8289680], [5, 0x8281cc0], [6, 0x8299aa0], [7, 0x82486a0], [0, 0x82e5560], [1, 0x82a8640], [2, 0x82c8640], [3, 0x82ffbe0], [4, 0x82ca6a0], [5, 0x82c2ce0], [6, 0x82daac0], [7, 0x82896c0]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f82480], [2, 0x7fa1c40], [3, 0x7fdba80], [4, 0x7fa6d80], [5, 0x7f8efa0], [6, 0x7fb40c0], [7, 0x7f530e0], [0, 0x7fc1c40]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7f1fb80]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f2d700]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7efe360], [2, 0x7f1db20], [3, 0x7f57960], [4, 0x7f22c60], [5, 0x7f0ae80], [6, 0x7f2ffa0], [7, 0x7ecefc0], [0, 0x7f3db20], [1, 0x7f1eb80], [2, 0x7f3e340], [3, 0x7f78180], [4, 0x7f43480], [5, 0x7f2b6a0], [6, 0x7f507c0], [7, 0x7eef7e0], [0, 0x7f5e340]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f3f3a0], [2, 0x7f5eb60], [3, 0x7f989a0], [4, 0x7f63ca0], [5, 0x7f4bec0], [6, 0x7f70fe0], [7, 0x7f10000], [0, 0x7f7eb60]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f41440], [2, 0x7f60c00], [3, 0x7f9aa40], [4, 0x7f65d40], [5, 0x7f4df60], [6, 0x7f73080], [7, 0x7f120a0], [0, 0x7f80c00], [1, 0x7f61c60], [2, 0x7f81420], [3, 0x7fbb260], [4, 0x7f86560], [5, 0x7f6e780], [6, 0x7f938a0], [7, 0x7f328c0], [0, 0x7fa1420]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f2ae20], [1, 0x7efba80], [2, 0x7f189a0], [3, 0x7f527e0], [4, 0x7f20380], [5, 0x7f085a0], [6, 0x7f1dae0], [7, 0x7ecc6e0]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fa8e20], [5, 0x7f91040], [6, 0x7fb6160], [7, 0x7f55180], [0, 0x7fc3ce0], [1, 0x7f84d60], [2, 0x7fa4520], [3, 0x7fde360], [4, 0x7fc9640], [5, 0x7fb1860], [6, 0x7fd6980], [7, 0x7f759a0], [0, 0x7fe4500], [1, 0x7fa5580], [2, 0x7fc4d40], [3, 0x7ffeb80]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fe9e60], [5, 0x7fd2080], [6, 0x7ff71a0], [7, 0x7f961c0], [0, 0x8004d20], [1, 0x7fc5da0], [2, 0x7fe5560], [3, 0x801f3a0]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7febf00], [5, 0x7fd4120], [6, 0x7ff9240], [7, 0x7f98260], [0, 0x8006dc0], [1, 0x7fc7e40], [2, 0x7fe7600], [3, 0x8021440], [4, 0x800c720], [5, 0x7ff4940], [6, 0x8019a60], [7, 0x7fb8a80], [0, 0x80275e0], [1, 0x7fe8660], [2, 0x8007e20], [3, 0x8041c60]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x802cf40], [5, 0x8015160], [6, 0x803a280], [7, 0x7fd92a0], [0, 0x8047e00], [1, 0x8008e80], [2, 0x8028640], [3, 0x8062480]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85df340]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x859d300]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x85c4cc0], [7, 0x85738c0], [0, 0x85cffa0], [1, 0x8593080], [2, 0x85a5d40], [3, 0x85ef760], [4, 0x85a7da0], [5, 0x85ad720], [6, 0x8605ce0], [7, 0x85b48e0], [0, 0x8610fc0], [1, 0x85d40a0], [2, 0x85e6d60], [3, 0x8630780], [4, 0x85e8dc0], [5, 0x85ee740], [6, 0x8646d00], [7, 0x85f5900], [0, 0x8651fe0], [1, 0x86150c0], [2, 0x8627d80], [3, 0x86717a0], [4, 0x8629de0], [5, 0x862f760], [6, 0x8687d20], [7, 0x8636920], [0, 0x8693000], [1, 0x86560e0], [2, 0x8668da0], [3, 0x86b27c0], [4, 0x866ae00], [5, 0x8670780]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x86c8d40], [7, 0x8677940], [0, 0x86d4020], [1, 0x8697100], [2, 0x86a9dc0], [3, 0x86f37e0], [4, 0x86abe20], [5, 0x86b17a0]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x86d0f60], [7, 0x867fb60], [0, 0x86dc240], [1, 0x869f320], [2, 0x86b1fe0], [3, 0x86fba00], [4, 0x86b4040], [5, 0x86b99c0], [6, 0x8711f80], [7, 0x86c0b80], [0, 0x871d260], [1, 0x86e0340], [2, 0x86f3000], [3, 0x873ca20], [4, 0x86f5060], [5, 0x86fa9e0], [6, 0x8752fa0], [7, 0x8701ba0], [0, 0x875e280], [1, 0x8721360], [2, 0x8734020], [3, 0x877da40], [4, 0x8736080], [5, 0x873ba00], [6, 0x8793fc0], [7, 0x8742bc0], [0, 0x879f2a0], [1, 0x8762380], [2, 0x8775040], [3, 0x87bea60], [4, 0x87770a0], [5, 0x877ca20]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x87d4fe0], [7, 0x8783be0], [0, 0x87e02c0], [1, 0x87a33a0], [2, 0x87b6060], [3, 0x87ffa80], [4, 0x87b80c0], [5, 0x87bda40]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87ba160]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x87d78c0]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x87864c0], [0, 0x87e5440], [1, 0x87a8520], [2, 0x87b8940], [3, 0x8802360], [4, 0x87ca580], [5, 0x87c0320], [6, 0x87e7ce0], [7, 0x87a6ce0], [0, 0x8805c60], [1, 0x87c8d40], [2, 0x87d9160], [3, 0x8822b80], [4, 0x87eada0], [5, 0x87e0b40], [6, 0x8808500]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x87c7500], [0, 0x8826480], [1, 0x87e9560], [2, 0x87f9980], [3, 0x88433a0], [4, 0x880b5c0], [5, 0x8801360], [6, 0x8828d20]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x87c95a0], [0, 0x8828520], [1, 0x87eb600], [2, 0x87fba20], [3, 0x8845440], [4, 0x880d660], [5, 0x8803400], [6, 0x882adc0], [7, 0x87e9dc0], [0, 0x8848d40], [1, 0x880be20], [2, 0x881c240], [3, 0x8865c60], [4, 0x882de80], [5, 0x8823c20], [6, 0x884b5e0]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x880a5e0], [0, 0x8869560], [1, 0x882c640], [2, 0x883ca60], [3, 0x8886480], [4, 0x884e6a0], [5, 0x8844440], [6, 0x886be00]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x84330a0], [2, 0x8455940], [3, 0x848cee0], [4, 0x8455100], [5, 0x844d740], [6, 0x8475100], [7, 0x8414120], [0, 0x84803e0], [1, 0x84538c0], [2, 0x8476160], [3, 0x84ad700], [4, 0x8475920], [5, 0x846df60], [6, 0x8495920], [7, 0x8434940], [0, 0x84a0c00]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x842a600], [1, 0x83ed6e0], [2, 0x840d6e0], [3, 0x8444c80], [4, 0x840f740], [5, 0x8407d80], [6, 0x841fb60], [7, 0x83ce760]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x842c6a0], [1, 0x83ef780], [2, 0x840f780], [3, 0x8446d20], [4, 0x84117e0], [5, 0x8409e20], [6, 0x8421c00], [7, 0x83d0800], [0, 0x844cec0], [1, 0x840ffa0], [2, 0x842ffa0], [3, 0x8467540], [4, 0x8432000], [5, 0x842a640], [6, 0x8442420], [7, 0x83f1020]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x846d6e0], [1, 0x84307c0], [2, 0x84507c0], [3, 0x8487d60], [4, 0x8452820], [5, 0x844ae60], [6, 0x8462c40], [7, 0x8411840]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8464ce0]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x846ffc0]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8326580], [1, 0x82e9660], [2, 0x8309660], [3, 0x8340c00], [4, 0x830b6c0], [5, 0x8303d00], [6, 0x831bae0], [7, 0x82ca6e0], [0, 0x83675a0], [1, 0x832a680], [2, 0x834a680], [3, 0x8381c20], [4, 0x834c6e0], [5, 0x8344d20], [6, 0x835cb00], [7, 0x830b700], [0, 0x83a85c0], [1, 0x836b6a0], [2, 0x838b6a0], [3, 0x83c2c40], [4, 0x838d700], [5, 0x8385d40], [6, 0x839db20], [7, 0x834c720], [0, 0x83e95e0], [1, 0x83ac6c0], [2, 0x83cc6c0], [3, 0x8403c60], [4, 0x83ce720], [5, 0x83c6d60], [6, 0x83deb40], [7, 0x838d740]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x84740e0], [2, 0x8496980], [3, 0x84cdf20], [4, 0x8496140], [5, 0x848e780], [6, 0x84b6140], [7, 0x8455160], [0, 0x84c1420]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x847c300], [2, 0x849eba0], [3, 0x84d6140], [4, 0x849e360], [5, 0x84969a0], [6, 0x84be360], [7, 0x845d380], [0, 0x84c9640], [1, 0x84bd320], [2, 0x84dfbc0], [3, 0x8517160], [4, 0x84df380], [5, 0x84d79c0], [6, 0x84ff380], [7, 0x849e3a0], [0, 0x850a660], [1, 0x84fe340], [2, 0x8520be0], [3, 0x8558180], [4, 0x85203a0], [5, 0x85189e0], [6, 0x85403a0], [7, 0x84df3c0], [0, 0x854b680], [1, 0x853f360], [2, 0x8561c00], [3, 0x85991a0], [4, 0x85613c0], [5, 0x8559a00], [6, 0x85813c0], [7, 0x85203e0], [0, 0x858c6a0]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8580380], [2, 0x85a2c20], [3, 0x85da1c0], [4, 0x85a23e0], [5, 0x859aa20], [6, 0x85c23e0], [7, 0x8561400], [0, 0x85cd6c0]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x85634a0]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8582c60]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e05c80], [5, 0x7dfda80], [6, 0x7e12fc0], [7, 0x7dc1bc0], [0, 0x7e20b40], [1, 0x7df17a0], [2, 0x7e0e6c0], [3, 0x7e48500], [4, 0x7e264a0], [5, 0x7e1e2a0], [6, 0x7e337e0], [7, 0x7de23e0], [0, 0x7e41360], [1, 0x7e11fc0], [2, 0x7e2eee0], [3, 0x7e68d20]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x76665e0], [5, 0x7660c80], [6, 0x7688640], [7, 0x76151e0], [0, 0x7683d40], [1, 0x76549a0], [2, 0x76718c0], [3, 0x76ab700]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7668680], [5, 0x7662d20], [6, 0x768a6e0], [7, 0x7617280], [0, 0x7685de0], [1, 0x7656a40], [2, 0x7673960], [3, 0x76ad7a0], [4, 0x7688ea0], [5, 0x7683540], [6, 0x76aaf00], [7, 0x7637aa0], [0, 0x76a6600], [1, 0x7677260], [2, 0x7694180], [3, 0x76cdfc0]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x76a96c0], [5, 0x76a3d60], [6, 0x76cb720], [7, 0x76582c0], [0, 0x76c6e20], [1, 0x7697a80], [2, 0x76b49a0], [3, 0x76ee7e0]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x765a360], [0, 0x76c8ec0], [1, 0x7699b20], [2, 0x76b6a40], [3, 0x76f0880], [4, 0x76abfa0], [5, 0x76a6640], [6, 0x76ce000], [7, 0x767ab80], [0, 0x76e96e0], [1, 0x76ba340], [2, 0x76d7260], [3, 0x77110a0], [4, 0x76cc7c0], [5, 0x76c6e60], [6, 0x76ee820]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x769b3a0], [0, 0x7709f00], [1, 0x76dab60], [2, 0x76f7a80], [3, 0x77318c0], [4, 0x76ecfe0], [5, 0x76e7680], [6, 0x770f040]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x769d440], [0, 0x770bfa0], [1, 0x76dcc00], [2, 0x76f9b20], [3, 0x7733960], [4, 0x76ef080], [5, 0x76e9720], [6, 0x77110e0], [7, 0x76bdc60], [0, 0x772c7c0], [1, 0x76fd420], [2, 0x771a340], [3, 0x7754180], [4, 0x770f8a0], [5, 0x7709f40], [6, 0x7731900]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x76de480], [0, 0x774cfe0], [1, 0x771dc40], [2, 0x773ab60], [3, 0x77749a0], [4, 0x77300c0], [5, 0x772a760], [6, 0x7752120]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x772c800]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x76e0d60]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x774f8c0], [1, 0x7722dc0], [2, 0x773fce0], [3, 0x7777280], [4, 0x77329a0], [5, 0x773cc20], [6, 0x7754a00], [7, 0x76f1180], [0, 0x77908e0], [1, 0x7763de0], [2, 0x7780d00], [3, 0x77b82a0], [4, 0x77739c0], [5, 0x777dc40], [6, 0x7795a20], [7, 0x77321a0], [0, 0x77d1900], [1, 0x77a4e00], [2, 0x77c1d20], [3, 0x77f92c0], [4, 0x77b49e0], [5, 0x77bec60], [6, 0x77d6a40], [7, 0x77731c0], [0, 0x7812920], [1, 0x77e5e20], [2, 0x7802d40], [3, 0x783a2e0], [4, 0x77f5a00], [5, 0x77ffc80], [6, 0x7817a60], [7, 0x77b41e0]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7853940], [1, 0x7826e40], [2, 0x7843d60], [3, 0x787b300], [4, 0x7836a20], [5, 0x7840ca0], [6, 0x7858a80], [7, 0x77f5200]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x785bb60], [1, 0x782f060], [2, 0x784bf80], [3, 0x7883520], [4, 0x783ec40], [5, 0x7848ec0], [6, 0x7860ca0], [7, 0x77fd420], [0, 0x789cb80], [1, 0x7870080], [2, 0x788cfa0], [3, 0x78c4540], [4, 0x787fc60], [5, 0x7889ee0], [6, 0x78a1cc0], [7, 0x783e440], [0, 0x78ddba0], [1, 0x78b10a0], [2, 0x78cdfc0], [3, 0x7905560], [4, 0x78c0c80], [5, 0x78caf00], [6, 0x78e2ce0], [7, 0x787f460], [0, 0x791ebc0], [1, 0x78f20c0], [2, 0x790efe0], [3, 0x7946580], [4, 0x7901ca0], [5, 0x790bf20], [6, 0x7923d00], [7, 0x78c0480]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x795fbe0], [1, 0x79330e0], [2, 0x7950000], [3, 0x79875a0], [4, 0x7942cc0], [5, 0x794cf40], [6, 0x7964d20], [7, 0x79014a0]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7568dc0]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7566d60]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75475a0], [2, 0x75548e0], [3, 0x759e300], [4, 0x7559a20], [5, 0x75540c0], [6, 0x75791e0], [7, 0x7505d80], [0, 0x7577180], [1, 0x7567dc0], [2, 0x7575100], [3, 0x75beb20], [4, 0x757a240], [5, 0x75748e0], [6, 0x7599a00], [7, 0x75265a0], [0, 0x75979a0]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75885e0], [2, 0x7595920], [3, 0x75df340], [4, 0x759aa60], [5, 0x7595100], [6, 0x75ba220], [7, 0x7546dc0], [0, 0x75b81c0]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x758a680], [2, 0x75979c0], [3, 0x75e13e0], [4, 0x759cb00], [5, 0x75971a0], [6, 0x75bc2c0], [7, 0x7548e60], [0, 0x75ba260], [1, 0x75aaea0], [2, 0x75b81e0], [3, 0x7601c00], [4, 0x75bd320], [5, 0x75b79c0], [6, 0x75dcae0], [7, 0x7569680], [0, 0x75daa80]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75cb6c0], [2, 0x75d8a00], [3, 0x7622420], [4, 0x75ddb40], [5, 0x75d81e0], [6, 0x75fd300], [7, 0x7589ea0], [0, 0x75fb2a0]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x75dfbe0], [5, 0x75da280], [6, 0x75ff3a0], [7, 0x758bf40], [0, 0x75fd340], [1, 0x75cdfa0], [2, 0x75db2e0], [3, 0x7624d00], [4, 0x7600400], [5, 0x75faaa0], [6, 0x761fbc0], [7, 0x75ac760], [0, 0x761db60], [1, 0x75ee7c0], [2, 0x75fbb00], [3, 0x7645520]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7620c20], [5, 0x761b2c0], [6, 0x76403e0], [7, 0x75ccf80], [0, 0x763e380], [1, 0x760efe0], [2, 0x761c320], [3, 0x7665d40]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7622cc0], [5, 0x761d360], [6, 0x7642480], [7, 0x75cf020], [0, 0x7640420], [1, 0x7611080], [2, 0x761e3c0], [3, 0x7667de0], [4, 0x76434e0], [5, 0x763db80], [6, 0x7662ca0], [7, 0x75ef840], [0, 0x7660c40], [1, 0x76318a0], [2, 0x763ebe0], [3, 0x7688600]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7663d00], [5, 0x765e3a0], [6, 0x76834c0], [7, 0x7610060], [0, 0x7681460], [1, 0x76520c0], [2, 0x765f400], [3, 0x76a8e20]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x76614a0]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7be34a0]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7beae80], [6, 0x7c003c0], [7, 0x7bac720], [0, 0x7c0b6a0], [1, 0x7bdeba0], [2, 0x7bfbac0], [3, 0x7c25d20], [4, 0x7bf38c0], [5, 0x7c2bea0], [6, 0x7c413e0], [7, 0x7bed740], [0, 0x7c4c6c0], [1, 0x7c1fbc0], [2, 0x7c3cae0], [3, 0x7c66d40], [4, 0x7c348e0], [5, 0x7c6cec0], [6, 0x7c82400], [7, 0x7c2e760], [0, 0x7c8d6e0], [1, 0x7c60be0], [2, 0x7c7db00], [3, 0x7ca7d60], [4, 0x7c75900], [5, 0x7cadee0], [6, 0x7cc3420], [7, 0x7c6f780], [0, 0x7cce700], [1, 0x7ca1c00], [2, 0x7cbeb20], [3, 0x7ce8d80], [4, 0x7cb6920]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ceef00], [6, 0x7d04440], [7, 0x7cb07a0], [0, 0x7d0f720], [1, 0x7ce2c20], [2, 0x7cffb40], [3, 0x7d29da0], [4, 0x7cf7940]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7cf7120], [6, 0x7d0c660], [7, 0x7cb89c0], [0, 0x7d17940], [1, 0x7ceae40], [2, 0x7d07d60], [3, 0x7d31fc0], [4, 0x7cffb60], [5, 0x7d38140], [6, 0x7d4d680], [7, 0x7cf99e0], [0, 0x7d58960], [1, 0x7d2be60], [2, 0x7d48d80], [3, 0x7d72fe0], [4, 0x7d40b80], [5, 0x7d79160], [6, 0x7d8e6a0], [7, 0x7d3aa00], [0, 0x7d99980], [1, 0x7d6ce80], [2, 0x7d89da0], [3, 0x7db4000], [4, 0x7d81ba0], [5, 0x7dba180], [6, 0x7dcf6c0], [7, 0x7d7ba20], [0, 0x7dda9a0], [1, 0x7dadea0], [2, 0x7dcadc0], [3, 0x7df5020], [4, 0x7dc2bc0]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7dfb1a0], [6, 0x7e106e0], [7, 0x7dbca40], [0, 0x7e1b9c0], [1, 0x7deeec0], [2, 0x7e0bde0], [3, 0x7e36040], [4, 0x7e03be0]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e380e0]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e46cc0]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e3eac0], [6, 0x7e54000], [7, 0x7e02c00], [0, 0x7e61b80], [1, 0x7e327e0], [2, 0x7e4f700], [3, 0x7e89540], [4, 0x7e570e0], [5, 0x7e5f2e0], [6, 0x7e74820], [7, 0x7e23420], [0, 0x7e823a0], [1, 0x7e53000], [2, 0x7e6ff20], [3, 0x7ea9d60], [4, 0x7e77900]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e7fb00], [6, 0x7e95040], [7, 0x7e43c40], [0, 0x7ea2bc0], [1, 0x7e73820], [2, 0x7e90740], [3, 0x7eca580], [4, 0x7e98120]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e81ba0], [6, 0x7e970e0], [7, 0x7e45ce0], [0, 0x7ea4c60], [1, 0x7e758c0], [2, 0x7e927e0], [3, 0x7ecc620], [4, 0x7e9a1c0], [5, 0x7ea23c0], [6, 0x7eb7900], [7, 0x7e66500], [0, 0x7ec5480], [1, 0x7e960e0], [2, 0x7eb3000], [3, 0x7eece40], [4, 0x7eba9e0]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ec2be0], [6, 0x7ed8120], [7, 0x7e86d20], [0, 0x7ee5ca0], [1, 0x7eb6900], [2, 0x7ed3820], [3, 0x7f0d660], [4, 0x7edb200]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7ee7d40], [1, 0x7eb89a0], [2, 0x7ed58c0], [3, 0x7f0f700], [4, 0x7edd2a0], [5, 0x7ec54c0], [6, 0x7edaa00], [7, 0x7e89600], [0, 0x7f08560], [1, 0x7ed91c0], [2, 0x7ef60e0], [3, 0x7f2ff20], [4, 0x7efdac0], [5, 0x7ee5ce0], [6, 0x7efb220], [7, 0x7ea9e20]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f28d80], [1, 0x7ef99e0], [2, 0x7f16900], [3, 0x7f50740], [4, 0x7f1e2e0], [5, 0x7f06500], [6, 0x7f1ba40], [7, 0x7eca640]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7b64ca0], [3, 0x7b9eae0], [4, 0x7b5a200], [5, 0x7b61be0], [6, 0x7b799c0], [7, 0x7b25d20], [0, 0x7b750c0], [1, 0x7b581a0], [2, 0x7b854c0], [3, 0x7bbf300], [4, 0x7b7aa20], [5, 0x7b82400], [6, 0x7b9a1e0], [7, 0x7b46540], [0, 0x7b958e0], [1, 0x7b789c0]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7a39200], [2, 0x7a56120], [3, 0x7a8d6c0], [4, 0x7a48de0], [5, 0x7a53060], [6, 0x7a6ae40], [7, 0x7a075c0], [0, 0x7a66540]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7a09660]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7a3bae0]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7a58a00], [3, 0x7a92840], [4, 0x7a4df60], [5, 0x7a55940], [6, 0x7a6d720], [7, 0x7a19a80], [0, 0x7a68e20], [1, 0x7a4bf00], [2, 0x7a99a20], [3, 0x7ad3860], [4, 0x7a8ef80], [5, 0x7a96960], [6, 0x7aae740], [7, 0x7a5aaa0], [0, 0x7aa9e40], [1, 0x7a8cf20], [2, 0x7adaa40], [3, 0x7b14880], [4, 0x7acffa0], [5, 0x7ad7980], [6, 0x7aef760], [7, 0x7a9bac0], [0, 0x7aeae60], [1, 0x7acdf40], [2, 0x7b1ba60], [3, 0x7b558a0], [4, 0x7b10fc0], [5, 0x7b189a0], [6, 0x7b30780], [7, 0x7adcae0], [0, 0x7b2be80], [1, 0x7b0ef60]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7b5ca80], [3, 0x7b968c0], [4, 0x7b51fe0], [5, 0x7b599c0], [6, 0x7b717a0], [7, 0x7b1db00], [0, 0x7b6cea0], [1, 0x7b4ff80]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7935180], [2, 0x79520a0], [3, 0x7989640], [4, 0x7944d60], [5, 0x794efe0], [6, 0x7966dc0], [7, 0x7903540], [0, 0x79624c0], [1, 0x79761a0], [2, 0x79930c0], [3, 0x79ca660], [4, 0x7985d80], [5, 0x7990000], [6, 0x79a7de0], [7, 0x7944560], [0, 0x79a34e0], [1, 0x79b71c0], [2, 0x79d40e0], [3, 0x7a0b680], [4, 0x79c6da0], [5, 0x79d1020], [6, 0x79e8e00], [7, 0x7985580], [0, 0x79e4500], [1, 0x79f81e0], [2, 0x7a15100], [3, 0x7a4c6a0], [4, 0x7a07dc0], [5, 0x7a12040], [6, 0x7a29e20], [7, 0x79c65a0], [0, 0x7a25520]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7ba5ce0], [3, 0x7bdfb20], [4, 0x7b9b240], [5, 0x7ba2c20], [6, 0x7bbaa00], [7, 0x7b66d60], [0, 0x7bb6100], [1, 0x7b991e0]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7bb81a0]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7ba85c0]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7be2400], [4, 0x7ba03c0], [5, 0x7ba7da0], [6, 0x7bbd2e0], [7, 0x7b69640], [0, 0x7bc85c0], [1, 0x7b9bac0], [2, 0x7bb89e0], [3, 0x7c02c20], [4, 0x7bc0be0], [5, 0x7bc85c0], [6, 0x7bddb00], [7, 0x7b89e60], [0, 0x7be8de0], [1, 0x7bbc2e0], [2, 0x7bd9200]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c23440], [4, 0x7be1400], [5, 0x7be8de0], [6, 0x7bfe320], [7, 0x7baa680], [0, 0x7c09600], [1, 0x7bdcb00], [2, 0x7bf9a20]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9310800], [2, 0x93065a0], [3, 0x936f780], [4, 0x9325520], [5, 0x92fbb00], [6, 0x9347dc0], [7, 0x92e4540], [0, 0x933bae0], [1, 0x9331020], [2, 0x9326dc0], [3, 0x938ffa0], [4, 0x9345d40], [5, 0x931c320], [6, 0x93685e0], [7, 0x9304d60], [0, 0x935c300]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x911d200], [6, 0x916bd60], [7, 0x910ad80], [0, 0x9162320], [1, 0x9125400], [2, 0x912ad80], [3, 0x9186c20], [4, 0x913a120]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9306540], [7, 0x92a2cc0], [0, 0x92fa260], [1, 0x92cf7c0], [2, 0x92c5560], [3, 0x932e740], [4, 0x92e44e0], [5, 0x92baac0], [6, 0x9326d60], [7, 0x92c34e0], [0, 0x931aa80], [1, 0x92effe0], [2, 0x92e5d80], [3, 0x934ef60], [4, 0x9304d00], [5, 0x92db2e0]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9351840], [2, 0x93475e0], [3, 0x93b07c0], [4, 0x9366560], [5, 0x933cb40], [6, 0x9388e00], [7, 0x9325580], [0, 0x937cb20]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9349680], [3, 0x93b2860], [4, 0x9368600], [5, 0x933ebe0], [6, 0x938aea0], [7, 0x9327620], [0, 0x937ebc0], [1, 0x9354120], [2, 0x9369ea0], [3, 0x93d3080], [4, 0x9388e20], [5, 0x935f400], [6, 0x93ab6c0], [7, 0x9347e40], [0, 0x939f3e0], [1, 0x9374940]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x938a6c0], [3, 0x93f38a0], [4, 0x93a9640], [5, 0x937fc20], [6, 0x93cbee0], [7, 0x9368660], [0, 0x93bfc00], [1, 0x9395160]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x936a700]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9397200]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x938cfa0], [3, 0x93f6180], [4, 0x93bbb00], [5, 0x9382500], [6, 0x93ce7c0], [7, 0x937ab20], [0, 0x93c24e0], [1, 0x93a7620], [2, 0x93cdfc0], [3, 0x94371a0], [4, 0x93fcb20], [5, 0x93c3520], [6, 0x940f7e0], [7, 0x93bbb40], [0, 0x9403500], [1, 0x93e8640], [2, 0x940efe0], [3, 0x94781c0], [4, 0x943db40], [5, 0x9404540], [6, 0x9450800], [7, 0x93fcb60], [0, 0x9444520], [1, 0x9429660], [2, 0x9450000], [3, 0x94b91e0], [4, 0x947eb60], [5, 0x9445560], [6, 0x9491820], [7, 0x943db80], [0, 0x9485540], [1, 0x946a680]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9491020], [3, 0x94fa200], [4, 0x94bfb80], [5, 0x9486580], [6, 0x94d2840], [7, 0x947eba0], [0, 0x94c6560], [1, 0x94ab6a0]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9499240], [3, 0x9502420], [4, 0x94c7da0], [5, 0x948e7a0], [6, 0x94daa60], [7, 0x9486dc0], [0, 0x94ce780], [1, 0x94b38c0], [2, 0x94da260], [3, 0x9543440], [4, 0x9508dc0], [5, 0x94cf7c0], [6, 0x951ba80], [7, 0x94c7de0], [0, 0x950f7a0], [1, 0x94f48e0], [2, 0x951b280], [3, 0x9584460], [4, 0x9549de0], [5, 0x95107e0], [6, 0x955caa0], [7, 0x9508e00], [0, 0x95507c0], [1, 0x9535900], [2, 0x955c2a0], [3, 0x95c5480], [4, 0x958ae00], [5, 0x9551800], [6, 0x959dac0], [7, 0x9549e20], [0, 0x95917e0], [1, 0x9576920]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x959d2c0], [3, 0x96064a0], [4, 0x95cbe20], [5, 0x9592820], [6, 0x95deae0], [7, 0x958ae40], [0, 0x95d2800], [1, 0x95b7940]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9205460]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x93ab6e0]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x92378e0], [7, 0x91d6900], [0, 0x922dea0], [1, 0x91f0f80], [2, 0x91f6900], [3, 0x92527a0], [4, 0x9215880], [5, 0x91e95c0], [6, 0x9258100], [7, 0x91f7120], [0, 0x924e6c0], [1, 0x92117a0], [2, 0x9217120], [3, 0x9272fc0], [4, 0x92360a0], [5, 0x9209de0]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9278920], [7, 0x9217940], [0, 0x926eee0], [1, 0x9231fc0], [2, 0x9237940], [3, 0x92937e0], [4, 0x92568c0], [5, 0x922a600]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x927a9c0], [7, 0x92199e0], [0, 0x9270f80], [1, 0x9234060], [2, 0x92399e0], [3, 0x9295880], [4, 0x9258960], [5, 0x922c6a0], [6, 0x929b1e0], [7, 0x923a200], [0, 0x92917a0], [1, 0x9254880], [2, 0x925a200], [3, 0x92b60a0], [4, 0x9279180], [5, 0x924cec0]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x92bba00], [7, 0x925aa20], [0, 0x92b1fc0], [1, 0x92750a0], [2, 0x927aa20], [3, 0x92d68c0], [4, 0x92999a0], [5, 0x926d6e0]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x91b5080], [3, 0x9210f20], [4, 0x91c4420], [5, 0x91a7d40], [6, 0x91f68a0], [7, 0x91958c0], [0, 0x91ece60], [1, 0x91aff40], [2, 0x91d58a0], [3, 0x9231740], [4, 0x91e4c40], [5, 0x91c8560], [6, 0x92170c0], [7, 0x91b60e0], [0, 0x920d680], [1, 0x91d0760]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92d8960], [4, 0x929ba40], [5, 0x926f780], [6, 0x92be2e0], [7, 0x925d300], [0, 0x92b48a0], [1, 0x927a220], [2, 0x927fba0]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92daa00], [4, 0x929dae0], [5, 0x9271820], [6, 0x92c0380], [7, 0x925f3a0], [0, 0x92b6940], [1, 0x927c2c0], [2, 0x9281c40], [3, 0x92fb220], [4, 0x92be300], [5, 0x9292040], [6, 0x92e0ba0], [7, 0x927fbc0], [0, 0x92d7160], [1, 0x929cae0], [2, 0x92a2460]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x931ba40], [4, 0x92deb20], [5, 0x92b2860], [6, 0x93013c0], [7, 0x92a03e0], [0, 0x92f7980], [1, 0x92bd300], [2, 0x92c2c80]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x92bf3a0]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x931e320]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9838180], [6, 0x9889580], [7, 0x98358e0], [0, 0x9878160], [1, 0x985d2a0], [2, 0x9862c20], [3, 0x98ac640], [4, 0x9891780], [5, 0x98791a0], [6, 0x98ca5a0], [7, 0x9876900], [0, 0x98b9180], [1, 0x989e2c0], [2, 0x98a3c40], [3, 0x98ed660], [4, 0x98d27a0], [5, 0x98ba1c0], [6, 0x990b5c0], [7, 0x98b7920], [0, 0x98fa1a0], [1, 0x98df2e0], [2, 0x98e4c60], [3, 0x992e680], [4, 0x99137c0], [5, 0x98fb1e0], [6, 0x994c5e0], [7, 0x98f8940], [0, 0x993b1c0], [1, 0x9920300], [2, 0x9925c80], [3, 0x996f6a0], [4, 0x99547e0]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97a74c0], [6, 0x97f6020], [7, 0x97a2380], [0, 0x97e74a0], [1, 0x97cc5e0], [2, 0x97c2380], [3, 0x981b980], [4, 0x97f0ee0]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x993c200], [6, 0x998d600], [7, 0x9939960], [0, 0x997c1e0], [1, 0x9961320], [2, 0x9966ca0], [3, 0x99b06c0], [4, 0x9995800], [5, 0x997d220], [6, 0x99ce620], [7, 0x997a980], [0, 0x99bd200], [1, 0x99a2340], [2, 0x99a7cc0], [3, 0x99f16e0], [4, 0x99d6820], [5, 0x99be240], [6, 0x9a0f640], [7, 0x99bb9a0], [0, 0x99fe220], [1, 0x99e3360], [2, 0x99e8ce0], [3, 0x9a32700], [4, 0x9a17840], [5, 0x99ff260], [6, 0x9a50660], [7, 0x99fc9c0], [0, 0x9a3f240], [1, 0x9a24380], [2, 0x9a29d00], [3, 0x9a73720], [4, 0x9a58860]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9a40280], [6, 0x9a91680], [7, 0x9a3d9e0], [0, 0x9a80260], [1, 0x9a653a0], [2, 0x9a6ad20], [3, 0x9ab4740], [4, 0x9a99880]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ab67e0]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9a42b60]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9a93f60], [7, 0x9a42b60], [0, 0x9a853e0], [1, 0x9a67c80], [2, 0x9a6d600], [3, 0x9ac6c00], [4, 0x9a9c160], [5, 0x9a52f80], [6, 0x9ab4780], [7, 0x9a63380], [0, 0x9aa5c00], [1, 0x9a884a0], [2, 0x9a8de20], [3, 0x9ae7420], [4, 0x9abc980], [5, 0x9a737a0]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9ad4fa0], [7, 0x9a83ba0], [0, 0x9ac6420], [1, 0x9aa8cc0], [2, 0x9aae640], [3, 0x9b07c40], [4, 0x9add1a0], [5, 0x9a93fc0]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9a85c40], [0, 0x9ac84c0], [1, 0x9aaad60], [2, 0x9ab06e0], [3, 0x9b09ce0], [4, 0x9adf240], [5, 0x9a96060], [6, 0x9ad7880], [7, 0x9aa6460], [0, 0x9ae8ce0], [1, 0x9acb580], [2, 0x9ad0f00], [3, 0x9b2a500], [4, 0x9affa60], [5, 0x9ab6880], [6, 0x9af80a0]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9ac6c80], [0, 0x9b09500], [1, 0x9aebda0], [2, 0x9af1720], [3, 0x9b4ad20], [4, 0x9b20280], [5, 0x9ad70a0], [6, 0x9b188c0]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9af37c0], [3, 0x9b4cdc0], [4, 0x9b22320], [5, 0x9ad9140], [6, 0x9b1a960], [7, 0x9ac9560], [0, 0x9b0bde0], [1, 0x9aee680], [2, 0x9b13fe0], [3, 0x9b6d5e0], [4, 0x9b42b40], [5, 0x9af9960], [6, 0x9b3b180], [7, 0x9ae9d80], [0, 0x9b2c600], [1, 0x9b0eea0]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b34800], [3, 0x9b8de00], [4, 0x9b63360], [5, 0x9b1a180], [6, 0x9b5b9a0], [7, 0x9b0a5a0], [0, 0x9b4ce20], [1, 0x9b2f6c0]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b368a0], [3, 0x9b8fea0], [4, 0x9b65400], [5, 0x9b1c220], [6, 0x9b5da40], [7, 0x9b0c640], [0, 0x9b4eec0], [1, 0x9b31760], [2, 0x9b570c0], [3, 0x9bb06c0], [4, 0x9b85c20], [5, 0x9b3ca40], [6, 0x9b7e260], [7, 0x9b2ce60], [0, 0x9b6f6e0], [1, 0x9b51f80]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x98360e0], [6, 0x98874e0], [7, 0x9833840], [0, 0x98760c0], [1, 0x985b200], [2, 0x9860b80], [3, 0x98aa5a0], [4, 0x988f6e0]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x959fba0]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x95ce700]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9595100], [6, 0x95e3c60], [7, 0x958ffc0], [0, 0x95d50e0], [1, 0x95ba220], [2, 0x95affc0], [3, 0x96095c0], [4, 0x95deb20], [5, 0x95d6120], [6, 0x9624c80], [7, 0x95d0fe0], [0, 0x9616100], [1, 0x95fb240], [2, 0x95f0fe0], [3, 0x964a5e0], [4, 0x961fb40], [5, 0x9617140], [6, 0x9665ca0], [7, 0x9612000], [0, 0x9657120], [1, 0x963c260], [2, 0x9632000], [3, 0x968b600], [4, 0x9660b60], [5, 0x9658160], [6, 0x96a6cc0], [7, 0x9653020], [0, 0x9698140], [1, 0x967d280], [2, 0x9673020], [3, 0x96cc620], [4, 0x96a1b80]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9699180], [6, 0x96e7ce0], [7, 0x9694040], [0, 0x96d9160], [1, 0x96be2a0], [2, 0x96b4040], [3, 0x970d640], [4, 0x96e2ba0]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x96a13a0], [6, 0x96eff00], [7, 0x969c260], [0, 0x96e1380], [1, 0x96c64c0], [2, 0x96bc260], [3, 0x9715860], [4, 0x96eadc0], [5, 0x96e23c0], [6, 0x9730f20], [7, 0x96dd280], [0, 0x97223a0], [1, 0x97074e0], [2, 0x96fd280], [3, 0x9756880], [4, 0x972bde0], [5, 0x97233e0], [6, 0x9771f40], [7, 0x971e2a0], [0, 0x97633c0], [1, 0x9748500], [2, 0x973e2a0], [3, 0x97978a0], [4, 0x976ce00], [5, 0x9764400], [6, 0x97b2f60], [7, 0x975f2c0], [0, 0x97a43e0], [1, 0x9789520], [2, 0x977f2c0], [3, 0x97d88c0], [4, 0x97ade20]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97a5420], [6, 0x97f3f80], [7, 0x97a02e0], [0, 0x97e5400], [1, 0x97ca540], [2, 0x97c02e0], [3, 0x98198e0], [4, 0x97eee40]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x97ca5a0]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97f9100]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97aff20], [6, 0x9801320], [7, 0x97ad680], [0, 0x97eff00], [1, 0x97d5040], [2, 0x97da9c0], [3, 0x98243e0], [4, 0x9809520], [5, 0x97d0740], [6, 0x9821b40], [7, 0x97cdea0], [0, 0x9810720], [1, 0x97f5860], [2, 0x97fb1e0], [3, 0x9844c00], [4, 0x9829d40]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97f0f60], [6, 0x9842360], [7, 0x97ee6c0], [0, 0x9830f40], [1, 0x9816080], [2, 0x981ba00], [3, 0x9865420], [4, 0x984a560]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97f3000], [6, 0x9844400], [7, 0x97f0760], [0, 0x9832fe0], [1, 0x9818120], [2, 0x981daa0], [3, 0x98674c0], [4, 0x984c600], [5, 0x9813820], [6, 0x9864c20], [7, 0x9810f80], [0, 0x9853800], [1, 0x9838940], [2, 0x983e2c0], [3, 0x9887ce0], [4, 0x986ce20]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9834040], [6, 0x9885440], [7, 0x98317a0], [0, 0x9874020], [1, 0x9859160], [2, 0x985eae0], [3, 0x98a8500], [4, 0x988d640]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8a69de0], [5, 0x8a5fb80], [6, 0x8a97120], [7, 0x8a36140], [0, 0x8a950c0], [1, 0x8a581a0], [2, 0x8a5b280], [3, 0x8ab4880], [4, 0x8a8a600], [5, 0x8a803a0], [6, 0x8ab7940], [7, 0x8a56960], [0, 0x8ab58e0], [1, 0x8a789c0], [2, 0x8a7baa0], [3, 0x8ad50a0]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8aaae20], [5, 0x8aa0bc0], [6, 0x8ad8160], [7, 0x8a77180], [0, 0x8ad6100], [1, 0x8a991e0], [2, 0x8a9c2c0], [3, 0x8af58c0]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8aacec0], [5, 0x8aa2c60], [6, 0x8ada200], [7, 0x8a79220], [0, 0x8ad81a0], [1, 0x8a9b280], [2, 0x8a9e360], [3, 0x8af7960], [4, 0x8acd6e0], [5, 0x8ac3480], [6, 0x8afaa20], [7, 0x8a99a40], [0, 0x8af89c0], [1, 0x8abbaa0], [2, 0x8abeb80], [3, 0x8b18180]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8aedf00], [5, 0x8ae3ca0], [6, 0x8b1b240], [7, 0x8aba260], [0, 0x8b191e0], [1, 0x8adc2c0], [2, 0x8adf3a0], [3, 0x8b389a0]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8a47d80]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c3eac0]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8bf4860], [5, 0x8bea600], [6, 0x8c24440], [7, 0x8bc3460], [0, 0x8c1fb40], [1, 0x8be2c20], [2, 0x8be5d00], [3, 0x8c4eee0], [4, 0x8c35880], [5, 0x8c2b620], [6, 0x8c65460], [7, 0x8c04480], [0, 0x8c60b60], [1, 0x8c23c40], [2, 0x8c26d20], [3, 0x8c8ff00], [4, 0x8c768a0], [5, 0x8c6c640], [6, 0x8ca6480], [7, 0x8c454a0], [0, 0x8ca1b80], [1, 0x8c64c60], [2, 0x8c67d40], [3, 0x8cd0f20], [4, 0x8cb78c0], [5, 0x8cad660], [6, 0x8ce74a0], [7, 0x8c864c0], [0, 0x8ce2ba0], [1, 0x8ca5c80], [2, 0x8ca8d60], [3, 0x8d11f40]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8cf88e0], [5, 0x8cee680], [6, 0x8d284c0], [7, 0x8cc74e0], [0, 0x8d23bc0], [1, 0x8ce6ca0], [2, 0x8ce9d80], [3, 0x8d52f60]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8d00b00], [5, 0x8cf68a0], [6, 0x8d306e0], [7, 0x8ccf700], [0, 0x8d2bde0], [1, 0x8ceeec0], [2, 0x8cf1fa0], [3, 0x8d5b180], [4, 0x8d41b20], [5, 0x8d378c0], [6, 0x8d71700], [7, 0x8d10720], [0, 0x8d6ce00], [1, 0x8d2fee0], [2, 0x8d32fc0], [3, 0x8d9c1a0], [4, 0x8d82b40], [5, 0x8d788e0], [6, 0x8db2720], [7, 0x8d51740], [0, 0x8dade20], [1, 0x8d70f00], [2, 0x8d73fe0], [3, 0x8ddd1c0], [4, 0x8dc3b60], [5, 0x8db9900], [6, 0x8df3740], [7, 0x8d92760], [0, 0x8deee40], [1, 0x8db1f20], [2, 0x8db5000], [3, 0x8e1e1e0]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e04b80], [5, 0x8dfa920], [6, 0x8e34760], [7, 0x8dd3780], [0, 0x8e2fe60], [1, 0x8df2f40], [2, 0x8df6020], [3, 0x8e5f200]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x89171a0]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88c9560]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8891780], [5, 0x8887520], [6, 0x88aeee0], [7, 0x884df00], [0, 0x88ace80], [1, 0x886ff60], [2, 0x8880380], [3, 0x88d9980], [4, 0x88b1fa0], [5, 0x88a7d40], [6, 0x88cf700], [7, 0x886e720], [0, 0x88cd6a0], [1, 0x8890780], [2, 0x88a0ba0], [3, 0x88fa1a0]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88d27c0], [5, 0x88c8560], [6, 0x88eff20], [7, 0x888ef40], [0, 0x88edec0], [1, 0x88b0fa0], [2, 0x88c13c0], [3, 0x891a9c0]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88d4860], [5, 0x88ca600], [6, 0x88f1fc0], [7, 0x8890fe0], [0, 0x88eff60], [1, 0x88b3040], [2, 0x88c3460], [3, 0x891ca60], [4, 0x88f5080], [5, 0x88eae20], [6, 0x89127e0], [7, 0x88b1800], [0, 0x8910780], [1, 0x88d3860], [2, 0x88e3c80], [3, 0x893d280]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x89158a0], [5, 0x890b640], [6, 0x8933000], [7, 0x88d2020], [0, 0x8930fa0], [1, 0x88f4080], [2, 0x89044a0], [3, 0x895daa0]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x88d40c0], [0, 0x8933040], [1, 0x88f6120], [2, 0x8906540], [3, 0x895fb40], [4, 0x8918180], [5, 0x890df20], [6, 0x89358e0], [7, 0x88f48e0], [0, 0x8953860], [1, 0x8916940], [2, 0x8926d60], [3, 0x8980360], [4, 0x89389a0], [5, 0x892e740], [6, 0x8956100]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8915100], [0, 0x8974080], [1, 0x8937160], [2, 0x8947580], [3, 0x89a0b80], [4, 0x89591c0], [5, 0x894ef60], [6, 0x8976920]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x883eb00], [3, 0x8888520], [4, 0x8850740], [5, 0x88464e0], [6, 0x886dea0], [7, 0x880cec0], [0, 0x886be40], [1, 0x882ef20], [2, 0x885f320], [3, 0x88a8d40], [4, 0x8870f60], [5, 0x8866d00], [6, 0x888e6c0], [7, 0x882d6e0], [0, 0x888c660], [1, 0x884f740]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8976120], [1, 0x8939200], [2, 0x8949620], [3, 0x89a2c20], [4, 0x895b260], [5, 0x8951000], [6, 0x89789c0], [7, 0x89275c0]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x897aa60]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8978a00]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x893bae0], [2, 0x894e7a0], [3, 0x89a7da0], [4, 0x895db40], [5, 0x89538e0], [6, 0x898ae80], [7, 0x8929ea0], [0, 0x8988e20], [1, 0x897cb00], [2, 0x898f7c0], [3, 0x89e8dc0], [4, 0x899eb60], [5, 0x8994900], [6, 0x89cbea0], [7, 0x896aec0], [0, 0x89c9e40], [1, 0x89bdb20], [2, 0x89d07e0], [3, 0x8a29de0], [4, 0x89dfb80], [5, 0x89d5920], [6, 0x8a0cec0], [7, 0x89abee0], [0, 0x8a0ae60], [1, 0x89feb40], [2, 0x8a11800], [3, 0x8a6ae00], [4, 0x8a20ba0], [5, 0x8a16940], [6, 0x8a4dee0], [7, 0x89ecf00], [0, 0x8a4be80]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8a3fb60], [2, 0x8a52820], [3, 0x8aabe20], [4, 0x8a61bc0], [5, 0x8a57960], [6, 0x8a8ef00], [7, 0x8a2df20], [0, 0x8a8cea0]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ae1440], [3, 0x8b3aa40], [4, 0x8af07e0], [5, 0x8ae6580], [6, 0x8b203c0], [7, 0x8abf3e0], [0, 0x8b1bac0], [1, 0x8adeba0], [2, 0x8b22460], [3, 0x8b7ba60], [4, 0x8b31800], [5, 0x8b275a0], [6, 0x8b613e0], [7, 0x8b00400], [0, 0x8b5cae0], [1, 0x8b1fbc0], [2, 0x8b63480], [3, 0x8bbca80], [4, 0x8b72820], [5, 0x8b685c0], [6, 0x8ba2400], [7, 0x8b41420], [0, 0x8b9db00], [1, 0x8b60be0], [2, 0x8ba44a0], [3, 0x8bfdaa0], [4, 0x8bb3840], [5, 0x8ba95e0], [6, 0x8be3420], [7, 0x8b82440], [0, 0x8bdeb20], [1, 0x8ba1c00]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x90e4340], [5, 0x90d7840], [6, 0x9123b00], [7, 0x90c2b20], [0, 0x911c960], [1, 0x90dfa40], [2, 0x90d57e0], [3, 0x9141260]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x90d7880]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x90e6c20]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x90da120], [6, 0x9128c80], [7, 0x90c7ca0], [0, 0x911f240], [1, 0x90e2320], [2, 0x90e7ca0], [3, 0x9143b40], [4, 0x90f7040], [5, 0x90fa940], [6, 0x91494a0], [7, 0x90e84c0], [0, 0x913fa60], [1, 0x9102b40], [2, 0x91084c0], [3, 0x9164360], [4, 0x9117860]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x911b160], [6, 0x9169cc0], [7, 0x9108ce0], [0, 0x9160280], [1, 0x9123360], [2, 0x9128ce0], [3, 0x9184b80], [4, 0x9138080]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x90a3300], [5, 0x9096800], [6, 0x90e2ac0], [7, 0x9081ae0], [0, 0x90db920], [1, 0x909ea00], [2, 0x90947a0], [3, 0x9100220], [4, 0x90c3b20], [5, 0x90b7020], [6, 0x91032e0], [7, 0x90a2300], [0, 0x90fc140], [1, 0x90bf220], [2, 0x90b4fc0], [3, 0x9120a40]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x911f2a0], [6, 0x916de00], [7, 0x910ce20], [0, 0x91643c0], [1, 0x91274a0], [2, 0x912ce20], [3, 0x9188cc0], [4, 0x913c1c0]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9166460], [1, 0x9129540], [2, 0x912eec0], [3, 0x918ad60], [4, 0x913e260], [5, 0x9121b80], [6, 0x91706e0], [7, 0x910f700], [0, 0x9186c80], [1, 0x9149d60], [2, 0x914f6e0], [3, 0x91ab580], [4, 0x915ea80], [5, 0x91423a0], [6, 0x9190f00], [7, 0x912ff20]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x91a74a0], [1, 0x916a580], [2, 0x916ff00], [3, 0x91cbda0], [4, 0x917f2a0], [5, 0x9162bc0], [6, 0x91b1720], [7, 0x9150740]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x91a9540], [1, 0x916c620], [2, 0x9171fa0], [3, 0x91cde40], [4, 0x9181340], [5, 0x9164c60], [6, 0x91b37c0], [7, 0x91527e0], [0, 0x91c9d60], [1, 0x918ce40], [2, 0x91927c0], [3, 0x91ee660], [4, 0x91a1b60], [5, 0x9185480], [6, 0x91d3fe0], [7, 0x9173000]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x91ea580], [1, 0x91ad660], [2, 0x91b2fe0], [3, 0x920ee80], [4, 0x91c2380], [5, 0x91a5ca0], [6, 0x91f4800], [7, 0x9193820]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8e398e0]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8e32740]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8df5820], [2, 0x8dfb1a0], [3, 0x8e64380], [4, 0x8e07ca0], [5, 0x8dfda40], [6, 0x8e49d00], [7, 0x8dd9140], [0, 0x8e42b60], [1, 0x8e36840], [2, 0x8e3c1c0], [3, 0x8ea53a0], [4, 0x8e48cc0], [5, 0x8e3ea60], [6, 0x8e8ad20], [7, 0x8e1a160], [0, 0x8e83b80], [1, 0x8e77860], [2, 0x8e7d1e0], [3, 0x8ee63c0], [4, 0x8e89ce0], [5, 0x8e7fa80], [6, 0x8ecbd40], [7, 0x8e5b180], [0, 0x8ec4ba0], [1, 0x8eb8880], [2, 0x8ebe200], [3, 0x8f273e0], [4, 0x8ecad00], [5, 0x8ec0aa0], [6, 0x8f0cd60], [7, 0x8e9c1a0], [0, 0x8f05bc0]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8ef98a0], [2, 0x8eff220], [3, 0x8f68400], [4, 0x8f0bd20], [5, 0x8f01ac0], [6, 0x8f4dd80], [7, 0x8edd1c0], [0, 0x8f46be0]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8f01ac0], [2, 0x8f07440], [3, 0x8f70620], [4, 0x8f13f40], [5, 0x8f09ce0], [6, 0x8f55fa0], [7, 0x8ee53e0], [0, 0x8f4ee00], [1, 0x8f42ae0], [2, 0x8f48460], [3, 0x8fb1640], [4, 0x8f54f60], [5, 0x8f4ad00], [6, 0x8f96fc0], [7, 0x8f26400], [0, 0x8f8fe20], [1, 0x8f83b00], [2, 0x8f89480], [3, 0x8ff2660], [4, 0x8f95f80], [5, 0x8f8bd20], [6, 0x8fd7fe0], [7, 0x8f67420], [0, 0x8fd0e40], [1, 0x8fc4b20], [2, 0x8fca4a0], [3, 0x9033680], [4, 0x8fd6fa0], [5, 0x8fccd40], [6, 0x9019000], [7, 0x8fa8440], [0, 0x9011e60]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9005b40], [2, 0x900b4c0], [3, 0x90746a0], [4, 0x9017fc0], [5, 0x900dd60], [6, 0x905a020], [7, 0x8fe9460], [0, 0x9052e80]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8feb500]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9008420]]}
  layer.12.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x900dda0], [3, 0x9079820], [4, 0x901d140], [5, 0x9010640], [6, 0x905c900], [7, 0x8ffb920], [0, 0x9055760], [1, 0x9018840], [2, 0x902e5c0], [3, 0x909a040], [4, 0x903d960], [5, 0x9030e60], [6, 0x907d120], [7, 0x901c140], [0, 0x9075f80], [1, 0x9039060]]}
  layer.12.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x904ede0], [3, 0x90ba860], [4, 0x905e180], [5, 0x9051680], [6, 0x909d940], [7, 0x903c960], [0, 0x90967a0], [1, 0x9059880]]}
  layer.12.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9050e80], [3, 0x90bc900], [4, 0x9060220], [5, 0x9053720], [6, 0x909f9e0], [7, 0x903ea00], [0, 0x9098840], [1, 0x905b920], [2, 0x90716a0], [3, 0x90dd120], [4, 0x9080a40], [5, 0x9073f40], [6, 0x90c0200], [7, 0x905f220], [0, 0x90b9060], [1, 0x907c140]]}
  layer.12.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9091ec0], [3, 0x90fd940], [4, 0x90a1260], [5, 0x9094760], [6, 0x90e0a20], [7, 0x907fa40], [0, 0x90d9880], [1, 0x909c960]]}
  layer.12.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5603580], [3, 0x5603580], [4, 0x55f39a0], [5, 0x5613160], [6, 0x56108c0], [7, 0x55fe440], [0, 0x560e860], [1, 0x5601520], [2, 0x5623da0], [3, 0x5623da0], [4, 0x56141c0], [5, 0x5633980], [6, 0x56310e0], [7, 0x561ec60], [0, 0x562f080], [1, 0x5621d40]]}
  layer.12.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bd2940], [6, 0x5bdd3e0], [7, 0x5bb8ae0], [0, 0x5bd8ae0], [1, 0x5bcb7a0], [2, 0x5bce040], [3, 0x5bce040], [4, 0x5bc0d00]]}
  layer.12.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bd49e0], [6, 0x5bdf480], [7, 0x5bbab80], [0, 0x5bdab80], [1, 0x5bcd840], [2, 0x5bd00e0], [3, 0x5bd00e0], [4, 0x5bc2da0], [5, 0x5bf5200], [6, 0x5bffca0], [7, 0x5bdb3a0], [0, 0x5bfb3a0], [1, 0x5bee060], [2, 0x5bf0900], [3, 0x5bf0900], [4, 0x5be35c0]]}
  layer.12.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c15a20], [6, 0x5c204c0], [7, 0x5bfbbc0], [0, 0x5c1bbc0], [1, 0x5c0e880], [2, 0x5c11120], [3, 0x5c11120], [4, 0x5c03de0]]}
  layer.12.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5c131c0]]}
  layer.12.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b6f8a0]]}
  layer.12.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d1c380], [6, 0x5d26e20], [7, 0x5d04dc0], [0, 0x5d24dc0], [1, 0x5d151e0], [2, 0x5d17a80], [3, 0x5d27660], [4, 0x5d0a740], [5, 0x5d5d3a0], [6, 0x5d67e40], [7, 0x5d45de0], [0, 0x5d65de0], [1, 0x5d56200], [2, 0x5d58aa0], [3, 0x5d68680], [4, 0x5d4b760], [5, 0x5d9e3c0], [6, 0x5da8e60], [7, 0x5d86e00], [0, 0x5da6e00], [1, 0x5d97220], [2, 0x5d99ac0], [3, 0x5da96a0], [4, 0x5d8c780], [5, 0x5ddf3e0], [6, 0x5de9e80], [7, 0x5dc7e20], [0, 0x5de7e20], [1, 0x5dd8240], [2, 0x5ddaae0], [3, 0x5dea6c0], [4, 0x5dcd7a0]]}
  layer.12.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e20400], [6, 0x5e2aea0], [7, 0x5e08e40], [0, 0x5e28e40], [1, 0x5e19260], [2, 0x5e1bb00], [3, 0x5e2b6e0], [4, 0x5e0e7c0]]}
  layer.12.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e28620], [6, 0x5e330c0], [7, 0x5e11060], [0, 0x5e31060], [1, 0x5e21480], [2, 0x5e23d20], [3, 0x5e33900], [4, 0x5e169e0], [5, 0x5e69640], [6, 0x5e740e0], [7, 0x5e52080], [0, 0x5e72080], [1, 0x5e624a0], [2, 0x5e64d40], [3, 0x5e74920], [4, 0x5e57a00], [5, 0x5eaa660], [6, 0x5eb5100], [7, 0x5e930a0], [0, 0x5eb30a0], [1, 0x5ea34c0], [2, 0x5ea5d60], [3, 0x5eb5940], [4, 0x5e98a20], [5, 0x5eeb680], [6, 0x5ef6120], [7, 0x5ed40c0], [0, 0x5ef40c0], [1, 0x5ee44e0], [2, 0x5ee6d80], [3, 0x5ef6960], [4, 0x5ed9a40]]}
  layer.12.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f2c6a0], [6, 0x5f37140], [7, 0x5f150e0], [0, 0x5f350e0], [1, 0x5f25500], [2, 0x5f27da0], [3, 0x5f37980], [4, 0x5f1aa60]]}
  layer.12.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5f39a20]]}
  layer.12.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5972920]]}
  layer.13.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x59604a0], [0, 0x59804a0], [1, 0x5963580], [2, 0x5975a00], [3, 0x5965e20], [4, 0x5958ae0], [5, 0x59782a0], [6, 0x5982d40], [7, 0x5980cc0], [0, 0x59a0cc0], [1, 0x5983da0], [2, 0x5996220], [3, 0x5986640], [4, 0x5979300], [5, 0x5998ac0], [6, 0x59a3560]]}
  layer.13.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x59a14e0], [0, 0x59c14e0], [1, 0x59a45c0], [2, 0x59b6a40], [3, 0x59a6e60], [4, 0x5999b20], [5, 0x59b92e0], [6, 0x59c3d80]]}
  layer.13.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x59a3580], [0, 0x59c3580], [1, 0x59a6660], [2, 0x59b8ae0], [3, 0x59a8f00], [4, 0x599bbc0], [5, 0x59bb380], [6, 0x59c5e20], [7, 0x59c3da0], [0, 0x59e3da0], [1, 0x59c6e80], [2, 0x59d9300], [3, 0x59c9720], [4, 0x59bc3e0], [5, 0x59dbba0], [6, 0x59e6640]]}
  layer.13.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x59e45c0], [0, 0x5a045c0], [1, 0x59e76a0], [2, 0x59f9b20], [3, 0x59e9f40], [4, 0x59dcc00], [5, 0x59fc3c0], [6, 0x5a06e60]]}
  layer.13.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59fbbc0], [3, 0x59ebfe0], [4, 0x59deca0], [5, 0x59fe460], [6, 0x5a08f00], [7, 0x59e6ea0], [0, 0x5a06ea0], [1, 0x59e9f80], [2, 0x5a1c3e0], [3, 0x5a0c800], [4, 0x59ff4c0], [5, 0x5a1ec80], [6, 0x5a29720], [7, 0x5a076c0], [0, 0x5a276c0], [1, 0x5a0a7a0]]}
  layer.13.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a3cc00], [3, 0x5a2d020], [4, 0x5a1fce0], [5, 0x5a3f4a0], [6, 0x5a49f40], [7, 0x5a27ee0], [0, 0x5a47ee0], [1, 0x5a2afc0]]}
  layer.13.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a3eca0], [3, 0x5a2f0c0], [4, 0x5a21d80], [5, 0x5a41540], [6, 0x5a4bfe0], [7, 0x5a29f80], [0, 0x5a49f80], [1, 0x5a2d060], [2, 0x5a5f4c0], [3, 0x5a4f8e0], [4, 0x5a425a0], [5, 0x5a61d60], [6, 0x5a6c800], [7, 0x5a4a7a0], [0, 0x5a6a7a0], [1, 0x5a4d880]]}
  layer.13.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5970880], [7, 0x595e400], [0, 0x597e400], [1, 0x59614e0], [2, 0x5973960], [3, 0x5963d80], [4, 0x5956a40], [5, 0x5976200]]}
  layer.13.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a6e0a0]]}
  layer.13.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a70940]]}
  layer.13.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a63600], [5, 0x5a85660], [6, 0x5a90100], [7, 0x5a6b800], [0, 0x5a8b800], [1, 0x5a7e4c0], [2, 0x5a80d60], [3, 0x5a80d60], [4, 0x5aa4620], [5, 0x5ac6680], [6, 0x5ad1120], [7, 0x5aac820], [0, 0x5acc820], [1, 0x5abf4e0], [2, 0x5ac1d80], [3, 0x5ac1d80], [4, 0x5ae5640], [5, 0x5b076a0], [6, 0x5b12140], [7, 0x5aed840], [0, 0x5b0d840], [1, 0x5b00500], [2, 0x5b02da0], [3, 0x5b02da0], [4, 0x5b26660], [5, 0x5b486c0], [6, 0x5b53160], [7, 0x5b2e860], [0, 0x5b4e860], [1, 0x5b41520], [2, 0x5b43dc0], [3, 0x5b43dc0]]}
  layer.13.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b67680], [5, 0x5b896e0], [6, 0x5b94180], [7, 0x5b6f880], [0, 0x5b8f880], [1, 0x5b82540], [2, 0x5b84de0], [3, 0x5b84de0]]}
  layer.13.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c18300], [6, 0x5c22da0], [7, 0x5c00d40], [0, 0x5c20d40], [1, 0x5c11160], [2, 0x5c13a00], [3, 0x5c235e0], [4, 0x5c066c0], [5, 0x5c59320], [6, 0x5c63dc0], [7, 0x5c41d60], [0, 0x5c61d60], [1, 0x5c52180], [2, 0x5c54a20], [3, 0x5c64600], [4, 0x5c476e0], [5, 0x5c9a340], [6, 0x5ca4de0], [7, 0x5c82d80], [0, 0x5ca2d80], [1, 0x5c931a0], [2, 0x5c95a40], [3, 0x5ca5620], [4, 0x5c88700], [5, 0x5cdb360], [6, 0x5ce5e00], [7, 0x5cc3da0], [0, 0x5ce3da0], [1, 0x5cd41c0], [2, 0x5cd6a60], [3, 0x5ce6640], [4, 0x5cc9720]]}
  layer.13.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61ccec0], [6, 0x61da200], [7, 0x61b81a0], [0, 0x61d5900], [1, 0x61d3060], [2, 0x61d5900], [3, 0x61f50c0], [4, 0x61c85c0]]}
  layer.13.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x61f7160]]}
  layer.13.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61cf7a0]]}
  layer.14.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x61dcae0], [7, 0x61bd320], [0, 0x61daa80], [1, 0x61d5940], [2, 0x61d81e0], [3, 0x6207580], [4, 0x61caea0], [5, 0x61dfbc0], [6, 0x61fd300], [7, 0x61ddb40], [0, 0x61fb2a0], [1, 0x61f6160], [2, 0x61f8a00], [3, 0x6227da0], [4, 0x61eb6c0], [5, 0x62003e0]]}
  layer.14.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61cae20], [6, 0x61d8160], [7, 0x61b6100], [0, 0x61d3860], [1, 0x61d0fc0], [2, 0x61d3860], [3, 0x61f3020], [4, 0x61c6520]]}
  layer.14.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x61fe360], [0, 0x621bac0], [1, 0x6216980], [2, 0x6219220], [3, 0x62485c0], [4, 0x620bee0], [5, 0x6220c00], [6, 0x621e360], [7, 0x621eb80], [0, 0x623c2e0], [1, 0x62371a0], [2, 0x6239a40], [3, 0x6268de0], [4, 0x622c700], [5, 0x6241420], [6, 0x623eb80]]}
  layer.14.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x623f3a0], [0, 0x625cb00], [1, 0x62579c0], [2, 0x625a260], [3, 0x6289600], [4, 0x624cf20], [5, 0x6261c40], [6, 0x625f3a0]]}
  layer.14.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x625c300], [3, 0x628b6a0], [4, 0x624efc0], [5, 0x6263ce0], [6, 0x6261440], [7, 0x6241c80], [0, 0x625f3e0], [1, 0x625a2a0], [2, 0x627cb20], [3, 0x62abec0], [4, 0x626f7e0], [5, 0x6284500], [6, 0x6281c60], [7, 0x62624a0], [0, 0x627fc00], [1, 0x627aac0]]}
  layer.14.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x629d340], [3, 0x62cc6e0], [4, 0x6290000], [5, 0x62a4d20], [6, 0x62a2480], [7, 0x6282cc0], [0, 0x62a0420], [1, 0x629b2e0]]}
  layer.14.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x629f3e0], [3, 0x62ce780], [4, 0x62920a0], [5, 0x62a6dc0], [6, 0x62a4520], [7, 0x6284d60], [0, 0x62a24c0], [1, 0x629d380], [2, 0x62bfc00], [3, 0x62eefa0], [4, 0x62b28c0], [5, 0x62c75e0], [6, 0x62c4d40], [7, 0x62a5580], [0, 0x62c2ce0], [1, 0x62bdba0]]}
  layer.14.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x62e0420], [3, 0x630f7c0], [4, 0x62d30e0], [5, 0x62e7e00], [6, 0x62e5560], [7, 0x62c5da0], [0, 0x62e3500], [1, 0x62de3c0]]}
  layer.14.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f27de0]]}
  layer.14.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5f49e40]]}
  layer.14.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5f1d340], [5, 0x5f32060], [6, 0x5f3cb00], [7, 0x5f1aaa0], [0, 0x5f3aaa0], [1, 0x5f38200], [2, 0x5f2aec0], [3, 0x5f5a260], [4, 0x5f5e360], [5, 0x5f73080], [6, 0x5f7db20], [7, 0x5f5bac0], [0, 0x5f7bac0], [1, 0x5f79220], [2, 0x5f6bee0], [3, 0x5f9b280], [4, 0x5f9f380], [5, 0x5fb40a0], [6, 0x5fbeb40], [7, 0x5f9cae0], [0, 0x5fbcae0], [1, 0x5fba240], [2, 0x5facf00], [3, 0x5fdc2a0], [4, 0x5fe03a0], [5, 0x5ff50c0], [6, 0x5fffb60], [7, 0x5fddb00], [0, 0x5ffdb00], [1, 0x5ffb260], [2, 0x5fedf20], [3, 0x601d2c0]]}
  layer.14.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x60213c0], [5, 0x60360e0], [6, 0x6040b80], [7, 0x601eb20], [0, 0x603eb20], [1, 0x603c280], [2, 0x602ef40], [3, 0x605e2e0]]}
  layer.14.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x60295e0], [5, 0x603e300], [6, 0x6048da0], [7, 0x6026d40], [0, 0x6046d40], [1, 0x60444a0], [2, 0x6037160], [3, 0x6066500], [4, 0x606a600], [5, 0x607f320], [6, 0x6089dc0], [7, 0x6067d60], [0, 0x6087d60], [1, 0x60854c0], [2, 0x6078180], [3, 0x60a7520], [4, 0x60ab620], [5, 0x60c0340], [6, 0x60cade0], [7, 0x60a8d80], [0, 0x60c8d80], [1, 0x60c64e0], [2, 0x60b91a0], [3, 0x60e8540], [4, 0x60ec640], [5, 0x6101360], [6, 0x610be00], [7, 0x60e9da0], [0, 0x6109da0], [1, 0x6107500], [2, 0x60fa1c0], [3, 0x6129560]]}
  layer.14.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x612d660], [5, 0x6142380], [6, 0x614ce20], [7, 0x612adc0], [0, 0x614adc0], [1, 0x6148520], [2, 0x613b1e0], [3, 0x616a580]]}
  layer.14.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x613d280]]}
  layer.14.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x612ff40]]}
  layer.15.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6144c60], [6, 0x6151fa0], [7, 0x612ff40], [0, 0x614d6a0], [1, 0x614ae00], [2, 0x614d6a0], [3, 0x616ce60], [4, 0x6140360], [5, 0x6165480], [6, 0x61727c0], [7, 0x6150760], [0, 0x616dec0], [1, 0x616b620], [2, 0x616dec0], [3, 0x618d680], [4, 0x6160b80]]}
  layer.15.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6185ca0], [6, 0x6192fe0], [7, 0x6170f80], [0, 0x618e6e0], [1, 0x618be40], [2, 0x618e6e0], [3, 0x61adea0], [4, 0x61813a0]]}
  layer.15.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6187d40], [6, 0x6195080], [7, 0x6173020], [0, 0x6190780], [1, 0x618dee0], [2, 0x6190780], [3, 0x61aff40], [4, 0x6183440], [5, 0x61a8560], [6, 0x61b58a0], [7, 0x6193840], [0, 0x61b0fa0], [1, 0x61ae700], [2, 0x61b0fa0], [3, 0x61d0760], [4, 0x61a3c60]]}
  layer.15.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61c8d80], [6, 0x61d60c0], [7, 0x61b4060], [0, 0x61d17c0], [1, 0x61cef20], [2, 0x61d17c0], [3, 0x61f0f80], [4, 0x61c4480]]}
  layer.15.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x51015e0], [3, 0x510e920], [4, 0x50fed40], [5, 0x511e500], [6, 0x510e920], [7, 0x50fed40], [0, 0x510f160], [1, 0x5101e20], [2, 0x5121e00], [3, 0x512f140], [4, 0x511f560], [5, 0x513ed20], [6, 0x512f140], [7, 0x511f560], [0, 0x512f980], [1, 0x5122640]]}
  layer.15.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5142620], [3, 0x514f960], [4, 0x513fd80], [5, 0x515f540], [6, 0x514f960], [7, 0x513fd80], [0, 0x51501a0], [1, 0x5142e60]]}
  layer.15.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x51446c0], [3, 0x5151a00], [4, 0x5141e20], [5, 0x51615e0], [6, 0x5151a00], [7, 0x5141e20], [0, 0x5152240], [1, 0x5144f00], [2, 0x5164ee0], [3, 0x5172220], [4, 0x5162640], [5, 0x5181e00], [6, 0x5172220], [7, 0x5162640], [0, 0x5172a60], [1, 0x5165720]]}
  layer.15.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5185700], [3, 0x5192a40], [4, 0x5182e60], [5, 0x51a2620], [6, 0x5192a40], [7, 0x5182e60], [0, 0x5193280], [1, 0x5185f40]]}
  layer.15.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x50bdd00]]}
  layer.15.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5187fe0]]}
  layer.15.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5195320], [4, 0x5187fe0], [5, 0x51a77a0], [6, 0x5195320], [7, 0x5185740], [0, 0x5195b60], [1, 0x5188820], [2, 0x5198400], [3, 0x51d6340], [4, 0x51c9000], [5, 0x51e87c0], [6, 0x51d6340], [7, 0x51c6760], [0, 0x51d6b80], [1, 0x51c9840], [2, 0x51d9420], [3, 0x5217360], [4, 0x520a020], [5, 0x52297e0], [6, 0x5217360], [7, 0x5207780], [0, 0x5217ba0], [1, 0x520a860], [2, 0x521a440], [3, 0x5258380], [4, 0x524b040], [5, 0x526a800], [6, 0x5258380], [7, 0x52487a0], [0, 0x5258bc0], [1, 0x524b880], [2, 0x525b460]]}
  layer.15.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x52993a0], [4, 0x528c060], [5, 0x52ab820], [6, 0x52993a0], [7, 0x52897c0], [0, 0x5299be0], [1, 0x528c8a0], [2, 0x529c480]]}
  layer.15.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x52a15c0], [4, 0x5294280], [5, 0x52b3a40], [6, 0x52a15c0], [7, 0x52919e0], [0, 0x52a1e00], [1, 0x5294ac0], [2, 0x52a46a0], [3, 0x52e25e0], [4, 0x52d52a0], [5, 0x52f4a60], [6, 0x52e25e0], [7, 0x52d2a00], [0, 0x52e2e20], [1, 0x52d5ae0], [2, 0x52e56c0], [3, 0x5323600], [4, 0x53162c0], [5, 0x5335a80], [6, 0x5323600], [7, 0x5313a20], [0, 0x5323e40], [1, 0x5316b00], [2, 0x53266e0], [3, 0x5364620], [4, 0x53572e0], [5, 0x5376aa0], [6, 0x5364620], [7, 0x5354a40], [0, 0x5364e60], [1, 0x5357b20], [2, 0x5367700]]}
  layer.15.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x53a5640], [4, 0x5398300], [5, 0x53b7ac0], [6, 0x53a5640], [7, 0x5395a60], [0, 0x53a5e80], [1, 0x5398b40], [2, 0x53a8720]]}
  layer.15.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5077b00]]}
  layer.15.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5077b00]]}
  layer.16.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5036ac0], [4, 0x5036ac0], [5, 0x5036ac0], [6, 0x5036ac0], [7, 0x5036ac0], [0, 0x5037300], [1, 0x5037300], [2, 0x5037300], [3, 0x50572e0], [4, 0x50572e0], [5, 0x50572e0], [6, 0x50572e0], [7, 0x50572e0], [0, 0x5057b20], [1, 0x5057b20], [2, 0x5057b20]]}
  layer.16.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5034a20], [4, 0x5034a20], [5, 0x5034a20], [6, 0x5034a20], [7, 0x5034a20], [0, 0x5035260], [1, 0x5035260], [2, 0x5035260]]}
  layer.16.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4ff39e0], [4, 0x4ff39e0], [5, 0x4ff39e0], [6, 0x4ff39e0], [7, 0x4ff39e0], [0, 0x4ff4220], [1, 0x4ff4220], [2, 0x4ff4220], [3, 0x5014200], [4, 0x5014200], [5, 0x5014200], [6, 0x5014200], [7, 0x5014200], [0, 0x5014a40], [1, 0x5014a40], [2, 0x5014a40]]}
  layer.16.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4ff1940], [4, 0x4ff1940], [5, 0x4ff1940], [6, 0x4ff1940], [7, 0x4ff1940], [0, 0x4ff2180], [1, 0x4ff2180], [2, 0x4ff2180]]}
  layer.16.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900], [1, 0x4fb0900], [2, 0x4fb0900], [3, 0x4fb0900], [4, 0x4fb0900], [5, 0x4fb0900], [6, 0x4fb0900], [7, 0x4fb0900], [0, 0x4fd1120], [1, 0x4fd1120], [2, 0x4fd1120], [3, 0x4fd1120], [4, 0x4fd1120], [5, 0x4fd1120], [6, 0x4fd1120], [7, 0x4fd1120]]}
  layer.16.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5077b00], [0, 0x5078340], [1, 0x5078340], [2, 0x5078340], [3, 0x5087f20], [4, 0x5078340], [5, 0x5087f20], [6, 0x5087f20]]}
  layer.16.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5079ba0], [0, 0x507a3e0], [1, 0x507a3e0], [2, 0x507a3e0], [3, 0x5089fc0], [4, 0x507a3e0], [5, 0x5089fc0], [6, 0x5089fc0], [7, 0x509a3c0], [0, 0x509ac00], [1, 0x509ac00], [2, 0x509ac00], [3, 0x50aa7e0], [4, 0x509ac00], [5, 0x50aa7e0], [6, 0x50aa7e0]]}
  layer.16.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50babe0], [0, 0x50bb420], [1, 0x50bb420], [2, 0x50bb420], [3, 0x50cb000], [4, 0x50bb420], [5, 0x50cb000], [6, 0x50cb000]]}
  layer.16.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50cd0a0]]}
  layer.16.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5077b00]]}
  layer.16.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5794180], [4, 0x5786e40], [5, 0x57a6600], [6, 0x57a14c0], [7, 0x578f040], [0, 0x57af040], [1, 0x5792120], [2, 0x57a45a0], [3, 0x57d51a0], [4, 0x57c7e60], [5, 0x57e7620], [6, 0x57e24e0], [7, 0x57d0060], [0, 0x57f0060], [1, 0x57d3140], [2, 0x57e55c0], [3, 0x58161c0], [4, 0x5808e80], [5, 0x5828640], [6, 0x5823500], [7, 0x5811080], [0, 0x5831080], [1, 0x5814160], [2, 0x58265e0], [3, 0x58571e0], [4, 0x5849ea0], [5, 0x5869660], [6, 0x5864520], [7, 0x58520a0], [0, 0x58720a0], [1, 0x5855180], [2, 0x5867600]]}
  layer.16.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5685600], [3, 0x5685600], [4, 0x5675a20], [5, 0x56951e0], [6, 0x5692940], [7, 0x56804c0], [0, 0x56908e0], [1, 0x56835a0]]}
  layer.16.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x568d820], [3, 0x568d820], [4, 0x567dc40], [5, 0x569d400], [6, 0x569ab60], [7, 0x56886e0], [0, 0x5698b00], [1, 0x568b7c0], [2, 0x56ce840], [3, 0x56ce840], [4, 0x56bec60], [5, 0x56de420], [6, 0x56dbb80], [7, 0x56c9700], [0, 0x56d9b20], [1, 0x56cc7e0], [2, 0x570f860], [3, 0x570f860], [4, 0x56ffc80], [5, 0x571f440], [6, 0x571cba0], [7, 0x570a720], [0, 0x571ab40], [1, 0x570d800], [2, 0x5750880], [3, 0x5750880], [4, 0x5740ca0], [5, 0x5760460], [6, 0x575dbc0], [7, 0x574b740], [0, 0x575bb60], [1, 0x574e820]]}
  layer.16.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x57918a0], [3, 0x57918a0], [4, 0x5781cc0], [5, 0x57a1480], [6, 0x579ebe0], [7, 0x578c760], [0, 0x579cb80], [1, 0x578f840]]}
  layer.16.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x579ec20]]}
  layer.16.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5794180]]}
  layer.17.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56445c0], [3, 0x56445c0], [4, 0x56349e0], [5, 0x56541a0], [6, 0x5651900], [7, 0x563f480], [0, 0x564f8a0], [1, 0x5642560], [2, 0x5664de0], [3, 0x5664de0], [4, 0x5655200], [5, 0x56749c0], [6, 0x5672120], [7, 0x565fca0], [0, 0x56700c0], [1, 0x5662d80]]}
  layer.17.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5898200], [4, 0x588aec0], [5, 0x58aa680], [6, 0x58a5540], [7, 0x58930c0], [0, 0x58b30c0], [1, 0x58961a0], [2, 0x58a8620]]}
  layer.17.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x589a2a0], [4, 0x588cf60], [5, 0x58ac720], [6, 0x58a75e0], [7, 0x5895160], [0, 0x58b5160], [1, 0x5898240], [2, 0x58aa6c0], [3, 0x58baac0], [4, 0x58ad780], [5, 0x58ccf40], [6, 0x58c7e00], [7, 0x58b5980], [0, 0x58d5980], [1, 0x58b8a60], [2, 0x58caee0]]}
  layer.17.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x58db2e0], [4, 0x58cdfa0], [5, 0x58ed760], [6, 0x58e8620], [7, 0x58d61a0], [0, 0x58f61a0], [1, 0x58d9280], [2, 0x58eb700]]}
  layer.17.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x58ea6c0], [7, 0x58d8240], [0, 0x58f8240], [1, 0x58db320], [2, 0x58ed7a0], [3, 0x58ddbc0], [4, 0x58d0880], [5, 0x58f0040], [6, 0x590aee0], [7, 0x58f8a60], [0, 0x5918a60], [1, 0x58fbb40], [2, 0x590dfc0], [3, 0x58fe3e0], [4, 0x58f10a0], [5, 0x5910860]]}
  layer.17.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x592b700], [7, 0x5919280], [0, 0x5939280], [1, 0x591c360], [2, 0x592e7e0], [3, 0x591ec00], [4, 0x59118c0], [5, 0x5931080]]}
  layer.17.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x592d7a0], [7, 0x591b320], [0, 0x593b320], [1, 0x591e400], [2, 0x5930880], [3, 0x5920ca0], [4, 0x5913960], [5, 0x5933120], [6, 0x594dfc0], [7, 0x593bb40], [0, 0x595bb40], [1, 0x593ec20], [2, 0x59510a0], [3, 0x59414c0], [4, 0x5934180], [5, 0x5953940]]}
  layer.17.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x596e7e0], [7, 0x595c360], [0, 0x597c360], [1, 0x595f440], [2, 0x59718c0], [3, 0x5961ce0], [4, 0x59549a0], [5, 0x5974160]]}
  layer.17.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x53aa7c0]]}
  layer.17.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53a8760]]}
  layer.17.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x539b420], [2, 0x53ad8a0], [3, 0x53ab000], [4, 0x539b420], [5, 0x53bd480], [6, 0x53babe0], [7, 0x5398b80], [0, 0x53b8b80], [1, 0x53dc440], [2, 0x53ee8c0], [3, 0x53ec020], [4, 0x53dc440], [5, 0x53fe4a0], [6, 0x53fbc00], [7, 0x53d9ba0], [0, 0x53f9ba0], [1, 0x541d460], [2, 0x542f8e0], [3, 0x542d040], [4, 0x541d460], [5, 0x543f4c0], [6, 0x543cc20], [7, 0x541abc0], [0, 0x543abc0], [1, 0x545e480], [2, 0x5470900], [3, 0x546e060], [4, 0x545e480], [5, 0x54804e0], [6, 0x547dc40], [7, 0x545bbe0], [0, 0x547bbe0]]}
  layer.17.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x549f4a0], [2, 0x54b1920], [3, 0x54af080], [4, 0x549f4a0], [5, 0x54c1500], [6, 0x54bec60], [7, 0x549cc00], [0, 0x54bcc00]]}
  layer.17.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x54a76c0], [2, 0x54b9b40], [3, 0x54b72a0], [4, 0x54a76c0], [5, 0x54c9720], [6, 0x54c6e80], [7, 0x54a4e20], [0, 0x54c4e20], [1, 0x54e86e0], [2, 0x54fab60], [3, 0x54f82c0], [4, 0x54e86e0], [5, 0x550a740], [6, 0x5507ea0], [7, 0x54e5e40], [0, 0x5505e40], [1, 0x5529700], [2, 0x553bb80], [3, 0x55392e0], [4, 0x5529700], [5, 0x554b760], [6, 0x5548ec0], [7, 0x5526e60], [0, 0x5546e60], [1, 0x556a720], [2, 0x557cba0], [3, 0x557a300], [4, 0x556a720], [5, 0x558c780], [6, 0x5589ee0], [7, 0x5567e80], [0, 0x5587e80]]}
  layer.17.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55ab740], [2, 0x55bdbc0], [3, 0x55bb320], [4, 0x55ab740], [5, 0x55cd7a0], [6, 0x55caf00], [7, 0x55a8ea0], [0, 0x55c8ea0]]}
  layer.17.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55aaf40]]}
  layer.17.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55ae020]]}
  layer.18.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55c04a0], [3, 0x55c04a0], [4, 0x55b08c0], [5, 0x55d0080], [6, 0x55cd7e0], [7, 0x55bb360], [0, 0x55cb780], [1, 0x55be440], [2, 0x55e0cc0], [3, 0x55e0cc0], [4, 0x55d10e0], [5, 0x55f08a0], [6, 0x55ee000], [7, 0x55dbb80], [0, 0x55ebfa0], [1, 0x55dec60]]}
  layer.18.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56014e0], [3, 0x56014e0], [4, 0x55f1900], [5, 0x56110c0], [6, 0x560e820], [7, 0x55fc3a0], [0, 0x560c7c0], [1, 0x55ff480]]}
  layer.18.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x50c05a0], [2, 0x50c05a0], [3, 0x50cd8e0], [4, 0x50bdd00], [5, 0x50dd4c0], [6, 0x50cd8e0], [7, 0x50bdd00], [0, 0x50ce120], [1, 0x50e0dc0], [2, 0x50e0dc0], [3, 0x50ee100], [4, 0x50de520], [5, 0x50fdce0], [6, 0x50ee100], [7, 0x50de520], [0, 0x50ee940]]}
  layer.18.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7478120], [6, 0x749d240], [7, 0x74399c0], [0, 0x749b1e0], [1, 0x747ba20], [2, 0x74864c0], [3, 0x74cfee0], [4, 0x748dea0]]}
  layer.18.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d44dc0], [7, 0x6cf39c0], [0, 0x6d52940], [1, 0x6d20d00], [2, 0x6d308e0], [3, 0x6d6cfc0], [4, 0x6d1b3a0], [5, 0x6d32960], [6, 0x6d655e0], [7, 0x6d141e0], [0, 0x6d73160], [1, 0x6d41520], [2, 0x6d51100], [3, 0x6d8d7e0], [4, 0x6d3bbc0], [5, 0x6d53180]]}
  layer.18.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d85e00], [7, 0x6d34a00], [0, 0x6d93980], [1, 0x6d61d40], [2, 0x6d71920], [3, 0x6dae000], [4, 0x6d5c3e0], [5, 0x6d739a0]]}
  layer.18.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d87ea0], [7, 0x6d36aa0], [0, 0x6d95a20], [1, 0x6d63de0], [2, 0x6d739c0], [3, 0x6db00a0], [4, 0x6d5e480], [5, 0x6d75a40], [6, 0x6da86c0], [7, 0x6d572c0], [0, 0x6db6240], [1, 0x6d84600], [2, 0x6d941e0], [3, 0x6dd08c0], [4, 0x6d7eca0], [5, 0x6d96260]]}
  layer.18.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6dc8ee0], [7, 0x6d77ae0], [0, 0x6dd6a60], [1, 0x6da4e20], [2, 0x6db4a00], [3, 0x6df10e0], [4, 0x6d9f4c0], [5, 0x6db6a80]]}
  layer.18.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6da1560]]}
  layer.18.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6dcb7c0]]}
  layer.18.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6d7a3c0], [0, 0x6ddbbe0], [1, 0x6da9fa0], [2, 0x6db72e0], [3, 0x6e035a0], [4, 0x6db1980], [5, 0x6db9360], [6, 0x6ddbbe0], [7, 0x6dbb3e0], [0, 0x6e1cc00], [1, 0x6deafc0], [2, 0x6df8300], [3, 0x6e445c0], [4, 0x6df29a0], [5, 0x6dfa380], [6, 0x6e1cc00], [7, 0x6dfc400], [0, 0x6e5dc20], [1, 0x6e2bfe0], [2, 0x6e39320], [3, 0x6e855e0], [4, 0x6e339c0], [5, 0x6e3b3a0], [6, 0x6e5dc20], [7, 0x6e3d420], [0, 0x6e9ec40], [1, 0x6e6d000], [2, 0x6e7a340], [3, 0x6ec6600], [4, 0x6e749e0], [5, 0x6e7c3c0], [6, 0x6e9ec40]]}
  layer.18.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6e7e440], [0, 0x6edfc60], [1, 0x6eae020], [2, 0x6ebb360], [3, 0x6f07620], [4, 0x6eb5a00], [5, 0x6ebd3e0], [6, 0x6edfc60]]}
  layer.18.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6e86660], [0, 0x6ee7e80], [1, 0x6eb6240], [2, 0x6ec3580], [3, 0x6f0f840], [4, 0x6ebdc20], [5, 0x6ec5600], [6, 0x6ee7e80], [7, 0x6ec7680], [0, 0x6f28ea0], [1, 0x6ef7260], [2, 0x6f045a0], [3, 0x6f50860], [4, 0x6efec40], [5, 0x6f06620], [6, 0x6f28ea0], [7, 0x6f086a0], [0, 0x6f69ec0], [1, 0x6f38280], [2, 0x6f455c0], [3, 0x6f91880], [4, 0x6f3fc60], [5, 0x6f47640], [6, 0x6f69ec0], [7, 0x6f496c0], [0, 0x6faaee0], [1, 0x6f792a0], [2, 0x6f865e0], [3, 0x6fd28a0], [4, 0x6f80c80], [5, 0x6f88660], [6, 0x6faaee0]]}
  layer.18.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f8a6e0], [0, 0x6febf00], [1, 0x6fba2c0], [2, 0x6fc7600], [3, 0x70138c0], [4, 0x6fc1ca0], [5, 0x6fc9680], [6, 0x6febf00]]}
  layer.18.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6c907e0]]}
  layer.18.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6c538e0]]}
  layer.19.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6c78a00], [7, 0x6c27600], [0, 0x6c769a0], [1, 0x6c54940], [2, 0x6c64520], [3, 0x6ca0c00], [4, 0x6c4c740], [5, 0x6c63d00], [6, 0x6c99220], [7, 0x6c47e20], [0, 0x6c971c0], [1, 0x6c75160], [2, 0x6c84d40], [3, 0x6cc1420], [4, 0x6c6cf60], [5, 0x6c84520]]}
  layer.19.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6cb9a40], [7, 0x6c68640], [0, 0x6cb79e0], [1, 0x6c95980], [2, 0x6ca5560], [3, 0x6ce1c40], [4, 0x6c8d780], [5, 0x6ca4d40]]}
  layer.19.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6cbbae0], [7, 0x6c6a6e0], [0, 0x6cb9a80], [1, 0x6c97a20], [2, 0x6ca7600], [3, 0x6ce3ce0], [4, 0x6c8f820], [5, 0x6ca6de0], [6, 0x6cdc300], [7, 0x6c8af00], [0, 0x6cda2a0], [1, 0x6cb8240], [2, 0x6cc7e20], [3, 0x6d04500], [4, 0x6cb0040], [5, 0x6cc7600]]}
  layer.19.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6cfcb20], [7, 0x6cab720], [0, 0x6cfaac0], [1, 0x6cd8a60], [2, 0x6ce8640], [3, 0x6d24d20], [4, 0x6cd0860], [5, 0x6ce7e20]]}
  layer.19.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c22ca0], [3, 0x6c4f7a0], [4, 0x6c0aec0], [5, 0x6c128a0], [6, 0x6c379c0], [7, 0x6be65c0], [0, 0x6c35960], [1, 0x6c13900], [2, 0x6c434c0], [3, 0x6c6ffc0], [4, 0x6c2b6e0], [5, 0x6c330c0], [6, 0x6c581e0], [7, 0x6c06de0], [0, 0x6c56180], [1, 0x6c34120]]}
  layer.19.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6cea6e0], [3, 0x6d26dc0], [4, 0x6cd2900], [5, 0x6ce9ec0], [6, 0x6cff400], [7, 0x6cae000], [0, 0x6cfd3a0], [1, 0x6cdb340]]}
  layer.19.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6cec780], [3, 0x6d28e60], [4, 0x6cd49a0], [5, 0x6cebf60], [6, 0x6d014a0], [7, 0x6cb00a0], [0, 0x6cff440], [1, 0x6cdd3e0], [2, 0x6d0cfa0], [3, 0x6d49680], [4, 0x6cf51c0], [5, 0x6d0c780], [6, 0x6d21cc0], [7, 0x6cd08c0], [0, 0x6d1fc60], [1, 0x6cfdc00]]}
  layer.19.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6d2d7c0], [3, 0x6d69ea0], [4, 0x6d159e0], [5, 0x6d2cfa0], [6, 0x6d424e0], [7, 0x6cf10e0], [0, 0x6d40480], [1, 0x6d1e420]]}
  layer.19.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6d42520]]}
  layer.19.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6df3180]]}
  layer.19.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x728a640], [7, 0x7226dc0], [0, 0x7285d40], [1, 0x7266580], [2, 0x72738c0], [3, 0x72bd2e0], [4, 0x726b6c0], [5, 0x7265d60], [6, 0x72cb660], [7, 0x7267de0], [0, 0x72c6d60], [1, 0x72a75a0], [2, 0x72b48e0], [3, 0x72fe300], [4, 0x72ac6e0], [5, 0x72a6d80], [6, 0x730c680], [7, 0x72a8e00], [0, 0x7307d80], [1, 0x72e85c0], [2, 0x72f5900], [3, 0x733f320], [4, 0x72ed700], [5, 0x72e7da0], [6, 0x734d6a0], [7, 0x72e9e20], [0, 0x7348da0], [1, 0x73295e0], [2, 0x7336920], [3, 0x7380340], [4, 0x732e720], [5, 0x7328dc0]]}
  layer.19.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x738e6c0], [7, 0x732ae40], [0, 0x7389dc0], [1, 0x736a600], [2, 0x7377940], [3, 0x73c1360], [4, 0x736f740], [5, 0x7369de0]]}
  layer.19.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x73968e0], [7, 0x7333060], [0, 0x7391fe0], [1, 0x7372820], [2, 0x737fb60], [3, 0x73c9580], [4, 0x7377960], [5, 0x7372000], [6, 0x73d7900], [7, 0x7374080], [0, 0x73d3000], [1, 0x73b3840], [2, 0x73c0b80], [3, 0x740a5a0], [4, 0x73b8980], [5, 0x73b3020], [6, 0x7418920], [7, 0x73b50a0], [0, 0x7414020], [1, 0x73f4860], [2, 0x7401ba0], [3, 0x744b5c0], [4, 0x73f99a0], [5, 0x73f4040], [6, 0x7459940], [7, 0x73f60c0], [0, 0x7455040], [1, 0x7435880], [2, 0x7442bc0], [3, 0x748c5e0], [4, 0x743a9c0], [5, 0x7435060]]}
  layer.19.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x749a960], [7, 0x74370e0], [0, 0x7496060], [1, 0x74768a0], [2, 0x7483be0], [3, 0x74cd600], [4, 0x747b9e0], [5, 0x7476080]]}
  layer.19.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x747da80]]}
  layer.19.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x747a1c0]]}
  layer.20.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x749f2e0], [7, 0x743ba60], [0, 0x749d280], [1, 0x747dac0], [2, 0x7488560], [3, 0x74d1f80], [4, 0x748ff40], [5, 0x748a5e0], [6, 0x74bfb00], [7, 0x745c280], [0, 0x74bdaa0], [1, 0x749e2e0], [2, 0x74a8d80], [3, 0x74f27a0], [4, 0x74b0760], [5, 0x74aae00]]}
  layer.20.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74e0320], [7, 0x747caa0], [0, 0x74de2c0], [1, 0x74beb00], [2, 0x74c95a0], [3, 0x7512fc0], [4, 0x74d0f80], [5, 0x74cb620]]}
  layer.20.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74e23c0], [7, 0x747eb40], [0, 0x74e0360], [1, 0x74c0ba0], [2, 0x74cb640], [3, 0x7515060], [4, 0x74d3020], [5, 0x74cd6c0], [6, 0x7502be0], [7, 0x749f360], [0, 0x7500b80], [1, 0x74e13c0], [2, 0x74ebe60], [3, 0x7535880], [4, 0x74f3840], [5, 0x74edee0]]}
  layer.20.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7523400], [7, 0x74bfb80], [0, 0x75213a0], [1, 0x7501be0], [2, 0x750c680], [3, 0x75560a0], [4, 0x7514060], [5, 0x750e700]]}
  layer.20.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7503c80], [2, 0x750e720], [3, 0x7558140], [4, 0x7516100], [5, 0x75107a0], [6, 0x7525ce0], [7, 0x74c2460], [0, 0x7523c80], [1, 0x75244a0], [2, 0x752ef40], [3, 0x7578960], [4, 0x7536920], [5, 0x7530fc0], [6, 0x7546500], [7, 0x74e2c80], [0, 0x75444a0]]}
  layer.20.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7544cc0], [2, 0x754f760], [3, 0x7599180], [4, 0x7557140], [5, 0x75517e0], [6, 0x7566d20], [7, 0x75034a0], [0, 0x7564cc0]]}
  layer.20.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x71d6940], [5, 0x71e0bc0], [6, 0x7203440], [7, 0x719fbc0], [0, 0x72013e0], [1, 0x71e1c20], [2, 0x71df380], [3, 0x7238980], [4, 0x71f7160], [5, 0x72013e0], [6, 0x7223c60], [7, 0x71c03e0], [0, 0x7221c00], [1, 0x7202440], [2, 0x71ffba0], [3, 0x72591a0]]}
  layer.20.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x71199e0], [4, 0x70c7dc0], [5, 0x70cf7a0], [6, 0x70f2020], [7, 0x7091040], [0, 0x70f2860], [1, 0x70c34c0], [2, 0x70d0800]]}
  layer.20.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70c5560]]}
  layer.20.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x711c2c0]]}
  layer.20.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70ca6a0], [5, 0x70d4920], [6, 0x70f71a0], [7, 0x7093920], [0, 0x70f5140], [1, 0x70d5980], [2, 0x70d30e0], [3, 0x712c6e0], [4, 0x710b6c0], [5, 0x7115940], [6, 0x71381c0], [7, 0x70d4940], [0, 0x7136160], [1, 0x71169a0], [2, 0x7114100], [3, 0x716d700], [4, 0x714c6e0], [5, 0x7156960], [6, 0x71791e0], [7, 0x7115960], [0, 0x7177180], [1, 0x71579c0], [2, 0x7155120], [3, 0x71ae720], [4, 0x718d700], [5, 0x7197980], [6, 0x71ba200], [7, 0x7156980], [0, 0x71b81a0], [1, 0x71989e0], [2, 0x7196140], [3, 0x71ef740]]}
  layer.20.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x71ce720], [5, 0x71d89a0], [6, 0x71fb220], [7, 0x71979a0], [0, 0x71f91c0], [1, 0x71d9a00], [2, 0x71d7160], [3, 0x7230760]]}
  layer.20.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7015960], [4, 0x6fc3d40], [5, 0x6fcb720], [6, 0x6fedfa0], [7, 0x6f8cfc0], [0, 0x6fee7e0], [1, 0x6fbf440], [2, 0x6fcc780], [3, 0x7056980], [4, 0x7004d60], [5, 0x700c740], [6, 0x702efc0], [7, 0x6fcdfe0], [0, 0x702f800], [1, 0x7000460], [2, 0x700d7a0], [3, 0x70979a0], [4, 0x7045d80], [5, 0x704d760], [6, 0x706ffe0], [7, 0x700f000], [0, 0x7070820], [1, 0x7041480], [2, 0x704e7c0], [3, 0x70d89c0], [4, 0x7086da0], [5, 0x708e780], [6, 0x70b1000], [7, 0x7050020], [0, 0x70b1840], [1, 0x70824a0], [2, 0x708f7e0]]}
  layer.20.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7217980], [5, 0x7221c00], [6, 0x7244480], [7, 0x71e0c00], [0, 0x7242420], [1, 0x7222c60], [2, 0x72203c0], [3, 0x72799c0]]}
  layer.20.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7222460]]}
  layer.20.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x721a260]]}
  layer.21.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x72244e0], [6, 0x7249600], [7, 0x71e5d80], [0, 0x7244d00], [1, 0x7225540], [2, 0x7232880], [3, 0x727c2a0], [4, 0x722a680], [5, 0x7244d00], [6, 0x7269e20], [7, 0x72065a0], [0, 0x7265520], [1, 0x7245d60], [2, 0x72530a0], [3, 0x729cac0], [4, 0x724aea0]]}
  layer.21.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6b48dc0], [6, 0x6b5e300], [7, 0x6b1cae0], [0, 0x6b6be80], [1, 0x6b49e20], [2, 0x6b59a00], [3, 0x6b86500], [4, 0x6b41c20]]}
  layer.21.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63f4a00], [7, 0x63d5240], [0, 0x6404e20], [1, 0x63f0100], [2, 0x63ffce0], [3, 0x641f4a0], [4, 0x63e7f00], [5, 0x63fcc20], [6, 0x6415220], [7, 0x63f5a60], [0, 0x6425640], [1, 0x6410920], [2, 0x6420500], [3, 0x643fcc0], [4, 0x6408720], [5, 0x641d440]]}
  layer.21.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6435a40], [7, 0x6416280], [0, 0x6445e60], [1, 0x6431140], [2, 0x6440d20], [3, 0x64604e0], [4, 0x6428f40], [5, 0x643dc60]]}
  layer.21.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64331e0], [2, 0x6442dc0], [3, 0x6462580], [4, 0x642afe0], [5, 0x643fd00], [6, 0x6438320], [7, 0x6418b60], [0, 0x6448740], [1, 0x6453a00], [2, 0x64635e0], [3, 0x6482da0], [4, 0x644b800], [5, 0x6460520], [6, 0x6458b40], [7, 0x6439380], [0, 0x6468f60]]}
  layer.21.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6474220], [2, 0x6483e00], [3, 0x64a35c0], [4, 0x646c020], [5, 0x6480d40], [6, 0x6479360], [7, 0x6459ba0], [0, 0x6489780]]}
  layer.21.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64762c0], [2, 0x6485ea0], [3, 0x64a5660], [4, 0x646e0c0], [5, 0x6482de0], [6, 0x647b400], [7, 0x645bc40], [0, 0x648b820], [1, 0x6496ae0], [2, 0x64a66c0], [3, 0x64c5e80], [4, 0x648e8e0], [5, 0x64a3600], [6, 0x649bc20], [7, 0x647c460], [0, 0x64ac040]]}
  layer.21.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64b7300], [2, 0x64c6ee0], [3, 0x64e66a0], [4, 0x64af100], [5, 0x64c3e20], [6, 0x64bc440], [7, 0x649cc80], [0, 0x64cc860]]}
  layer.21.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x64be4e0]]}
  layer.21.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x64ce900]]}
  layer.21.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64b9be0], [2, 0x64c97c0], [3, 0x64f8b60], [4, 0x64b19e0], [5, 0x64c6700], [6, 0x64ce900], [7, 0x649f560], [0, 0x64ded20], [1, 0x64fac00], [2, 0x650a7e0], [3, 0x6539b80], [4, 0x64f2a00], [5, 0x6507720], [6, 0x650f920], [7, 0x64e0580], [0, 0x651fd40], [1, 0x653bc20], [2, 0x654b800], [3, 0x657aba0], [4, 0x6533a20], [5, 0x6548740], [6, 0x6550940], [7, 0x65215a0], [0, 0x6560d60], [1, 0x657cc40], [2, 0x658c820], [3, 0x65bbbc0], [4, 0x6574a40], [5, 0x6589760], [6, 0x6591960], [7, 0x65625c0], [0, 0x65a1d80]]}
  layer.21.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x65bdc60], [2, 0x65cd840], [3, 0x65fcbe0], [4, 0x65b5a60], [5, 0x65ca780], [6, 0x65d2980], [7, 0x65a35e0], [0, 0x65e2da0]]}
  layer.21.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x65c5e80], [2, 0x65d5a60], [3, 0x6604e00], [4, 0x65bdc80], [5, 0x65d29a0], [6, 0x65daba0], [7, 0x65ab800], [0, 0x65eafc0], [1, 0x6606ea0], [2, 0x6616a80], [3, 0x6645e20], [4, 0x65feca0], [5, 0x66139c0], [6, 0x661bbc0], [7, 0x65ec820], [0, 0x662bfe0], [1, 0x6647ec0], [2, 0x6657aa0], [3, 0x6686e40], [4, 0x663fcc0], [5, 0x66549e0], [6, 0x665cbe0], [7, 0x662d840], [0, 0x666d000], [1, 0x6688ee0], [2, 0x6698ac0], [3, 0x66c7e60], [4, 0x6680ce0], [5, 0x6695a00], [6, 0x669dc00], [7, 0x666e860], [0, 0x66ae020]]}
  layer.21.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x66c9f00], [2, 0x66d9ae0], [3, 0x6708e80], [4, 0x66c1d00], [5, 0x66d6a20], [6, 0x66dec20], [7, 0x66af880], [0, 0x66ef040]]}
  layer.21.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x62e55a0]]}
  layer.21.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x62e2d00]]}
  layer.22.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63120a0], [4, 0x62d8260], [5, 0x62ecf80], [6, 0x62e7e40], [7, 0x62c8680], [0, 0x62f59c0], [1, 0x62e0ca0], [2, 0x62f3120], [3, 0x63328c0], [4, 0x62f8a80], [5, 0x630d7a0], [6, 0x6308660], [7, 0x62e8ea0], [0, 0x63161e0], [1, 0x63014c0], [2, 0x6313940]]}
  layer.22.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63530e0], [4, 0x63192a0], [5, 0x632dfc0], [6, 0x6328e80], [7, 0x63096c0], [0, 0x6336a00], [1, 0x6321ce0], [2, 0x6334160]]}
  layer.22.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6355180], [4, 0x631b340], [5, 0x6330060], [6, 0x632af20], [7, 0x630b760], [0, 0x6338aa0], [1, 0x6323d80], [2, 0x6336200], [3, 0x63759a0], [4, 0x633bb60], [5, 0x6350880], [6, 0x634b740], [7, 0x632bf80], [0, 0x63592c0], [1, 0x63445a0], [2, 0x6356a20]]}
  layer.22.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63961c0], [4, 0x635c380], [5, 0x63710a0], [6, 0x636bf60], [7, 0x634c7a0], [0, 0x6379ae0], [1, 0x6364dc0], [2, 0x6377240]]}
  layer.22.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x636e000], [7, 0x634e840], [0, 0x637bb80], [1, 0x6366e60], [2, 0x63792e0], [3, 0x6398aa0], [4, 0x635ec60], [5, 0x6373980], [6, 0x638e820], [7, 0x636f060], [0, 0x639c3a0], [1, 0x6387680], [2, 0x6399b00], [3, 0x63b92c0], [4, 0x637f480], [5, 0x63941a0]]}
  layer.22.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63af040], [7, 0x638f880], [0, 0x63bcbc0], [1, 0x63a7ea0], [2, 0x63ba320], [3, 0x63d9ae0], [4, 0x639fca0], [5, 0x63b49c0]]}
  layer.22.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63b10e0], [7, 0x6391920], [0, 0x63bec60], [1, 0x63a9f40], [2, 0x63bc3c0], [3, 0x63dbb80], [4, 0x63a1d40], [5, 0x63b6a60], [6, 0x63d1900], [7, 0x63b2140], [0, 0x63df480], [1, 0x63ca760], [2, 0x63dcbe0], [3, 0x63fc3a0], [4, 0x63c2560], [5, 0x63d7280]]}
  layer.22.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63f2120], [7, 0x63d2960], [0, 0x63ffca0], [1, 0x63eaf80], [2, 0x63fd400], [3, 0x641cbc0], [4, 0x63e2d80], [5, 0x63f7aa0]]}
  layer.22.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x64e8740]]}
  layer.22.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x693bb20]]}
  layer.22.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6909ee0], [0, 0x6959280], [1, 0x6934980], [2, 0x6944560], [3, 0x6973900], [4, 0x692f020], [5, 0x6936a00], [6, 0x694bf40], [7, 0x694af00], [0, 0x699a2a0], [1, 0x69759a0], [2, 0x6985580], [3, 0x69b4920], [4, 0x6970040], [5, 0x6977a20], [6, 0x698cf60], [7, 0x698bf20], [0, 0x69db2c0], [1, 0x69b69c0], [2, 0x69c65a0], [3, 0x69f5940], [4, 0x69b1060], [5, 0x69b8a40], [6, 0x69cdf80], [7, 0x69ccf40], [0, 0x6a1c2e0], [1, 0x69f79e0], [2, 0x6a075c0], [3, 0x6a36960], [4, 0x69f2080], [5, 0x69f9a60], [6, 0x6a0efa0]]}
  layer.22.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6a0df60], [0, 0x6a5d300], [1, 0x6a38a00], [2, 0x6a485e0], [3, 0x6a77980], [4, 0x6a330a0], [5, 0x6a3aa80], [6, 0x6a4ffc0]]}
  layer.22.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6a16180], [0, 0x6a65520], [1, 0x6a40c20], [2, 0x6a50800], [3, 0x6a7fba0], [4, 0x6a3b2c0], [5, 0x6a42ca0], [6, 0x6a581e0], [7, 0x6a571a0], [0, 0x6aa6540], [1, 0x6a81c40], [2, 0x6a91820], [3, 0x6ac0bc0], [4, 0x6a7c2e0], [5, 0x6a83cc0], [6, 0x6a99200], [7, 0x6a981c0], [0, 0x6ae7560], [1, 0x6ac2c60], [2, 0x6ad2840], [3, 0x6b01be0], [4, 0x6abd300], [5, 0x6ac4ce0], [6, 0x6ada220], [7, 0x6ad91e0], [0, 0x6b28580], [1, 0x6b03c80], [2, 0x6b13860], [3, 0x6b42c00], [4, 0x6afe320], [5, 0x6b05d00], [6, 0x6b1b240]]}
  layer.22.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b1a200], [0, 0x6b695a0], [1, 0x6b44ca0], [2, 0x6b54880], [3, 0x6b83c20], [4, 0x6b3f340], [5, 0x6b46d20], [6, 0x6b5c260]]}
  layer.22.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x691ec00]]}
  layer.22.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6b603a0]]}
  layer.23.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b1eb80], [0, 0x6b6df20], [1, 0x6b4bec0], [2, 0x6b5baa0], [3, 0x6b885a0], [4, 0x6b43cc0], [5, 0x6b4b6a0], [6, 0x6b707c0], [7, 0x6b3f3a0], [0, 0x6b8e740], [1, 0x6b6c6e0], [2, 0x6b7c2c0], [3, 0x6ba8dc0], [4, 0x6b644e0], [5, 0x6b6bec0], [6, 0x6b90fe0]]}
  layer.23.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b5fbc0], [0, 0x6baef60], [1, 0x6b8cf00], [2, 0x6b9cae0], [3, 0x6bc95e0], [4, 0x6b84d00], [5, 0x6b8c6e0], [6, 0x6bb1800]]}
  layer.23.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b61c60], [0, 0x6bb1000], [1, 0x6b8efa0], [2, 0x6b9eb80], [3, 0x6bcb680], [4, 0x6b86da0], [5, 0x6b8e780], [6, 0x6bb38a0], [7, 0x6b82480], [0, 0x6bd1820], [1, 0x6baf7c0], [2, 0x6bbf3a0], [3, 0x6bebea0], [4, 0x6ba75c0], [5, 0x6baefa0], [6, 0x6bd40c0]]}
  layer.23.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ba2ca0], [0, 0x6bf2040], [1, 0x6bcffe0], [2, 0x6bdfbc0], [3, 0x6c0c6c0], [4, 0x6bc7de0], [5, 0x6bcf7c0], [6, 0x6bf48e0]]}
  layer.23.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [2, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6be1c60], [3, 0x6c0e760], [4, 0x6bc9e80], [5, 0x6bd1860], [6, 0x6bf6980], [7, 0x6ba5580], [0, 0x6bf4920], [1, 0x6bd28c0], [2, 0x6c02480], [3, 0x6c2ef80], [4, 0x6bea6a0], [5, 0x6bf2080], [6, 0x6c171a0], [7, 0x6bc5da0], [0, 0x6c15140], [1, 0x6bf30e0]]}
  layer.23.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x685aae0], [4, 0x6816200], [5, 0x682af20], [6, 0x6830880], [7, 0x68014e0], [0, 0x6850880], [1, 0x681c3a0], [2, 0x683bb60]]}
  layer.23.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x66e3da0], [3, 0x6713140], [4, 0x66cbfc0], [5, 0x66e0ce0], [6, 0x66e8ee0], [7, 0x66b9b40], [0, 0x66f9300], [1, 0x66d4a00], [2, 0x67045c0], [3, 0x6733960], [4, 0x66ec7e0], [5, 0x6701500], [6, 0x6709700], [7, 0x66da360], [0, 0x6719b20], [1, 0x66f5220]]}
  layer.23.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6724de0], [3, 0x6754180], [4, 0x670d000], [5, 0x6721d20], [6, 0x6729f20], [7, 0x66fab80], [0, 0x673a340], [1, 0x6715a40]]}
  layer.23.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x673c3e0]]}
  layer.23.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x67276c0]]}
  layer.23.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6756a60], [4, 0x6712180], [5, 0x6726ea0], [6, 0x672c800], [7, 0x66fd460], [0, 0x674c800], [1, 0x6718320], [2, 0x6737ae0], [3, 0x6797a80], [4, 0x67531a0], [5, 0x6767ec0], [6, 0x676d820], [7, 0x673e480], [0, 0x678d820], [1, 0x6759340], [2, 0x6778b00], [3, 0x67d8aa0], [4, 0x67941c0], [5, 0x67a8ee0], [6, 0x67ae840], [7, 0x677f4a0], [0, 0x67ce840], [1, 0x679a360], [2, 0x67b9b20], [3, 0x6819ac0], [4, 0x67d51e0], [5, 0x67e9f00], [6, 0x67ef860], [7, 0x67c04c0], [0, 0x680f860], [1, 0x67db380], [2, 0x67fab40]]}
  layer.23.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x66dbb80], [3, 0x670af20], [4, 0x66c3da0], [5, 0x66d8ac0], [6, 0x66e0cc0], [7, 0x66b1920], [0, 0x66f10e0], [1, 0x66cc7e0]]}
  layer.23.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x685cb80], [4, 0x68182a0], [5, 0x682cfc0], [6, 0x6832920], [7, 0x6803580], [0, 0x6852920], [1, 0x681e440], [2, 0x683dc00], [3, 0x689dba0], [4, 0x68592c0], [5, 0x686dfe0], [6, 0x6873940], [7, 0x68445a0], [0, 0x6893940], [1, 0x685f460], [2, 0x687ec20], [3, 0x68debc0], [4, 0x689a2e0], [5, 0x68af000], [6, 0x68b4960], [7, 0x68855c0], [0, 0x68d4960], [1, 0x68a0480], [2, 0x68bfc40], [3, 0x691fbe0], [4, 0x68db300], [5, 0x68f0020], [6, 0x68f5980], [7, 0x68c65e0], [0, 0x6915980], [1, 0x68e14a0], [2, 0x6900c60]]}
  layer.23.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6960c00], [4, 0x691c320], [5, 0x6931040], [6, 0x69369a0], [7, 0x6907600], [0, 0x69569a0], [1, 0x69224c0], [2, 0x6941c80]]}
  layer.23.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6924560]]}
  layer.23.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x69634e0]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8071900]]}
  lc.input_tensor.attention_mask_s_brcst_m2_23_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80ab740]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x805ec60]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80a9760]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8115a20]]}
  dc.input_tensor.layernorm_38.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x80d6260], [2, 0x80f6260]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81300a0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80fab60]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x810af80]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f2cec0]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7efdb20]]}
  dc.input_tensor.layernorm_52.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f1aa40], [3, 0x7f54880]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f22420]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f0a640]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7ece780]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f84520]]}
  lc.input_tensor.attention_mask_s_brcst_m2_22_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa3ce0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fddb20]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x802efe0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8017200]]}
  dc.input_tensor.layernorm_91.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x803c320], [7, 0x7fdb340]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8049ea0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x810a740]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x85a7560]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x87d7080]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8785c80]]}
  dc.input_tensor.layernorm_105.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x87e2360], [1, 0x87a5440]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x87b8100]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x85a5500]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x87bfae0]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x880c680]]}
  lc.input_tensor.attention_mask_s_brcst_m2_21_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x886b600]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x882e6e0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x846f780]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8432860]]}
  dc.input_tensor.layernorm_144.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8452860], [3, 0x8489e00]]}
  lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x84548c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x844cf00]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x84138e0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8582420]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x85a4cc0]]}
  dc.input_tensor.layernorm_158.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85dc260], [4, 0x85a4480]]}
  lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x859cac0]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x85c4480]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x85cf760]]}
  input_1_multiply_175_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x76ab760]]}
  lc.input_tensor.attention_mask_s_brcst_m2_20_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x76a5e00]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x76cd7c0]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x76e0520]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76aaec0]]}
  dc.input_tensor.layernorm_197.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x771fce0], [2, 0x773cc00]]}
  lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7776a40]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7732160]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77541c0]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7961c80]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75daaa0]]}
  dc.input_tensor.layernorm_211.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7551800], [3, 0x759b220]]}
  lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x75591e0]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7553880]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7505540]]}
  input_1_multiply_228_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75cd760]]}
  lc.input_tensor.attention_mask_s_brcst_m2_19_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7546d60]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76244c0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7665da0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7660440]]}
  dc.input_tensor.layernorm_250.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7685560], [7, 0x7612100]]}
  lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7683500]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7654160]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x774f080]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7dfd240]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7e12780]]}
  dc.input_tensor.layernorm_264.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7dbeae0], [0, 0x7e1da60]]}
  lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7df0f60]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7e0de80]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c254e0]]}
  input_1_multiply_281_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ec4c80]]}
  lc.input_tensor.attention_mask_s_brcst_m2_18_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7eda1c0]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7e88dc0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7a3b2a0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7a581c0]]}
  dc.input_tensor.layernorm_303.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7a8f760], [4, 0x7a4ae80]]}
  lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7a55100]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7a6cee0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7a685e0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7ba7d80]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7be1bc0]]}
  dc.input_tensor.layernorm_317.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b9d2e0], [5, 0x7ba4cc0]]}
  lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7bbcaa0]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7b68e00]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7b9b280]]}
  input_1_multiply_334_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x933b2a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_17_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x92e3d00]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9347580]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x938c760]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x93f5940]]}
  dc.input_tensor.layernorm_356.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x92e1400], [5, 0x92b79e0]]}
  lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9381cc0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x93cdf80]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x93c1ca0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x959f360]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9608540]]}
  dc.input_tensor.layernorm_370.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9277140], [2, 0x927cac0]]}
  lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x91f60c0]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9251f60]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x91e8d80]]}
  input_1_multiply_387_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x92bdaa0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_16_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x925cac0]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x92b4060]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x931dae0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x92e0bc0]]}
  dc.input_tensor.layernorm_409.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x92b4900], [6, 0x9303460]]}
  lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x92a2480]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x92f9a20]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x92c4d20]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9a42320]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9a93720]]}
  dc.input_tensor.layernorm_423.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9a3fa80], [0, 0x9a82300]]}
  lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9a67440]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9a6cdc0]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9a9b920]]}
  input_1_multiply_440_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9ac8d20]]}
  lc.input_tensor.attention_mask_s_brcst_m2_15_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b0b5a0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9aede40]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9ad7040]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x95948c0]]}
  dc.input_tensor.layernorm_462.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x95e0b80], [7, 0x958cee0]]}
  lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x95d48a0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x95b99e0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9608d80]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x95cdec0]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x97af6e0]]}
  dc.input_tensor.layernorm_476.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x97fe240], [7, 0x97aa5a0]]}
  lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x97ef6c0]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x97d4800]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9823ba0]]}
  input_1_multiply_493_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x93538e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_14_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8a5aa40]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8ab4040]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8aeffa0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8ae5d40]]}
  dc.input_tensor.layernorm_515.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8b1d2e0], [7, 0x8abc300]]}
  lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8b1b280]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8ade360]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8be54c0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e06c20]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8dfc9c0]]}
  dc.input_tensor.layernorm_529.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8e36800], [7, 0x8dd5820]]}
  lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8e31f00]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8df4fe0]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x887fb40]]}
  input_1_multiply_546_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8917940]]}
  lc.input_tensor.attention_mask_s_brcst_m2_13_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x890d6e0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x89350a0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89781c0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x893b2a0]]}
  dc.input_tensor.layernorm_568.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x894b6c0], [3, 0x89a4cc0]]}
  lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x895d300]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x89530a0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8929660]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x90e63e0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x90d98e0]]}
  dc.input_tensor.layernorm_582.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9125ba0], [7, 0x90c4bc0]]}
  lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x911ea00]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x90e1ae0]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9143300]]}
  input_1_multiply_599_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9121340]]}
  lc.input_tensor.attention_mask_s_brcst_m2_12_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x916fea0]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x910eec0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x91ec620]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x91af700]]}
  dc.input_tensor.layernorm_621.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9076740], [4, 0x901a060]]}
  lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e07460]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8dfd200]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8dd8900]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9007be0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x900d560]]}
  dc.input_tensor.layernorm_635.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8df80c0], [3, 0x8e612a0]]}
  lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x900fe00]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x905c0c0]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9054f20]]}
  input_1_multiply_652_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9093f60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8801b20]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x90ff9e0]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c17ac0]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5c22560]]}
  dc.input_tensor.layernorm_674.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5bfdc60], [0, 0x5c1dc60]]}
  lc.input_tensor.layernorm_674.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5c10920]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c131c0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5c05e80]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f2e740]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5f391e0]]}
  dc.input_tensor.layernorm_688.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5f17180], [0, 0x5f37180]]}
  lc.input_tensor.layernorm_688.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f275a0]]}
  lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f29e40]]}
  lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a7fce0]]}
  input_1_multiply_705_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x59e6660]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5a06660]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x59e9740]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a70100]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a62dc0]]}
  dc.input_tensor.layernorm_727.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a82580], [6, 0x5a8d020]]}
  lc.input_tensor.layernorm_727.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5a6afc0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5a8afc0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a80520]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61cef60]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x61dc2a0]]}
  dc.input_tensor.layernorm_741.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x61ba240], [0, 0x61d79a0]]}
  lc.input_tensor.layernorm_741.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x61d5100]]}
  lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61d79a0]]}
  lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x61ca660]]}
  input_1_multiply_758_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6241440]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x625eba0]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6259a60]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x62e24c0]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6144420]]}
  dc.input_tensor.layernorm_780.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f2ef80], [6, 0x5f39a20]]}
  lc.input_tensor.layernorm_780.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5f1a260]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f3a260]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f2a680]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x612f700]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5f1cb00]]}
  dc.input_tensor.layernorm_794.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x614eec0], [7, 0x612ce60]]}
  lc.input_tensor.layernorm_794.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x614ce60]]}
  lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x614a5c0]]}
  lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x616c620]]}
  input_1_multiply_811_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x621db20]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50bd4c0]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51015e0]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x51877a0]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5194ae0]]}
  dc.input_tensor.layernorm_833.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5184f00], [5, 0x51a46c0]]}
  lc.input_tensor.layernorm_833.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5194ae0]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5184f00]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5187fe0]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x53a76e0]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x539a3a0]]}
  dc.input_tensor.layernorm_847.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x53b9b60], [6, 0x53a76e0]]}
  lc.input_tensor.layernorm_847.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5397b00]]}
  lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5195320]]}
  lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5077b00]]}
  input_1_multiply_864_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4ff1940]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ff1940]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4ff1940]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50bcc80]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x50bd4c0]]}
  dc.input_tensor.layernorm_886.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x50bd4c0], [2, 0x50bd4c0]]}
  lc.input_tensor.layernorm_886.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x50cd0a0]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x50bd4c0]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x50cd0a0]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5793940]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5793940]]}
  dc.input_tensor.layernorm_900.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5783d60], [5, 0x57a3520]]}
  lc.input_tensor.layernorm_900.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x57a0c80]]}
  lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x578e800]]}
  lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x57918e0]]}
  input_1_multiply_917_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x58dd380]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x58d0040]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x58ef800]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55ad7e0]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x539abe0]]}
  dc.input_tensor.layernorm_939.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x53aa7c0], [3, 0x53a7f20]]}
  lc.input_tensor.layernorm_939.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x539abe0]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x53bcc40]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5398340]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53a7f20]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55bfc60]]}
  dc.input_tensor.layernorm_953.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55bd3c0], [4, 0x55ad7e0]]}
  lc.input_tensor.layernorm_953.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55cf840]]}
  lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x55ccfa0]]}
  lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x55caf40]]}
  input_1_multiply_970_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6d6c780]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6d1ab60]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6d32120]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6dcaf80]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6d79b80]]}
  dc.input_tensor.layernorm_992.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6dd8b00], [1, 0x6da6ec0]]}
  lc.input_tensor.layernorm_992.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6db6aa0]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6d300a0]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6db8b20]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f8c780]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6fedfa0]]}
  dc.input_tensor.layernorm_1006.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6fbc360], [2, 0x6fc96a0]]}
  lc.input_tensor.layernorm_1006.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6cdab00]]}
  lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c63ce0]]}
  lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c4bf00]]}
  input_1_multiply_1023_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6cfebc0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6cad7c0]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6cfcb60]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6d2f860]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6d6bf40]]}
  dc.input_tensor.layernorm_1045.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6d17a80], [5, 0x6d2f040]]}
  lc.input_tensor.layernorm_1045.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d44580]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6cf3180]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d204c0]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x749ca00]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7439180]]}
  dc.input_tensor.layernorm_1059.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7498100], [1, 0x7478940]]}
  lc.input_tensor.layernorm_1059.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7485c80]]}
  lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x74cf6a0]]}
  lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7265520]]}
  input_1_multiply_1076_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x75254a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x74c1c20]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7523440]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x711ba80]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70c9e60]]}
  dc.input_tensor.layernorm_1098.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70d1840], [6, 0x70f40c0]]}
  lc.input_tensor.layernorm_1098.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x70930e0]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70f4900]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x70d28a0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7219a20]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7223ca0]]}
  dc.input_tensor.layernorm_1112.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7246520], [7, 0x71e2ca0]]}
  lc.input_tensor.layernorm_1112.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x72444c0]]}
  lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7224d00]]}
  lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x727ba60]]}
  input_1_multiply_1129_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6437ae0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6418320]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6447f00]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64b93a0]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x64c8f80]]}
  dc.input_tensor.layernorm_1151.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x63e4e20], [5, 0x63f9b40]]}
  lc.input_tensor.layernorm_1151.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x64b11a0]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x64c5ec0]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x649ed20]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x66cbfa0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x635e420]]}
  dc.input_tensor.layernorm_1165.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x62d5180], [5, 0x62e9ea0]]}
  lc.input_tensor.layernorm_1165.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x62e7600]]}
  lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x62c7e40]]}
  lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x62e0460]]}
  input_1_multiply_1182_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6398260]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6311860]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6373140]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63f41c0]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x63d4a00]]}
  dc.input_tensor.layernorm_1204.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6401d40], [1, 0x63ed020]]}
  lc.input_tensor.layernorm_1204.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x63ff4a0]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x641ec60]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x69361c0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b1c2a0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b6b640]]}
  dc.input_tensor.layernorm_1218.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6b46d40], [2, 0x6b56920]]}
  lc.input_tensor.layernorm_1218.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6b85cc0]]}
  lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6b413e0]]}
  lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6b4ae60]]}
  input_1_multiply_1235_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ba4d40]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6bf40e0]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6bd2080]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6726e80]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6756220]]}
  dc.input_tensor.layernorm_1257.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x670f0a0], [5, 0x6723dc0]]}
  lc.input_tensor.layernorm_1257.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x672bfc0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x66fcc20]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6717ae0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6962ca0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x691e3c0]]}
  dc.input_tensor.layernorm_1271.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x69330e0], [6, 0x6938a40]]}
  lc.input_tensor.layernorm_1271.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x69096a0]]}
  lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6958a40]]}
  lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6943d20]]}

  # epoch_to_epoch
  e2e_add_37_0:                                                                 {input: add_37, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0x9b5d260], [6, 0x9b9ea80]]}
  e2e_layernorm_38.dc.reduce_avg.0.lc1_0:                                       {input: layernorm_38.dc.reduce_avg.0.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9b4d680], [0, 0x9b8ff00]]}
  e2e_layernorm_38.dc.add.10_0:                                                 {input: layernorm_38.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb712480], [5, 0xb6c92a0], [6, 0xb70aac0], [7, 0x9fdf6c0]]}
  e2e_gelu_44_0:                                                                {input: gelu_44, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcf292c0], [6, 0xcf6aae0], [7, 0xb83f6e0], [0, 0xa32df60], [1, 0xa496800], [2, 0xa7a7960], [3, 0xbd54f60], [4, 0xd27e4c0], [5, 0xe7892e0], [6, 0xe7cab00], [7, 0xd09f700], [0, 0xbb8df80], [1, 0xbcf6820], [2, 0xc007980], [3, 0xd5b4f80], [4, 0xeade4e0]]}
  e2e_layernorm_52.dc.add.10_0:                                                 {input: layernorm_52.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xe8ff720], [0, 0xd3edfa0]]}
  e2e_matmul_55_0:                                                              {input: matmul_55, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xd556840], [2, 0xd8679a0], [3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0xffe9300], [6, 0x1002ab20], [7, 0x1015f740], [0, 0xec4dfc0], [1, 0xdb6e860], [2, 0xde7f9c0], [3, 0xa1e8f00], [4, 0xa1be460], [5, 0x10601320], [6, 0x10642b40], [7, 0x10777760], [0, 0xf265fe0]]}
  e2e_attention_mask_s_brcst_m2_22_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_22_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9b727a0]]}
  e2e_layernorm_91.dc.multiply.9_0:                                             {input: layernorm_91.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xe186880], [2, 0xe4979e0], [3, 0xee14fa0], [4, 0x1033e500]]}
  e2e_layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xb6c92a0]]}
  e2e_gelu_97_0:                                                                {input: gelu_97, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xb83f6e0], [0, 0xbb8df80], [1, 0xbcf6820], [2, 0xc007980], [3, 0xbd54f60], [4, 0xb712480], [5, 0xbee92c0], [6, 0xcf6aae0], [7, 0xd09f700], [0, 0xd3edfa0], [1, 0xf9e68a0]]}
  e2e_layernorm_91.dc.add.10_0:                                                 {input: layernorm_91.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xb70aac0], [7, 0x9fdf6c0], [0, 0xa32df60], [1, 0xa496800]]}
  e2e_layernorm_105.dc.add.10_0:                                                {input: layernorm_105.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xfcf7a00], [3, 0xd5b4f80], [4, 0xd27e4c0], [5, 0xd7492e0]]}
  e2e_matmul_108_0:                                                             {input: matmul_108, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xe7cab00], [7, 0xe8ff720], [0, 0xec4dfc0], [1, 0xd556840], [2, 0xd8679a0], [3, 0xee14fa0], [4, 0xeade4e0], [5, 0xb6c92a0], [6, 0xede2b20], [7, 0xef17740], [0, 0xf265fe0], [1, 0xdb6e860], [2, 0xde7f9c0], [3, 0xf42cfc0], [4, 0xf0f6500], [5, 0xefa9300]]}
  e2e_attention_mask_s_brcst_m2_21_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_21_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9e83900]]}
  e2e_layernorm_144.dc.multiply.9_0:                                            {input: layernorm_144.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xf3fab40], [7, 0xf52f760], [0, 0xf87e000], [1, 0xe186880]]}
  e2e_layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xe4979e0]]}
  e2e_gelu_150_0:                                                               {input: gelu_150, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9fdf6c0], [0, 0xa32df60], [1, 0xa496800], [2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0xb6c92a0], [6, 0xb70aac0], [7, 0xb83f6e0], [0, 0xbb8df80], [1, 0xbcf6820], [2, 0xc007980], [3, 0xd5b4f80], [4, 0xd27e4c0], [5, 0xcf292c0], [6, 0xcf6aae0]]}
  e2e_layernorm_144.dc.add.10_0:                                                {input: layernorm_144.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60], [4, 0xb712480], [5, 0x9b5d260], [6, 0x9b9ea80]]}
  e2e_layernorm_158.dc.add.10_0:                                                {input: layernorm_158.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xdccf740], [0, 0xe01dfe0], [1, 0xe186880], [2, 0xde7f9c0]]}
  e2e_matmul_161_0:                                                             {input: matmul_161, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xd09f700], [0, 0xd3edfa0], [1, 0xd556840], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xeade4e0], [5, 0xe7892e0], [6, 0xe7cab00], [7, 0xd6b7720], [0, 0xda05fc0], [1, 0xdb6e860], [2, 0xd8679a0], [3, 0xf42cfc0], [4, 0xf0f6500], [5, 0xeda1300], [6, 0xede2b20]]}
  e2e_attention_mask_s_brcst_m2_20_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_20_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb430f00]]}
  e2e_layernorm_197.dc.multiply.9_0:                                            {input: layernorm_197.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd27e4c0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9fdf6c0]]}
  e2e_layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xfa44fe0]]}
  e2e_gelu_203_0:                                                               {input: gelu_203, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ba6440], [5, 0xb6c92a0], [6, 0xb70aac0], [7, 0xb83f6e0], [0, 0xbb8df80], [1, 0xbcf6820], [2, 0xc007980], [3, 0xbd54f60], [4, 0xb712480], [5, 0xcf292c0], [6, 0xcf6aae0], [7, 0xd09f700], [0, 0xd3edfa0], [1, 0xd556840], [2, 0xd8679a0], [3, 0xd5b4f80]]}
  e2e_layernorm_197.dc.add.10_0:                                                {input: layernorm_197.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xa32df60], [1, 0xa496800], [2, 0xa7a7960], [3, 0x9bd0ee0]]}
  e2e_layernorm_211.dc.add.10_0:                                                {input: layernorm_211.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd27e4c0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9fdf6c0]]}
  e2e_matmul_214_0:                                                             {input: matmul_214, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xeade4e0], [5, 0xe7892e0], [6, 0xe7cab00], [7, 0xe8ff720], [0, 0xec4dfc0], [1, 0xedb6860], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xf0f6500], [5, 0xeda1300], [6, 0xede2b20], [7, 0xef17740], [0, 0xf265fe0], [1, 0xf3ce880], [2, 0xf0c79c0], [3, 0xf42cfc0]]}
  e2e_attention_mask_s_brcst_m2_19_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_19_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb406460]]}
  e2e_layernorm_250.dc.multiply.9_0:                                            {input: layernorm_250.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa496800], [2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0xb712480]]}
  e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xf87e000]]}
  e2e_gelu_256_0:                                                               {input: gelu_256, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xb6c92a0], [6, 0xb70aac0], [7, 0xb83f6e0], [0, 0xa32df60], [1, 0xbcf6820], [2, 0xc007980], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0xcf292c0], [6, 0xcf6aae0], [7, 0xd09f700], [0, 0xbb8df80], [1, 0xd556840], [2, 0xd8679a0], [3, 0xd5b4f80], [4, 0xf70e520]]}
  e2e_layernorm_250.dc.add.10_0:                                                {input: layernorm_250.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9fdf6c0], [0, 0xd3edfa0]]}
  e2e_layernorm_264.dc.add.10_0:                                                {input: layernorm_264.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa496800], [2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0xdeae500]]}
  e2e_matmul_267_0:                                                             {input: matmul_267, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xedb6860], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xd27e4c0], [5, 0xe7892e0], [6, 0xe7cab00], [7, 0xe8ff720], [0, 0xec4dfc0], [1, 0xf3ce880], [2, 0xf0c79c0], [3, 0xf42cfc0], [4, 0xd8964e0], [5, 0xeda1300], [6, 0xede2b20], [7, 0xef17740], [0, 0xf265fe0]]}
  e2e_attention_mask_s_brcst_m2_18_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_18_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xb3bd280]]}
  e2e_layernorm_303.dc.multiply.9_0:                                            {input: layernorm_303.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9b9ea80], [7, 0x9fdf6c0], [0, 0xa32df60], [1, 0xbcf6820]]}
  e2e_layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xf3b9320]]}
  e2e_gelu_309_0:                                                               {input: gelu_309, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc007980], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0x9b5d260], [6, 0xb70aac0], [7, 0xb83f6e0], [0, 0xbb8df80], [1, 0xd556840], [2, 0xd8679a0], [3, 0xd5b4f80], [4, 0xb406460], [5, 0xb6c92a0], [6, 0xcf6aae0], [7, 0xd09f700], [0, 0xd3edfa0], [1, 0xf9e68a0]]}
  e2e_layernorm_303.dc.add.10_0:                                                {input: layernorm_303.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xcf292c0]]}
  e2e_layernorm_317.dc.add.10_0:                                                {input: layernorm_317.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xe7cab00], [7, 0xe8ff720], [0, 0xec4dfc0], [1, 0xa496800]]}
  e2e_matmul_320_0:                                                             {input: matmul_320, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b778e0], [3, 0xee14fa0], [4, 0xeade4e0], [5, 0xe7892e0], [6, 0x9b9ea80], [7, 0x9fdf6c0], [0, 0xa32df60], [1, 0xedb6860], [2, 0xf0c79c0], [3, 0xf42cfc0], [4, 0xf0f6500], [5, 0xeda1300], [6, 0xa1b6aa0], [7, 0xa5f76e0], [0, 0xa945f80], [1, 0xf3ce880]]}
  e2e_attention_mask_s_brcst_m2_17_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_17_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xb3feaa0]]}
  e2e_layernorm_356.dc.multiply.9_0:                                            {input: layernorm_356.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0x9b5d260], [6, 0xb70aac0]]}
  e2e_layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xf6df9e0]]}
  e2e_gelu_362_0:                                                               {input: gelu_362, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xac0f700], [0, 0xaf5dfa0], [1, 0xbcf6820], [2, 0xa7a7960], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0xb3bd280], [6, 0xcf6aae0], [7, 0xc46f720], [0, 0xc7bdfc0], [1, 0xd556840], [2, 0xc007980], [3, 0xd5b4f80], [4, 0xb406460], [5, 0xcc1d2a0], [6, 0x1002ab20]]}
  e2e_layernorm_356.dc.add.10_0:                                                {input: layernorm_356.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xdccf740], [0, 0xe01dfe0], [1, 0xa496800], [2, 0xd8679a0]]}
  e2e_layernorm_370.dc.add.10_0:                                                {input: layernorm_370.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xee14fa0], [4, 0xeade4e0], [5, 0xe47d2c0], [6, 0xe7cab00]]}
  e2e_matmul_373_0:                                                             {input: matmul_373, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9fdf6c0], [0, 0xa32df60], [1, 0xedb6860], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa5f76e0], [0, 0xa945f80], [1, 0xf3ce880], [2, 0xf0c79c0], [3, 0xa1e8f00], [4, 0xd8964e0], [5, 0xa175280], [6, 0xa1b6aa0]]}
  e2e_attention_mask_s_brcst_m2_16_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_16_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9cd36a0]]}
  e2e_layernorm_409.dc.multiply.9_0:                                            {input: layernorm_409.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xaf5dfa0], [1, 0xa496800], [2, 0xa7a7960], [3, 0xbd54f60]]}
  e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xf52f760]]}
  e2e_gelu_415_0:                                                               {input: gelu_415, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ba6440], [5, 0xa78d2a0], [6, 0xa7ceac0], [7, 0xac0f700], [0, 0xc7bdfc0], [1, 0xbcf6820], [2, 0xc007980], [3, 0xd5b4f80], [4, 0xb406460], [5, 0xbfed2c0], [6, 0xc02eae0], [7, 0xc46f720], [0, 0xe01dfe0], [1, 0xd556840], [2, 0xd8679a0], [3, 0x10674fc0]]}
  e2e_layernorm_409.dc.add.10_0:                                                {input: layernorm_409.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd27e4c0], [5, 0xd84d2e0], [6, 0xd88eb00], [7, 0xdccf740]]}
  e2e_layernorm_423.dc.add.10_0:                                                {input: layernorm_423.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xf87e000], [1, 0xedb6860], [2, 0xf0c79c0], [3, 0xee14fa0]]}
  e2e_matmul_426_0:                                                             {input: matmul_426, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xeade4e0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xa32df60], [1, 0xa496800], [2, 0x9b778e0], [3, 0xbd54f60], [4, 0xf0f6500], [5, 0xa175280], [6, 0xa1b6aa0], [7, 0xa1656a0], [0, 0xa945f80], [1, 0xaaae820], [2, 0xa7a7960], [3, 0xc36cf80]]}
  e2e_attention_mask_s_brcst_m2_15_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_15_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9d15f20]]}
  e2e_layernorm_462.dc.multiply.9_0:                                            {input: layernorm_462.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa78d2a0], [6, 0xa7ceac0], [7, 0xa77d6c0], [0, 0xaf5dfa0]]}
  e2e_layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf70e520]]}
  e2e_gelu_468_0:                                                               {input: gelu_468, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xb0c6840], [2, 0xadbf980], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xbfed2c0], [6, 0xc02eae0], [7, 0xbfdd6e0], [0, 0xc7bdfc0], [1, 0xc926860], [2, 0xc61f9a0], [3, 0xc984fa0], [4, 0x9ba6440], [5, 0xd84d2e0], [6, 0xd88eb00], [7, 0xd83d700], [0, 0xe01dfe0]]}
  e2e_layernorm_462.dc.add.10_0:                                                {input: layernorm_462.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xe186880], [2, 0xde7f9c0], [3, 0xe1e4fc0], [4, 0xb406460]]}
  e2e_layernorm_476.dc.add.10_0:                                                {input: layernorm_476.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa78d2a0], [6, 0xa7ceac0], [7, 0xa77d6c0], [0, 0xaf5dfa0]]}
  e2e_matmul_479_0:                                                             {input: matmul_479, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xa32df60], [1, 0xa496800], [2, 0xa7a7960], [3, 0xbd54f60], [4, 0xeade4e0], [5, 0xa175280], [6, 0xa1b6aa0], [7, 0xa1656a0], [0, 0xa945f80], [1, 0xaaae820], [2, 0x9b778e0], [3, 0xc36cf80], [4, 0xf0f6500]]}
  e2e_attention_mask_s_brcst_m2_14_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_14_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e7e7c0]]}
  e2e_layernorm_515.dc.multiply.9_0:                                            {input: layernorm_515.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xadbf980], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xbfed2c0]]}
  e2e_layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xf9e68a0]]}
  e2e_gelu_521_0:                                                               {input: gelu_521, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc61f9a0], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xa32df60], [1, 0xc926860], [2, 0xde7f9c0], [3, 0xd5b4f80], [4, 0xb406460], [5, 0xd84d2e0], [6, 0xd88eb00], [7, 0xd83d700], [0, 0xe01dfe0], [1, 0xe186880]]}
  e2e_layernorm_515.dc.add.10_0:                                                {input: layernorm_515.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc02eae0], [7, 0xbfdd6e0], [0, 0xc7bdfc0], [1, 0xb0c6840]]}
  e2e_layernorm_529.dc.add.10_0:                                                {input: layernorm_529.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xf6df9e0], [3, 0xee14fa0], [4, 0xeade4e0], [5, 0xf0ad300]]}
  e2e_matmul_532_0:                                                             {input: matmul_532, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xb3feaa0], [7, 0xb3ad6a0], [0, 0xbb8df80], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xb3bd280], [6, 0xba16ac0], [7, 0xb9c56c0], [0, 0xc1a5fa0], [1, 0xa496800], [2, 0xa7a7960], [3, 0xa1e8f00], [4, 0xd8964e0], [5, 0xb9d52a0]]}
  e2e_attention_mask_s_brcst_m2_13_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_13_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa18f920]]}
  e2e_layernorm_568.dc.multiply.9_0:                                            {input: layernorm_568.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xf0eeb20], [7, 0xf09d720], [0, 0xf87e000], [1, 0xf9e68a0]]}
  e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xadbf980]]}
  e2e_gelu_574_0:                                                               {input: gelu_574, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9b4d680], [0, 0xa32df60], [1, 0xa496800], [2, 0xb5df9a0], [3, 0x9bd0ee0], [4, 0xb406460], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xb3ad6a0], [0, 0xbb8df80], [1, 0xbcf6820], [2, 0xce3f9c0], [3, 0xd5b4f80], [4, 0xd27e4c0], [5, 0xcc1d2a0], [6, 0xcc5eac0]]}
  e2e_layernorm_568.dc.add.10_0:                                                {input: layernorm_568.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60], [4, 0x9ba6440], [5, 0x9b5d260], [6, 0x9b9ea80]]}
  e2e_layernorm_582.dc.add.10_0:                                                {input: layernorm_582.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xd83d700], [0, 0xe01dfe0], [1, 0xdb6e860], [2, 0xe69f9e0]]}
  e2e_matmul_585_0:                                                             {input: matmul_585, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xcc0d6c0], [0, 0xd3edfa0], [1, 0x9b727a0], [2, 0xa7a7960], [3, 0xee14fa0], [4, 0xeade4e0], [5, 0xe47d2c0], [6, 0xe4beae0], [7, 0xd2256e0], [0, 0xda05fc0], [1, 0xd556840], [2, 0x9b778e0], [3, 0xf42cfc0], [4, 0xf0f6500], [5, 0xea952e0], [6, 0xead6b00]]}
  e2e_attention_mask_s_brcst_m2_12_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_12_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb73cf20]]}
  e2e_layernorm_621.dc.reciprocal.7_0:                                          {input: layernorm_621.dc.reciprocal.7, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb430f00], [4, 0xcc66480]]}
  e2e_layernorm_621.dc.subtract.1_0:                                            {input: layernorm_621.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xa32df60], [1, 0xa496800], [2, 0xadbf980], [3, 0x9bd0ee0], [4, 0xd27e4c0]]}
  e2e_gelu_627_0:                                                               {input: gelu_627, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xaa992a0], [6, 0xaadaac0], [7, 0xaa896c0], [0, 0xaf5df80], [1, 0xb3d2840], [2, 0xb9ef9a0], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0xc2f92c0], [6, 0xc33aae0], [7, 0xf09d720], [0, 0xf87e000], [1, 0xf3ce880], [2, 0xfeffa00], [3, 0xd5b4f80], [4, 0xb406460]]}
  e2e_layernorm_621.dc.add.10_0:                                                {input: layernorm_621.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xdb592e0], [6, 0xdb9ab00], [7, 0xc2e96e0], [0, 0xc7bdfa0]]}
  e2e_layernorm_635.dc.add.10_0:                                                {input: layernorm_635.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xcc32860], [2, 0xd24f9c0], [3, 0xee14fa0], [4, 0xe1ba500]]}
  e2e_matmul_638_0:                                                             {input: matmul_638, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0x9e9bf20], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xf3b9300], [6, 0xf3fab20], [7, 0xdb49700], [0, 0xa4b3f40], [1, 0xa496800], [2, 0xa7a7960], [3, 0xab0cf20], [4, 0xfa1a520]]}
  e2e_attention_mask_input_op_fork_nop1_0:                                      {input: attention_mask_input_op_fork_nop1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b778e0]]}
  e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0:                   {input: attention_mask_input_op_fork_nop1_input_op_fork_nop0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xa021f40]]}
  e2e_attention_mask_s_brcst_m2_11_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_11_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa78d280]]}
  e2e_layernorm_674.dc.multiply.9_0:                                            {input: layernorm_674.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xf9d1320], [6, 0xfa12b40], [7, 0x108fd740], [0, 0xe01dfc0]]}
  e2e_layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xb3d2840]]}
  e2e_gelu_680_0:                                                               {input: gelu_680, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xadbf980], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0xaa992a0], [6, 0xaadaac0], [7, 0xe161720], [0, 0xaacbf60], [1, 0xe492880], [2, 0xeaaf9e0], [3, 0xd5b4f80], [4, 0xb406460], [5, 0xc2f92c0], [6, 0xc33aae0], [7, 0xaa896c0], [0, 0xc32bf80], [1, 0xfcf28a0]]}
  e2e_layernorm_674.dc.add.10_0:                                                {input: layernorm_674.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc61f9a0], [3, 0xee14fa0], [4, 0xe1ba500], [5, 0xdb592e0]]}
  e2e_layernorm_688.dc.add.10_0:                                                {input: layernorm_688.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xdb9ab00], [7, 0xc2e96e0], [0, 0xf87dfe0], [1, 0xbbf2860]]}
  e2e_matmul_691_0:                                                             {input: matmul_691, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xdb49700], [0, 0x9e9bf20], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0xab0cf20], [4, 0xfa1a520], [5, 0xa175280], [6, 0xf3fab20], [7, 0x9b4d680], [0, 0xa4b3f40], [1, 0xb3d2840]]}
  e2e_attention_mask_s_brcst_m2_10_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_10_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa7ceaa0]]}
  e2e_layernorm_727.dc.multiply.9_0:                                            {input: layernorm_727.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60], [4, 0xe1ba500], [5, 0xa78d2a0], [6, 0xaadaac0]]}
  e2e_layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xde7f9c0]]}
  e2e_gelu_733_0:                                                               {input: gelu_733, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xaa896c0], [0, 0xaacbf60], [1, 0xd452880], [2, 0xadbf980], [3, 0xd5b4f80], [4, 0x9ba6440], [5, 0xbfed2c0], [6, 0xc33aae0], [7, 0xe161720], [0, 0xc32bf80], [1, 0xecb28a0], [2, 0xc61f9a0], [3, 0xee14fa0], [4, 0xb406460], [5, 0xd84d2e0], [6, 0xfa12b40]]}
  e2e_layernorm_727.dc.add.10_0:                                                {input: layernorm_727.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xc2e96e0], [0, 0xdb8bfa0], [1, 0xb3d2840], [2, 0xe69f9e0]]}
  e2e_layernorm_741.dc.add.10_0:                                                {input: layernorm_741.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60], [4, 0xe1ba500], [5, 0xa78d2a0], [6, 0xa7ceac0]]}
  e2e_matmul_744_0:                                                             {input: matmul_744, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xdb49700], [0, 0x9e9bf20], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0xab0cf20], [4, 0xfa1a520], [5, 0xa175280], [6, 0xa1b6aa0], [7, 0x9b4d680], [0, 0xa4b3f40], [1, 0xcc32860], [2, 0xa7a7960]]}
  e2e_attention_mask_s_brcst_m2_9_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_9_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa77d6a0]]}
  e2e_layernorm_780.dc.multiply.9_0:                                            {input: layernorm_780.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xaacbf60], [1, 0xb3d2840], [2, 0xadbf980], [3, 0xd5b4f80]]}
  e2e_layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xf9c1740]]}
  e2e_gelu_786_0:                                                               {input: gelu_786, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ba6440], [5, 0xbfed2c0], [6, 0xc02eae0], [7, 0xe161720], [0, 0xc32bf80], [1, 0xd24a880], [2, 0xc61f9a0], [3, 0xee14fa0], [4, 0xb406460], [5, 0xd84d2e0], [6, 0xd88eb00], [7, 0xaa896c0], [0, 0xdb8bfa0], [1, 0xeaaa8a0], [2, 0xde7f9c0], [3, 0x10674fc0]]}
  e2e_layernorm_780.dc.add.10_0:                                                {input: layernorm_780.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe1ba500], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xc2e96e0]]}
  e2e_layernorm_794.dc.add.10_0:                                                {input: layernorm_794.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xf3ebfc0], [1, 0x1030a8c0], [2, 0xf6df9e0], [3, 0xbd54f60]]}
  e2e_matmul_797_0:                                                             {input: matmul_797, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd27e4c0], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xdb49700], [0, 0x9e9bf20], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xfa1a520], [5, 0xb9d52a0], [6, 0xba16ac0], [7, 0x9b4d680], [0, 0xa4b3f40], [1, 0xa496800], [2, 0xa7a7960], [3, 0xab0cf20]]}
  e2e_attention_mask_s_brcst_m2_8_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_8_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b8ff00]]}
  e2e_layernorm_833.dc.multiply.9_0:                                            {input: layernorm_833.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa1656a0], [0, 0xaacbf60]]}
  e2e_layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10032540]]}
  e2e_gelu_839_0:                                                               {input: gelu_839, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xb3d2840], [2, 0xadbf980], [3, 0xd5b4f80], [4, 0xe1ba500], [5, 0xbfed2c0], [6, 0xc02eae0], [7, 0xb9c56c0], [0, 0xc32bf80], [1, 0xcc32860], [2, 0xc61f9a0], [3, 0xee14fa0], [4, 0x9ba6440], [5, 0xd84d2e0], [6, 0xd88eb00], [7, 0xe161720], [0, 0xdb8bfa0]]}
  e2e_layernorm_833.dc.add.10_0:                                                {input: layernorm_833.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xe492880], [2, 0xde7f9c0], [3, 0xbd54f60], [4, 0xb406460]]}
  e2e_layernorm_847.dc.add.10_0:                                                {input: layernorm_847.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa1656a0], [0, 0xa7bff40]]}
  e2e_matmul_850_0:                                                             {input: matmul_850, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xb3bd280], [6, 0xb3feaa0], [7, 0x9b4d680], [0, 0x9b8ff00], [1, 0x9b727a0], [2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0xfa1a520], [5, 0xb9d52a0], [6, 0xba16ac0], [7, 0xd2256e0], [0, 0xa1a7f20], [1, 0xa496800], [2, 0x9b778e0], [3, 0xab0cf20], [4, 0xd27e4c0]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xb0c6820]]}
  e2e_layernorm_886.dc.multiply.9_0:                                            {input: layernorm_886.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xfcf28a0], [2, 0xf6df9e0], [3, 0x10674fc0], [4, 0x10032540]]}
  e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xbfed2c0]]}
  e2e_gelu_892_0:                                                               {input: gelu_892, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc02eae0], [7, 0xb9c56c0], [0, 0xc01ff60], [1, 0xb3d2840], [2, 0xadbf980], [3, 0xbd54f60], [4, 0xe1ba500], [5, 0xc80d2e0], [6, 0xd88eb00], [7, 0xd83d700], [0, 0xd87ff80], [1, 0xcc32860], [2, 0xc61f9a0], [3, 0xd5b4f80], [4, 0x9ba6440], [5, 0xe06d300]]}
  e2e_layernorm_886.dc.add.10_0:                                                {input: layernorm_886.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9b9ea80], [7, 0x9b4d680], [0, 0x9b8ff00], [1, 0xe492880]]}
  e2e_layernorm_900.dc.add.10_0:                                                {input: layernorm_900.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xde7f9c0], [3, 0xee14fa0], [4, 0xb406460], [5, 0xa78d2a0]]}
  e2e_matmul_903_0:                                                             {input: matmul_903, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xfa1a520], [5, 0x9b5d260], [6, 0xb3feaa0], [7, 0xb3ad6a0], [0, 0xb3eff20], [1, 0x9b727a0], [2, 0xa7a7960], [3, 0xab0cf20], [4, 0xd27e4c0], [5, 0xa175280], [6, 0xba16ac0], [7, 0xd2256e0], [0, 0xba07f40], [1, 0xa496800]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa18f900]]}
  e2e_layernorm_939.dc.multiply.9_0:                                            {input: layernorm_939.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xf0eeb20], [7, 0xf09d720], [0, 0xf0dffa0], [1, 0xfcf28a0]]}
  e2e_layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xadbf980]]}
  e2e_gelu_945_0:                                                               {input: gelu_945, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9b4d680], [0, 0x9b8ff00], [1, 0xa496800], [2, 0xb5df9a0], [3, 0xd5b4f80], [4, 0xb406460], [5, 0x9b5d260], [6, 0xb3feaa0], [7, 0xb3ad6a0], [0, 0xb3eff20], [1, 0xbcf6820], [2, 0xce3f9c0], [3, 0xee14fa0], [4, 0xe1ba500], [5, 0xd84d2e0], [6, 0xcc5eac0]]}
  e2e_layernorm_939.dc.add.10_0:                                                {input: layernorm_939.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60], [4, 0x9ba6440], [5, 0xbfed2c0], [6, 0x9b9ea80]]}
  e2e_layernorm_953.dc.add.10_0:                                                {input: layernorm_953.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xcc0d6c0], [0, 0xcc4ff40], [1, 0xd556840], [2, 0xe69f9e0]]}
  e2e_matmul_956_0:                                                             {input: matmul_956, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0xb3bd280], [6, 0xe4beae0], [7, 0xe46d6e0], [0, 0xe4aff60], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0xab0cf20], [4, 0xfa1a520], [5, 0xb9d52a0], [6, 0xead6b00], [7, 0xea85700], [0, 0xeac7f80], [1, 0xedb6860], [2, 0xa7a7960]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa800f00]]}
  e2e_layernorm_992.dc.multiply.9_0:                                            {input: layernorm_992.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10674fc0], [4, 0x10032540], [5, 0xf0ad300], [6, 0xf0eeb20]]}
  e2e_layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9b4d680]]}
  e2e_gelu_998_0:                                                               {input: gelu_998, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe1ba500], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa36d6a0], [0, 0xb3eff20], [1, 0xbcf6820], [2, 0xc61f9a0], [3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xbbcd6c0], [0, 0xcc4ff40], [1, 0xd556840], [2, 0xde7f9c0], [3, 0xd5b4f80]]}
  e2e_layernorm_992.dc.add.10_0:                                                {input: layernorm_992.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b8ff00], [1, 0xa496800], [2, 0xadbf980], [3, 0xbd54f60]]}
  e2e_layernorm_1006.dc.add.10_0:                                               {input: layernorm_1006.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb406460], [5, 0xcc1d2a0], [6, 0xcc5eac0], [7, 0xd42d6e0]]}
  e2e_matmul_1009_0:                                                            {input: matmul_1009, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xe4aff60], [1, 0x9b727a0], [2, 0xa7a7960], [3, 0xb430f00], [4, 0xd27e4c0], [5, 0xe47d2c0], [6, 0xe4beae0], [7, 0x9b4d680], [0, 0xeac7f80], [1, 0xedb6860], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xfa1a520], [5, 0xea952e0], [6, 0xead6b00], [7, 0xec8d700]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdeae4e0]]}
  e2e_layernorm_1045.dc.multiply.9_0:                                           {input: layernorm_1045.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa496800], [2, 0xadbf980], [3, 0x9bd0ee0], [4, 0x9ba6440]]}
  e2e_layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xf0dffa0]]}
  e2e_gelu_1051_0:                                                              {input: gelu_1051, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa1656a0], [0, 0x9b8ff00], [1, 0xbcf6820], [2, 0xc61f9a0], [3, 0xbd54f60], [4, 0xe1ba500], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xb9c56c0], [0, 0xb3eff20], [1, 0xd556840], [2, 0xde7f9c0], [3, 0xd5b4f80], [4, 0x10032540]]}
  e2e_layernorm_1045.dc.add.10_0:                                               {input: layernorm_1045.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcc1d2a0], [6, 0xcc5eac0], [7, 0xd2256e0], [0, 0xcc4ff40]]}
  e2e_layernorm_1059.dc.add.10_0:                                               {input: layernorm_1059.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa496800], [2, 0xadbf980], [3, 0x9bd0ee0], [4, 0x9ba6440]]}
  e2e_matmul_1062_0:                                                            {input: matmul_1062, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9b727a0], [2, 0xa7a7960], [3, 0xb430f00], [4, 0xfa1a520], [5, 0xe47d2c0], [6, 0xe4beae0], [7, 0x9b4d680], [0, 0xe4aff60], [1, 0xedb6860], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xd27e4c0], [5, 0xea952e0], [6, 0xead6b00], [7, 0xea85700], [0, 0xeac7f80]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa18a7e0]]}
  e2e_layernorm_1098.dc.multiply.9_0:                                           {input: layernorm_1098.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xf0ad300], [6, 0xf0eeb20], [7, 0xf09d720], [0, 0xf0dffa0]]}
  e2e_layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xbcf6820]]}
  e2e_gelu_1104_0:                                                              {input: gelu_1104, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc61f9a0], [3, 0xbd54f60], [4, 0xb406460], [5, 0x9b5d260], [6, 0x9b9ea80], [7, 0xa1656a0], [0, 0x9b8ff00], [1, 0xc516840], [2, 0xde7f9c0], [3, 0xd5b4f80], [4, 0xd8964e0], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xb9c56c0], [0, 0xb3eff20], [1, 0xf3ce880]]}
  e2e_layernorm_1098.dc.add.10_0:                                               {input: layernorm_1098.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa7a7960], [3, 0x9bd0ee0], [4, 0x9ba6440], [5, 0xcc1d2a0]]}
  e2e_layernorm_1112.dc.add.10_0:                                               {input: layernorm_1112.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xd88eb00], [7, 0xd83d700], [0, 0xd87ff80], [1, 0x9b727a0]]}
  e2e_matmul_1115_0:                                                            {input: matmul_1115, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xcc5eac0], [7, 0x9b4d680], [0, 0xcc4ff40], [1, 0xdd76860], [2, 0xc007980], [3, 0xb430f00], [4, 0xd27e4c0], [5, 0xe47d2c0], [6, 0xd276ae0], [7, 0xd2256e0], [0, 0xd267f60], [1, 0xe38e880], [2, 0x9b778e0], [3, 0xee14fa0], [4, 0xf0f6500], [5, 0xea952e0]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa49b940]]}
  e2e_layernorm_1151.dc.multiply.9_0:                                           {input: layernorm_1151.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xf6df9e0], [3, 0xf42cfc0], [4, 0xf70e520], [5, 0xf0ad300]]}
  e2e_layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9b9ea80]]}
  e2e_gelu_1157_0:                                                              {input: gelu_1157, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd0ee0], [4, 0xd27e4c0], [5, 0x9b5d260], [6, 0xa3beaa0], [7, 0xb9c56c0], [0, 0xb3eff20], [1, 0x9b727a0], [2, 0xc007980], [3, 0xbd54f60], [4, 0x9ba6440], [5, 0xb3bd280], [6, 0xbc1eac0], [7, 0xd2256e0], [0, 0xcc4ff40], [1, 0xcc327e0], [2, 0xd8679a0]]}
  e2e_layernorm_1151.dc.add.10_0:                                               {input: layernorm_1151.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa1656a0], [0, 0x9b8ff00], [1, 0xb3d27c0], [2, 0xa7a7960]]}
  e2e_layernorm_1165.dc.add.10_0:                                               {input: layernorm_1165.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd5b4f80], [4, 0xb406460], [5, 0xcc1d2a0], [6, 0xd47eae0]]}
  e2e_matmul_1168_0:                                                            {input: matmul_1168, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9b4d680], [0, 0xe4aff60], [1, 0xe492800], [2, 0x9b778e0], [3, 0xb430f00], [4, 0xeade4e0], [5, 0xe47d2c0], [6, 0x9b9ea80], [7, 0xea85700], [0, 0xeac7f80], [1, 0xeaaa820], [2, 0xa18f900], [3, 0xee14fa0], [4, 0xf0f6500], [5, 0xea952e0], [6, 0xecdeb00]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xba48f40]]}
  e2e_layernorm_1204.dc.multiply.9_0:                                           {input: layernorm_1204.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xf09d720], [0, 0xf0dffa0], [1, 0xf0c2840], [2, 0xf0c79c0]]}
  e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbd54f60]]}
  e2e_gelu_1210_0:                                                              {input: gelu_1210, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b8ff00], [1, 0x9b727a0], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xb406460], [5, 0xb3bd280], [6, 0xba16ac0], [7, 0xb9c56c0], [0, 0xb3eff20], [1, 0xb3d27c0], [2, 0xb3d7900], [3, 0xc574f80], [4, 0xd27e4c0], [5, 0xcc1d2a0], [6, 0xd276ae0], [7, 0xd2256e0]]}
  e2e_layernorm_1204.dc.add.10_0:                                               {input: layernorm_1204.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ba6440], [5, 0x9b5d260], [6, 0xa1b6aa0], [7, 0xa1656a0]]}
  e2e_layernorm_1218.dc.add.10_0:                                               {input: layernorm_1218.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xcc4ff40], [1, 0xcc327e0], [2, 0xcc37920], [3, 0xddd4fa0]]}
  e2e_matmul_1221_0:                                                            {input: matmul_1221, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xeade4e0], [5, 0xe47d2c0], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xe4aff60], [1, 0xe492800], [2, 0xe497940], [3, 0xb430f00], [4, 0xf0f6500], [5, 0xea952e0], [6, 0xead6b00], [7, 0xea85700], [0, 0xeac7f80], [1, 0xeaaa820], [2, 0xeaaf960], [3, 0xba48f20]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcf724a0]]}
  e2e_layernorm_1257.dc.multiply.9_0:                                           {input: layernorm_1257.dc.multiply.9, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xf70e520], [5, 0xf0ad300], [6, 0xf0eeb20], [7, 0xf09d720]]}
  e2e_layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b8ff00]]}
  e2e_gelu_1263_0:                                                              {input: gelu_1263, type: queue, entries: 128, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b5d260], [6, 0x9b9ea80], [7, 0x9b4d680], [0, 0xa3aff20], [1, 0xb3d27c0], [2, 0xb3d7900], [3, 0xb430f00], [4, 0x9ba6440], [5, 0xb3bd280], [6, 0xb3feaa0], [7, 0xb3ad6a0], [0, 0xbc0ff40], [1, 0xcc327e0], [2, 0xcc37920], [3, 0xcc90f20], [4, 0xb406460]]}
  e2e_layernorm_1257.dc.add.10_0:                                               {input: layernorm_1257.dc.add.10, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9b727a0], [2, 0x9b778e0], [3, 0x9bd0ee0], [4, 0xd27e4c0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_8: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_14: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_16: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    attention_mask_input_op_fork_nop0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_input_op_fork_nop0_input_op_fork_nop0: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_23_1.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_23_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_17: {type: add, grid_loc: [2, 9], grid_size: [2, 1], inputs: [multiply_16, attention_mask_s_brcst_m2_23_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_18.dc.exp.0: {type: exp, grid_loc: [4, 8], grid_size: [2, 3], inputs: [add_17],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [softmax_18.dc.exp.0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_18.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 11], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_18.dc.multiply.3: {type: multiply, grid_loc: [6, 0], grid_size: [2, 2], inputs: [softmax_18.dc.exp.0, softmax_18.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_22: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_29: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [softmax_18.dc.multiply.3, matmul_22],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_33: {type: matmul, grid_loc: [6, 4], grid_size: [2, 8], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_37: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    attention_mask_s_brcst_m2_22_1.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_22_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_21_1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_21_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_20_1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_20_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_19_1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_19_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_18_1.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_18_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_17_1.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_17_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_16_1.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_16_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_15_1.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_15_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_14_1.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_14_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_13_1.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_13_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_12_1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_12_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_input_op_fork_nop1: {type: nop, grid_loc: [3, 8], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_1:
    target_device: 0
    input_count: 128
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_add_37_0, e2e_layernorm_38.dc.reduce_avg.0.lc1_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [2, 2], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_38.dc.add.5: {type: add, grid_loc: [1, 4], grid_size: [2, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.sqrt.6: {type: sqrt, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_38.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 6], grid_size: [2, 1], inputs: [layernorm_38.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_38.dc.reciprocal.7, lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_38.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_38.dc.multiply.9: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_38.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_38.dc.add.10: {type: add, grid_loc: [3, 4], grid_size: [2, 2], inputs: [layernorm_38.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    attention_mask_input_op_fork_nop1_input_op_fork_nop0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_attention_mask_input_op_fork_nop1_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, e2e_attention_mask_input_op_fork_nop1_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, e2e_attention_mask_input_op_fork_nop1_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, e2e_attention_mask_input_op_fork_nop1_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, e2e_attention_mask_input_op_fork_nop1_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_2:
    target_device: 0
    input_count: 128
    matmul_41: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_layernorm_38.dc.add.10_0, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_44: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_41],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_3:
    target_device: 0
    input_count: 128
    matmul_47: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_44_0, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_51: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_47, e2e_layernorm_38.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_52.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_52.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_52.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_52.dc.reciprocal.7, lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_52.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_52.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_52.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_52.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_52.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_55: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_52.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_61: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_52.dc.add.10_0, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_67: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_55_0, matmul_61],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_69: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_70: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_69, e2e_attention_mask_s_brcst_m2_22_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_71.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_70],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_71.dc.exp.0, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_71.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_71.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_71.dc.exp.0, softmax_71.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_75: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_52.dc.add.10_0, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_82: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_71.dc.multiply.3, matmul_75],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_86: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_90: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_86, e2e_layernorm_52.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_91.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_91.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_91.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_91.dc.reciprocal.7, lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_91.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_91.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_91.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_5:
    target_device: 0
    input_count: 128
    layernorm_91.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_91.dc.multiply.9_0, e2e_layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_94: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_91.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_97: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_94],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_6:
    target_device: 0
    input_count: 128
    matmul_100: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_97_0, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_104: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_100, e2e_layernorm_91.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_105.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_105.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_105.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_105.dc.reciprocal.7, lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_105.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_105.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_105.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_105.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_105.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_108: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_105.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_7:
    target_device: 0
    input_count: 128
    matmul_114: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_105.dc.add.10_0, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_120: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_108_0, matmul_114],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_122: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_123: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_122, e2e_attention_mask_s_brcst_m2_21_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_124.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_123],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_124.dc.exp.0, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_124.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_124.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_124.dc.exp.0, softmax_124.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_128: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_105.dc.add.10_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_135: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_124.dc.multiply.3, matmul_128],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_139: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_143: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_139, e2e_layernorm_105.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_144.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_144.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_144.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_144.dc.reciprocal.7, lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_144.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_144.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_144.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_8:
    target_device: 0
    input_count: 128
    layernorm_144.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_144.dc.multiply.9_0, e2e_layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_147: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_144.dc.add.10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_150: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_147],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_9:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_157: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_153, e2e_layernorm_144.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_158.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_158.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_158.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_158.dc.reciprocal.7, lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_158.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_158.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_158.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_158.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_158.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_161: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_158.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_10:
    target_device: 0
    input_count: 128
    matmul_167: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_158.dc.add.10_0, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_173: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_161_0, matmul_167],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_175: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_176: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_175, e2e_attention_mask_s_brcst_m2_20_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_177.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_176],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_177.dc.exp.0, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_177.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_177.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_177.dc.exp.0, softmax_177.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_181: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_158.dc.add.10_0, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_188: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_177.dc.multiply.3, matmul_181],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_192: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_196: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_192, e2e_layernorm_158.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_197.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_197.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_197.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_197.dc.reciprocal.7, lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_197.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_197.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_197.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_11:
    target_device: 0
    input_count: 128
    layernorm_197.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_197.dc.multiply.9_0, e2e_layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_200: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_197.dc.add.10, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_203: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_200],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_12:
    target_device: 0
    input_count: 128
    matmul_206: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_203_0, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_210: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_206, e2e_layernorm_197.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_211.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_211.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_211.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_211.dc.reciprocal.7, lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_211.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_211.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_211.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_211.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_211.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_214: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_211.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_13:
    target_device: 0
    input_count: 128
    matmul_220: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_211.dc.add.10_0, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_226: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_214_0, matmul_220],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_228: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_229: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_228, e2e_attention_mask_s_brcst_m2_19_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_230.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_229],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_230.dc.exp.0, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_230.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_230.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_230.dc.exp.0, softmax_230.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_234: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_211.dc.add.10_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_241: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_230.dc.multiply.3, matmul_234],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_245: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_249: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_245, e2e_layernorm_211.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_250.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_250.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_250.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_250.dc.reciprocal.7, lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_250.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_250.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_250.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_14:
    target_device: 0
    input_count: 128
    layernorm_250.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_250.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_253: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_250.dc.add.10, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_256: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_253],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_15:
    target_device: 0
    input_count: 128
    matmul_259: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_256_0, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_263: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_259, e2e_layernorm_250.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_264.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_264.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_264.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_264.dc.reciprocal.7, lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_264.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_264.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_264.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_264.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_264.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_267: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_264.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_16:
    target_device: 0
    input_count: 128
    matmul_273: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_264.dc.add.10_0, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_279: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_267_0, matmul_273],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_281: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_282: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_281, e2e_attention_mask_s_brcst_m2_18_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_283.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_282],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_283.dc.exp.0, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_283.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_283.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_283.dc.exp.0, softmax_283.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_287: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_264.dc.add.10_0, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_294: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_283.dc.multiply.3, matmul_287],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_298: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_302: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_298, e2e_layernorm_264.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_303.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_303.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_303.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_303.dc.reciprocal.7, lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_303.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_303.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_303.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_17:
    target_device: 0
    input_count: 128
    layernorm_303.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_303.dc.multiply.9_0, e2e_layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_306: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_303.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_309: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_306],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_18:
    target_device: 0
    input_count: 128
    matmul_312: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_309_0, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_316: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_312, e2e_layernorm_303.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_317.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_317.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_317.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_317.dc.reciprocal.7, lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_317.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_317.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_317.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_317.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_317.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_320: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_317.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_19:
    target_device: 0
    input_count: 128
    matmul_326: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_317.dc.add.10_0, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_332: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_320_0, matmul_326],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_334: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_335: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_334, e2e_attention_mask_s_brcst_m2_17_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_336.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_335],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_336.dc.exp.0, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_336.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_336.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_336.dc.exp.0, softmax_336.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_340: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_317.dc.add.10_0, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_347: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_336.dc.multiply.3, matmul_340],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_351: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_355: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_351, e2e_layernorm_317.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_356.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_356.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_356.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_356.dc.reciprocal.7, lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_356.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_356.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_356.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_20:
    target_device: 0
    input_count: 128
    layernorm_356.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_356.dc.multiply.9_0, e2e_layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_359: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_356.dc.add.10, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_362: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_359],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_21:
    target_device: 0
    input_count: 128
    matmul_365: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_362_0, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_369: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_365, e2e_layernorm_356.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_370.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_370.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_370.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_370.dc.reciprocal.7, lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_370.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_370.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_370.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_370.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_370.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_373: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_370.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_22:
    target_device: 0
    input_count: 128
    matmul_379: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_370.dc.add.10_0, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_385: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_373_0, matmul_379],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_387: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_388: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_387, e2e_attention_mask_s_brcst_m2_16_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_389.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_388],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_389.dc.exp.0, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_389.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_389.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_389.dc.exp.0, softmax_389.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_393: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_370.dc.add.10_0, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_400: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_389.dc.multiply.3, matmul_393],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_404: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_408: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_404, e2e_layernorm_370.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_409.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_409.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_409.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_409.dc.reciprocal.7, lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_409.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_409.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_409.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_23:
    target_device: 0
    input_count: 128
    layernorm_409.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_409.dc.multiply.9_0, e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_412: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_409.dc.add.10, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_415: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_412],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_24:
    target_device: 0
    input_count: 128
    matmul_418: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_415_0, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_422: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_418, e2e_layernorm_409.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_423.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_423.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_423.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_423.dc.reciprocal.7, lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_423.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_423.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_423.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_423.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_423.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_426: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_423.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_25:
    target_device: 0
    input_count: 128
    matmul_432: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_423.dc.add.10_0, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_438: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_426_0, matmul_432],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_440: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_441: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_440, e2e_attention_mask_s_brcst_m2_15_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_442.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_441],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_442.dc.exp.0, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_442.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_442.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_442.dc.exp.0, softmax_442.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_446: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_423.dc.add.10_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_453: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_442.dc.multiply.3, matmul_446],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_457: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_461: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_457, e2e_layernorm_423.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_462.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_462.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_462.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_462.dc.reciprocal.7, lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_462.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_462.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_462.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_26:
    target_device: 0
    input_count: 128
    layernorm_462.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_462.dc.multiply.9_0, e2e_layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_465: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_462.dc.add.10, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_468: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_465],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_27:
    target_device: 0
    input_count: 128
    matmul_471: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_468_0, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_475: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_471, e2e_layernorm_462.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_476.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_476.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_476.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_476.dc.reciprocal.7, lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_476.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_476.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_476.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_476.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_476.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_479: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_476.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_28:
    target_device: 0
    input_count: 128
    matmul_485: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_476.dc.add.10_0, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_491: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_479_0, matmul_485],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_493: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_494: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_493, e2e_attention_mask_s_brcst_m2_14_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_495.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_494],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_495.dc.exp.0, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_495.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_495.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_495.dc.exp.0, softmax_495.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_499: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_476.dc.add.10_0, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_506: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_495.dc.multiply.3, matmul_499],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_510: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_514: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_510, e2e_layernorm_476.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_515.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_515.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_515.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_515.dc.reciprocal.7, lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_515.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_515.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_515.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_29:
    target_device: 0
    input_count: 128
    layernorm_515.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_515.dc.multiply.9_0, e2e_layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_518: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_515.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_521: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_518],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_30:
    target_device: 0
    input_count: 128
    matmul_524: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_521_0, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_528: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_524, e2e_layernorm_515.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_529.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_529.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_529.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_529.dc.reciprocal.7, lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_529.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_529.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_529.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_529.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_529.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_532: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_529.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_31:
    target_device: 0
    input_count: 128
    matmul_538: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_529.dc.add.10_0, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_544: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_532_0, matmul_538],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_546: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_547: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_546, e2e_attention_mask_s_brcst_m2_13_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_548.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_547],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_548.dc.exp.0, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_548.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_548.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_548.dc.exp.0, softmax_548.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_552: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_529.dc.add.10_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_559: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_548.dc.multiply.3, matmul_552],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_563: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_567: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_563, e2e_layernorm_529.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_568.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_568.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_568.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_568.dc.reciprocal.7, lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_568.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_568.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_568.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_32:
    target_device: 0
    input_count: 128
    layernorm_568.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_568.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_571: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_568.dc.add.10, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_574: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_571],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_33:
    target_device: 0
    input_count: 128
    matmul_577: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_574_0, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_581: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_577, e2e_layernorm_568.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_582.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_582.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_582.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_582.dc.reciprocal.7, lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_582.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_582.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_582.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_582.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_582.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_585: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_582.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_34:
    target_device: 0
    input_count: 128
    matmul_591: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_582.dc.add.10_0, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_597: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_585_0, matmul_591],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_599: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_600: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_599, e2e_attention_mask_s_brcst_m2_12_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_601.dc.exp.0: {type: exp, grid_loc: [2, 8], grid_size: [2, 3], inputs: [add_600],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [softmax_601.dc.exp.0, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_601.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 0], grid_size: [2, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_601.dc.multiply.3: {type: multiply, grid_loc: [3, 1], grid_size: [2, 2], inputs: [softmax_601.dc.exp.0, softmax_601.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_605: {type: matmul, grid_loc: [4, 3], grid_size: [2, 8], inputs: [e2e_layernorm_582.dc.add.10_0, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_612: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [softmax_601.dc.multiply.3, matmul_605],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_616: {type: matmul, grid_loc: [6, 2], grid_size: [2, 8], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_620: {type: add, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_616, e2e_layernorm_582.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [2, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [8, 4], grid_size: [2, 4], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [6, 10], grid_size: [2, 2], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [2, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_621.dc.add.5: {type: add, grid_loc: [8, 9], grid_size: [2, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.sqrt.6: {type: sqrt, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_621.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_621.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_35:
    target_device: 0
    input_count: 128
    layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_621.dc.reciprocal.7_0, lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_621.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_621.dc.subtract.1_0, layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_621.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 2], inputs: [layernorm_621.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_621.dc.add.10: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [layernorm_621.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_624: {type: matmul, grid_loc: [2, 0], grid_size: [6, 8], inputs: [layernorm_621.dc.add.10, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_627: {type: gelu, grid_loc: [8, 0], grid_size: [2, 8], inputs: [matmul_624],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_36:
    target_device: 0
    input_count: 128
    matmul_630: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_627_0, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_634: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_630, e2e_layernorm_621.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_635.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_635.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_635.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_635.dc.reciprocal.7, lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_635.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_635.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_635.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_635.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_635.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_638: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_635.dc.add.10, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_37:
    target_device: 0
    input_count: 128
    matmul_644: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_635.dc.add.10_0, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_650: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_638_0, matmul_644],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_652: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_650, input_1_multiply_652_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_653: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_652, e2e_attention_mask_s_brcst_m2_11_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_654.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_653],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_654.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_654.dc.exp.0, lc.input_tensor.softmax_654.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_654.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_654.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_654.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_654.dc.exp.0, softmax_654.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_658: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_635.dc.add.10_0, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_665: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_654.dc.multiply.3, matmul_658],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_669: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_665, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_673: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_669, e2e_layernorm_635.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_673, lc.input_tensor.layernorm_674.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_674.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_673, layernorm_674.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_674.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_674.dc.subtract.1, layernorm_674.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_674.dc.multiply.2, lc.input_tensor.layernorm_674.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_674.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_674.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_674.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_674.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_674.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_674.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_674.dc.reciprocal.7, lc.input_tensor.layernorm_674.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_674.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_674.dc.subtract.1, layernorm_674.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_674.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_674.dc.multiply.8, layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_38:
    target_device: 0
    input_count: 128
    layernorm_674.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_674.dc.multiply.9_0, e2e_layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_677: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_674.dc.add.10, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_680: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_677],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_39:
    target_device: 0
    input_count: 128
    matmul_683: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_680_0, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_687: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_683, e2e_layernorm_674.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_687, lc.input_tensor.layernorm_688.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_688.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_687, layernorm_688.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_688.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_688.dc.subtract.1, layernorm_688.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_688.dc.multiply.2, lc.input_tensor.layernorm_688.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_688.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_688.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_688.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_688.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_688.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_688.dc.reciprocal.7, lc.input_tensor.layernorm_688.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_688.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_688.dc.subtract.1, layernorm_688.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_688.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_688.dc.multiply.8, layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_688.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_688.dc.multiply.9, layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_691: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_688.dc.add.10, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_40:
    target_device: 0
    input_count: 128
    matmul_697: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_688.dc.add.10_0, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_703: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_691_0, matmul_697],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_705: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_703, input_1_multiply_705_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_706: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_705, e2e_attention_mask_s_brcst_m2_10_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_707.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_706],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_707.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_707.dc.exp.0, lc.input_tensor.softmax_707.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_707.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_707.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_707.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_707.dc.exp.0, softmax_707.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_711: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_688.dc.add.10_0, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_718: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_707.dc.multiply.3, matmul_711],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_722: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_718, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_726: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_722, e2e_layernorm_688.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_726, lc.input_tensor.layernorm_727.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_727.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_726, layernorm_727.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_727.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_727.dc.subtract.1, layernorm_727.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_727.dc.multiply.2, lc.input_tensor.layernorm_727.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_727.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_727.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_727.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_727.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_727.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_727.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_727.dc.reciprocal.7, lc.input_tensor.layernorm_727.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_727.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_727.dc.subtract.1, layernorm_727.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_727.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_727.dc.multiply.8, layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_41:
    target_device: 0
    input_count: 128
    layernorm_727.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_727.dc.multiply.9_0, e2e_layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_730: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_727.dc.add.10, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_733: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_730],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_42:
    target_device: 0
    input_count: 128
    matmul_736: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_733_0, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_740: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_736, e2e_layernorm_727.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_740, lc.input_tensor.layernorm_741.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_741.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_740, layernorm_741.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_741.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_741.dc.subtract.1, layernorm_741.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_741.dc.multiply.2, lc.input_tensor.layernorm_741.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_741.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_741.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_741.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_741.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_741.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_741.dc.reciprocal.7, lc.input_tensor.layernorm_741.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_741.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_741.dc.subtract.1, layernorm_741.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_741.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_741.dc.multiply.8, layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_741.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_741.dc.multiply.9, layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_744: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_741.dc.add.10, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_43:
    target_device: 0
    input_count: 128
    matmul_750: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_741.dc.add.10_0, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_756: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_744_0, matmul_750],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_758: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_756, input_1_multiply_758_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_759: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_758, e2e_attention_mask_s_brcst_m2_9_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_760.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_759],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_760.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_760.dc.exp.0, lc.input_tensor.softmax_760.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_760.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_760.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_760.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_760.dc.exp.0, softmax_760.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_764: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_741.dc.add.10_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_771: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_760.dc.multiply.3, matmul_764],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_775: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_771, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_779: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_775, e2e_layernorm_741.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_779, lc.input_tensor.layernorm_780.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_780.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_779, layernorm_780.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_780.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_780.dc.subtract.1, layernorm_780.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_780.dc.multiply.2, lc.input_tensor.layernorm_780.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_780.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_780.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_780.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_780.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_780.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_780.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_780.dc.reciprocal.7, lc.input_tensor.layernorm_780.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_780.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_780.dc.subtract.1, layernorm_780.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_780.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_780.dc.multiply.8, layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_44:
    target_device: 0
    input_count: 128
    layernorm_780.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_780.dc.multiply.9_0, e2e_layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_783: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_780.dc.add.10, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_786: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_783],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_45:
    target_device: 0
    input_count: 128
    matmul_789: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_786_0, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_793: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_789, e2e_layernorm_780.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_793, lc.input_tensor.layernorm_794.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_794.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_793, layernorm_794.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_794.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_794.dc.subtract.1, layernorm_794.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_794.dc.multiply.2, lc.input_tensor.layernorm_794.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_794.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_794.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_794.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_794.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_794.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_794.dc.reciprocal.7, lc.input_tensor.layernorm_794.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_794.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_794.dc.subtract.1, layernorm_794.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_794.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_794.dc.multiply.8, layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_794.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_794.dc.multiply.9, layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_797: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_794.dc.add.10, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_46:
    target_device: 0
    input_count: 128
    matmul_803: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_794.dc.add.10_0, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_809: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_797_0, matmul_803],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_811: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_809, input_1_multiply_811_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_812: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_811, e2e_attention_mask_s_brcst_m2_8_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_813.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_812],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_813.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_813.dc.exp.0, lc.input_tensor.softmax_813.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_813.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_813.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_813.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_813.dc.exp.0, softmax_813.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_817: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_794.dc.add.10_0, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_824: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_813.dc.multiply.3, matmul_817],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_828: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_824, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_832: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_828, e2e_layernorm_794.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_832, lc.input_tensor.layernorm_833.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_833.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_832, layernorm_833.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_833.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_833.dc.subtract.1, layernorm_833.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_833.dc.multiply.2, lc.input_tensor.layernorm_833.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_833.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_833.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_833.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_833.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_833.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_833.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_833.dc.reciprocal.7, lc.input_tensor.layernorm_833.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_833.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_833.dc.subtract.1, layernorm_833.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_833.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_833.dc.multiply.8, layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_47:
    target_device: 0
    input_count: 128
    layernorm_833.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_833.dc.multiply.9_0, e2e_layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_836: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_833.dc.add.10, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_839: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_836],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_48:
    target_device: 0
    input_count: 128
    matmul_842: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_839_0, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_846: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_842, e2e_layernorm_833.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_846, lc.input_tensor.layernorm_847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_847.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_846, layernorm_847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_847.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_847.dc.subtract.1, layernorm_847.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_847.dc.multiply.2, lc.input_tensor.layernorm_847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_847.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_847.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_847.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_847.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_847.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_847.dc.reciprocal.7, lc.input_tensor.layernorm_847.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_847.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_847.dc.subtract.1, layernorm_847.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_847.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_847.dc.multiply.8, layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_847.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_847.dc.multiply.9, layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_850: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_847.dc.add.10, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_49:
    target_device: 0
    input_count: 128
    matmul_856: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_847.dc.add.10_0, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_862: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_850_0, matmul_856],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_864: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_862, input_1_multiply_864_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_865: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_864, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_866.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_865],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_866.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_866.dc.exp.0, lc.input_tensor.softmax_866.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_866.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_866.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_866.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_866.dc.exp.0, softmax_866.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_870: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_847.dc.add.10_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_877: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_866.dc.multiply.3, matmul_870],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_881: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_877, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_885: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_881, e2e_layernorm_847.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_885, lc.input_tensor.layernorm_886.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_886.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_885, layernorm_886.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_886.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_886.dc.subtract.1, layernorm_886.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_886.dc.multiply.2, lc.input_tensor.layernorm_886.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_886.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_886.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_886.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_886.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_886.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_886.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_886.dc.reciprocal.7, lc.input_tensor.layernorm_886.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_886.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_886.dc.subtract.1, layernorm_886.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_886.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_886.dc.multiply.8, layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_50:
    target_device: 0
    input_count: 128
    layernorm_886.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_886.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_889: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_886.dc.add.10, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_892: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_889],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_51:
    target_device: 0
    input_count: 128
    matmul_895: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_892_0, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_899: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_895, e2e_layernorm_886.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_899, lc.input_tensor.layernorm_900.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_900.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_899, layernorm_900.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_900.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_900.dc.subtract.1, layernorm_900.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_900.dc.multiply.2, lc.input_tensor.layernorm_900.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_900.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_900.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_900.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_900.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_900.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_900.dc.reciprocal.7, lc.input_tensor.layernorm_900.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_900.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_900.dc.subtract.1, layernorm_900.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_900.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_900.dc.multiply.8, layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_900.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_900.dc.multiply.9, layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_903: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_900.dc.add.10, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_52:
    target_device: 0
    input_count: 128
    matmul_909: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_900.dc.add.10_0, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_915: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_903_0, matmul_909],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_917: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_915, input_1_multiply_917_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_918: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_917, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_919.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_918],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_919.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_919.dc.exp.0, lc.input_tensor.softmax_919.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_919.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_919.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_919.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_919.dc.exp.0, softmax_919.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_923: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_900.dc.add.10_0, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_930: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_919.dc.multiply.3, matmul_923],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_934: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_930, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_938: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_934, e2e_layernorm_900.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_938, lc.input_tensor.layernorm_939.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_939.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_938, layernorm_939.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_939.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_939.dc.subtract.1, layernorm_939.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_939.dc.multiply.2, lc.input_tensor.layernorm_939.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_939.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_939.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_939.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_939.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_939.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_939.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_939.dc.reciprocal.7, lc.input_tensor.layernorm_939.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_939.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_939.dc.subtract.1, layernorm_939.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_939.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_939.dc.multiply.8, layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_53:
    target_device: 0
    input_count: 128
    layernorm_939.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_939.dc.multiply.9_0, e2e_layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_942: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_939.dc.add.10, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_945: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_942],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_54:
    target_device: 0
    input_count: 128
    matmul_948: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_945_0, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_952: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_948, e2e_layernorm_939.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_952, lc.input_tensor.layernorm_953.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_953.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_952, layernorm_953.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_953.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_953.dc.subtract.1, layernorm_953.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_953.dc.multiply.2, lc.input_tensor.layernorm_953.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_953.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_953.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_953.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_953.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_953.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_953.dc.reciprocal.7, lc.input_tensor.layernorm_953.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_953.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_953.dc.subtract.1, layernorm_953.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_953.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_953.dc.multiply.8, layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_953.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_953.dc.multiply.9, layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_956: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_953.dc.add.10, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_55:
    target_device: 0
    input_count: 128
    matmul_962: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_953.dc.add.10_0, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_968: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_956_0, matmul_962],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_970: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_968, input_1_multiply_970_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_971: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_970, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_972.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_971],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_972.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_972.dc.exp.0, lc.input_tensor.softmax_972.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_972.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_972.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_972.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_972.dc.exp.0, softmax_972.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_976: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_953.dc.add.10_0, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_983: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_972.dc.multiply.3, matmul_976],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_987: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_983, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_991: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_987, e2e_layernorm_953.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_991, lc.input_tensor.layernorm_992.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_992.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_991, layernorm_992.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_992.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_992.dc.subtract.1, layernorm_992.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_992.dc.multiply.2, lc.input_tensor.layernorm_992.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_992.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_992.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_992.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_992.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_992.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_992.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_992.dc.reciprocal.7, lc.input_tensor.layernorm_992.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_992.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_992.dc.subtract.1, layernorm_992.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_992.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_992.dc.multiply.8, layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_56:
    target_device: 0
    input_count: 128
    layernorm_992.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_992.dc.multiply.9_0, e2e_layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_995: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_992.dc.add.10, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_998: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_995],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_57:
    target_device: 0
    input_count: 128
    matmul_1001: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_998_0, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1005: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1001, e2e_layernorm_992.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1005, lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1006.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1005, layernorm_1006.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1006.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1006.dc.subtract.1, layernorm_1006.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1006.dc.multiply.2, lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1006.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1006.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1006.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1006.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1006.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1006.dc.reciprocal.7, lc.input_tensor.layernorm_1006.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1006.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1006.dc.subtract.1, layernorm_1006.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1006.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1006.dc.multiply.8, layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1006.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1006.dc.multiply.9, layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1009: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_1006.dc.add.10, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_58:
    target_device: 0
    input_count: 128
    matmul_1015: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1006.dc.add.10_0, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1021: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_1009_0, matmul_1015],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1023: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_1021, input_1_multiply_1023_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1024: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_1023, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_1025.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_1024],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1025.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_1025.dc.exp.0, lc.input_tensor.softmax_1025.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_1025.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_1025.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1025.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_1025.dc.exp.0, softmax_1025.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_1029: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1006.dc.add.10_0, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1036: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_1025.dc.multiply.3, matmul_1029],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1040: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1036, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1044: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_1040, e2e_layernorm_1006.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_1044, lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1045.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_1044, layernorm_1045.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1045.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1045.dc.subtract.1, layernorm_1045.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1045.dc.multiply.2, lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1045.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1045.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1045.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1045.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1045.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1045.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1045.dc.reciprocal.7, lc.input_tensor.layernorm_1045.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1045.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_1045.dc.subtract.1, layernorm_1045.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1045.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_1045.dc.multiply.8, layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_59:
    target_device: 0
    input_count: 128
    layernorm_1045.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1045.dc.multiply.9_0, e2e_layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1048: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_1045.dc.add.10, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_1051: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1048],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_60:
    target_device: 0
    input_count: 128
    matmul_1054: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_1051_0, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1058: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1054, e2e_layernorm_1045.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1058, lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1059.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1058, layernorm_1059.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1059.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1059.dc.subtract.1, layernorm_1059.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1059.dc.multiply.2, lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1059.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1059.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1059.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1059.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1059.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1059.dc.reciprocal.7, lc.input_tensor.layernorm_1059.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1059.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1059.dc.subtract.1, layernorm_1059.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1059.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1059.dc.multiply.8, layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1059.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1059.dc.multiply.9, layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1062: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_1059.dc.add.10, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_61:
    target_device: 0
    input_count: 128
    matmul_1068: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1059.dc.add.10_0, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1074: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_1062_0, matmul_1068],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1076: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_1074, input_1_multiply_1076_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1077: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_1076, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_1078.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_1077],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1078.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_1078.dc.exp.0, lc.input_tensor.softmax_1078.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_1078.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_1078.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1078.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_1078.dc.exp.0, softmax_1078.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_1082: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1059.dc.add.10_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1089: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_1078.dc.multiply.3, matmul_1082],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1093: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1089, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1097: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_1093, e2e_layernorm_1059.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_1097, lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1098.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_1097, layernorm_1098.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1098.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1098.dc.subtract.1, layernorm_1098.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1098.dc.multiply.2, lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1098.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1098.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1098.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1098.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1098.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1098.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1098.dc.reciprocal.7, lc.input_tensor.layernorm_1098.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1098.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_1098.dc.subtract.1, layernorm_1098.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1098.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_1098.dc.multiply.8, layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_62:
    target_device: 0
    input_count: 128
    layernorm_1098.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1098.dc.multiply.9_0, e2e_layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1101: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_1098.dc.add.10, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_1104: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1101],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_63:
    target_device: 0
    input_count: 128
    matmul_1107: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_1104_0, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1111: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1107, e2e_layernorm_1098.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1111, lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1112.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1111, layernorm_1112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1112.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1112.dc.subtract.1, layernorm_1112.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1112.dc.multiply.2, lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1112.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1112.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1112.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1112.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1112.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1112.dc.reciprocal.7, lc.input_tensor.layernorm_1112.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1112.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1112.dc.subtract.1, layernorm_1112.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1112.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1112.dc.multiply.8, layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1112.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1112.dc.multiply.9, layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1115: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_1112.dc.add.10, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_64:
    target_device: 0
    input_count: 128
    matmul_1121: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1112.dc.add.10_0, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1127: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_1115_0, matmul_1121],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1129: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_1127, input_1_multiply_1129_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1130: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_1129, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_1131.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_1130],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1131.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_1131.dc.exp.0, lc.input_tensor.softmax_1131.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_1131.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_1131.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1131.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_1131.dc.exp.0, softmax_1131.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_1135: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1112.dc.add.10_0, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1142: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_1131.dc.multiply.3, matmul_1135],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1146: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1142, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1150: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_1146, e2e_layernorm_1112.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_1150, lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1151.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_1150, layernorm_1151.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1151.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1151.dc.subtract.1, layernorm_1151.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1151.dc.multiply.2, lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1151.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1151.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1151.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1151.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1151.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1151.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1151.dc.reciprocal.7, lc.input_tensor.layernorm_1151.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1151.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_1151.dc.subtract.1, layernorm_1151.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1151.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_1151.dc.multiply.8, layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_65:
    target_device: 0
    input_count: 128
    layernorm_1151.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1151.dc.multiply.9_0, e2e_layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1154: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_1151.dc.add.10, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_1157: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1154],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_66:
    target_device: 0
    input_count: 128
    matmul_1160: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_1157_0, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1164: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1160, e2e_layernorm_1151.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1164, lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1165.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1164, layernorm_1165.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1165.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1165.dc.subtract.1, layernorm_1165.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1165.dc.multiply.2, lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1165.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1165.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1165.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1165.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1165.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1165.dc.reciprocal.7, lc.input_tensor.layernorm_1165.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1165.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1165.dc.subtract.1, layernorm_1165.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1165.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1165.dc.multiply.8, layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1165.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1165.dc.multiply.9, layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1168: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_1165.dc.add.10, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_67:
    target_device: 0
    input_count: 128
    matmul_1174: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1165.dc.add.10_0, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1180: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_1168_0, matmul_1174],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1182: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_1180, input_1_multiply_1182_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1183: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_1182, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_1184.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 3], inputs: [add_1183],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1184.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_1184.dc.exp.0, lc.input_tensor.softmax_1184.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_1184.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [softmax_1184.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1184.dc.multiply.3: {type: multiply, grid_loc: [2, 5], grid_size: [2, 2], inputs: [softmax_1184.dc.exp.0, softmax_1184.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_1188: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1165.dc.add.10_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1195: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [softmax_1184.dc.multiply.3, matmul_1188],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1199: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1195, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1203: {type: add, grid_loc: [4, 8], grid_size: [2, 4], inputs: [matmul_1199, e2e_layernorm_1165.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [2, 1], inputs: [add_1203, lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1204.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_1203, layernorm_1204.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1204.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1204.dc.subtract.1, layernorm_1204.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1204.dc.multiply.2, lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1204.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1204.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1204.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1204.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1204.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1204.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1204.dc.reciprocal.7, lc.input_tensor.layernorm_1204.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1204.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_1204.dc.subtract.1, layernorm_1204.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1204.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_1204.dc.multiply.8, layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_68:
    target_device: 0
    input_count: 128
    layernorm_1204.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1204.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1207: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_1204.dc.add.10, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_1210: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1207],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_69:
    target_device: 0
    input_count: 128
    matmul_1213: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_1210_0, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1217: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1213, e2e_layernorm_1204.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1217, lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1218.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1217, layernorm_1218.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1218.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1218.dc.subtract.1, layernorm_1218.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1218.dc.multiply.2, lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1218.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1218.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1218.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1218.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1218.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1218.dc.reciprocal.7, lc.input_tensor.layernorm_1218.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1218.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1218.dc.subtract.1, layernorm_1218.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1218.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1218.dc.multiply.8, layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1218.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1218.dc.multiply.9, layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1221: {type: matmul, grid_loc: [8, 2], grid_size: [2, 8], inputs: [layernorm_1218.dc.add.10, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_70:
    target_device: 0
    input_count: 128
    matmul_1227: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1218.dc.add.10_0, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1233: {type: matmul, grid_loc: [0, 8], grid_size: [2, 2], inputs: [e2e_matmul_1221_0, matmul_1227],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1235: {type: multiply, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_1233, input_1_multiply_1235_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1236: {type: add, grid_loc: [0, 11], grid_size: [2, 1], inputs: [multiply_1235, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    softmax_1237.dc.exp.0: {type: exp, grid_loc: [2, 8], grid_size: [2, 3], inputs: [add_1236],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1237.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [softmax_1237.dc.exp.0, lc.input_tensor.softmax_1237.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_1237.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_1237.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    softmax_1237.dc.multiply.3: {type: multiply, grid_loc: [4, 1], grid_size: [2, 2], inputs: [softmax_1237.dc.exp.0, softmax_1237.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_1241: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [e2e_layernorm_1218.dc.add.10_0, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1248: {type: matmul, grid_loc: [4, 3], grid_size: [2, 2], inputs: [softmax_1237.dc.multiply.3, matmul_1241],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 12], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1252: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1248, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1256: {type: add, grid_loc: [4, 5], grid_size: [2, 4], inputs: [matmul_1252, e2e_layernorm_1218.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [2, 1], inputs: [add_1256, lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1257.dc.subtract.1: {type: subtract, grid_loc: [6, 8], grid_size: [2, 4], inputs: [add_1256, layernorm_1257.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1257.dc.multiply.2: {type: multiply, grid_loc: [4, 10], grid_size: [2, 2], inputs: [layernorm_1257.dc.subtract.1, layernorm_1257.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1257.dc.multiply.2, lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1257.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1257.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1257.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1257.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1257.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1257.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1257.dc.reciprocal.7, lc.input_tensor.layernorm_1257.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1257.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 4], inputs: [layernorm_1257.dc.subtract.1, layernorm_1257.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1257.dc.multiply.9: {type: multiply, grid_loc: [8, 10], grid_size: [2, 2], inputs: [layernorm_1257.dc.multiply.8, layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_71:
    target_device: 0
    input_count: 128
    layernorm_1257.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1257.dc.multiply.9_0, e2e_layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_1260: {type: matmul, grid_loc: [0, 2], grid_size: [6, 8], inputs: [layernorm_1257.dc.add.10, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    gelu_1263: {type: gelu, grid_loc: [6, 0], grid_size: [2, 8], inputs: [matmul_1260],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_72:
    target_device: 0
    input_count: 128
    matmul_1266: {type: matmul, grid_loc: [0, 0], grid_size: [6, 8], inputs: [e2e_gelu_1263_0, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1270: {type: add, grid_loc: [0, 8], grid_size: [2, 4], inputs: [matmul_1266, e2e_layernorm_1257.dc.add.10_0],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1270, lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1271.dc.subtract.1: {type: subtract, grid_loc: [4, 8], grid_size: [2, 4], inputs: [add_1270, layernorm_1271.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1271.dc.multiply.2: {type: multiply, grid_loc: [2, 9], grid_size: [2, 2], inputs: [layernorm_1271.dc.subtract.1, layernorm_1271.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1271.dc.multiply.2, lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1271.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1271.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1271.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_1271.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true}}
    layernorm_1271.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1271.dc.reciprocal.7, lc.input_tensor.layernorm_1271.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1271.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 4], inputs: [layernorm_1271.dc.subtract.1, layernorm_1271.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1271.dc.multiply.9: {type: multiply, grid_loc: [6, 9], grid_size: [2, 2], inputs: [layernorm_1271.dc.multiply.8, layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1271.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_1271.dc.multiply.9, layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0, $gptr_q57: 0, $gptr_q55: 0, $lptr_q55: 0, $gptr_q54: 0, $gptr_q53: 0, $gptr_q52: 0, $lptr_q52: 0, $gptr_q51: 0, $lptr_q51: 0, $gptr_q50: 0, $gptr_q14: 0, $lptr_q14: 0, $gptr_q42: 0, $lptr_q59: 0, $lptr_q20: 0, $gptr_q24: 0, $lptr_q18: 0, $lptr_q46: 0, $gptr_q23: 0, $lptr_q15: 0, $gptr_q21: 0, $lptr_q1: 0, $gptr_q56: 0, $gptr_q26: 0, $lptr_q9: 0, $lptr_q10: 0, $lptr_q27: 0, $lptr_q28: 0, $gptr_q16: 0, $lptr_q11: 0, $gptr_q66: 0, $lptr_q4: 0, $gptr_q25: 0, $gptr_q68: 0, $gptr_q72: 0, $lptr_q30: 0, $gptr_q70: 0, $gptr_q62: 0, $lptr_q68: 0, $lptr_q70: 0, $gptr_q4: 0, $lptr_q43: 0, $gptr_q27: 0, $lptr_q44: 0, $gptr_q19: 0, $gptr_q30: 0, $lptr_q71: 0, $gptr_q61: 0, $lptr_q64: 0, $gptr_q34: 0, $lptr_q72: 0, $lptr_q26: 0, $gptr_q58: 0, $lptr_q50: 0, $lptr_q21: 0, $lptr_q12: 0, $gptr_q46: 0, $lptr_q65: 0, $lptr_q67: 0, $gptr_q17: 0, $gptr_q22: 0, $lptr_q24: 0, $gptr_q65: 0, $gptr_q29: 0, $gptr_q41: 0, $lptr_q58: 0, $gptr_q59: 0, $lptr_q19: 0, $lptr_q63: 0, $gptr_q12: 0, $lptr_q62: 0, $lptr_q54: 0, $gptr_q18: 0, $lptr_q37: 0, $gptr_q3: 0, $lptr_q61: 0, $lptr_q22: 0, $lptr_q16: 0, $gptr_q60: 0, $gptr_q71: 0, $gptr_q69: 0, $lptr_q53: 0, $gptr_q1: 0, $lptr_q60: 0, $gptr_q15: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q2_shadow: 0, $lptr_q66: 0, $lptr_q3: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q17: 0, $gptr_q20: 0, $lptr_q69: 0, $lptr_q6: 0, $gptr_q6: 0, $lptr_q35: 0, $gptr_q7: 0, $lptr_q41: 0, $gptr_q38: 0, $lptr_q42: 0, $lptr_q8: 0, $lptr_q45: 0, $gptr_q8: 0, $lptr_q40: 0, $gptr_q9: 0, $gptr_q11: 0, $gptr_q13: 0, $lptr_q29: 0, $lptr_q31: 0, $lptr_q23: 0, $gptr_q39: 0, $lptr_q56: 0, $gptr_q63: 0, $gptr_q31: 0, $lptr_q32: 0, $lptr_q13: 0, $gptr_q43: 0, $gptr_q32: 0, $lptr_q33: 0, $gptr_q33: 0, $gptr_q64: 0, $gptr_q10: 0, $lptr_q34: 0, $gptr_q28: 0, $gptr_q35: 0, $lptr_q36: 0, $lptr_q57: 0, $gptr_q36: 0, $lptr_q7: 0, $gptr_q37: 0, $lptr_q49: 0, $gptr_q48: 0, $lptr_q39: 0, $gptr_q67: 0, $gptr_q40: 0, $lptr_q38: 0, $gptr_q44: 0, $lptr_q25: 0, $gptr_q45: 0, $lptr_q47: 0, $gptr_q47: 0, $lptr_q48: 0, $gptr_q49: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_add_37_0, e2e_layernorm_38.dc.reduce_avg.0.lc1_0, e2e_attention_mask_input_op_fork_nop1_0, e2e_attention_mask_s_brcst_m2_22_1.lc1_0, e2e_attention_mask_s_brcst_m2_21_1.lc1_0, e2e_attention_mask_s_brcst_m2_20_1.lc1_0, e2e_attention_mask_s_brcst_m2_19_1.lc1_0, e2e_attention_mask_s_brcst_m2_18_1.lc1_0, e2e_attention_mask_s_brcst_m2_17_1.lc1_0, e2e_attention_mask_s_brcst_m2_16_1.lc1_0, e2e_attention_mask_s_brcst_m2_15_1.lc1_0, e2e_attention_mask_s_brcst_m2_14_1.lc1_0, e2e_attention_mask_s_brcst_m2_13_1.lc1_0, e2e_attention_mask_s_brcst_m2_12_1.lc1_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_23_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_22_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_21_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_20_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_19_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_18_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_17_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_16_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_15_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_14_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_13_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_12_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_38.dc.add.10_0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               e2e_add_37_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_38.dc.reduce_avg.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_37_0, e2e_layernorm_38.dc.reduce_avg.0.lc1_0, e2e_attention_mask_input_op_fork_nop1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_gelu_44_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               e2e_layernorm_38.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_52.dc.add.10_0, e2e_matmul_55_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               e2e_layernorm_38.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_38.dc.add.10_0, e2e_gelu_44_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_91.dc.multiply.9_0, e2e_layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_4, queue_settings: {
               e2e_layernorm_52.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_attention_mask_s_brcst_m2_22_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_52.dc.add.10_0, e2e_matmul_55_0, e2e_attention_mask_s_brcst_m2_22_1.lc1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_91.dc.add.10_0, e2e_gelu_97_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               e2e_layernorm_91.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_91.dc.multiply.9_0, e2e_layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_105.dc.add.10_0, e2e_matmul_108_0]
    -   execute: {graph_name: fwd_6, queue_settings: {
               e2e_layernorm_91.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_91.dc.add.10_0, e2e_gelu_97_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_144.dc.multiply.9_0, e2e_layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_7, queue_settings: {
               e2e_layernorm_105.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_attention_mask_s_brcst_m2_21_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_105.dc.add.10_0, e2e_matmul_108_0, e2e_attention_mask_s_brcst_m2_21_1.lc1_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_144.dc.add.10_0, e2e_gelu_150_0]
    -   execute: {graph_name: fwd_8, queue_settings: {
               e2e_layernorm_144.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_144.dc.multiply.9_0, e2e_layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_158.dc.add.10_0, e2e_matmul_161_0]
    -   execute: {graph_name: fwd_9, queue_settings: {
               e2e_layernorm_144.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_144.dc.add.10_0, e2e_gelu_150_0]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_197.dc.multiply.9_0, e2e_layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_10, queue_settings: {
               e2e_layernorm_158.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_attention_mask_s_brcst_m2_20_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_158.dc.add.10_0, e2e_matmul_161_0, e2e_attention_mask_s_brcst_m2_20_1.lc1_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_197.dc.add.10_0, e2e_gelu_203_0]
    -   execute: {graph_name: fwd_11, queue_settings: {
               e2e_layernorm_197.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_197.dc.multiply.9_0, e2e_layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_211.dc.add.10_0, e2e_matmul_214_0]
    -   execute: {graph_name: fwd_12, queue_settings: {
               e2e_layernorm_197.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_gelu_203_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_197.dc.add.10_0, e2e_gelu_203_0]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_250.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_13, queue_settings: {
               e2e_layernorm_211.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_214_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_attention_mask_s_brcst_m2_19_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_211.dc.add.10_0, e2e_matmul_214_0, e2e_attention_mask_s_brcst_m2_19_1.lc1_0]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_250.dc.add.10_0, e2e_gelu_256_0]
    -   execute: {graph_name: fwd_14, queue_settings: {
               e2e_layernorm_250.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_250.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_264.dc.add.10_0, e2e_matmul_267_0]
    -   execute: {graph_name: fwd_15, queue_settings: {
               e2e_layernorm_250.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_gelu_256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_250.dc.add.10_0, e2e_gelu_256_0]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_303.dc.multiply.9_0, e2e_layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_16, queue_settings: {
               e2e_layernorm_264.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_267_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_attention_mask_s_brcst_m2_18_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_264.dc.add.10_0, e2e_matmul_267_0, e2e_attention_mask_s_brcst_m2_18_1.lc1_0]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_303.dc.add.10_0, e2e_gelu_309_0]
    -   execute: {graph_name: fwd_17, queue_settings: {
               e2e_layernorm_303.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_303.dc.multiply.9_0, e2e_layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_317.dc.add.10_0, e2e_matmul_320_0]
    -   execute: {graph_name: fwd_18, queue_settings: {
               e2e_layernorm_303.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_gelu_309_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_303.dc.add.10_0, e2e_gelu_309_0]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_356.dc.multiply.9_0, e2e_layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_19, queue_settings: {
               e2e_layernorm_317.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_matmul_320_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_attention_mask_s_brcst_m2_17_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_317.dc.add.10_0, e2e_matmul_320_0, e2e_attention_mask_s_brcst_m2_17_1.lc1_0]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_356.dc.add.10_0, e2e_gelu_362_0]
    -   execute: {graph_name: fwd_20, queue_settings: {
               e2e_layernorm_356.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_356.dc.multiply.9_0, e2e_layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_370.dc.add.10_0, e2e_matmul_373_0]
    -   execute: {graph_name: fwd_21, queue_settings: {
               e2e_layernorm_356.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_gelu_362_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_356.dc.add.10_0, e2e_gelu_362_0]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_409.dc.multiply.9_0, e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_22, queue_settings: {
               e2e_layernorm_370.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_373_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_attention_mask_s_brcst_m2_16_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_370.dc.add.10_0, e2e_matmul_373_0, e2e_attention_mask_s_brcst_m2_16_1.lc1_0]
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_409.dc.add.10_0, e2e_gelu_415_0]
    -   execute: {graph_name: fwd_23, queue_settings: {
               e2e_layernorm_409.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_409.dc.multiply.9_0, e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_423.dc.add.10_0, e2e_matmul_426_0]
    -   execute: {graph_name: fwd_24, queue_settings: {
               e2e_layernorm_409.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_gelu_415_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_409.dc.add.10_0, e2e_gelu_415_0]
    -   varinst: [$gptr_q24, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_462.dc.multiply.9_0, e2e_layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_25, queue_settings: {
               e2e_layernorm_423.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_matmul_426_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_attention_mask_s_brcst_m2_15_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_423.dc.add.10_0, e2e_matmul_426_0, e2e_attention_mask_s_brcst_m2_15_1.lc1_0]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_462.dc.add.10_0, e2e_gelu_468_0]
    -   execute: {graph_name: fwd_26, queue_settings: {
               e2e_layernorm_462.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_462.dc.multiply.9_0, e2e_layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q26, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_476.dc.add.10_0, e2e_matmul_479_0]
    -   execute: {graph_name: fwd_27, queue_settings: {
               e2e_layernorm_462.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_462.dc.add.10_0, e2e_gelu_468_0]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_515.dc.multiply.9_0, e2e_layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_28, queue_settings: {
               e2e_layernorm_476.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_matmul_479_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_attention_mask_s_brcst_m2_14_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_476.dc.add.10_0, e2e_matmul_479_0, e2e_attention_mask_s_brcst_m2_14_1.lc1_0]
    -   varinst: [$gptr_q28, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_515.dc.add.10_0, e2e_gelu_521_0]
    -   execute: {graph_name: fwd_29, queue_settings: {
               e2e_layernorm_515.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_515.dc.multiply.9_0, e2e_layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_529.dc.add.10_0, e2e_matmul_532_0]
    -   execute: {graph_name: fwd_30, queue_settings: {
               e2e_layernorm_515.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_gelu_521_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_515.dc.add.10_0, e2e_gelu_521_0]
    -   varinst: [$gptr_q30, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_568.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_31, queue_settings: {
               e2e_layernorm_529.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_matmul_532_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_attention_mask_s_brcst_m2_13_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_529.dc.add.10_0, e2e_matmul_532_0, e2e_attention_mask_s_brcst_m2_13_1.lc1_0]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_568.dc.add.10_0, e2e_gelu_574_0]
    -   execute: {graph_name: fwd_32, queue_settings: {
               e2e_layernorm_568.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_568.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q32, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_582.dc.add.10_0, e2e_matmul_585_0]
    -   execute: {graph_name: fwd_33, queue_settings: {
               e2e_layernorm_568.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_gelu_574_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_568.dc.add.10_0, e2e_gelu_574_0]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_621.dc.subtract.1_0, e2e_layernorm_621.dc.reciprocal.7_0, e2e_attention_mask_s_brcst_m2_11_1.lc1_0, e2e_attention_mask_s_brcst_m2_10_1.lc1_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   execute: {graph_name: fwd_34, queue_settings: {
               e2e_layernorm_582.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_matmul_585_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_attention_mask_s_brcst_m2_12_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_582.dc.add.10_0, e2e_matmul_585_0, e2e_attention_mask_s_brcst_m2_12_1.lc1_0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0]
    -   varinst: [$gptr_q34, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_621.dc.add.10_0, e2e_gelu_627_0]
    -   execute: {graph_name: fwd_35, queue_settings: {
               e2e_layernorm_621.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_layernorm_621.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_621.dc.subtract.1_0, e2e_layernorm_621.dc.reciprocal.7_0]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_635.dc.add.10_0, e2e_matmul_638_0]
    -   execute: {graph_name: fwd_36, queue_settings: {
               e2e_layernorm_621.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e_gelu_627_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_621.dc.add.10_0, e2e_gelu_627_0]
    -   varinst: [$gptr_q36, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_674.dc.multiply.9_0, e2e_layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_37, queue_settings: {
               e2e_layernorm_635.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               e2e_matmul_638_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               e2e_attention_mask_s_brcst_m2_11_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_652_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_635.dc.add.10_0, e2e_matmul_638_0, e2e_attention_mask_s_brcst_m2_11_1.lc1_0]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_674.dc.add.10_0, e2e_gelu_680_0]
    -   execute: {graph_name: fwd_38, queue_settings: {
               e2e_layernorm_674.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_674.dc.multiply.9_0, e2e_layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q38, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_688.dc.add.10_0, e2e_matmul_691_0]
    -   execute: {graph_name: fwd_39, queue_settings: {
               e2e_layernorm_674.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               e2e_gelu_680_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_674.dc.add.10_0, e2e_gelu_680_0]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_727.dc.multiply.9_0, e2e_layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_40, queue_settings: {
               e2e_layernorm_688.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e_matmul_691_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e_attention_mask_s_brcst_m2_10_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_688.dc.add.10_0, e2e_matmul_691_0, e2e_attention_mask_s_brcst_m2_10_1.lc1_0]
    -   varinst: [$gptr_q40, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_727.dc.add.10_0, e2e_gelu_733_0]
    -   execute: {graph_name: fwd_41, queue_settings: {
               e2e_layernorm_727.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               e2e_layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_727.dc.multiply.9_0, e2e_layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_741.dc.add.10_0, e2e_matmul_744_0]
    -   execute: {graph_name: fwd_42, queue_settings: {
               e2e_layernorm_727.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e_gelu_733_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_727.dc.add.10_0, e2e_gelu_733_0]
    -   varinst: [$gptr_q42, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_780.dc.multiply.9_0, e2e_layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_43, queue_settings: {
               e2e_layernorm_741.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_matmul_744_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_attention_mask_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_758_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_741.dc.add.10_0, e2e_matmul_744_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_780.dc.add.10_0, e2e_gelu_786_0]
    -   execute: {graph_name: fwd_44, queue_settings: {
               e2e_layernorm_780.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e_layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_780.dc.multiply.9_0, e2e_layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q44, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_794.dc.add.10_0, e2e_matmul_797_0]
    -   execute: {graph_name: fwd_45, queue_settings: {
               e2e_layernorm_780.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               e2e_gelu_786_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_780.dc.add.10_0, e2e_gelu_786_0]
    -   varinst: [$gptr_q45, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_833.dc.multiply.9_0, e2e_layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_46, queue_settings: {
               e2e_layernorm_794.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_matmul_797_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_attention_mask_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_794.dc.add.10_0, e2e_matmul_797_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_833.dc.add.10_0, e2e_gelu_839_0]
    -   execute: {graph_name: fwd_47, queue_settings: {
               e2e_layernorm_833.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               e2e_layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_833.dc.multiply.9_0, e2e_layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_847.dc.add.10_0, e2e_matmul_850_0]
    -   execute: {graph_name: fwd_48, queue_settings: {
               e2e_layernorm_833.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               e2e_gelu_839_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_833.dc.add.10_0, e2e_gelu_839_0]
    -   varinst: [$gptr_q48, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_886.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_49, queue_settings: {
               e2e_layernorm_847.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_matmul_850_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_864_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_847.dc.add.10_0, e2e_matmul_850_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_886.dc.add.10_0, e2e_gelu_892_0]
    -   execute: {graph_name: fwd_50, queue_settings: {
               e2e_layernorm_886.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_886.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q50, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_900.dc.add.10_0, e2e_matmul_903_0]
    -   execute: {graph_name: fwd_51, queue_settings: {
               e2e_layernorm_886.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               e2e_gelu_892_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_886.dc.add.10_0, e2e_gelu_892_0]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_939.dc.multiply.9_0, e2e_layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_52, queue_settings: {
               e2e_layernorm_900.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               e2e_matmul_903_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_900.dc.add.10_0, e2e_matmul_903_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0]
    -   varinst: [$gptr_q52, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q52, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_939.dc.add.10_0, e2e_gelu_945_0]
    -   execute: {graph_name: fwd_53, queue_settings: {
               e2e_layernorm_939.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               e2e_layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_939.dc.multiply.9_0, e2e_layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q53, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q53, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_953.dc.add.10_0, e2e_matmul_956_0]
    -   execute: {graph_name: fwd_54, queue_settings: {
               e2e_layernorm_939.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               e2e_gelu_945_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_939.dc.add.10_0, e2e_gelu_945_0]
    -   varinst: [$gptr_q54, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q54, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_992.dc.multiply.9_0, e2e_layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_55, queue_settings: {
               e2e_layernorm_953.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e_matmul_956_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_970_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_953.dc.add.10_0, e2e_matmul_956_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0]
    -   varinst: [$gptr_q55, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q55, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_992.dc.add.10_0, e2e_gelu_998_0]
    -   execute: {graph_name: fwd_56, queue_settings: {
               e2e_layernorm_992.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               e2e_layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_992.dc.multiply.9_0, e2e_layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q56, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q56, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1006.dc.add.10_0, e2e_matmul_1009_0]
    -   execute: {graph_name: fwd_57, queue_settings: {
               e2e_layernorm_992.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               e2e_gelu_998_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_992.dc.add.10_0, e2e_gelu_998_0]
    -   varinst: [$gptr_q57, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q57, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1045.dc.multiply.9_0, e2e_layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_58, queue_settings: {
               e2e_layernorm_1006.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               e2e_matmul_1009_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1006.dc.add.10_0, e2e_matmul_1009_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   varinst: [$gptr_q58, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q58, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1045.dc.add.10_0, e2e_gelu_1051_0]
    -   execute: {graph_name: fwd_59, queue_settings: {
               e2e_layernorm_1045.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               e2e_layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1045.dc.multiply.9_0, e2e_layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q59, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q59, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1059.dc.add.10_0, e2e_matmul_1062_0]
    -   execute: {graph_name: fwd_60, queue_settings: {
               e2e_layernorm_1045.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               e2e_gelu_1051_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1045.dc.add.10_0, e2e_gelu_1051_0]
    -   varinst: [$gptr_q60, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q60, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1098.dc.multiply.9_0, e2e_layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_61, queue_settings: {
               e2e_layernorm_1059.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               e2e_matmul_1062_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1076_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1059.dc.add.10_0, e2e_matmul_1062_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0]
    -   varinst: [$gptr_q61, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q61, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1098.dc.add.10_0, e2e_gelu_1104_0]
    -   execute: {graph_name: fwd_62, queue_settings: {
               e2e_layernorm_1098.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               e2e_layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1098.dc.multiply.9_0, e2e_layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q62, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q62, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1112.dc.add.10_0, e2e_matmul_1115_0]
    -   execute: {graph_name: fwd_63, queue_settings: {
               e2e_layernorm_1098.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               e2e_gelu_1104_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1098.dc.add.10_0, e2e_gelu_1104_0]
    -   varinst: [$gptr_q63, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q63, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1151.dc.multiply.9_0, e2e_layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_64, queue_settings: {
               e2e_layernorm_1112.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               e2e_matmul_1115_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1112.dc.add.10_0, e2e_matmul_1115_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0]
    -   varinst: [$gptr_q64, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q64, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1151.dc.add.10_0, e2e_gelu_1157_0]
    -   execute: {graph_name: fwd_65, queue_settings: {
               e2e_layernorm_1151.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65},
               e2e_layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1151.dc.multiply.9_0, e2e_layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q65, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q65, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1165.dc.add.10_0, e2e_matmul_1168_0]
    -   execute: {graph_name: fwd_66, queue_settings: {
               e2e_layernorm_1151.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               e2e_gelu_1157_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1151.dc.add.10_0, e2e_gelu_1157_0]
    -   varinst: [$gptr_q66, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q66, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1204.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_67, queue_settings: {
               e2e_layernorm_1165.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               e2e_matmul_1168_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1182_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1165.dc.add.10_0, e2e_matmul_1168_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0]
    -   varinst: [$gptr_q67, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q67, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1204.dc.add.10_0, e2e_gelu_1210_0]
    -   execute: {graph_name: fwd_68, queue_settings: {
               e2e_layernorm_1204.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1204.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q68, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q68, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1218.dc.add.10_0, e2e_matmul_1221_0]
    -   execute: {graph_name: fwd_69, queue_settings: {
               e2e_layernorm_1204.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               e2e_gelu_1210_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1204.dc.add.10_0, e2e_gelu_1210_0]
    -   varinst: [$gptr_q69, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q69, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1257.dc.multiply.9_0, e2e_layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_70, queue_settings: {
               e2e_layernorm_1218.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               e2e_matmul_1221_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1218.dc.add.10_0, e2e_matmul_1221_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q70, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q70, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1257.dc.add.10_0, e2e_gelu_1263_0]
    -   execute: {graph_name: fwd_71, queue_settings: {
               e2e_layernorm_1257.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               e2e_layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1257.dc.multiply.9_0, e2e_layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q71, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q71, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_72, queue_settings: {
               e2e_layernorm_1257.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72},
               e2e_gelu_1263_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1257.dc.add.10_0, e2e_gelu_1263_0]
    -   varinst: [$gptr_q72, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q72, incwrap, $c_microbatch_size, 256]
    - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_1271]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.05
