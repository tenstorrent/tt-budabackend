# git checkout ef840c36
# pytest pybuda/test/benchmark/benchmark.py -m bert -c base -opt 3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_FUSE_REDUCE=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1 PYBUDA_DISABLE_DRAM0=1 PYBUDA_FUSED_OP_MULTIPLIER=2 --loop_count 100 --auto_transpose

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                   {input: HOST, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x73420e0], [3, 0x5f18a60], [4, 0x5ef3bc0], [5, 0x58f0480]]}

  # output
  bert_encoders.output_layernorm_635:               {input: _fused_op_47_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x52db7a0], [1, 0x6ccba40]]}
  layer.0.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f39be0], [5, 0x59364a0]]}
  layer.0.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7388100], [3, 0x5f5ea80]]}
  layer.0.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x58bdf80], [1, 0x72e55e0]]}
  layer.0.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58704e0], [7, 0x586f360]]}
  layer.0.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x581ab00], [7, 0x5819980]]}
  layer.0.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ec69a0], [4, 0x5ea1b00]]}
  layer.0.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x72de820], [2, 0x733b7a0]]}
  layer.0.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ec0080]]}
  layer.0.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7334e80]]}
  layer.0.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7240fe0], [2, 0x72e6260], [3, 0x5e71460], [4, 0x5e52600], [5, 0x589a640], [6, 0x581dfa0], [7, 0x581ce20], [1, 0x728fc00]]}
  layer.0.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x58e9b40], [6, 0x586d040], [7, 0x586bec0], [1, 0x72e1cc0], [2, 0x733ec40], [3, 0x5f155c0], [4, 0x5ef0720], [5, 0x58ecfe0]]}
  layer.0.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5993060], [6, 0x58cd520], [7, 0x5917fa0], [1, 0x738ad80], [2, 0x74308c0], [3, 0x6000da0], [4, 0x5fdc7c0], [5, 0x59e1c80]]}
  layer.0.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5fdad40], [5, 0x5992320], [6, 0x58cc7e0], [7, 0x5917260], [1, 0x738a040], [2, 0x742fb80], [3, 0x6000060], [4, 0x5fdba80]]}
  layer.0.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58c5ec0]]}
  layer.0.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x598ba00]]}
  layer.1.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5fb0fc0], [4, 0x5f8c120]]}
  layer.1.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x58c1420], [1, 0x72e8a80]]}
  layer.1.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x58c7d60], [1, 0x733ab40]]}
  layer.1.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5988560], [6, 0x58c2a20]]}
  layer.1.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x72ebf20], [2, 0x73da1c0]]}
  layer.1.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58bf580], [7, 0x58c48c0]]}
  layer.1.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f3d080], [5, 0x5939940]]}
  layer.1.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x73d6d20], [3, 0x5fad6a0]]}
  layer.1.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x576e5c0]]}
  layer.1.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5750d40]]}
  layer.1.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5cba540], [5, 0x5702120], [6, 0x571f9a0], [7, 0x56d30a0], [1, 0x7100740], [2, 0x71f0ce0], [3, 0x5d27fc0], [4, 0x5d09160]]}
  layer.1.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d21680], [4, 0x5cb70a0], [5, 0x56fec80], [6, 0x571c500], [7, 0x56cfc00], [1, 0x70fd2a0], [2, 0x71ed840], [3, 0x5d24b20]]}
  layer.1.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7150000], [3, 0x5cd2a60], [4, 0x5c68480], [5, 0x56b0060], [6, 0x56cd8e0], [7, 0x5680fe0], [1, 0x70ae680], [2, 0x719ec20]]}
  layer.1.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x70acc00], [2, 0x714f2c0], [3, 0x5cd1d20], [4, 0x5c67740], [5, 0x56af320], [6, 0x56ccba0], [7, 0x56802a0], [1, 0x70ad940]]}
  layer.1.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ccb400]]}
  layer.1.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x71489a0]]}
  layer.2.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5631200], [1, 0x705dfe0]]}
  layer.2.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56ab5a0], [6, 0x56c9280]]}
  layer.2.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5c7c7e0], [4, 0x5c18240]]}
  layer.2.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x705ab40], [2, 0x7145500]]}
  layer.2.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e039e0], [5, 0x584ba20]]}
  layer.2.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x72e2dc0], [3, 0x5e6dfc0]]}
  layer.2.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57cad60], [1, 0x71f23c0]]}
  layer.2.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5848580], [6, 0x5817660]]}
  layer.2.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57c4440]]}
  layer.2.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d76be0]]}
  layer.2.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57aad40], [6, 0x57c85c0], [7, 0x5775820], [1, 0x71a2ec0], [2, 0x72938c0], [3, 0x5e1ef20], [4, 0x5db4940], [5, 0x57f9960]]}
  layer.2.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5dae000], [5, 0x57a78a0], [6, 0x57c5120], [7, 0x5772380], [1, 0x719fa20], [2, 0x7290420], [3, 0x5e1ba80], [4, 0x5db14a0]]}
  layer.2.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d7e240], [4, 0x5d5f3e0], [5, 0x5758c80], [6, 0x5776500], [7, 0x5723760], [1, 0x7150e00], [2, 0x7241800], [3, 0x5dcce60]]}
  layer.2.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x723fd80], [3, 0x5d7d500], [4, 0x5d5e6a0], [5, 0x5757f40], [6, 0x57757c0], [7, 0x5722a20], [1, 0x71500c0], [2, 0x7240ac0]]}
  layer.2.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d57d80]]}
  layer.2.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cc17e0]]}
  layer.3.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x75c1ca0], [2, 0x7616020]]}
  layer.3.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x624ca20], [4, 0x635b560]]}
  layer.3.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x76b57a0], [2, 0x7757e60]]}
  layer.3.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5bac7c0], [7, 0x5b66380]]}
  layer.3.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7709240], [3, 0x61fde00]]}
  layer.3.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b62ee0], [1, 0x76b2300]]}
  layer.3.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7610d40], [2, 0x76680e0]]}
  layer.3.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6357c40], [5, 0x5cbdec0]]}
  layer.3.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5ba5a20]]}
  layer.3.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cb75a0]]}
  layer.3.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62ba400], [5, 0x5c68980], [6, 0x5b56e00], [7, 0x5b139e0], [1, 0x7662e00], [2, 0x76ba1a0], [3, 0x61aed60], [4, 0x6309020]]}
  layer.3.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x61a8420], [4, 0x62b6f60], [5, 0x5c654e0], [6, 0x5b53960], [7, 0x5b10540], [1, 0x765f960], [2, 0x76b6d00], [3, 0x61ab8c0]]}
  layer.3.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6400420], [5, 0x5cd60a0], [6, 0x5c0a0c0], [7, 0x5c08b00], [1, 0x775af40], [2, 0x77aea00], [3, 0x62eed40], [4, 0x644f040]]}
  layer.3.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5bafc60], [7, 0x5b69820], [1, 0x77043c0], [2, 0x77a6a80], [3, 0x624fec0], [4, 0x635ea00], [5, 0x5cc8100], [6, 0x5bb09a0]]}
  layer.3.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c037a0]]}
  layer.3.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ccf780]]}
  layer.4.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x629fca0], [4, 0x63b1800]]}
  layer.4.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x77571c0], [2, 0x77ab0e0]]}
  layer.4.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5bb4b80], [7, 0x5bb9600]]}
  layer.4.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x63ae360], [5, 0x5ccc2e0]]}
  layer.4.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b6a9e0], [1, 0x77085a0]]}
  layer.4.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cc8e40], [6, 0x5bb16e0]]}
  layer.4.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6250c00], [4, 0x635f740]]}
  layer.4.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7705100], [2, 0x77a77c0]]}
  layer.4.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7519080]]}
  layer.4.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a5ab20]]}
  layer.4.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5973140], [7, 0x5a0bf00], [1, 0x74ca460], [2, 0x7524820], [3, 0x60ad2c0], [4, 0x6122340], [5, 0x5ad6040], [6, 0x59c1d60]]}
  layer.4.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5acf700], [6, 0x596fca0], [7, 0x5a08a60], [1, 0x74c6fc0], [2, 0x7521380], [3, 0x60a9e20], [4, 0x611eea0], [5, 0x5ad2ba0]]}
  layer.4.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6081660], [5, 0x5a80ae0], [6, 0x5921080], [7, 0x59b9e40], [1, 0x74783a0], [2, 0x74d2760], [3, 0x605b200], [4, 0x60d0280]]}
  layer.4.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6059780], [4, 0x6080920], [5, 0x5a7fda0], [6, 0x5920340], [7, 0x59b9100], [1, 0x7477660], [2, 0x74d1a20], [3, 0x605a4c0]]}
  layer.4.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x607a000]]}
  layer.4.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6052e60]]}
  layer.5.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x73d9e20], [2, 0x7482980]]}
  layer.5.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x591c5c0], [7, 0x5967040]]}
  layer.5.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x602b3e0], [5, 0x5a308a0]]}
  layer.5.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x747f4e0], [3, 0x604f9c0]]}
  layer.5.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x596a4e0], [1, 0x7428a40]]}
  layer.5.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b504c0], [7, 0x5b0d0a0]]}
  layer.5.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6268340], [5, 0x5c168c0]]}
  layer.5.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7664c40], [3, 0x61a4f80]]}
  layer.5.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6261a20]]}
  layer.5.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x619e660]]}
  layer.5.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6170f60], [5, 0x5b24c60], [6, 0x5a10980], [7, 0x5a61440], [1, 0x751f9a0], [2, 0x7573d20], [3, 0x60fc7c0], [4, 0x61bfb80]]}
  layer.5.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b062e0], [1, 0x75be800], [2, 0x7612b80], [3, 0x619b1c0], [4, 0x625e580], [5, 0x5c12b40], [6, 0x5b4c740], [7, 0x5b09780]]}
  layer.5.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5aaef00], [7, 0x5ab76c0], [1, 0x756fbe0], [2, 0x75c3f60], [3, 0x614c5a0], [4, 0x620f960], [5, 0x5bc3f20], [6, 0x5afdb20]]}
  layer.5.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5bc24a0], [6, 0x5aae1c0], [7, 0x5ab6980], [1, 0x756eea0], [2, 0x75c3220], [3, 0x614b860], [4, 0x620ec20], [5, 0x5bc31e0]]}
  layer.5.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5ab0060]]}
  layer.5.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7428de0]]}
  layer.6.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b73880], [6, 0x5a5f5a0]]}
  layer.6.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6b33d60], [2, 0x6b39940]]}
  layer.6.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x51f9e40], [6, 0x524e1c0]]}
  layer.6.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x58e0d80], [4, 0x591a440]]}
  layer.6.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x51ff5a0], [7, 0x528cb80]]}
  layer.6.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5916fa0], [5, 0x51f69a0]]}
  layer.6.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c8a8c0], [3, 0x5892160]]}
  layer.6.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5871800], [5, 0x5151200]]}
  layer.6.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x588ab00]]}
  layer.6.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c83260]]}
  layer.6.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6c2c300], [2, 0x6c34640], [3, 0x583bee0], [4, 0x58c6d60], [5, 0x51a6760], [6, 0x51af7c0], [7, 0x523cda0], [1, 0x6c7af20]]}
  layer.6.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5236460], [1, 0x6c28e60], [2, 0x6c311a0], [3, 0x5838a40], [4, 0x58c38c0], [5, 0x51a32c0], [6, 0x51ac320], [7, 0x5239900]]}
  layer.6.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x510eae0], [7, 0x51e7840], [1, 0x6bda240], [2, 0x6be2580], [3, 0x57e9e20], [4, 0x5874ca0], [5, 0x51546a0], [6, 0x515d700]]}
  layer.6.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6cc9b40], [2, 0x6c89b80], [3, 0x5891420], [4, 0x5916260], [5, 0x51f5c60], [6, 0x51fe860], [7, 0x528be40], [1, 0x6cca880]]}
  layer.6.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x596fe20]]}
  layer.6.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x58eb440]]}
  layer.7.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6d69700], [2, 0x6d2c300]]}
  layer.7.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x52eeea0], [7, 0x532dce0]]}
  layer.7.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5921200], [5, 0x5297b00]]}
  layer.7.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6cd9960], [3, 0x58e4220]]}
  layer.7.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5248ee0], [6, 0x52a0280]]}
  layer.7.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x58e76c0], [4, 0x591dd60]]}
  layer.7.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6d1a660], [2, 0x6cdce00]]}
  layer.7.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x529cde0], [7, 0x532a3c0]]}
  layer.7.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  layer.7.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5056520]]}
  layer.7.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x56e2c20], [5, 0x5007900], [6, 0x500d940], [7, 0x5052c20], [1, 0x6a96520], [2, 0x6a9c100], [3, 0x56ec100], [4, 0x5731840]]}
  layer.7.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x56e57c0], [4, 0x56df780], [5, 0x5004460], [6, 0x500a4a0], [7, 0x504f780], [1, 0x6a93080], [2, 0x6a98c60], [3, 0x56e8c60]]}
  layer.7.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x69fb420], [3, 0x5696ba0], [4, 0x5690b60], [5, 0x4fb5840], [6, 0x4fbb880], [7, 0x5000b60], [1, 0x6a44460], [2, 0x6a4a040]]}
  layer.7.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6a429e0], [2, 0x69fa6e0], [3, 0x5695e60], [4, 0x568fe20], [5, 0x4fb4b00], [6, 0x4fbab40], [7, 0x4fffe20], [1, 0x6a43720]]}
  layer.7.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x568f540]]}
  layer.7.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x69f3dc0]]}
  layer.8.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0d80], [1, 0x69f3dc0]]}
  layer.8.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0d80], [6, 0x4fb7220]]}
  layer.8.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5640920], [4, 0x5640920]]}
  layer.8.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69f0920], [2, 0x69f0920]]}
  layer.8.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5822760], [5, 0x50ff140]]}
  layer.8.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6bdf0e0], [3, 0x57e6980]]}
  layer.8.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5198c20], [1, 0x6b8b620]]}
  layer.8.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x514dd60], [6, 0x510b640]]}
  layer.8.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5192300]]}
  layer.8.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5104d20]]}
  layer.8.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50a1840], [1, 0x6ae5140], [2, 0x6aead20], [3, 0x573ad20], [4, 0x5780460], [5, 0x505ce40], [6, 0x5062e80], [7, 0x50f0460]]}
  layer.8.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x57dfbc0], [4, 0x581f2c0], [5, 0x50fbca0], [6, 0x5101880], [7, 0x518ee60], [1, 0x6b878a0], [2, 0x6bdb360], [3, 0x57e3060]]}
  layer.8.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b3db20], [3, 0x5790fa0], [4, 0x57d06a0], [5, 0x50ad080], [6, 0x50b2c60], [7, 0x5140240], [1, 0x6b38c80], [2, 0x6b8c740]]}
  layer.8.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6b37200], [2, 0x6b3cde0], [3, 0x5790260], [4, 0x57cf960], [5, 0x50ac340], [6, 0x50b1f20], [7, 0x513f500], [1, 0x6b37f40]]}
  layer.8.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5789940]]}
  layer.8.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x505c560]]}
  layer.9.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x562b5c0], [7, 0x55db840]]}
  layer.9.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5c0db60], [5, 0x5609fc0]]}
  layer.9.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7058340], [3, 0x5bd7d80]]}
  layer.9.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55d1a60], [1, 0x6ffe840]]}
  layer.9.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5b89160], [4, 0x5bbef40]]}
  layer.9.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ffb3a0], [2, 0x7054ea0]]}
  layer.9.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x558a460], [7, 0x5582e40]]}
  layer.9.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f55c00], [2, 0x6faf700]]}
  layer.9.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x557b7e0]]}
  layer.9.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5582e00]]}
  layer.9.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x556a880], [6, 0x55341e0], [7, 0x552cbc0], [1, 0x6fab160], [2, 0x7004c60], [3, 0x5b39380], [4, 0x5b6f160], [5, 0x55b94a0]]}
  layer.9.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b68820], [5, 0x55673e0], [6, 0x5530d40], [7, 0x5529720], [1, 0x6fa7cc0], [2, 0x70017c0], [3, 0x5b35ee0], [4, 0x5b6bcc0]]}
  layer.9.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5a986a0], [4, 0x5b19c00], [5, 0x55187c0], [6, 0x54e2120], [7, 0x54dab00], [1, 0x6f590a0], [2, 0x6fb2ba0], [3, 0x5ae72c0]]}
  layer.9.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56080c0], [6, 0x5589720], [7, 0x5582100], [1, 0x6ffa660], [2, 0x7054160], [3, 0x5b88420], [4, 0x5bbe200], [5, 0x5608e00]]}
  layer.9.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7054220]]}
  layer.9.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x562a460]]}
  layer.10.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x565c500], [6, 0x567a1e0]]}
  layer.10.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5c78a60], [4, 0x5c14920]]}
  layer.10.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7005600], [2, 0x70f6000]]}
  layer.10.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x55d9500], [7, 0x55d4f00]]}
  layer.10.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x70a73e0], [3, 0x5c29e40]]}
  layer.10.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55d83a0], [1, 0x7002160]]}
  layer.10.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x560d460], [6, 0x55dc9a0]]}
  layer.10.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5c269a0], [4, 0x5c11000]]}
  layer.10.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a1bf00]]}
  layer.10.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x59e6120]]}
  layer.10.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e6be40], [3, 0x5997500], [4, 0x59cd2e0], [5, 0x5389320], [6, 0x5398820], [7, 0x53dcde0], [1, 0x6e63f80], [2, 0x6ebaa60]]}
  layer.10.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6e5d640], [2, 0x6e689a0], [3, 0x5994060], [4, 0x59c9e40], [5, 0x5385e80], [6, 0x5395380], [7, 0x53d9940], [1, 0x6e60ae0]]}
  layer.10.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x533c100], [1, 0x6e0ea20], [2, 0x6e19d80], [3, 0x5945440], [4, 0x597b220], [5, 0x5337260], [6, 0x5346760], [7, 0x538ad20]]}
  layer.10.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5344ce0], [7, 0x533b3c0], [1, 0x6e0dce0], [2, 0x6e19040], [3, 0x5944700], [4, 0x597a4e0], [5, 0x5336520], [6, 0x5345a20]]}
  layer.10.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6e073c0]]}
  layer.10.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5334aa0]]}
  layer.11.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x52e7480], [6, 0x52f60c0]]}
  layer.11.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x58f2640], [4, 0x5976bc0]]}
  layer.11.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6db87a0], [2, 0x6d7b800]]}
  layer.11.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x52f2c20], [7, 0x5331600]]}
  layer.11.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6dca420], [3, 0x58f5ae0]]}
  layer.11.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x54dec80], [7, 0x54d7660]]}
  layer.11.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5acafe0], [5, 0x54c9ba0]]}
  layer.11.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6fac260], [3, 0x5a95200]]}
  layer.11.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ac46c0]]}
  layer.11.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5a8e8e0]]}
  layer.11.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x53d7f40], [6, 0x53e7440], [7, 0x542ba00], [1, 0x6eb2ba0], [2, 0x6f09680], [3, 0x59eca40], [4, 0x5a22820], [5, 0x5426b60]]}
  layer.11.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x54d08a0], [1, 0x6f51a00], [2, 0x6fa84e0], [3, 0x5a8b440], [4, 0x5ac1220], [5, 0x54c5e20], [6, 0x54daf00], [7, 0x54d3d40]]}
  layer.11.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x543d6c0], [7, 0x5481c80], [1, 0x6f02de0], [2, 0x6f598c0], [3, 0x5a3c820], [4, 0x5a72600], [5, 0x5477200], [6, 0x548c2e0]]}
  layer.11.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5475780], [6, 0x543c980], [7, 0x5480f40], [1, 0x6f020a0], [2, 0x6f58b80], [3, 0x5a3bae0], [4, 0x5a718c0], [5, 0x54764c0]]}
  layer.11.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x547a620]]}
  layer.11.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5436060]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58bf100]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x72e5160]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x586ba40]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x586cbc0]]}
  dc.input_tensor.layernorm_38.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ea1220], [5, 0x58e9260]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5fffbe0]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x742f700]]}
  dc.input_tensor.layernorm_52.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5916980], [1, 0x7389760]]}
  input_1_multiply_69_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f8bca0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5fb0b40]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58171e0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x723f900]]}
  dc.input_tensor.layernorm_91.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5721cc0], [1, 0x714f360]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x56c8e00]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x56cc720]]}
  dc.input_tensor.layernorm_105.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5c66e60], [5, 0x56aea40]]}
  input_1_multiply_122_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5630d80]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x567fe20]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e03560]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5e6db40]]}
  dc.input_tensor.layernorm_144.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x71f1ae0], [2, 0x72e24e0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x714fc40]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57225a0]]}
  dc.input_tensor.layernorm_158.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5757660], [6, 0x5774ee0]]}
  input_1_multiply_175_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cc1360]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x635b0e0]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x61fd980]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7708dc0]]}
  dc.input_tensor.layernorm_197.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b62600], [1, 0x76b1a20]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x62ee8c0]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x77ae580]]}
  dc.input_tensor.layernorm_211.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c08220], [1, 0x775a660]]}
  input_1_multiply_228_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x629f820]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x77aac60]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5bac340]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b6a560]]}
  dc.input_tensor.layernorm_250.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7573440], [3, 0x60fbee0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74d15a0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x591c140]]}
  dc.input_tensor.layernorm_264.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a7f4c0], [6, 0x591fa60]]}
  input_1_multiply_281_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x73d99a0]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5966bc0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x76108c0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b0cc20]]}
  dc.input_tensor.layernorm_303.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c15fe0], [6, 0x5b4fbe0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x620e7a0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x614b3e0]]}
  dc.input_tensor.layernorm_317.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x756e5c0], [2, 0x75c2940]]}
  input_1_multiply_334_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6cd94e0]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ccb5c0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x528b9c0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x51fe3e0]]}
  dc.input_tensor.layernorm_356.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5915980], [5, 0x51f5380]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6db8320]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5331180]]}
  dc.input_tensor.layernorm_370.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x52e6720], [6, 0x52f2340]]}
  input_1_multiply_387_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6d69280]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x532d860]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5248a60]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x591d8e0]]}
  dc.input_tensor.layernorm_409.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d2ba20], [3, 0x58eab60]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fff9a0]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fba6c0]]}
  dc.input_tensor.layernorm_423.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x568f540], [5, 0x4fb4220]]}
  input_1_multiply_440_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5871380]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x57e6500]]}
  dc.input_tensor.layernorm_462.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6b8ad40], [2, 0x6bde800]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x513f080]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x50b1aa0]]}
  dc.input_tensor.layernorm_476.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x57cf080], [5, 0x50aba60]]}
  input_1_multiply_493_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x55d9080]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5609b40]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5bbdd80]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5b87fa0]]}
  dc.input_tensor.layernorm_515.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ff9d80], [2, 0x7053880]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56ab120]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5c17dc0]]}
  dc.input_tensor.layernorm_529.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7144c20], [3, 0x5c7bf00]]}
  input_1_multiply_546_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x565c080]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5c144a0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x70a6f60]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7001ce0]]}
  dc.input_tensor.layernorm_568.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f54ea0], [2, 0x6fab980]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x53360a0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x597a060]]}
  dc.input_tensor.layernorm_582.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d7af20], [3, 0x58f1d60]]}
  input_1_multiply_599_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x52e7000]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5976740]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f55780]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x54d71e0]]}
  dc.input_tensor.layernorm_621.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x54c92c0], [6, 0x54de3a0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a71440]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5a3b660]]}
  dc.input_tensor.layernorm_635.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f017c0], [2, 0x6f582a0]]}

  # epoch_to_epoch
  e2e_gelu_150_0:                                   {input: gelu_150, type: queue, entries: 128, grid_size: [1, 2], t: 1, mblock: [2, 12], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900], [2, 0x4fb0900]]}
  e2e__fused_op_10_0:                               {input: _fused_op_10, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900], [4, 0x4fb0900]]}
  e2e__fused_op_22_0:                               {input: _fused_op_22, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5d24cc0], [6, 0x5c58ce0]]}
  e2e_matmul_453_0:                                 {input: matmul_453, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x77fd620]]}
  e2e__fused_op_31_0:                               {input: _fused_op_31, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c57720], [1, 0x77a9b60]]}
  e2e__fused_op_43_0:                               {input: _fused_op_43, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x633d960], [4, 0x649dc60]]}
  e2e_matmul_597_0:                                 {input: matmul_597, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x63b4ce0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_22: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 4], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0, _fused_op_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_1, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    _fused_op_2: {type: fused_op, grid_loc: [1, 3], grid_size: [2, 1], inputs: [matmul_33, hidden_states, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0, matmul_33, hidden_states, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0, dc.input_tensor.layernorm_38.4, matmul_33, hidden_states, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_41: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [_fused_op_2, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_44: {type: gelu, grid_loc: [1, 5], grid_size: [1, 2], inputs: [matmul_41],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_2__fused_op_3: {type: nop, grid_loc: [1, 7], grid_size: [1, 2], inputs: [_fused_op_2],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_3: {type: fused_op, grid_loc: [1, 9], grid_size: [2, 1], inputs: [matmul_47, buffer_0__fused_op_2__fused_op_3, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0, matmul_47, buffer_0__fused_op_2__fused_op_3, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0, dc.input_tensor.layernorm_52.4, matmul_47, buffer_0__fused_op_2__fused_op_3, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_55: {type: matmul, grid_loc: [2, 8], grid_size: [1, 2], inputs: [_fused_op_3, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_61: {type: matmul, grid_loc: [2, 10], grid_size: [1, 2], inputs: [_fused_op_3, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_67: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_75: {type: matmul, grid_loc: [4, 4], grid_size: [1, 2], inputs: [_fused_op_3, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_4: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_5: {type: fused_op, grid_loc: [3, 9], grid_size: [2, 1], inputs: [_fused_op_4, lc.input_tensor.softmax_71.dc.reduce_sum.1.0, _fused_op_4], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [_fused_op_5, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_3__fused_op_6: {type: nop, grid_loc: [4, 8], grid_size: [1, 2], inputs: [_fused_op_3],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_6: {type: fused_op, grid_loc: [4, 10], grid_size: [2, 1], inputs: [matmul_86, buffer_0__fused_op_3__fused_op_6, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0, matmul_86, buffer_0__fused_op_3__fused_op_6, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0, dc.input_tensor.layernorm_91.4, matmul_86, buffer_0__fused_op_3__fused_op_6, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_94: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_6, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_97: {type: gelu, grid_loc: [5, 8], grid_size: [1, 2], inputs: [matmul_94],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_6__fused_op_7: {type: nop, grid_loc: [5, 10], grid_size: [1, 2], inputs: [_fused_op_6],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_7: {type: fused_op, grid_loc: [6, 8], grid_size: [2, 1], inputs: [matmul_100, buffer_0__fused_op_6__fused_op_7, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0, matmul_100, buffer_0__fused_op_6__fused_op_7, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0, dc.input_tensor.layernorm_105.4, matmul_100, buffer_0__fused_op_6__fused_op_7, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_108: {type: matmul, grid_loc: [6, 10], grid_size: [1, 2], inputs: [_fused_op_7, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_114: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [_fused_op_7, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_120: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_128: {type: matmul, grid_loc: [7, 9], grid_size: [1, 2], inputs: [_fused_op_7, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [7, 3], grid_size: [1, 4], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_9: {type: fused_op, grid_loc: [7, 7], grid_size: [2, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_124.dc.reduce_sum.1.0, _fused_op_8], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [_fused_op_9, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_7__fused_op_10: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [_fused_op_7],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_10: {type: fused_op, grid_loc: [8, 4], grid_size: [2, 1], inputs: [matmul_139, buffer_0__fused_op_7__fused_op_10, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0, matmul_139, buffer_0__fused_op_7__fused_op_10, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0, dc.input_tensor.layernorm_144.4, matmul_139, buffer_0__fused_op_7__fused_op_10, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_147: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [_fused_op_10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_150: {type: gelu, grid_loc: [8, 6], grid_size: [1, 2], inputs: [matmul_147],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_1:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_10__fused_op_11: {type: nop, grid_loc: [0, 8], grid_size: [1, 2], inputs: [e2e__fused_op_10_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_11: {type: fused_op, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_153, buffer_0__fused_op_10__fused_op_11, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0, matmul_153, buffer_0__fused_op_10__fused_op_11, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0, dc.input_tensor.layernorm_158.4, matmul_153, buffer_0__fused_op_10__fused_op_11, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_161: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_11, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_167: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_11, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_173: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_181: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [_fused_op_11, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_12: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 4], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_13: {type: fused_op, grid_loc: [1, 9], grid_size: [2, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_177.dc.reduce_sum.1.0, _fused_op_12], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_13, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_11__fused_op_14: {type: nop, grid_loc: [2, 4], grid_size: [1, 2], inputs: [_fused_op_11],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_14: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_192, buffer_0__fused_op_11__fused_op_14, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0, matmul_192, buffer_0__fused_op_11__fused_op_14, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0, dc.input_tensor.layernorm_197.4, matmul_192, buffer_0__fused_op_11__fused_op_14, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_200: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [_fused_op_14, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_203: {type: gelu, grid_loc: [2, 8], grid_size: [1, 2], inputs: [matmul_200],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_14__fused_op_15: {type: nop, grid_loc: [2, 10], grid_size: [1, 2], inputs: [_fused_op_14],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_15: {type: fused_op, grid_loc: [3, 8], grid_size: [2, 1], inputs: [matmul_206, buffer_0__fused_op_14__fused_op_15, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0, matmul_206, buffer_0__fused_op_14__fused_op_15, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0, dc.input_tensor.layernorm_211.4, matmul_206, buffer_0__fused_op_14__fused_op_15, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_214: {type: matmul, grid_loc: [3, 9], grid_size: [1, 2], inputs: [_fused_op_15, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_220: {type: matmul, grid_loc: [4, 9], grid_size: [1, 2], inputs: [_fused_op_15, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_226: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_234: {type: matmul, grid_loc: [5, 4], grid_size: [1, 2], inputs: [_fused_op_15, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_16: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 4], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_17: {type: fused_op, grid_loc: [4, 11], grid_size: [2, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_230.dc.reduce_sum.1.0, _fused_op_16],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_17, matmul_234],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [5, 7], grid_size: [1, 2], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_15__fused_op_18: {type: nop, grid_loc: [5, 9], grid_size: [1, 2], inputs: [_fused_op_15],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_18: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_245, buffer_0__fused_op_15__fused_op_18, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0, matmul_245, buffer_0__fused_op_15__fused_op_18, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0, dc.input_tensor.layernorm_250.4, matmul_245, buffer_0__fused_op_15__fused_op_18, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_253: {type: matmul, grid_loc: [6, 2], grid_size: [1, 8], inputs: [_fused_op_18, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_256: {type: gelu, grid_loc: [6, 10], grid_size: [1, 2], inputs: [matmul_253],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_18__fused_op_19: {type: nop, grid_loc: [7, 8], grid_size: [1, 2], inputs: [_fused_op_18],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_19: {type: fused_op, grid_loc: [7, 10], grid_size: [2, 1], inputs: [matmul_259, buffer_0__fused_op_18__fused_op_19, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0, matmul_259, buffer_0__fused_op_18__fused_op_19, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0, dc.input_tensor.layernorm_264.4, matmul_259, buffer_0__fused_op_18__fused_op_19, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_267: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [_fused_op_19, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_273: {type: matmul, grid_loc: [8, 2], grid_size: [1, 2], inputs: [_fused_op_19, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_279: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_287: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [_fused_op_19, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_20: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 4], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_21: {type: fused_op, grid_loc: [8, 9], grid_size: [2, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_283.dc.reduce_sum.1.0, _fused_op_20], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [_fused_op_21, matmul_287],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [9, 2], grid_size: [1, 2], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_19__fused_op_22: {type: nop, grid_loc: [9, 4], grid_size: [1, 2], inputs: [_fused_op_19],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_22: {type: fused_op, grid_loc: [9, 6], grid_size: [2, 1], inputs: [matmul_298, buffer_0__fused_op_19__fused_op_22, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0, matmul_298, buffer_0__fused_op_19__fused_op_22, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0, dc.input_tensor.layernorm_303.4, matmul_298, buffer_0__fused_op_19__fused_op_22, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_2:
    target_device: 0
    input_count: 128
    matmul_306: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e__fused_op_22_0, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_309: {type: gelu, grid_loc: [0, 8], grid_size: [1, 2], inputs: [matmul_306],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_22__fused_op_23: {type: nop, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_22_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_23: {type: fused_op, grid_loc: [1, 8], grid_size: [2, 1], inputs: [matmul_312, buffer_0__fused_op_22__fused_op_23, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0, matmul_312, buffer_0__fused_op_22__fused_op_23, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0, dc.input_tensor.layernorm_317.4, matmul_312, buffer_0__fused_op_22__fused_op_23, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_320: {type: matmul, grid_loc: [1, 10], grid_size: [1, 2], inputs: [_fused_op_23, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_326: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [_fused_op_23, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_332: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_340: {type: matmul, grid_loc: [2, 9], grid_size: [1, 2], inputs: [_fused_op_23, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_24: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 4], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_25: {type: fused_op, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_336.dc.reduce_sum.1.0, _fused_op_24], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_25, matmul_340],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_23__fused_op_26: {type: nop, grid_loc: [3, 2], grid_size: [1, 2], inputs: [_fused_op_23],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_26: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 1], inputs: [matmul_351, buffer_0__fused_op_23__fused_op_26, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0, matmul_351, buffer_0__fused_op_23__fused_op_26, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0, dc.input_tensor.layernorm_356.4, matmul_351, buffer_0__fused_op_23__fused_op_26, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_359: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_26, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_362: {type: gelu, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_359],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_26__fused_op_27: {type: nop, grid_loc: [3, 8], grid_size: [1, 2], inputs: [_fused_op_26],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_27: {type: fused_op, grid_loc: [3, 10], grid_size: [2, 1], inputs: [matmul_365, buffer_0__fused_op_26__fused_op_27, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0, matmul_365, buffer_0__fused_op_26__fused_op_27, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0, dc.input_tensor.layernorm_370.4, matmul_365, buffer_0__fused_op_26__fused_op_27, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_373: {type: matmul, grid_loc: [4, 8], grid_size: [1, 2], inputs: [_fused_op_27, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_379: {type: matmul, grid_loc: [4, 10], grid_size: [1, 2], inputs: [_fused_op_27, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_385: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_393: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [_fused_op_27, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_28: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 4], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_29: {type: fused_op, grid_loc: [5, 9], grid_size: [2, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_389.dc.reduce_sum.1.0, _fused_op_28], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [_fused_op_29, matmul_393],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_404: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_27__fused_op_30: {type: nop, grid_loc: [6, 8], grid_size: [1, 2], inputs: [_fused_op_27],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_30: {type: fused_op, grid_loc: [6, 10], grid_size: [2, 1], inputs: [matmul_404, buffer_0__fused_op_27__fused_op_30, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0, matmul_404, buffer_0__fused_op_27__fused_op_30, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0, dc.input_tensor.layernorm_409.4, matmul_404, buffer_0__fused_op_27__fused_op_30, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_412: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [_fused_op_30, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_415: {type: gelu, grid_loc: [7, 8], grid_size: [1, 2], inputs: [matmul_412],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_30__fused_op_31: {type: nop, grid_loc: [7, 10], grid_size: [1, 2], inputs: [_fused_op_30],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_31: {type: fused_op, grid_loc: [8, 8], grid_size: [2, 1], inputs: [matmul_418, buffer_0__fused_op_30__fused_op_31, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0, matmul_418, buffer_0__fused_op_30__fused_op_31, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0, dc.input_tensor.layernorm_423.4, matmul_418, buffer_0__fused_op_30__fused_op_31, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_426: {type: matmul, grid_loc: [8, 10], grid_size: [1, 2], inputs: [_fused_op_31, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_432: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [_fused_op_31, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_438: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_446: {type: matmul, grid_loc: [9, 9], grid_size: [1, 2], inputs: [_fused_op_31, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_32: {type: fused_op, grid_loc: [9, 3], grid_size: [1, 4], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_33: {type: fused_op, grid_loc: [9, 7], grid_size: [2, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_442.dc.reduce_sum.1.0, _fused_op_32], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [_fused_op_33, matmul_446],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}

  fwd_3:
    target_device: 0
    input_count: 128
    matmul_457: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_453_0, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_31__fused_op_34: {type: nop, grid_loc: [0, 2], grid_size: [1, 2], inputs: [e2e__fused_op_31_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_34: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_457, buffer_0__fused_op_31__fused_op_34, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0, matmul_457, buffer_0__fused_op_31__fused_op_34, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0, dc.input_tensor.layernorm_462.4, matmul_457, buffer_0__fused_op_31__fused_op_34, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_465: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [_fused_op_34, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_468: {type: gelu, grid_loc: [0, 6], grid_size: [1, 2], inputs: [matmul_465],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_471: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_468, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_34__fused_op_35: {type: nop, grid_loc: [0, 8], grid_size: [1, 2], inputs: [_fused_op_34],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_35: {type: fused_op, grid_loc: [0, 10], grid_size: [2, 1], inputs: [matmul_471, buffer_0__fused_op_34__fused_op_35, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0, matmul_471, buffer_0__fused_op_34__fused_op_35, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0, dc.input_tensor.layernorm_476.4, matmul_471, buffer_0__fused_op_34__fused_op_35, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_479: {type: matmul, grid_loc: [1, 8], grid_size: [1, 2], inputs: [_fused_op_35, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_485: {type: matmul, grid_loc: [1, 10], grid_size: [1, 2], inputs: [_fused_op_35, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_491: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_499: {type: matmul, grid_loc: [3, 4], grid_size: [1, 2], inputs: [_fused_op_35, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_36: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 4], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_37: {type: fused_op, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_495.dc.reduce_sum.1.0, _fused_op_36], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_37, matmul_499],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_35__fused_op_38: {type: nop, grid_loc: [3, 8], grid_size: [1, 2], inputs: [_fused_op_35],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_38: {type: fused_op, grid_loc: [3, 10], grid_size: [2, 1], inputs: [matmul_510, buffer_0__fused_op_35__fused_op_38, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0, matmul_510, buffer_0__fused_op_35__fused_op_38, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0, dc.input_tensor.layernorm_515.4, matmul_510, buffer_0__fused_op_35__fused_op_38, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_518: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_38, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_521: {type: gelu, grid_loc: [4, 8], grid_size: [1, 2], inputs: [matmul_518],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_38__fused_op_39: {type: nop, grid_loc: [4, 10], grid_size: [1, 2], inputs: [_fused_op_38],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_39: {type: fused_op, grid_loc: [5, 8], grid_size: [2, 1], inputs: [matmul_524, buffer_0__fused_op_38__fused_op_39, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0, matmul_524, buffer_0__fused_op_38__fused_op_39, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0, dc.input_tensor.layernorm_529.4, matmul_524, buffer_0__fused_op_38__fused_op_39, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_532: {type: matmul, grid_loc: [5, 10], grid_size: [1, 2], inputs: [_fused_op_39, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_538: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [_fused_op_39, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_544: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_552: {type: matmul, grid_loc: [6, 9], grid_size: [1, 2], inputs: [_fused_op_39, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_40: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 4], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_41: {type: fused_op, grid_loc: [6, 7], grid_size: [2, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_548.dc.reduce_sum.1.0, _fused_op_40], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [_fused_op_41, matmul_552],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_563: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_39__fused_op_42: {type: nop, grid_loc: [7, 2], grid_size: [1, 2], inputs: [_fused_op_39],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_42: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 1], inputs: [matmul_563, buffer_0__fused_op_39__fused_op_42, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0, matmul_563, buffer_0__fused_op_39__fused_op_42, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0, dc.input_tensor.layernorm_568.4, matmul_563, buffer_0__fused_op_39__fused_op_42, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_571: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [_fused_op_42, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_574: {type: gelu, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_571],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_42__fused_op_43: {type: nop, grid_loc: [7, 8], grid_size: [1, 2], inputs: [_fused_op_42],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_43: {type: fused_op, grid_loc: [7, 10], grid_size: [2, 1], inputs: [matmul_577, buffer_0__fused_op_42__fused_op_43, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0, matmul_577, buffer_0__fused_op_42__fused_op_43, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0, dc.input_tensor.layernorm_582.4, matmul_577, buffer_0__fused_op_42__fused_op_43, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_585: {type: matmul, grid_loc: [8, 8], grid_size: [1, 2], inputs: [_fused_op_43, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_591: {type: matmul, grid_loc: [8, 10], grid_size: [1, 2], inputs: [_fused_op_43, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_597: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_605: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [e2e__fused_op_43_0, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_44: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_matmul_597_0, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_45: {type: fused_op, grid_loc: [0, 8], grid_size: [2, 1], inputs: [_fused_op_44, lc.input_tensor.softmax_601.dc.reduce_sum.1.0, _fused_op_44], grid_transpose: true,
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_45, matmul_605],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_43__fused_op_46: {type: nop, grid_loc: [0, 6], grid_size: [1, 2], inputs: [e2e__fused_op_43_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_46: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_616, buffer_0__fused_op_43__fused_op_46, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0, matmul_616, buffer_0__fused_op_43__fused_op_46, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0, dc.input_tensor.layernorm_621.4, matmul_616, buffer_0__fused_op_43__fused_op_46, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_624: {type: matmul, grid_loc: [1, 2], grid_size: [1, 8], inputs: [_fused_op_46, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_627: {type: gelu, grid_loc: [2, 2], grid_size: [1, 2], inputs: [matmul_624],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [2, 4], grid_size: [1, 8], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    buffer_0__fused_op_46__fused_op_47: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [_fused_op_46],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_47: {type: fused_op, grid_loc: [3, 0], grid_size: [2, 1], inputs: [matmul_630, buffer_0__fused_op_46__fused_op_47, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0, matmul_630, buffer_0__fused_op_46__fused_op_47, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0, dc.input_tensor.layernorm_635.4, matmul_630, buffer_0__fused_op_46__fused_op_47, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_10_tms: [broadcast: {r: 4}], input_9_tms: [broadcast: {r: 4}], input_5_tms: [broadcast: {r: 24}], input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_47_output_nop_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_47], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0, $gptr_q4_shadow: 0, $gptr_q2_shadow: 0, $gptr_q8: 0, $gptr_q6_shadow: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q9: 0, $lptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $gptr_q1_shadow: 0, $gptr_q7: 0, $gptr_q4: 0}
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_453_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_43_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_matmul_597_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  1: 
    inputs: 3
    intermediates: 2
    schedules: 
      -
        - softmax_18.dc.reduce_sum.1.lc1: { type: matmul, inputs: [input0, input1], attributes: {m_k: 1, u_kt: 4}, mblock: [1, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [intermed0], pop_last: [intermed0], mblock: [1, 1], ublock: [2, 1], output: intermed1}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input2, intermed1], input_1_tms: [broadcast: {c: 4}], pop_last: [intermed1], mblock: [1, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 11
    intermediates: 3
    schedules: 
      -
        - add_37: { type: add, inputs: [input0, input1], mblock: [1, 6], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.reduce_avg.0.lc1: { type: matmul, inputs: [intermed0, input2], attributes: {m_k: 6, u_kt: 4}, pop: [intermed0], mblock: [1, 1], ublock: [2, 1], output: intermed1}
      -
        - add_37: { type: add, inputs: [input3, input4], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.subtract.1: { type: subtract, inputs: [dest, intermed1], input_1_tms: [broadcast: {c: 24}], mblock: [1, 6], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.multiply.2: { type: multiply, inputs: [intermed0, intermed0], pop: [intermed0], mblock: [1, 6], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.reduce_avg.3.lc1: { type: matmul, inputs: [intermed0, input5], attributes: {m_k: 6, u_kt: 4}, pop: [intermed0], mblock: [1, 1], ublock: [2, 1], output: intermed2}
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [intermed2, input6], pop_last: [intermed2], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: intermed0}
      -
        - add_37: { type: add, inputs: [input7, input8], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.subtract.1: { type: subtract, inputs: [dest, intermed1], input_1_tms: [broadcast: {c: 24}], pop_last: [intermed1], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [dest, intermed0], input_1_tms: [broadcast: {c: 24}, tile_broadcast: c], pop_last: [intermed0], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input9], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input10], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_635]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.03
