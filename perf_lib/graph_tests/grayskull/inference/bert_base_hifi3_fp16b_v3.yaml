# git checkout ef840c36
# pytest pybuda/test/benchmark/benchmark.py -m bert -c base -opt 3 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=85000 PYBUDA_DISABLE_DYNAMIC_DRAM=1 PYBUDA_DISABLE_DRAM0=1 PYBUDA_FUSED_OP_MULTIPLIER=2 --loop_count 100 --auto_transpose

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                   {input: HOST, type: queue, entries: 256, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6046b20], [5, 0x5fff180]]}

  # output
  bert_encoders.output_layernorm_635:               {input: _fused_op_47_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x54cdf40], [4, 0x5512000], [5, 0x5563c20], [6, 0x558be00]]}
  layer.0.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6debf40], [2, 0x6d225a0], [3, 0x6089be0], [4, 0x6193d60]]}
  layer.0.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x614ab40], [5, 0x61031a0], [6, 0x602b480], [7, 0x60ae4c0]]}
  layer.0.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x60ab3e0], [1, 0x6de8e60], [2, 0x6d1f4c0], [3, 0x6086b00]]}
  layer.0.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5fe1a20], [7, 0x60621c0], [1, 0x6d9fc40], [2, 0x6cd62a0]]}
  layer.0.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5f36f60], [7, 0x5fc29c0], [1, 0x6cba300], [2, 0x6bf1180]]}
  layer.0.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d50880], [2, 0x6c86ee0], [3, 0x6036f00], [4, 0x5ff7760]]}
  layer.0.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5ff4680], [5, 0x5fefd60], [6, 0x5fd87a0], [7, 0x6058f40]]}
  layer.0.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5fcc480]]}
  layer.0.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fe3a40]]}
  layer.0.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5f5b060], [4, 0x5f19020], [5, 0x5f51600], [6, 0x5f3a040], [7, 0x5fc5aa0], [1, 0x6cbd3e0], [2, 0x6bf4260], [3, 0x5fa4280], [4, 0x5f62240], [5, 0x5f9a820], [6, 0x5f83260], [7, 0x600ecc0], [1, 0x6d06600], [2, 0x6c3d480], [3, 0x5fed4a0], [4, 0x5fab460]]}
  layer.0.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ff2e40], [6, 0x5fdb880], [7, 0x605c020], [1, 0x6d99aa0], [2, 0x6cd0100], [3, 0x6080120], [4, 0x6040980], [5, 0x5ff8fe0]]}
  layer.0.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6e07f00], [3, 0x616ed20], [4, 0x6278ea0], [5, 0x61e82e0], [6, 0x60d67a0], [7, 0x61a21c0], [1, 0x6e97a80], [2, 0x6e51120], [3, 0x61b7f40], [4, 0x62c20c0], [5, 0x6231500], [6, 0x611f9c0], [7, 0x61eb3e0], [1, 0x6ee0ca0], [2, 0x6e9a340], [3, 0x6201160]]}
  layer.0.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61e5200], [6, 0x60d36c0], [7, 0x619f0e0], [1, 0x6e949a0]]}
  layer.0.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6192dc0]]}
  layer.0.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x60c73a0]]}
  layer.1.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6dbdc80], [3, 0x61252c0], [4, 0x622f440], [5, 0x619bfe0]]}
  layer.1.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x60746a0], [7, 0x60f76e0], [1, 0x6def020], [2, 0x6d25680]]}
  layer.1.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x61e6220], [5, 0x6152dc0], [6, 0x607e180], [7, 0x6149ba0]]}
  layer.1.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6146ac0], [1, 0x6e84540], [2, 0x6dbaba0], [3, 0x61221e0]]}
  layer.1.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e3b320], [2, 0x6d71980], [3, 0x60d8fc0], [4, 0x619d000]]}
  layer.1.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6199f20], [5, 0x614f4a0], [6, 0x607a860], [7, 0x61439e0]]}
  layer.1.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x60fa7c0], [1, 0x6df2100], [2, 0x6d28760], [3, 0x608fda0]]}
  layer.1.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x608ccc0], [4, 0x6196e40], [5, 0x614c3c0], [6, 0x6077780]]}
  layer.1.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5e3a7e0]]}
  layer.1.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5de9c00]]}
  layer.1.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5c25460], [5, 0x5cec560], [6, 0x5d577c0], [7, 0x5da83a0], [1, 0x6a65680], [2, 0x6999c60], [3, 0x5cbea40], [4, 0x5c6e680], [5, 0x5d35780], [6, 0x5da09e0], [7, 0x5df15c0], [1, 0x6aae8a0], [2, 0x69e2e80], [3, 0x5d07c60], [4, 0x5cb78a0], [5, 0x5d7e9a0]]}
  layer.1.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5cb2700], [4, 0x5c1f2c0], [5, 0x5ce63c0], [6, 0x5d51620], [7, 0x5da2200], [1, 0x6a5f4e0], [2, 0x6993ac0], [3, 0x5cb88a0]]}
  layer.1.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6983e80], [2, 0x68b8460], [3, 0x5c202c0], [4, 0x5b8ce80], [5, 0x5c53f80], [6, 0x5cbf1e0], [7, 0x5d0fdc0], [1, 0x69cd0a0], [2, 0x6901680], [3, 0x5c694e0], [4, 0x5bd60a0], [5, 0x5c9d1a0], [6, 0x5d08400], [7, 0x5d58fe0], [1, 0x6a162c0], [2, 0x694a8a0]]}
  layer.1.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b89da0], [5, 0x5c50ea0], [6, 0x5cbc100], [7, 0x5d0cce0]]}
  layer.1.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5cafde0]]}
  layer.1.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c44b80]]}
  layer.2.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6939c00], [2, 0x686ea00], [3, 0x5bd6860], [4, 0x5b40b80]]}
  layer.2.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b3daa0], [5, 0x5c41aa0], [6, 0x5cacd00], [7, 0x5d08ba0]]}
  layer.2.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5cbf980], [1, 0x68f09e0], [2, 0x68257e0], [3, 0x5b8d640]]}
  layer.2.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5b8a560], [4, 0x5b3a9c0], [5, 0x5c3e9c0], [6, 0x5ca9c20]]}
  layer.2.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6ba7f60], [3, 0x5f11e40], [4, 0x5ecfe00], [5, 0x5f083e0]]}
  layer.2.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f05300], [6, 0x5f33e80], [7, 0x5fbf8e0], [1, 0x6cb7220]]}
  layer.2.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6c6e000], [2, 0x6b5ed40], [3, 0x5ec8c20], [4, 0x5e86be0]]}
  layer.2.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5e83b00], [5, 0x5f02220], [6, 0x5f30da0], [7, 0x5fbc800]]}
  layer.2.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5f24a80]]}
  layer.2.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5d00ac0]]}
  layer.2.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5decd80], [4, 0x5da84a0], [5, 0x5e6f5a0], [6, 0x5e92640], [7, 0x5f29360], [1, 0x6bdab60], [2, 0x6acc0c0], [3, 0x5e35fa0], [4, 0x5df16c0], [5, 0x5eb87c0], [6, 0x5edb860], [7, 0x5f72580], [1, 0x6c23d80], [2, 0x6b152e0], [3, 0x5e7f1c0], [4, 0x5e3a8e0]]}
  layer.2.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6abfd80], [3, 0x5de6be0], [4, 0x5da2300], [5, 0x5e69400], [6, 0x5e8c4a0], [7, 0x5f231c0], [1, 0x6bd49c0], [2, 0x6ac5f20]]}
  layer.2.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5e47b60], [1, 0x6af9360], [2, 0x6a2d940], [3, 0x5d547a0], [4, 0x5d0fec0], [5, 0x5dd6fc0], [6, 0x5dfa060], [7, 0x5e90d80], [1, 0x6b42580], [2, 0x6a76b60], [3, 0x5d9d9c0], [4, 0x5d590e0], [5, 0x5e201e0], [6, 0x5e43280], [7, 0x5ed9fa0], [1, 0x6b8b7a0]]}
  layer.2.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5d516c0], [4, 0x5d0cde0], [5, 0x5dd3ee0], [6, 0x5df6f80]]}
  layer.2.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dc7bc0]]}
  layer.2.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x73b17c0]]}
  layer.3.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6534520], [1, 0x71e1c20], [2, 0x71a4500], [3, 0x6545980]]}
  layer.3.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x673d6c0], [6, 0x66314e0], [7, 0x673f760], [1, 0x73f1fa0]]}
  layer.3.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x73a8d80], [2, 0x73685a0], [3, 0x66c6180], [4, 0x674e300]]}
  layer.3.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x674b220], [5, 0x673a5e0], [6, 0x662e400], [7, 0x673c680]]}
  layer.3.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x66f13c0], [6, 0x65e51e0], [7, 0x66f3460], [1, 0x735fb60]]}
  layer.3.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x735ca80], [2, 0x7364c80], [3, 0x66c2860], [4, 0x6748140]]}
  layer.3.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6657540], [5, 0x66007c0], [6, 0x64ff8a0], [7, 0x66103c0]]}
  layer.3.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x66bf780], [4, 0x6745060], [5, 0x66ee2e0], [6, 0x65e2100]]}
  layer.3.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x66e1fc0]]}
  layer.3.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6738d40]]}
  layer.3.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7288de0], [3, 0x65e4120], [4, 0x66a6900], [5, 0x664fb80], [6, 0x654ec60], [7, 0x665f780], [1, 0x72c9e00], [2, 0x72d2000], [3, 0x662d340], [4, 0x66efb20], [5, 0x6698da0], [6, 0x6597e80], [7, 0x66a89a0], [1, 0x7313020], [2, 0x731b220], [3, 0x6676560]]}
  layer.3.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x72bdac0], [2, 0x7282c40], [3, 0x65ddf80], [4, 0x66a0760], [5, 0x66499e0], [6, 0x6548ac0], [7, 0x66595e0], [1, 0x72c3c60]]}
  layer.3.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x74591c0], [3, 0x6728280], [4, 0x67b0400], [5, 0x6796da0], [6, 0x6716e40], [7, 0x6822000], [1, 0x74d70e0], [2, 0x74a23e0], [3, 0x67714a0], [4, 0x67f9620], [5, 0x67dffc0], [6, 0x6760060], [7, 0x686b220], [1, 0x7520300], [2, 0x74eb600], [3, 0x67ba6c0]]}
  layer.3.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x670f3a0], [4, 0x6797520], [5, 0x67407a0], [6, 0x66345c0]]}
  layer.3.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x67a40e0]]}
  layer.3.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x671bf60]]}
  layer.4.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x66ccbc0], [7, 0x67d85a0], [1, 0x748d680], [2, 0x740ffa0]]}
  layer.4.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x740cec0], [3, 0x6718e80], [4, 0x67a1000], [5, 0x6792c60]]}
  layer.4.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6842840], [5, 0x68291e0], [6, 0x67a9280], [7, 0x68b4440]]}
  layer.4.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x678fb80], [6, 0x66c9ae0], [7, 0x67d54c0], [1, 0x748a5a0]]}
  layer.4.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x66808c0], [7, 0x678c2a0], [1, 0x7441380], [2, 0x73c3ca0]]}
  layer.4.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x73c0bc0], [3, 0x6715560], [4, 0x679d6e0], [5, 0x678caa0]]}
  layer.4.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6743880], [6, 0x66376a0], [7, 0x6743080], [1, 0x73f8160]]}
  layer.4.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x73f5080], [2, 0x73bdae0], [3, 0x6712480], [4, 0x679a600]]}
  layer.4.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63c51e0]]}
  layer.4.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7023540]]}
  layer.4.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x631f0a0], [1, 0x6fd7a60], [2, 0x6f91100], [3, 0x6332da0], [4, 0x64807c0], [5, 0x63ef3e0], [6, 0x629d0c0], [7, 0x63682c0], [1, 0x7020c80], [2, 0x6fda320], [3, 0x637bfc0], [4, 0x64c99e0], [5, 0x6438600], [6, 0x62e62e0], [7, 0x63b14e0], [1, 0x7069ea0]]}
  layer.4.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6290d80], [7, 0x6318f00], [1, 0x6fd18c0], [2, 0x6f8af60], [3, 0x632cc00], [4, 0x647a620], [5, 0x63e9240], [6, 0x6296f20]]}
  layer.4.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x639efc0], [5, 0x630dbe0], [6, 0x61fe940], [7, 0x6286ac0], [1, 0x6f3f480], [2, 0x6ef8b20], [3, 0x629a7c0], [4, 0x63e81e0], [5, 0x6356e00], [6, 0x6247b60], [7, 0x62cfce0], [1, 0x6f886a0], [2, 0x6f41d40], [3, 0x62e39e0], [4, 0x6431400], [5, 0x63a0020]]}
  layer.4.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x62839e0], [1, 0x6f3c3a0], [2, 0x6ef5a40], [3, 0x62976e0]]}
  layer.4.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6ee9720]]}
  layer.4.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f30080]]}
  layer.5.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6354d40], [5, 0x62c4180], [6, 0x61b4ee0], [7, 0x623a7c0]]}
  layer.5.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x62376e0], [1, 0x6f2cfa0], [2, 0x6ee6640], [3, 0x62935a0]]}
  layer.5.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x624a380], [4, 0x630bb20], [5, 0x627af60], [6, 0x616bcc0]]}
  layer.5.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6168be0], [7, 0x6234600], [1, 0x6f29ec0], [2, 0x6ee3560]]}
  layer.5.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x65c71a0], [1, 0x72748a0], [2, 0x7239a20], [3, 0x6594d60]]}
  layer.5.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6591c80], [4, 0x6654460], [5, 0x65fd6e0], [6, 0x64fc7c0]]}
  layer.5.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x64b35a0], [7, 0x657df80], [1, 0x722b680], [2, 0x71f0800]]}
  layer.5.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71ed720], [3, 0x658eba0], [4, 0x6651380], [5, 0x65fa600]]}
  layer.5.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6645060]]}
  layer.5.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x632f500]]}
  layer.5.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x651df40], [6, 0x63d6ee0], [7, 0x64a20e0], [1, 0x714f7e0], [2, 0x71120c0], [3, 0x64b3540], [4, 0x65b2c20], [5, 0x6567160], [6, 0x6420100], [7, 0x64eb300], [1, 0x7198a00], [2, 0x715b2e0], [3, 0x64fc760], [4, 0x65fbe40], [5, 0x65b0380], [6, 0x6469320]]}
  layer.5.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x65a68e0], [5, 0x6517da0], [6, 0x63d0d40], [7, 0x649bf40], [1, 0x7149640], [2, 0x710bf20], [3, 0x64ad3a0], [4, 0x65aca80]]}
  layer.5.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x70308c0], [3, 0x63d1d40], [4, 0x65144a0], [5, 0x6485960], [6, 0x633e900], [7, 0x6409b00], [1, 0x70b7200], [2, 0x7079ae0], [3, 0x641af60], [4, 0x655d6c0], [5, 0x64ceb80], [6, 0x6387b20], [7, 0x6452d20], [1, 0x7100420], [2, 0x70c2d00], [3, 0x6464180]]}
  layer.5.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6482880], [6, 0x633b820], [7, 0x6406a20], [1, 0x70b4120]]}
  layer.5.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e87620]]}
  layer.5.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x63fa700]]}
  layer.6.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e38e60], [2, 0x5dfbf60], [3, 0x518f880], [4, 0x51caf20]]}
  layer.6.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5588d20], [7, 0x5551780], [1, 0x613cec0], [2, 0x60f4d00]]}
  layer.6.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60abae0], [3, 0x5484d20], [4, 0x54c8de0], [5, 0x551aa00]]}
  layer.6.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5517920], [6, 0x5585c40], [7, 0x554e6a0], [1, 0x6139de0]]}
  layer.6.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x553ca20], [7, 0x5505480], [1, 0x60f0bc0], [2, 0x60628c0]]}
  layer.6.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x605f7e0], [3, 0x5481400], [4, 0x54c54c0], [5, 0x5514840]]}
  layer.6.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5342480], [5, 0x5391800], [6, 0x53c4ca0], [7, 0x53475c0]]}
  layer.6.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x53ebee0], [4, 0x542ffa0], [5, 0x547f320], [6, 0x54a7500]]}
  layer.6.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5473000]]}
  layer.6.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5423c80]]}
  layer.6.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5ef1500], [3, 0x5310880], [4, 0x5391840], [5, 0x53e0bc0], [6, 0x5414060], [7, 0x5396980], [1, 0x5f828e0], [2, 0x5f3a720], [3, 0x5359aa0], [4, 0x53daa60], [5, 0x5429de0], [6, 0x545d280], [7, 0x53dfba0], [1, 0x5fcbb00], [2, 0x5f83940], [3, 0x53a2cc0]]}
  layer.6.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f765a0], [2, 0x5eeb360], [3, 0x530a6e0], [4, 0x538b6a0], [5, 0x53daa20], [6, 0x540dec0], [7, 0x53907e0], [1, 0x5f7c740]]}
  layer.6.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5429e20], [1, 0x6015560], [2, 0x5fcd3a0], [3, 0x53eefc0], [4, 0x5433080], [5, 0x5482400], [6, 0x54aa5e0], [7, 0x5473040], [1, 0x605e780], [2, 0x60165c0], [3, 0x54381e0], [4, 0x547c2a0], [5, 0x54cb620], [6, 0x54f3800], [7, 0x54bc260], [1, 0x60a79a0]]}
  layer.6.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55f0780], [1, 0x619efc0], [2, 0x619f7e0], [3, 0x55f99e0]]}
  layer.6.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61934c0]]}
  layer.6.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6192ca0]]}
  layer.7.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x55f0f80], [5, 0x5645440], [6, 0x5624c40], [7, 0x55a7560]]}
  layer.7.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55a4480], [1, 0x618fbc0], [2, 0x61903e0], [3, 0x55f58a0]]}
  layer.7.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5554860], [1, 0x613ffa0], [2, 0x60f7de0], [3, 0x5517160]]}
  layer.7.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x618d300], [3, 0x55f27c0], [4, 0x55edea0], [5, 0x5642360]]}
  layer.7.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55a95a0], [4, 0x55a4c80], [5, 0x55f9140], [6, 0x55db1e0]]}
  layer.7.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x55d8100], [7, 0x55a0b60], [1, 0x618c2a0], [2, 0x618a220]]}
  layer.7.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6141000], [3, 0x5560380], [4, 0x555ba60], [5, 0x55aff20]]}
  layer.7.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55ace40], [6, 0x55d5020], [7, 0x559da80], [1, 0x61891c0]]}
  layer.7.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x512c7c0]]}
  layer.7.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x50f1120]]}
  layer.7.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ccb3c0], [2, 0x5c8e4c0], [3, 0x505ece0], [4, 0x509a380], [5, 0x5126600], [6, 0x51255c0], [7, 0x50e4de0], [1, 0x5d145e0], [2, 0x5cd76e0], [3, 0x50a7f00], [4, 0x50e35a0], [5, 0x516f820], [6, 0x516e7e0], [7, 0x512e000], [1, 0x5d5d800], [2, 0x5d20900]]}
  layer.7.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50d8aa0], [1, 0x5cc5220], [2, 0x5c88320], [3, 0x5058b40], [4, 0x50941e0], [5, 0x5120460], [6, 0x511f420], [7, 0x50dec40]]}
  layer.7.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5044e00], [6, 0x5043dc0], [7, 0x5046660], [1, 0x5c32de0], [2, 0x5bf5ee0], [3, 0x4fc6700], [4, 0x5001da0], [5, 0x508e020], [6, 0x508cfe0], [7, 0x508f880], [1, 0x5c7c000], [2, 0x5c3f100], [3, 0x500f920], [4, 0x504afc0], [5, 0x50d7240], [6, 0x50d6200]]}
  layer.7.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5c2fd00], [2, 0x5bf2e00], [3, 0x4fc3620], [4, 0x4ffecc0]]}
  layer.7.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb7300]]}
  layer.7.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5be6ae0]]}
  layer.8.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4ffab80], [6, 0x4ffa360], [7, 0x4ffcc00], [1, 0x5be6ae0]]}
  layer.8.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5be3a00], [2, 0x5be3a00], [3, 0x4fb4220], [4, 0x4ffab80]]}
  layer.8.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4fb1960], [5, 0x4fb1960], [6, 0x4fb1140], [7, 0x4fb39e0]]}
  layer.8.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900], [1, 0x5be0920], [2, 0x5be0920], [3, 0x4fb1140]]}
  layer.8.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5274160], [4, 0x52af800], [5, 0x52feb80], [6, 0x532f780]]}
  layer.8.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x53444e0], [1, 0x5f734c0], [2, 0x5ee8280], [3, 0x5307600]]}
  layer.8.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x52be3e0], [4, 0x52f9260], [5, 0x53485e0], [6, 0x537ba80]]}
  layer.8.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x53789a0], [7, 0x5341400], [1, 0x5f703e0], [2, 0x5ee51a0]]}
  layer.8.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f640c0]]}
  layer.8.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x53350e0]]}
  layer.8.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x51b7a00], [7, 0x5177220], [1, 0x5da6a20], [2, 0x5d69b20], [3, 0x50fd440], [4, 0x5138ae0], [5, 0x51c4d60], [6, 0x5200c20], [7, 0x51c0440], [1, 0x5defc40], [2, 0x5db2d40], [3, 0x5146660], [4, 0x5181d00], [5, 0x520df80], [6, 0x5249e40], [7, 0x5209660]]}
  layer.8.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5ed7e00], [3, 0x526dfc0], [4, 0x52a9660], [5, 0x52f89e0], [6, 0x53295e0], [7, 0x532ef40], [1, 0x5f5df20], [2, 0x5eddfa0]]}
  layer.8.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x52538e0], [1, 0x5e828c0], [2, 0x5e459c0], [3, 0x51dbb80], [4, 0x5217220], [5, 0x52665a0], [6, 0x52971a0], [7, 0x529cb00], [1, 0x5ecbae0], [2, 0x5e8ebe0], [3, 0x5224da0], [4, 0x5260440], [5, 0x52af7c0], [6, 0x52e03c0], [7, 0x52e5d20], [1, 0x5f14d00]]}
  layer.8.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x51d8aa0], [4, 0x5214140], [5, 0x52634c0], [6, 0x52940c0]]}
  layer.8.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52571a0]]}
  layer.8.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x51b8a40]]}
  layer.9.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5be3ae0], [1, 0x685ac80], [2, 0x67cc980], [3, 0x5b31f40]]}
  layer.9.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5b91620], [1, 0x67c4f20], [2, 0x6736c20], [3, 0x5a99940]]}
  layer.9.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a50720], [4, 0x5a51f80], [5, 0x5b58820], [6, 0x5bc42a0]]}
  layer.9.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5bc11c0], [7, 0x5b8e540], [1, 0x67c1e40], [2, 0x6733b40]]}
  layer.9.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5b45320], [1, 0x6778c20], [2, 0x66ea920], [3, 0x5a07500]]}
  layer.9.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a04420], [4, 0x5a4e660], [5, 0x5b54f00], [6, 0x5bbe0e0]]}
  layer.9.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b74ec0], [7, 0x5afc100], [1, 0x672fa00], [2, 0x66a1700]]}
  layer.9.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x65b84a0], [2, 0x6527900], [3, 0x58c7520], [4, 0x5911760]]}
  layer.9.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a3f260]]}
  layer.9.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x59f5020]]}
  layer.9.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6653b60], [2, 0x65c2fc0], [3, 0x5962be0], [4, 0x59ace20], [5, 0x5abe980], [6, 0x5ae1a20], [7, 0x5a69480], [1, 0x669cd80], [2, 0x660c1e0], [3, 0x59abe00], [4, 0x59f6040], [5, 0x5b07ba0], [6, 0x5b2ac40], [7, 0x5ab26a0], [1, 0x66e5fa0], [2, 0x6655400]]}
  layer.9.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5a5d140], [1, 0x664d9c0], [2, 0x65bce20], [3, 0x595ca40], [4, 0x59a6c80], [5, 0x5ab87e0], [6, 0x5adb880], [7, 0x5a632e0]]}
  layer.9.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59dd180], [6, 0x5a00220], [7, 0x59cad00], [1, 0x65bb580], [2, 0x652a9e0], [3, 0x58ca600], [4, 0x5914840], [5, 0x5a263a0], [6, 0x5a49440], [7, 0x5a13f20], [1, 0x66047a0], [2, 0x6573c00], [3, 0x5913820], [4, 0x595da60], [5, 0x5a6f5c0], [6, 0x5a92660]]}
  layer.9.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x669e620], [3, 0x5a01340], [4, 0x5a4b580], [5, 0x5b51e20]]}
  layer.9.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5b7e240]]}
  layer.9.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6818c80]]}
  layer.10.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bf4740], [6, 0x5c601c0], [7, 0x5c75f20], [1, 0x68a6f80]]}
  layer.10.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x68a3ea0], [2, 0x6815ba0], [3, 0x5b7b160], [4, 0x5b36880]]}
  layer.10.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5aed660], [5, 0x5bab520], [6, 0x5c16fa0], [7, 0x5c2cd00]]}
  layer.10.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a9b1a0], [5, 0x5ba1a40], [6, 0x5c0d4c0], [7, 0x5b94700]]}
  layer.10.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6811a60], [2, 0x6783760], [3, 0x5ae8d20], [4, 0x5aa4440]]}
  layer.10.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5aa1360], [5, 0x5ba7c00], [6, 0x5c13680], [7, 0x5be0a00]]}
  layer.10.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5b977e0], [1, 0x67c8840], [2, 0x673a540], [3, 0x5a9fb00]]}
  layer.10.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a9ca20], [4, 0x5a9e280], [5, 0x5ba4b20], [6, 0x5c105a0]]}
  layer.10.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x57fd200]]}
  layer.10.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5878040]]}
  layer.10.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x56e9e00], [5, 0x57c0ae0], [6, 0x57e5c00], [7, 0x576adc0], [1, 0x6361fe0], [2, 0x62d3ce0], [3, 0x56adf60], [4, 0x5733020], [5, 0x5809d00], [6, 0x582ee20], [7, 0x57b3fe0], [1, 0x63ab200], [2, 0x631cf00], [3, 0x56f7180], [4, 0x577c240], [5, 0x5852f20]]}
  layer.10.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x56a1c20], [4, 0x56e3c60], [5, 0x57ba940], [6, 0x57dfa60], [7, 0x5764c20], [1, 0x635be40], [2, 0x62cdb40], [3, 0x56a7dc0]]}
  layer.10.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x62807e0], [2, 0x61f24e0], [3, 0x560f7e0], [4, 0x5651820], [5, 0x5728500], [6, 0x574d620], [7, 0x56d27e0], [1, 0x62c9a00], [2, 0x623b700], [3, 0x5658a00], [4, 0x569aa40], [5, 0x5771720], [6, 0x5796840], [7, 0x571ba00], [1, 0x6312c20], [2, 0x6284920]]}
  layer.10.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x564e740], [5, 0x5725420], [6, 0x574a540], [7, 0x56cf700]]}
  layer.10.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5642420]]}
  layer.10.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5602c80]]}
  layer.11.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x56b8100], [7, 0x563d2c0], [1, 0x61ee3a0], [2, 0x61a8a80]]}
  layer.11.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61a59a0], [3, 0x55ffba0], [4, 0x563f340], [5, 0x56d9120]]}
  layer.11.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x568ff00], [6, 0x566eee0], [7, 0x55f40a0], [1, 0x61a5180]]}
  layer.11.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x61a20a0], [2, 0x61a28c0], [3, 0x55fcac0], [4, 0x563c260]]}
  layer.11.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x56dc200], [6, 0x5701320], [7, 0x56864e0], [1, 0x62375c0]]}
  layer.11.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x590e680], [5, 0x59da0a0], [6, 0x59fd140], [7, 0x59c7c20]]}
  layer.11.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x597ea00], [1, 0x656f280], [2, 0x64de6e0], [3, 0x587e300]]}
  layer.11.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x587b220], [4, 0x590b5a0], [5, 0x59d6fc0], [6, 0x59fa060]]}
  layer.11.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59caca0]]}
  layer.11.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x58ff280]]}
  layer.11.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x63f4420], [2, 0x6366120], [3, 0x57403a0], [4, 0x57c5460], [5, 0x589c140], [6, 0x5884360], [7, 0x5809520], [1, 0x643d640], [2, 0x63af340], [3, 0x57895c0], [4, 0x580e680], [5, 0x58e5360], [6, 0x58cd580], [7, 0x5852740], [1, 0x6486860], [2, 0x63f8560]]}
  layer.11.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6562700], [2, 0x64d6ca0], [3, 0x5874020], [4, 0x58f90e0], [5, 0x59c4b00], [6, 0x59f2e60], [7, 0x5977800], [1, 0x65688a0]]}
  layer.11.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5917800], [7, 0x589c1a0], [1, 0x64d02c0], [2, 0x6444860], [3, 0x57e1be0], [4, 0x5866ca0], [5, 0x59326c0], [6, 0x5960a20], [7, 0x58e53c0], [1, 0x65194e0], [2, 0x648da80], [3, 0x582ae00], [4, 0x58afec0], [5, 0x597b8e0], [6, 0x59a9c40], [7, 0x592e5e0]]}
  layer.11.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6441780], [3, 0x57deb00], [4, 0x5863bc0], [5, 0x592f5e0]]}
  layer.11.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x58578a0]]}
  layer.11.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x57d27e0]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x602ac40]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x60862c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x60366c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c866a0]]}
  dc.input_tensor.layernorm_38.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6057ee0], [1, 0x6d4f820]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6278660]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x616e4e0]]}
  dc.input_tensor.layernorm_52.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e93940], [2, 0x6e06ea0]]}
  input_1_multiply_69_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x607d940]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6152580]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f019e0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5d50e80]]}
  dc.input_tensor.layernorm_91.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6af7ac0], [2, 0x6a2c0a0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x68f01a0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x68b7c20]]}
  dc.input_tensor.layernorm_105.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5d0bc80], [1, 0x6982e20]]}
  input_1_multiply_122_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6824fa0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5c1fa80]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5ec83e0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6b5e500]]}
  dc.input_tensor.layernorm_144.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5fbb7a0], [1, 0x6c6cfa0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6a2d100]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6af8b20]]}
  dc.input_tensor.layernorm_158.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5df5f20], [7, 0x5e46b00]]}
  input_1_multiply_175_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x66c5940]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7367d60]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7364440]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x735c240]]}
  dc.input_tensor.layernorm_197.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65e10a0], [7, 0x66f1bc0]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x74d68a0]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x68217c0]]}
  dc.input_tensor.layernorm_211.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6795d40], [6, 0x6715de0]]}
  input_1_multiply_228_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x67a07c0]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6718640]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x66f2c20]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6742840]]}
  dc.input_tensor.layernorm_250.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6512c00], [5, 0x6481820]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x61fe100]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x630b2e0]]}
  dc.input_tensor.layernorm_264.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6296680], [4, 0x639df60]]}
  input_1_multiply_281_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x627a720]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x630d3a0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x722ae40]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x657d740]]}
  dc.input_tensor.layernorm_303.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x65f95a0], [6, 0x64b2540]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6513c60]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63d1500]]}
  dc.input_tensor.layernorm_317.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70b30c0], [2, 0x702f860]]}
  input_1_multiply_334_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x54c85a0]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x54844e0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5fccb60]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6014d20]]}
  dc.input_tensor.layernorm_356.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x54a64a0], [7, 0x5428dc0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x566de60]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x568e660]]}
  dc.input_tensor.layernorm_370.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55f8980], [4, 0x563a1a0]]}
  input_1_multiply_387_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x618f380]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55a3c40]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x555b220]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5624400]]}
  dc.input_tensor.layernorm_409.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900], [5, 0x4fb0900]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5045e20]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5043580]]}
  dc.input_tensor.layernorm_423.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4ffdc60], [5, 0x5043da0]]}
  input_1_multiply_440_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5347da0]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x52f8a20]]}
  dc.input_tensor.layernorm_462.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5ee4140], [3, 0x52bd380]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e45180]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e82080]]}
  dc.input_tensor.layernorm_476.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5293060], [7, 0x5252880]]}
  input_1_multiply_493_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b57fe0]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a51740]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x672f1c0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5afb8c0]]}
  dc.input_tensor.layernorm_515.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b50dc0], [6, 0x5b73e60]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5cbf140]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5ca93e0]]}
  dc.input_tensor.layernorm_529.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b39960], [5, 0x5c3d960]]}
  input_1_multiply_546_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5c16760]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5baace0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6739d00]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x67c8000]]}
  dc.input_tensor.layernorm_568.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x64dce40], [3, 0x587a1c0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x560efa0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x61f1ca0]]}
  dc.input_tensor.layernorm_582.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x563b200], [5, 0x568eea0]]}
  input_1_multiply_599_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55f3860]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x566e6a0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x64ddea0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x656ea40]]}
  dc.input_tensor.layernorm_621.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x59f9000], [7, 0x597d9a0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64cfa80]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x589b960]]}
  dc.input_tensor.layernorm_635.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x592e580], [6, 0x59167a0]]}

  # epoch_to_epoch
  e2e__fused_op_6_0:                                {input: _fused_op_6, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900], [2, 0x4fb0900]]}
  e2e_gelu_150_0:                                   {input: gelu_150, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x68038e0], [4, 0x688ba60], [5, 0x6872400]]}
  e2e__fused_op_10_0:                               {input: _fused_op_10, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7569520], [2, 0x7534820]]}
  e2e_matmul_245_0:                                 {input: matmul_245, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x67f24a0], [7, 0x68fd660], [1, 0x8199540], [2, 0x8164840]]}
  e2e_buffer_0__fused_op_15_add_249_0:              {input: buffer_0__fused_op_15_add_249, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8883900]]}
  e2e_gelu_309_0:                                   {input: gelu_309, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6e0a4c0], [7, 0x6f15680], [1, 0x87b1560]]}
  e2e__fused_op_22_0:                               {input: _fused_op_22, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x890ba80], [5, 0x88f2420]]}
  e2e_matmul_404_0:                                 {input: matmul_404, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x877c860], [3, 0xa0e3920], [4, 0x953baa0], [5, 0x9522440]]}
  e2e_buffer_0__fused_op_27_add_408_0:              {input: buffer_0__fused_op_27_add_408, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8e8a4e0]]}
  e2e_gelu_468_0:                                   {input: gelu_468, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d94880], [3, 0xa6fb940], [4, 0x9b53ac0]]}
  e2e__fused_op_34_0:                               {input: _fused_op_34, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8f956a0], [1, 0xa831580]]}
  e2e_matmul_563_0:                                 {input: matmul_563, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9b3a460], [6, 0xa6ea500], [7, 0x9bc56c0], [1, 0xb4615a0]]}
  e2e_buffer_0__fused_op_39_add_567_0:              {input: buffer_0__fused_op_39_add_567, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xae148a0]]}
  e2e_gelu_627_0:                                   {input: gelu_627, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc77b960], [4, 0xbbd3ae0], [5, 0xa152480]]}
  e2e_buffer_0__fused_op_46_add_634_0:              {input: buffer_0__fused_op_46_add_634, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xad02520]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_1, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 6], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_37: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_37, layernorm_38.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_2: {type: fused_op, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4, layernorm_38.dc.subtract.1, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_41: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [_fused_op_2, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_44: {type: gelu, grid_loc: [2, 5], grid_size: [1, 3], inputs: [matmul_41],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [2, 8], grid_size: [4, 4], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_2_add_51: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [matmul_47, buffer_0__fused_op_2_add_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_3: {type: fused_op, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4, layernorm_52.dc.subtract.1, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_55: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_61: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_67: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_4: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 2], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [_fused_op_4, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_75: {type: matmul, grid_loc: [7, 1], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_4],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_5, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [7, 6], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_3_add_90: {type: nop, grid_loc: [7, 10], grid_size: [1, 1], inputs: [_fused_op_3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [7, 11], grid_size: [1, 1], inputs: [matmul_86, buffer_0__fused_op_3_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_6: {type: fused_op, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4, layernorm_91.dc.subtract.1, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_1:
    target_device: 0
    input_count: 128
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_6_0, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_97: {type: gelu, grid_loc: [0, 8], grid_size: [1, 3], inputs: [matmul_94],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [1, 8], grid_size: [4, 4], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_104: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_100, e2e__fused_op_6_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_7: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4, layernorm_105.dc.subtract.1, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_108: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_114: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_120: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_8: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 2], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_128: {type: matmul, grid_loc: [4, 3], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_9: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_9, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_7_add_143: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [_fused_op_7],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_139, buffer_0__fused_op_7_add_143],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_10: {type: fused_op, grid_loc: [5, 10], grid_size: [2, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4, layernorm_144.dc.subtract.1, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_147: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_150: {type: gelu, grid_loc: [6, 8], grid_size: [1, 3], inputs: [matmul_147],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_2:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_10_add_157: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_10_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_153, buffer_0__fused_op_10_add_157],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_11: {type: fused_op, grid_loc: [0, 10], grid_size: [2, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4, layernorm_158.dc.subtract.1, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_161: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_167: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_173: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 2], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_181: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_13: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_12],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_13, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_11_add_196: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_11],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_196: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_192, buffer_0__fused_op_11_add_196],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_14: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4, layernorm_197.dc.subtract.1, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_200: {type: matmul, grid_loc: [5, 0], grid_size: [2, 8], inputs: [_fused_op_14, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_203: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_200],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_14_add_210: {type: nop, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [matmul_206, buffer_0__fused_op_14_add_210],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_15: {type: fused_op, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4, layernorm_211.dc.subtract.1, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_214: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_220: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_226: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_16: {type: fused_op, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_234: {type: matmul, grid_loc: [9, 2], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [9, 1], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_16],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [_fused_op_17, matmul_234],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [9, 7], grid_size: [1, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_15_add_249: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [_fused_op_15],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 128
    add_249: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_245_0, e2e_buffer_0__fused_op_15_add_249_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_18: {type: fused_op, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4, layernorm_250.dc.subtract.1, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_253: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_18, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_256: {type: gelu, grid_loc: [0, 7], grid_size: [1, 3], inputs: [matmul_253],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [1, 8], grid_size: [4, 4], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_18_add_263: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_18],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_263: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_259, buffer_0__fused_op_18_add_263],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_19: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4, layernorm_264.dc.subtract.1, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_267: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_273: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_279: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_287: {type: matmul, grid_loc: [5, 3], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_21: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_20],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_21, matmul_287],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_19_add_302: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_19],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_302: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [matmul_298, buffer_0__fused_op_19_add_302],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_22: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4, layernorm_303.dc.subtract.1, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_306: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [_fused_op_22, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_309: {type: gelu, grid_loc: [6, 8], grid_size: [1, 3], inputs: [matmul_306],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_312: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_309_0, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_22_add_316: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_22_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_316: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_312, buffer_0__fused_op_22_add_316],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_23: {type: fused_op, grid_loc: [0, 10], grid_size: [2, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4, layernorm_317.dc.subtract.1, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_320: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_326: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_332: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_24: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 2], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_340: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_24],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_25, matmul_340],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_23_add_355: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_23],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_355: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_351, buffer_0__fused_op_23_add_355],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_26: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4, layernorm_356.dc.subtract.1, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_359: {type: matmul, grid_loc: [5, 0], grid_size: [2, 8], inputs: [_fused_op_26, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_362: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_359],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_26_add_369: {type: nop, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_26],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_369: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [matmul_365, buffer_0__fused_op_26_add_369],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_27: {type: fused_op, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4, layernorm_370.dc.subtract.1, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_373: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_379: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_385: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_28: {type: fused_op, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_393: {type: matmul, grid_loc: [9, 2], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [9, 1], grid_size: [1, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_28],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [_fused_op_29, matmul_393],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_404: {type: matmul, grid_loc: [9, 7], grid_size: [1, 4], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_27_add_408: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [_fused_op_27],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5:
    target_device: 0
    input_count: 128
    add_408: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_404_0, e2e_buffer_0__fused_op_27_add_408_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_30: {type: fused_op, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4, layernorm_409.dc.subtract.1, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_412: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_30, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_415: {type: gelu, grid_loc: [0, 7], grid_size: [1, 3], inputs: [matmul_412],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [1, 8], grid_size: [4, 4], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_30_add_422: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_30],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_422: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_418, buffer_0__fused_op_30_add_422],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_31: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4, layernorm_423.dc.subtract.1, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_426: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_432: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_438: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_32: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_446: {type: matmul, grid_loc: [5, 3], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_33: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_32],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_33, matmul_446],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_457: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_31_add_461: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_31],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_461: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [matmul_457, buffer_0__fused_op_31_add_461],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_34: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4, layernorm_462.dc.subtract.1, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_465: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [_fused_op_34, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_468: {type: gelu, grid_loc: [6, 8], grid_size: [1, 3], inputs: [matmul_465],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_6:
    target_device: 0
    input_count: 128
    matmul_471: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_468_0, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_34_add_475: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_34_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_475: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_471, buffer_0__fused_op_34_add_475],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_35: {type: fused_op, grid_loc: [0, 10], grid_size: [2, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4, layernorm_476.dc.subtract.1, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_479: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_485: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_491: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 2], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_499: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_37: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_36],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_37, matmul_499],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_35_add_514: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_35],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_514: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_510, buffer_0__fused_op_35_add_514],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_38: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4, layernorm_515.dc.subtract.1, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_518: {type: matmul, grid_loc: [5, 0], grid_size: [2, 8], inputs: [_fused_op_38, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_521: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_518],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_38_add_528: {type: nop, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_38],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_528: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [matmul_524, buffer_0__fused_op_38_add_528],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_39: {type: fused_op, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4, layernorm_529.dc.subtract.1, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_532: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_538: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_544: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_40: {type: fused_op, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_552: {type: matmul, grid_loc: [9, 2], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [9, 1], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_40],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [_fused_op_41, matmul_552],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_563: {type: matmul, grid_loc: [9, 7], grid_size: [1, 4], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_39_add_567: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [_fused_op_39],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7:
    target_device: 0
    input_count: 128
    add_567: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_563_0, e2e_buffer_0__fused_op_39_add_567_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_42: {type: fused_op, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4, layernorm_568.dc.subtract.1, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_571: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_42, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_574: {type: gelu, grid_loc: [0, 7], grid_size: [1, 3], inputs: [matmul_571],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [1, 8], grid_size: [4, 4], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    buffer_0__fused_op_42_add_581: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_42],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_581: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_577, buffer_0__fused_op_42_add_581],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_43: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4, layernorm_582.dc.subtract.1, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_585: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_591: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_597: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_44, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_605: {type: matmul, grid_loc: [5, 2], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_45: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_44],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [_fused_op_45, matmul_605],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0__fused_op_43_add_620: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_43],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_620: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [matmul_616, buffer_0__fused_op_43_add_620],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_46: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4, layernorm_621.dc.subtract.1, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_624: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [_fused_op_46, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_627: {type: gelu, grid_loc: [6, 9], grid_size: [1, 3], inputs: [matmul_624],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0__fused_op_46_add_634: {type: nop, grid_loc: [6, 8], grid_size: [1, 1], inputs: [_fused_op_46],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8:
    target_device: 0
    input_count: 128
    matmul_630: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_627_0, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_634: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [matmul_630, e2e_buffer_0__fused_op_46_add_634_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_47: {type: fused_op, grid_loc: [0, 9], grid_size: [2, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4, layernorm_635.dc.subtract.1, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 80, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_47_output_nop_0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_47], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $gptr_q1_shadow: 0, $lptr_q1: 0, $gptr_q2_shadow: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $gptr_q13: 0, $gptr_q12: 0, $lptr_q0: 0, $lptr_q3: 0, $lptr_q10: 0, $lptr_q14: 0, $gptr_q11: 0, $gptr_q1: 0, $lptr_q9: 0, $lptr_q11: 0, $lptr_q15: 0, $gptr_q6_shadow: 0, $lptr_q13: 0, $gptr_q16: 0, $lptr_q8: 0, $gptr_q15: 0, $lptr_q12: 0, $gptr_q8_shadow: 0, $gptr_q4: 0, $gptr_q12_shadow: 0, $gptr_q6: 0, $lptr_q16: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q4_shadow: 0, $gptr_q5: 0, $gptr_q10: 0, $gptr_q10_shadow: 0, $lptr_q5: 0, $gptr_q9: 0, $lptr_q7: 0, $gptr_q7: 0, $lptr_q6: 0, $lptr_q4: 0}
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q8, set, $gptr_q8_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_245_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_buffer_0__fused_op_15_add_249_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_309_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_404_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_buffer_0__fused_op_27_add_408_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_matmul_563_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_buffer_0__fused_op_39_add_567_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_8, queue_settings: {
               e2e_gelu_627_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_buffer_0__fused_op_46_add_634_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 2], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 2], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 4}], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: intermed0}
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 24}, tile_broadcast: c], pop_last: [intermed0], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input3], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input4], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_635]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.03
