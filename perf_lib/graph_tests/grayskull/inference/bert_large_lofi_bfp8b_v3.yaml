# git checkout ef840c36
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NO_FUSE_MATMUL_BIAS=1 PYBUDA_NLP_MANUAL_TARGET=185000 PYBUDA_DISABLE_DRAM0=1 --auto_transpose

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                     {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6d05d40], [2, 0x6e25360], [3, 0x6dcfe60]]}

  # output
  bert_encoders.output_layernorm_1271:               {input: _fused_op_239_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5677820], [7, 0x56c5fc0], [1, 0x563c2e0], [2, 0x5666d60]]}
  layer.0.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6cfd120]]}
  layer.0.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ddf340], [3, 0x6d89e40], [4, 0x6e02c00], [5, 0x6f74fc0]]}
  layer.0.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e6d1e0]]}
  layer.0.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6fbafe0], [6, 0x6e75e00], [7, 0x6ecd180], [1, 0x6e1dd60]]}
  layer.0.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e645c0]]}
  layer.0.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6ee7e80], [4, 0x6e490a0], [5, 0x7001000], [6, 0x6ebbe20]]}
  layer.0.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6f131a0]]}
  layer.0.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7047020]]}
  layer.0.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6f01e40]]}
  layer.0.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6f1bdc0], [1, 0x6e64200], [2, 0x6f3dc80], [3, 0x6f2f900], [4, 0x6e90b20], [5, 0x704fc40], [6, 0x6f0aa60], [7, 0x6f61de0], [1, 0x6eaa220], [2, 0x6f83ca0], [3, 0x6f75920], [4, 0x6ed6b40], [5, 0x7095c60], [6, 0x6f50a80], [7, 0x6fa7e00], [1, 0x6ef0240]]}
  layer.0.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ca5460], [2, 0x6d87680]]}
  layer.0.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6caca00], [3, 0x6bdd160], [4, 0x6c55f20], [5, 0x6e0de80], [6, 0x6d060a0], [7, 0x6de7a00], [1, 0x6c179c0], [2, 0x6cf2a20], [3, 0x6c23180], [4, 0x6c9bf40], [5, 0x6e53ea0], [6, 0x6d4c0c0], [7, 0x6e2da20], [1, 0x6c5d9e0], [2, 0x6d38a40], [3, 0x6c691a0]]}
  layer.0.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6ce1f60]]}
  layer.0.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d7ea60]]}
  layer.0.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6caf1c0]]}
  layer.1.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6ceab80], [5, 0x6e9a340], [6, 0x6d92560], [7, 0x6e754a0]]}
  layer.1.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6c0eda0]]}
  layer.1.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6cb7de0], [4, 0x6d30ba0], [5, 0x6ee0360], [6, 0x6dd8580]]}
  layer.1.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6ebb4c0]]}
  layer.1.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6cfde00], [4, 0x6d76bc0], [5, 0x6f26380], [6, 0x6e1e5a0]]}
  layer.1.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6ec40e0]]}
  layer.1.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6cb7100], [2, 0x6d99320], [3, 0x6d43e20], [4, 0x6dbcbe0]]}
  layer.1.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6f6c3a0]]}
  layer.1.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x71927e0]]}
  layer.1.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x706a600]]}
  layer.1.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7145660], [3, 0x7161d60], [4, 0x7107540], [5, 0x7292fe0], [6, 0x7145660], [7, 0x719b400], [1, 0x7073220], [2, 0x718b680], [3, 0x71a7d80], [4, 0x714d560], [5, 0x72d9000], [6, 0x718b680], [7, 0x71e1420], [1, 0x70b9240], [2, 0x71d16a0], [3, 0x71edda0]]}
  layer.1.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7193580], [5, 0x731f020]]}
  layer.1.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x71d16a0], [7, 0x7227440], [1, 0x70ff260], [2, 0x72176c0], [3, 0x7233dc0], [4, 0x71a4da0], [5, 0x7330840], [6, 0x72176c0], [7, 0x726d460], [1, 0x7145280], [2, 0x725d6e0], [3, 0x7279de0], [4, 0x71eadc0], [5, 0x7376860], [6, 0x725d6e0], [7, 0x72b3480]]}
  layer.1.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x718b2a0]]}
  layer.1.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x72a3700]]}
  layer.1.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x72f94a0]]}
  layer.2.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7193ec0], [2, 0x72ac320], [3, 0x72c0280], [4, 0x7232840]]}
  layer.2.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x73be2e0]]}
  layer.2.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x72ac320], [7, 0x73020c0], [1, 0x71d9ee0], [2, 0x72f2340]]}
  layer.2.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x73062a0]]}
  layer.2.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6f1cb60], [5, 0x70dbc80], [6, 0x6f96aa0], [7, 0x6fede20]]}
  layer.2.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f36260]]}
  layer.2.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6fd28e0], [3, 0x6fbbdc0], [4, 0x6f62b80], [5, 0x7121ca0]]}
  layer.2.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6fdcac0]]}
  layer.2.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6fc9cc0]]}
  layer.2.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7167cc0]]}
  layer.2.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6fe56e0], [7, 0x70342c0], [1, 0x6f3f300], [2, 0x701a360], [3, 0x7003840], [4, 0x6fa9020], [5, 0x71708e0], [6, 0x702b700], [7, 0x707a2e0], [1, 0x6f85320], [2, 0x7060380], [3, 0x7049860], [4, 0x6fef040], [5, 0x71b6900], [6, 0x7071720], [7, 0x70c0300]]}
  layer.2.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6fcb340], [2, 0x70a63a0]]}
  layer.2.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x708f880], [4, 0x7035060], [5, 0x71fc920], [6, 0x70b7740], [7, 0x7106320], [1, 0x6fdcb60], [2, 0x70b7bc0], [3, 0x70d58a0], [4, 0x707b080], [5, 0x7242940], [6, 0x70fd760], [7, 0x714c340], [1, 0x7022b80], [2, 0x70fdbe0], [3, 0x711b8c0], [4, 0x70c10a0]]}
  layer.2.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7288960]]}
  layer.2.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6cae840]]}
  layer.2.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x67135c0]]}
  layer.3.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x67bf5a0], [5, 0x690c660], [6, 0x68bdec0], [7, 0x6969180]]}
  layer.3.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x684f760]]}
  layer.3.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67ea4a0], [3, 0x671c1e0], [4, 0x68055c0], [5, 0x6952680]]}
  layer.3.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6903ee0]]}
  layer.3.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68304c0], [3, 0x6762200], [4, 0x684b5e0], [5, 0x69986a0]]}
  layer.3.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67e1880]]}
  layer.3.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x69af620], [1, 0x6858800], [2, 0x68764e0], [3, 0x67a8220]]}
  layer.3.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6891600]]}
  layer.3.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68bc500]]}
  layer.3.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x67ee240]]}
  layer.3.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x689a220], [5, 0x69deb40], [6, 0x690d400], [7, 0x69f70a0], [1, 0x68a0280], [2, 0x68c5120], [3, 0x67f6e60], [4, 0x68e0240], [5, 0x6a24b60], [6, 0x6953420], [7, 0x6a3d0c0], [1, 0x68e62a0], [2, 0x690b140], [3, 0x683ce80], [4, 0x6926260], [5, 0x6a6ab80]]}
  layer.3.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6908d20], [1, 0x67f7aa0]]}
  layer.3.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x682e0a0], [1, 0x671ce20], [2, 0x66f53c0], [3, 0x65fac40], [4, 0x66a6c20], [5, 0x67acb60], [6, 0x679b7c0], [7, 0x68740c0], [1, 0x6762e40], [2, 0x673b3e0], [3, 0x6640c60], [4, 0x66ecc40], [5, 0x67f2b80], [6, 0x67e17e0], [7, 0x68ba0e0], [1, 0x67a8e60]]}
  layer.3.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6781400]]}
  layer.3.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6900100]]}
  layer.3.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x67eee80]]}
  layer.4.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x678a020], [3, 0x6687100], [4, 0x67330e0], [5, 0x683a600]]}
  layer.4.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6829260]]}
  layer.4.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x65b4c20], [4, 0x6660c00], [5, 0x6766b40], [6, 0x67557a0]]}
  layer.4.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67d0040]]}
  layer.4.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6880620], [6, 0x6831e80], [7, 0x691a540], [1, 0x68092c0]]}
  layer.4.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67d8c60]]}
  layer.4.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x66cd5a0], [4, 0x6779580], [5, 0x68c6640], [6, 0x6877ea0]]}
  layer.4.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6960560]]}
  layer.4.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6a27860]]}
  layer.4.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6aa0620]]}
  layer.4.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6c23920], [6, 0x6b507a0], [7, 0x6c76260], [1, 0x6adae80], [2, 0x6affd20], [3, 0x6a30480], [4, 0x6aa9240], [5, 0x6c69940], [6, 0x6b967c0], [7, 0x6cbc280], [1, 0x6b20ea0], [2, 0x6b45d40], [3, 0x6a764a0], [4, 0x6aef260], [5, 0x6caf960], [6, 0x6bdc7e0]]}
  layer.4.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6d022a0], [1, 0x6b66ec0]]}
  layer.4.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b8bd60], [3, 0x6abc4c0], [4, 0x6b35280], [5, 0x6cf5980], [6, 0x6c22800], [7, 0x6d13ac0], [1, 0x6b786e0], [2, 0x6bd1d80], [3, 0x6b024e0], [4, 0x6b7b2a0], [5, 0x6d3b9a0], [6, 0x6c68820], [7, 0x6d59ae0], [1, 0x6bbe700], [2, 0x6c17da0], [3, 0x6b48500]]}
  layer.4.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6bc12c0]]}
  layer.4.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c5ddc0]]}
  layer.4.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6b8e520]]}
  layer.5.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6bc9ee0], [5, 0x6d81e40], [6, 0x6cb7460], [7, 0x6da1560]]}
  layer.5.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6c06180]]}
  layer.5.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c669e0], [3, 0x6b97140], [4, 0x6c0ff00], [5, 0x6dc7e60]]}
  layer.5.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6cfd480]]}
  layer.5.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6a830e0], [1, 0x692c2c0], [2, 0x6951160], [3, 0x6882ea0]]}
  layer.5.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x696c280]]}
  layer.5.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6ab0ba0], [6, 0x69a2060], [7, 0x6ac9100], [1, 0x69722e0]]}
  layer.5.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6997180]]}
  layer.5.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6b0f120]]}
  layer.5.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6999440]]}
  layer.5.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x699fda0], [3, 0x68c9340], [4, 0x6975320], [5, 0x6af8620], [6, 0x69e9ae0], [7, 0x6b17d40], [1, 0x69b8780], [2, 0x69e5dc0], [3, 0x690f360], [4, 0x69bb340], [5, 0x6b3e640], [6, 0x6a2fb00], [7, 0x6b5dd60], [1, 0x69fe7a0], [2, 0x6a2bde0], [3, 0x6955380]]}
  layer.5.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6a01360], [5, 0x6b84660]]}
  layer.5.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6a75b20], [7, 0x6ba3d80], [1, 0x6a447c0], [2, 0x6a71e00], [3, 0x699b3a0], [4, 0x6a12b80], [5, 0x6b95e80], [6, 0x6abbb40], [7, 0x6be9da0], [1, 0x6a8a7e0], [2, 0x6ab7e20], [3, 0x69e13c0], [4, 0x6a58ba0], [5, 0x6bdbea0], [6, 0x6b01b60], [7, 0x6c2fdc0]]}
  layer.5.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ad0800]]}
  layer.5.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6b47b80]]}
  layer.5.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7a40440]]}
  layer.6.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x78aa680], [5, 0x79b61a0], [6, 0x78a2c00], [7, 0x7934360]]}
  layer.6.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7a1cb80]]}
  layer.6.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7ae3820], [5, 0x7b7d2e0], [6, 0x7a8da60], [7, 0x7ae1dc0]]}
  layer.6.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x79c4ae0]]}
  layer.6.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7b29840], [5, 0x7bc3300], [6, 0x7ad3a80], [7, 0x7b27de0]]}
  layer.6.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x79cd700]]}
  layer.6.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7b372c0], [6, 0x7a47a40], [7, 0x7a9bda0], [1, 0x797eac0]]}
  layer.6.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7a25c20]]}
  layer.6.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x79d6320]]}
  layer.6.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7a52100]]}
  layer.6.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7a2e840], [4, 0x7b6fce0], [5, 0x7c097a0], [6, 0x7b1b500], [7, 0x7b6f860], [1, 0x79def40], [2, 0x7a5ad20], [3, 0x7a74860], [4, 0x7bb5d00], [5, 0x7c4f7c0], [6, 0x7b61520], [7, 0x7bb5880], [1, 0x7a24f60], [2, 0x7aa0d40], [3, 0x7aba880], [4, 0x7bfbd20]]}
  layer.6.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7c957e0], [6, 0x7ba7540]]}
  layer.6.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x78fbee0], [4, 0x79cb320], [5, 0x7a99a40], [6, 0x7975560], [7, 0x79c98c0], [1, 0x7866a40], [2, 0x79283c0], [3, 0x7941f00], [4, 0x7a11340], [5, 0x7adfa60], [6, 0x79bb580], [7, 0x7a0f8e0], [1, 0x78aca60], [2, 0x796e3e0], [3, 0x7987f20], [4, 0x7a57360]]}
  layer.6.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7a4ae00]]}
  layer.6.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x78f32c0]]}
  layer.6.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x79c2700]]}
  layer.7.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7a53a20], [6, 0x792f540], [7, 0x79838a0], [1, 0x7820a20]]}
  layer.7.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x791f7a0]]}
  layer.7.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x77d8fa0], [2, 0x78d7d20], [3, 0x78ad2a0], [4, 0x797c6e0]]}
  layer.7.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7b25a80]]}
  layer.7.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x78f2a80], [2, 0x79b4400], [3, 0x79cdf40], [4, 0x7a9d380]]}
  layer.7.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7b2e6a0]]}
  layer.7.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7a01a20], [7, 0x7a55d80], [1, 0x7938aa0], [2, 0x79fa420]]}
  layer.7.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7a13f60]]}
  layer.7.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7e0e5c0]]}
  layer.7.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7d2bae0]]}
  layer.7.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7d77240], [1, 0x7c668a0], [2, 0x7cd2d20], [3, 0x7c72060], [4, 0x7de6b80], [5, 0x7e171e0], [6, 0x7d34700], [7, 0x7dbd260], [1, 0x7cac8c0], [2, 0x7d18d40], [3, 0x7cb8080], [4, 0x7e2cba0], [5, 0x7e5d200], [6, 0x7d7a720], [7, 0x7e03280], [1, 0x7cf28e0]]}
  layer.7.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7d5ed60], [3, 0x7cfe0a0]]}
  layer.7.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7e72bc0], [5, 0x7ea3220], [6, 0x7dc0740], [7, 0x7e492a0], [1, 0x7d38900], [2, 0x7d70580], [3, 0x7d0f8c0], [4, 0x7eb8be0], [5, 0x7ee9240], [6, 0x7e06760], [7, 0x7e8f2c0], [1, 0x7d7e920], [2, 0x7db65a0], [3, 0x7d558e0], [4, 0x7efec00], [5, 0x7f2f260]]}
  layer.7.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7e4c780]]}
  layer.7.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7d9b900]]}
  layer.7.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7f44c20]]}
  layer.8.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7f75280], [6, 0x7e553a0], [7, 0x7ed5760], [1, 0x7dc63a0]]}
  layer.8.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7dfe020]]}
  layer.8.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7da4520], [4, 0x7f4d840], [5, 0x7fbb2a0], [6, 0x7e9b3c0]]}
  layer.8.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7f1b780]]}
  layer.8.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7a6af80], [2, 0x7ae6d60], [3, 0x7b008a0], [4, 0x7c41d40]]}
  layer.8.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7ca7000]]}
  layer.8.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7bb8d60], [7, 0x7c044c0], [1, 0x7ab0fa0], [2, 0x7b2cd80]]}
  layer.8.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7b468c0]]}
  layer.8.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7af6fc0]]}
  layer.8.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7bfb8a0]]}
  layer.8.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7c881e0], [5, 0x7cb00a0], [6, 0x7c007e0], [7, 0x7c4bf40], [1, 0x7affbe0], [2, 0x7b74800], [3, 0x7b50f40], [4, 0x7cce200], [5, 0x7cf60c0], [6, 0x7c46800], [7, 0x7c91f60], [1, 0x7b45c00], [2, 0x7bba820], [3, 0x7b96f60], [4, 0x7d14220], [5, 0x7d3c0e0]]}
  layer.8.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7c8c820], [7, 0x7cd7f80]]}
  layer.8.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7b8bc20], [2, 0x7c00840], [3, 0x7bdcf80], [4, 0x7d5a240], [5, 0x7d82100], [6, 0x7c9e040], [7, 0x7ce97a0], [1, 0x7bd1c40], [2, 0x7c46860], [3, 0x7c22fa0], [4, 0x7da0260], [5, 0x7dc8120], [6, 0x7ce4060], [7, 0x7d2f7c0], [1, 0x7c17c60], [2, 0x7c8c880]]}
  layer.8.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7c68fc0]]}
  layer.8.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7a494e0]]}
  layer.8.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7c5dc80]]}
  layer.9.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7459480], [7, 0x74a6a80], [1, 0x7342a80], [2, 0x745aee0]]}
  layer.9.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7476000]]}
  layer.9.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x742e160], [5, 0x75b9c00], [6, 0x749f4a0], [7, 0x74ecaa0]]}
  layer.9.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7388aa0]]}
  layer.9.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7474180], [5, 0x75ffc20], [6, 0x74e54c0], [7, 0x7532ac0]]}
  layer.9.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x73916c0]]}
  layer.9.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7414ec0], [3, 0x742ffe0], [4, 0x73e8140], [5, 0x7573be0]]}
  layer.9.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x74ba1a0]]}
  layer.9.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74a2de0]]}
  layer.9.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7480b00]]}
  layer.9.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x74c2dc0], [5, 0x76460c0], [6, 0x752b960], [7, 0x757a540], [1, 0x739bd40], [2, 0x74aba00], [3, 0x7489720], [4, 0x7508de0], [5, 0x768c0e0], [6, 0x7571980], [7, 0x75c0560], [1, 0x73e1d60], [2, 0x74f1a20], [3, 0x74cf740], [4, 0x754ee00], [5, 0x76d2100]]}
  layer.9.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x75b79a0], [7, 0x7606580]]}
  layer.9.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x738e580], [1, 0x7267980], [2, 0x73429e0], [3, 0x735db00], [4, 0x72d00c0], [5, 0x745bb60], [6, 0x737e800], [7, 0x73d45a0], [1, 0x72ad9a0], [2, 0x7388a00], [3, 0x73a3b20], [4, 0x73160e0], [5, 0x74a1b80], [6, 0x73c4820], [7, 0x741a5c0], [1, 0x72f39c0]]}
  layer.9.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x73c6f00]]}
  layer.9.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x730eec0]]}
  layer.9.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7281480]]}
  layer.10.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x73cfb20], [6, 0x72f27c0], [7, 0x7348560], [1, 0x7221960]]}
  layer.10.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7339dc0]]}
  layer.10.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7317ae0], [4, 0x728a0a0], [5, 0x7415b40], [6, 0x73387e0]]}
  layer.10.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7278860]]}
  layer.10.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x735c100], [5, 0x74e7ba0], [6, 0x740a840], [7, 0x74605e0]]}
  layer.10.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x73399e0]]}
  layer.10.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x73ceea0], [3, 0x73e9fc0], [4, 0x73a2120], [5, 0x752dbc0]]}
  layer.10.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7450860]]}
  layer.10.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x77cd6a0]]}
  layer.10.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7660aa0]]}
  layer.10.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x772abc0], [3, 0x76c2d40], [4, 0x7780de0], [5, 0x788c900], [6, 0x7744b60], [7, 0x77d62c0], [1, 0x76696c0], [2, 0x7770be0], [3, 0x7708d60], [4, 0x77c6e00], [5, 0x78d2920], [6, 0x778ab80], [7, 0x781c2e0], [1, 0x76af6e0], [2, 0x77b6c00], [3, 0x774ed80]]}
  layer.10.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x780ce20], [5, 0x7918940]]}
  layer.10.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x77d0ba0], [7, 0x7862300], [1, 0x76f5700], [2, 0x77fcc20], [3, 0x7794da0], [4, 0x781e640], [5, 0x792a160], [6, 0x7816bc0], [7, 0x78a8320], [1, 0x773b720], [2, 0x7842c40], [3, 0x77dadc0], [4, 0x7864660], [5, 0x7970180], [6, 0x785cbe0], [7, 0x78ee340]]}
  layer.10.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7781740]]}
  layer.10.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x778a360]]}
  layer.10.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x78890e0]]}
  layer.11.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7821260], [4, 0x78f06a0], [5, 0x79fc1c0], [6, 0x78e8c20]]}
  layer.11.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x797a380]]}
  layer.11.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7792f80], [2, 0x7891d00], [3, 0x7867280], [4, 0x79366c0]]}
  layer.11.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7a421e0]]}
  layer.11.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7502a00], [2, 0x7609f20], [3, 0x75aa840], [4, 0x76688e0]]}
  layer.11.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x75a17a0]]}
  layer.11.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7620e60], [5, 0x77a4160], [6, 0x7655200], [7, 0x76a3de0]]}
  layer.11.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74f9de0]]}
  layer.11.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x769b220]]}
  layer.11.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x76e9e00]]}
  layer.11.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7427d80], [2, 0x7537a40], [3, 0x7515760], [4, 0x7594e20], [5, 0x7718120], [6, 0x75c91c0], [7, 0x7617da0], [1, 0x746dda0], [2, 0x757da60], [3, 0x755b780], [4, 0x75dae40], [5, 0x775e140], [6, 0x760f1e0], [7, 0x765ddc0], [1, 0x74b3dc0], [2, 0x75c3a80]]}
  layer.11.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x77ebbe0], [6, 0x76a3e40]]}
  layer.11.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x76f2a20], [1, 0x7548a20], [2, 0x764ff40], [3, 0x75f0860], [4, 0x76ae900], [5, 0x77fd400], [6, 0x76b5660], [7, 0x7738a40], [1, 0x758ea40], [2, 0x7695f60], [3, 0x7636880], [4, 0x76f4920], [5, 0x7843420], [6, 0x76fb680], [7, 0x777ea60], [1, 0x75d4a60]]}
  layer.11.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x76dbf80]]}
  layer.11.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x77c4a80]]}
  layer.11.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x72a3700]]}
  layer.12.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x761aa80], [2, 0x76e4ba0], [3, 0x767cd20], [4, 0x773adc0]]}
  layer.12.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5470f80]]}
  layer.12.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x557a960], [4, 0x55393c0], [5, 0x55aafc0], [6, 0x56bd840]]}
  layer.12.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x570bfe0]]}
  layer.12.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x55c0980], [4, 0x557f3e0], [5, 0x55f0fe0], [6, 0x5703860]]}
  layer.12.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5714c00]]}
  layer.12.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5682780], [2, 0x56ad200], [3, 0x56069a0], [4, 0x55c5400]]}
  layer.12.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x55a23a0]]}
  layer.12.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x560b420]]}
  layer.12.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5638a60]]}
  layer.12.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x574b2e0], [7, 0x571dca0], [1, 0x56c8c20], [2, 0x56f4c80], [3, 0x564e420], [4, 0x5614040], [5, 0x5641680], [6, 0x5791300], [7, 0x5763cc0], [1, 0x570ec40], [2, 0x573aca0], [3, 0x5694440], [4, 0x565a060], [5, 0x56876a0], [6, 0x57d7320], [7, 0x57a9ce0]]}
  layer.12.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5754c60], [2, 0x5780cc0]]}
  layer.12.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x56da460], [4, 0x56a0080], [5, 0x56cd6c0], [6, 0x581d340], [7, 0x57efd00], [1, 0x5766480], [2, 0x57924e0], [3, 0x5720480], [4, 0x56e60a0], [5, 0x57136e0], [6, 0x5863360], [7, 0x5835d20], [1, 0x57ac4a0], [2, 0x57d8500], [3, 0x57664a0], [4, 0x572c0c0]]}
  layer.12.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5527700]]}
  layer.12.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55eb340]]}
  layer.12.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5561660]]}
  layer.13.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x558c0e0], [3, 0x54a8480], [4, 0x54e16e0], [5, 0x5515ee0]]}
  layer.13.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5628760]]}
  layer.13.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55f3f60], [1, 0x556a280], [2, 0x55d2100], [3, 0x54ee4a0]]}
  layer.13.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x55834c0]]}
  layer.13.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5639f80], [1, 0x55b02a0], [2, 0x5618120], [3, 0x55344c0]]}
  layer.13.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5530320]]}
  layer.13.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x555c380], [6, 0x5631800], [7, 0x567ffa0], [1, 0x55f62c0]]}
  layer.13.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x565e140]]}
  layer.13.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5998460]]}
  layer.13.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59912a0]]}
  layer.13.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5926400], [4, 0x59279e0], [5, 0x5955020], [6, 0x5a57f40], [7, 0x59e4d60], [1, 0x59a1080], [2, 0x5999ec0], [3, 0x596c420], [4, 0x596da00], [5, 0x599b040], [6, 0x5a9df60], [7, 0x5a2ad80], [1, 0x59e70a0], [2, 0x59dfee0], [3, 0x59b2440], [4, 0x59b3a20]]}
  layer.13.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x59e1060], [6, 0x5ae3f80]]}
  layer.13.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a70da0], [1, 0x5a2d0c0], [2, 0x5a25f00], [3, 0x59f8460], [4, 0x59f9a40], [5, 0x59f2880], [6, 0x5af57a0], [7, 0x5ab6dc0], [1, 0x5a730e0], [2, 0x5a6bf20], [3, 0x5a3e480], [4, 0x5a3fa60], [5, 0x5a388a0], [6, 0x5b3b7c0], [7, 0x5afcde0], [1, 0x5ab9100]]}
  layer.13.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ab1f40]]}
  layer.13.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5abab60]]}
  layer.13.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5a84920]]}
  layer.14.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a85f00], [5, 0x5ac48e0], [6, 0x5bc7800], [7, 0x5b88e20]]}
  layer.14.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5b45140]]}
  layer.14.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ac3780], [3, 0x5a8d540], [4, 0x5acbf20], [5, 0x5b0a900]]}
  layer.14.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c0d820]]}
  layer.14.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5834380], [6, 0x597b860], [7, 0x5910e20], [1, 0x58c5f80]]}
  layer.14.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5907d80]]}
  layer.14.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x587e500], [2, 0x58aa560], [3, 0x5838500], [4, 0x57fe120]]}
  layer.14.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x582b760]]}
  layer.14.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x587e520]]}
  layer.14.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5844140]]}
  layer.14.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5759700], [6, 0x58a9380], [7, 0x587bd40], [1, 0x57f24c0], [2, 0x581e520], [3, 0x57ac4c0], [4, 0x57720e0], [5, 0x579f720], [6, 0x58ef3a0], [7, 0x58c1d60], [1, 0x58384e0], [2, 0x5864540], [3, 0x57f24e0], [4, 0x57b8100], [5, 0x57e5740], [6, 0x59353c0]]}
  layer.14.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x58f1fe0], [3, 0x5887140]]}
  layer.14.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x584cd60], [5, 0x587a3a0], [6, 0x59c1880], [7, 0x5956e40], [1, 0x590bfa0], [2, 0x5903800], [3, 0x5898960], [4, 0x5892d80], [5, 0x58c03c0], [6, 0x5a078a0], [7, 0x599ce60], [1, 0x5951fc0], [2, 0x5949820], [3, 0x58de980], [4, 0x58d8da0], [5, 0x59063e0]]}
  layer.14.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a4d8c0]]}
  layer.14.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x591edc0]]}
  layer.14.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x594c400]]}
  layer.15.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a7e8c0], [6, 0x5b817e0], [7, 0x5b42e00], [1, 0x5aff120]]}
  layer.15.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x50085e0]]}
  layer.15.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x508b580], [7, 0x508b580], [1, 0x508ba00], [2, 0x5083260]]}
  layer.15.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5011200]]}
  layer.15.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x50d15a0], [7, 0x50d15a0], [1, 0x50d1a20], [2, 0x50c9280]]}
  layer.15.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5019e20]]}
  layer.15.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x50478c0], [5, 0x500a4c0], [6, 0x51175c0], [7, 0x51175c0]]}
  layer.15.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x503e820]]}
  layer.15.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x515d5e0]]}
  layer.15.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x515d5e0]]}
  layer.15.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5120660], [2, 0x510f720], [3, 0x5022ec0], [4, 0x508f340], [5, 0x5051f40], [6, 0x5166200], [7, 0x5166200], [1, 0x5166680], [2, 0x5155740], [3, 0x5068ee0], [4, 0x50d5360], [5, 0x5097f60], [6, 0x51ac220], [7, 0x51ac220], [1, 0x51ac6a0], [2, 0x519b760]]}
  layer.15.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x50aef00], [4, 0x511b380]]}
  layer.15.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x50ddf80], [6, 0x51f2240], [7, 0x51f2240], [1, 0x51f26c0], [2, 0x51e1780], [3, 0x50c0720], [4, 0x512cba0], [5, 0x5123fa0], [6, 0x5238260], [7, 0x5238260], [1, 0x52386e0], [2, 0x52277a0], [3, 0x5106740], [4, 0x5172bc0], [5, 0x5169fc0], [6, 0x527e280]]}
  layer.15.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5117a40]]}
  layer.15.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  layer.15.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  layer.16.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0d80], [2, 0x4fb0d80], [3, 0x4fb0d80], [4, 0x4fb2360]]}
  layer.16.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb2360]]}
  layer.16.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb9520], [7, 0x4fb9520], [1, 0x4ff6da0], [2, 0x4ff6da0]]}
  layer.16.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4ff6da0]]}
  layer.16.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fff540], [7, 0x4fff540], [1, 0x503cdc0], [2, 0x503cdc0]]}
  layer.16.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fff9c0]]}
  layer.16.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4ff8800], [5, 0x4fbb400], [6, 0x5045560], [7, 0x5045560]]}
  layer.16.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5082de0]]}
  layer.16.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x52f2700]]}
  layer.16.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x532b960]]}
  layer.16.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5329f20], [6, 0x5479ba0], [7, 0x5479ba0], [1, 0x542d2c0], [2, 0x53d67e0], [3, 0x52fb320], [4, 0x5334580], [5, 0x536ff40], [6, 0x54bfbc0], [7, 0x54bfbc0], [1, 0x54732e0], [2, 0x541c800], [3, 0x5341340], [4, 0x537a5a0], [5, 0x53b5f60], [6, 0x5505be0]]}
  layer.16.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5505be0], [1, 0x54b9300]]}
  layer.16.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5462820], [3, 0x5387360], [4, 0x53c05c0], [5, 0x53fbf80], [6, 0x554bc00], [7, 0x5517400], [1, 0x54cab20], [2, 0x54a8840], [3, 0x53cd380], [4, 0x54065e0], [5, 0x5441fa0], [6, 0x5591c20], [7, 0x555d420], [1, 0x5510b40], [2, 0x54ee860], [3, 0x54133a0]]}
  layer.16.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x544c600]]}
  layer.16.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5534880]]}
  layer.16.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x54593c0]]}
  layer.17.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5455220], [5, 0x5488440], [6, 0x55d80c0], [7, 0x55a4ea0]]}
  layer.17.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x55585c0]]}
  layer.17.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x553d4a0], [3, 0x5461fe0], [4, 0x549b240], [5, 0x54ce460]]}
  layer.17.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x561e0e0]]}
  layer.17.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5358f00], [1, 0x5350be0], [2, 0x53028a0], [3, 0x5220220]]}
  layer.17.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x52f9800]]}
  layer.17.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x51d87a0], [4, 0x5244c20], [5, 0x523c020], [6, 0x53502e0]]}
  layer.17.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x53502e0]]}
  layer.17.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5282040]]}
  layer.17.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5396300]]}
  layer.17.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x527e280], [1, 0x527e700], [2, 0x526d7c0], [3, 0x514c760], [4, 0x51b8be0], [5, 0x51affe0], [6, 0x52c42a0], [7, 0x52c42a0], [1, 0x52c4720], [2, 0x52b37e0], [3, 0x5192780], [4, 0x51fec00], [5, 0x51f6000], [6, 0x530a2c0], [7, 0x530a2c0], [1, 0x530a740]]}
  layer.17.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x528c6a0], [5, 0x528ac60]]}
  layer.17.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x539ef20], [7, 0x539ef20], [1, 0x5396c00], [2, 0x53488c0], [3, 0x5266240], [4, 0x529dec0], [5, 0x529c480], [6, 0x53e4f40], [7, 0x53e4f40], [1, 0x53dcc20], [2, 0x538e8e0], [3, 0x52ac260], [4, 0x52e3ee0], [5, 0x52e24a0], [6, 0x542af60], [7, 0x542af60]]}
  layer.17.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5422c40]]}
  layer.17.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5470f80]]}
  layer.17.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5001420]]}
  layer.18.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6683740], [7, 0x6799440], [1, 0x6690960], [2, 0x66a6300]]}
  layer.18.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x614f5e0]]}
  layer.18.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6108000], [5, 0x618dfc0], [6, 0x61afe40], [7, 0x62d54a0]]}
  layer.18.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6259b40]]}
  layer.18.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x614e020], [5, 0x61d3fe0], [6, 0x61f5e60], [7, 0x631b4c0]]}
  layer.18.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6262760]]}
  layer.18.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6229960], [3, 0x6158680], [4, 0x6194040], [5, 0x621a000]]}
  layer.18.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x62208c0]]}
  layer.18.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6260020]]}
  layer.18.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x623d8e0]]}
  layer.18.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6362f40], [1, 0x626b800], [2, 0x626fe00], [3, 0x61a0100], [4, 0x61dbac0], [5, 0x6268c40], [6, 0x6246500], [7, 0x63a8f60], [1, 0x62b1820], [2, 0x62b5e20], [3, 0x61e6120], [4, 0x6221ae0], [5, 0x62aec60], [6, 0x628c520], [7, 0x63eef80], [1, 0x62f7840]]}
  layer.18.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x62fbe40], [3, 0x622c140]]}
  layer.18.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6267b00], [5, 0x62f4c80], [6, 0x62d2540], [7, 0x6434fa0], [1, 0x633d860], [2, 0x630d660], [3, 0x623d960], [4, 0x62adb20], [5, 0x633aca0], [6, 0x6318560], [7, 0x647afc0], [1, 0x6383880], [2, 0x6353680], [3, 0x6283980], [4, 0x62f3b40], [5, 0x6380cc0]]}
  layer.18.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6247e80]]}
  layer.18.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x602d380]]}
  layer.18.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60b3340]]}
  layer.19.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x60d51c0], [7, 0x6202fc0], [1, 0x6201e60], [2, 0x6194400]]}
  layer.19.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6100520]]}
  layer.19.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6035fa0], [5, 0x60bbf60], [6, 0x611b1e0], [7, 0x6248fe0]]}
  layer.19.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x60cc5a0]]}
  layer.19.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x607bfc0], [5, 0x6101f80], [6, 0x6161200], [7, 0x628f000]]}
  layer.19.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6250aa0]]}
  layer.19.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x61da8a0], [3, 0x61095c0], [4, 0x60c1fe0], [5, 0x6147fa0]]}
  layer.19.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x61a7220]]}
  layer.19.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x653f640]]}
  layer.19.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x643c720]]}
  layer.19.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64b3aa0], [5, 0x657c5e0], [6, 0x6559ea0], [7, 0x666fba0], [1, 0x65328c0], [2, 0x6548260], [3, 0x6445340], [4, 0x64f9ac0], [5, 0x65c2600], [6, 0x659fec0], [7, 0x66b5bc0], [1, 0x65788e0], [2, 0x658e280], [3, 0x648b360], [4, 0x653fae0], [5, 0x6608620]]}
  layer.19.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x65e5ee0], [7, 0x66fbbe0]]}
  layer.19.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65be900], [2, 0x65d42a0], [3, 0x64d1380], [4, 0x6585b00], [5, 0x664e640], [6, 0x65f7700], [7, 0x670d400], [1, 0x6604920], [2, 0x661a2c0], [3, 0x65173a0], [4, 0x65cbb20], [5, 0x6694660], [6, 0x663d720], [7, 0x6753420], [1, 0x664a940], [2, 0x66602e0]]}
  layer.19.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x655d3c0]]}
  layer.19.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6565fe0]]}
  layer.19.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6611fc0]]}
  layer.20.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x66dab00], [6, 0x66c9760], [7, 0x67df460], [1, 0x66d6980]]}
  layer.20.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x66ec320]]}
  layer.20.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x656ec00], [4, 0x661abe0], [5, 0x6720b20], [6, 0x670f780]]}
  layer.20.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6825480]]}
  layer.20.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6439200], [7, 0x65934c0], [1, 0x645e980], [2, 0x646d160]]}
  layer.20.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64558e0]]}
  layer.20.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x64256e0], [3, 0x63559e0], [4, 0x63c5ba0], [5, 0x6452d20]]}
  layer.20.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x64305e0]]}
  layer.20.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x640bbc0]]}
  layer.20.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6498d40]]}
  layer.20.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x635e580], [7, 0x64c0fe0], [1, 0x63c98a0], [2, 0x63996a0], [3, 0x62c99a0], [4, 0x6339b60], [5, 0x63c6ce0], [6, 0x63a45a0], [7, 0x6507000], [1, 0x640f8c0], [2, 0x63df6c0], [3, 0x630f9c0], [4, 0x637fb80], [5, 0x640cd00], [6, 0x63ea5c0], [7, 0x654d020]]}
  layer.20.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x639d460], [4, 0x64147e0]]}
  layer.20.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x64a1960], [6, 0x647f220], [7, 0x65d94e0], [1, 0x64a49a0], [2, 0x64b3180], [3, 0x63aec80], [4, 0x6426000], [5, 0x64e7980], [6, 0x64c5240], [7, 0x661f500], [1, 0x64ea9c0], [2, 0x64f91a0], [3, 0x63f4ca0], [4, 0x646c020], [5, 0x652d9a0], [6, 0x650b260]]}
  layer.20.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6665520]]}
  layer.20.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x65739c0]]}
  layer.20.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6551280]]}
  layer.21.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x60c0940], [3, 0x6069e60], [4, 0x5fa0ec0], [5, 0x6064280]]}
  layer.21.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c66f60]]}
  layer.21.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5ca9f40], [1, 0x5c28e60], [2, 0x5bed040], [3, 0x5ba5ec0]]}
  layer.21.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b6a0a0]]}
  layer.21.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5ceff60], [1, 0x5c6ee80], [2, 0x5c33060], [3, 0x5bebee0]]}
  layer.21.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b72cc0]]}
  layer.21.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5be78e0], [6, 0x5c70000], [7, 0x5d35f80], [1, 0x5cb4ea0]]}
  layer.21.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5bde840]]}
  layer.21.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5cfaec0]]}
  layer.21.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c7aae0]]}
  layer.21.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5c33960], [4, 0x5b7bd60], [5, 0x5c2dd80], [6, 0x5cb7a80], [7, 0x5d7da00], [1, 0x5d03ae0], [2, 0x5c83700], [3, 0x5c79980], [4, 0x5bc1d80], [5, 0x5c73da0], [6, 0x5cfdaa0], [7, 0x5dc3a20], [1, 0x5d49b00], [2, 0x5cc9720], [3, 0x5cbf9a0], [4, 0x5c07da0]]}
  layer.21.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cb9dc0], [6, 0x5d43ac0]]}
  layer.21.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5e09a40], [1, 0x5d8fb20], [2, 0x5d0f740], [3, 0x5d059c0], [4, 0x5c4ddc0], [5, 0x5ccb5e0], [6, 0x5d552e0], [7, 0x5e4fa60], [1, 0x5dd5b40], [2, 0x5d55760], [3, 0x5d4b9e0], [4, 0x5c93de0], [5, 0x5d11600], [6, 0x5d9b300], [7, 0x5e95a80], [1, 0x5e1bb60]]}
  layer.21.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b583e0]]}
  layer.21.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bcf2c0]]}
  layer.21.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5b4e1e0]]}
  layer.22.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5b123c0], [3, 0x5ad39e0], [4, 0x5b123c0], [5, 0x5b52380]]}
  layer.22.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c17ea0]]}
  layer.22.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bd7ee0], [1, 0x5b56e00], [2, 0x5b583e0], [3, 0x5b19a00]]}
  layer.22.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5b097a0]]}
  layer.22.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c1df00], [1, 0x5b9ce20], [2, 0x5b9e400], [3, 0x5b5fa20]]}
  layer.22.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b61000]]}
  layer.22.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b98820], [6, 0x5c20f40], [7, 0x5c63f20], [1, 0x5be2e40]]}
  layer.22.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5be4420]]}
  layer.22.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5efd5c0]]}
  layer.22.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f540a0]]}
  layer.22.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x60559e0], [1, 0x6017480], [2, 0x5f970a0], [3, 0x5f405c0], [4, 0x5e42e20], [5, 0x5f061e0], [6, 0x5f5ccc0], [7, 0x609ba00], [1, 0x605d4a0], [2, 0x5fdd0c0], [3, 0x5f865e0], [4, 0x5e88e40], [5, 0x5f4c200], [6, 0x5fa2ce0], [7, 0x60e1a20], [1, 0x60a34c0]]}
  layer.22.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x60230e0], [3, 0x5fcc600]]}
  layer.22.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ecee60], [5, 0x5f92220], [6, 0x5fe8d00], [7, 0x6127a40], [1, 0x60e94e0], [2, 0x6034900], [3, 0x5fdde20], [4, 0x5f14e80], [5, 0x5fd8240], [6, 0x602ed20], [7, 0x616da60], [1, 0x612f500], [2, 0x607a920], [3, 0x6023e40], [4, 0x5f5aea0], [5, 0x601e260]]}
  layer.22.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6074d40]]}
  layer.22.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x607d960]]}
  layer.22.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x61b3f00]]}
  layer.23.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x61759a0], [2, 0x6106960], [3, 0x60afe80], [4, 0x5fe6ee0]]}
  layer.23.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60aa2a0]]}
  layer.23.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6086580], [7, 0x61bcb20], [1, 0x61bb9c0], [2, 0x614c980]]}
  layer.23.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x60f5ea0]]}
  layer.23.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5e76400], [3, 0x5e63ee0], [4, 0x5d6eee0], [5, 0x5e2b0e0]]}
  layer.23.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d65e40]]}
  layer.23.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5de3660], [6, 0x5e6d360], [7, 0x5f67ae0], [1, 0x5eedbc0]]}
  layer.23.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5e6d7e0]]}
  layer.23.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5fadb00]]}
  layer.23.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5f33be0]]}
  layer.23.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5d9b780], [3, 0x5d91a00], [4, 0x5cd9e00], [5, 0x5d57620], [6, 0x5de1320], [7, 0x5edbaa0], [1, 0x5e61b80], [2, 0x5de17a0], [3, 0x5dd7a20], [4, 0x5d1fe20], [5, 0x5d9d640], [6, 0x5e27340], [7, 0x5f21ac0], [1, 0x5ea7ba0], [2, 0x5e277c0], [3, 0x5e1da40]]}
  layer.23.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5eb4de0], [7, 0x5fb6720]]}
  layer.23.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5f3c800], [2, 0x5ebc420], [3, 0x5ea9f00], [4, 0x5db4f00], [5, 0x5e71100], [6, 0x5ec6600], [7, 0x5fc7f40], [1, 0x5f82820], [2, 0x5f02440], [3, 0x5eeff20], [4, 0x5dfaf20], [5, 0x5eb7120], [6, 0x5f0c620], [7, 0x600df60], [1, 0x5fc8840], [2, 0x5f48460]]}
  layer.23.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f35f40]]}
  layer.23.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x600e860]]}
  layer.23.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f8e480]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6eccd00]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6e48c20]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6e63d80]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f3d800]]}
  dc.input_tensor.layernorm_38.4:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6f2dea0], [4, 0x6e8f0c0]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6e99ec0]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6d920e0]]}
  dc.input_tensor.layernorm_52.4:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6e73a40], [1, 0x6ca3a00]]}
  input_1_multiply_69_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6cb6c80]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d98ea0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f3d380]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x71070c0]]}
  dc.input_tensor.layernorm_91.4:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7291580], [6, 0x7143c00]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x71618e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x72bfe00]]}
  dc.input_tensor.layernorm_105.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7230de0], [5, 0x73bc880]]}
  input_1_multiply_122_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6fa8ba0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6fbb940]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7033e40]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f3ee80]]}
  dc.input_tensor.layernorm_144.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7018900], [3, 0x7001de0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7143780]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7192360]]}
  dc.input_tensor.layernorm_158.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7068ba0], [2, 0x7143c00]]}
  input_1_multiply_175_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x69af1a0]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6858380]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x69de6c0]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x690cf80]]}
  dc.input_tensor.layernorm_197.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x69f5640], [1, 0x689e820]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6686c80]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6732c60]]}
  dc.input_tensor.layernorm_211.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6838ba0], [6, 0x6827800]]}
  input_1_multiply_228_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x66cd120]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6779100]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x684f2e0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x690cb00]]}
  dc.input_tensor.layernorm_250.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6ad9420], [2, 0x6afe2c0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6d819c0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6c75de0]]}
  dc.input_tensor.layernorm_264.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6d9fb00], [1, 0x6c04720]]}
  input_1_multiply_281_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6de7580]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69b8300]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x68c8ec0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6974ea0]]}
  dc.input_tensor.layernorm_303.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6af6bc0], [6, 0x69e8080]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6afde40]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6a273e0]]}
  dc.input_tensor.layernorm_317.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6a9ebc0], [5, 0x6c21ec0]]}
  input_1_multiply_334_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7a49060]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7a257a0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7b6f860]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7c09320]]}
  dc.input_tensor.layernorm_356.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7b19aa0], [7, 0x7b6de00]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x792f0c0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7983420]]}
  dc.input_tensor.layernorm_370.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x781efc0], [2, 0x791dd40]]}
  input_1_multiply_387_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7a015a0]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7a55900]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7ae33a0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7de6700]]}
  dc.input_tensor.layernorm_409.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7b72da0], [3, 0x7b4f4e0]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7cd28a0]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ed52e0]]}
  dc.input_tensor.layernorm_423.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7dc4940], [2, 0x7dfc5c0]]}
  input_1_multiply_440_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7e0c3c0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7c71be0]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7c87d60]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7cafc20]]}
  dc.input_tensor.layernorm_462.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7bfed80], [7, 0x7c4a4e0]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7de6280]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7e0e140]]}
  dc.input_tensor.layernorm_476.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7d2a080], [7, 0x7d757e0]]}
  input_1_multiply_493_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74a0f00]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x747ec20]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7645c40]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x752b4e0]]}
  dc.input_tensor.layernorm_515.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7578ae0], [1, 0x739a2e0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x72f2340]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x73480e0]]}
  dc.input_tensor.layernorm_529.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x721ff00], [2, 0x7338360]]}
  input_1_multiply_546_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x73cea20]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x73e9b40]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x74a6600]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7342600]]}
  dc.input_tensor.layernorm_568.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74a1380], [3, 0x747f0a0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7888c60]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7820de0]]}
  dc.input_tensor.layernorm_582.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x788aea0], [6, 0x7743100]]}
  input_1_multiply_599_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x792ec40]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7982fa0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7609aa0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x75aa3c0]]}
  dc.input_tensor.layernorm_621.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7666e80], [5, 0x77ea180]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x767c8a0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x773a940]]}
  dc.input_tensor.layernorm_635.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7889440], [6, 0x77416a0]]}
  input_1_multiply_652_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5682300]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x56acd80]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x571d820]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x56c87a0]]}
  dc.input_tensor.layernorm_674.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x56f3220], [3, 0x564c9c0]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x54a8000]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x54e1260]]}
  dc.input_tensor.layernorm_688.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5514480], [6, 0x5626d00]]}
  input_1_multiply_705_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x555bf00]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5631380]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x557a4e0]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5538f40]]}
  dc.input_tensor.layernorm_727.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5637000], [6, 0x5749880]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5a844a0]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a85a80]]}
  dc.input_tensor.layernorm_741.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a564e0], [7, 0x59e3300]]}
  input_1_multiply_758_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bcee40]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5b4dd60]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x597b3e0]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x59109a0]]}
  dc.input_tensor.layernorm_780.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x58c4520], [2, 0x58f0580]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x59e2e80]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5997fe0]]}
  dc.input_tensor.layernorm_794.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x598f840], [3, 0x59249a0]]}
  input_1_multiply_811_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5047440]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x500a040]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x510f2a0]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5022a40]]}
  dc.input_tensor.layernorm_833.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x508d8e0], [5, 0x50504e0]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  dc.input_tensor.layernorm_847.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900], [5, 0x4fb0900]]}
  input_1_multiply_864_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4ff8380]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fbaf80]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5082de0]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  dc.input_tensor.layernorm_886.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55a3440], [1, 0x5556b60]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5487fc0]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x55d7c40]]}
  dc.input_tensor.layernorm_900.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x542b860], [2, 0x53d4d80]]}
  input_1_multiply_917_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55eaec0]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x55611e0]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5350760]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5302420]]}
  dc.input_tensor.layernorm_939.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x521e7c0], [4, 0x528ac40]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x53d4900]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x52f2280]]}
  dc.input_tensor.layernorm_953.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5329f00], [5, 0x53284c0]]}
  input_1_multiply_970_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x62294e0]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6158200]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x626b380]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x626f980]]}
  dc.input_tensor.layernorm_992.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x619e6a0], [4, 0x61da060]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6202b40]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62019e0]]}
  dc.input_tensor.layernorm_1006.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x61929a0], [3, 0x60feac0]]}
  input_1_multiply_1023_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x61da420]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6109140]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x62d5020]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62596c0]]}
  dc.input_tensor.layernorm_1045.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x623be80], [7, 0x63614e0]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6611b40]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x66da680]]}
  dc.input_tensor.layernorm_1059.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x666e140], [1, 0x6530e60]]}
  input_1_multiply_1076_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x671c9a0]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x66f4f40]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6593040]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x645e500]]}
  dc.input_tensor.layernorm_1098.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x646b700], [3, 0x639ba00]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65309e0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x653f1c0]]}
  dc.input_tensor.layernorm_1112.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x643acc0], [4, 0x64b2040]]}
  input_1_multiply_1129_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5be7460]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c6fb80]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b7b8e0]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c2d900]]}
  dc.input_tensor.layernorm_1151.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cb6020], [7, 0x5d7bfa0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ad3560]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b11f40]]}
  dc.input_tensor.layernorm_1165.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b50920], [6, 0x5c16440]]}
  input_1_multiply_1182_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b983a0]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c20ac0]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ba5a40]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5b69c20]]}
  dc.input_tensor.layernorm_1204.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c79080], [3, 0x5c31f00]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x61b3a80]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6175520]]}
  dc.input_tensor.layernorm_1218.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f3eb60], [4, 0x5e413c0]]}
  input_1_multiply_1235_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x602cf00]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60b2ec0]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5e63a60]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d6ea60]]}
  dc.input_tensor.layernorm_1257.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5e29680], [6, 0x5eb3380]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e40f40]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5efd140]]}
  dc.input_tensor.layernorm_1271.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f52640], [7, 0x6053f80]]}

  # epoch_to_epoch
  e2e__fused_op_10_0:                                {input: _fused_op_10, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7e06c40], [3, 0x7dea540]]}
  e2e_matmul_61_0:                                   {input: matmul_61, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7f93860], [5, 0x80012c0], [6, 0x7ee13e0], [7, 0x7f243a0]]}
  e2e__fused_op_9_0:                                 {input: _fused_op_9, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7e0c840], [2, 0x9846c60]]}
  e2e__fused_op_16_0:                                {input: _fused_op_16, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x982a560], [4, 0x8cb3880]]}
  e2e__fused_op_26_0:                                {input: _fused_op_26, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8d212e0], [6, 0x8c01400]]}
  e2e__fused_op_36_0:                                {input: _fused_op_36, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7f243a0], [1, 0x7e0c840]]}
  e2e__fused_op_46_0:                                {input: _fused_op_46, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7e06c40], [3, 0x7dea540]]}
  e2e__fused_op_56_0:                                {input: _fused_op_56, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7f93860], [5, 0x80012c0]]}
  e2e__fused_op_66_0:                                {input: _fused_op_66, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7ee13e0], [7, 0x7f243a0]]}
  e2e__fused_op_76_0:                                {input: _fused_op_76, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7e0c840], [2, 0x7e06c40]]}
  e2e__fused_op_86_0:                                {input: _fused_op_86, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7dea540], [4, 0x7f93860]]}
  e2e__fused_op_96_0:                                {input: _fused_op_96, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x80012c0], [6, 0x7ee13e0]]}
  e2e__fused_op_106_0:                               {input: _fused_op_106, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7f243a0], [1, 0x7e0c840]]}
  e2e__fused_op_116_0:                               {input: _fused_op_116, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7e06c40], [3, 0x7dea540]]}
  e2e__fused_op_126_0:                               {input: _fused_op_126, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7f93860], [5, 0x80012c0]]}
  e2e__fused_op_136_0:                               {input: _fused_op_136, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7ee13e0], [7, 0x7f243a0]]}
  e2e__fused_op_146_0:                               {input: _fused_op_146, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7e0c840], [2, 0x7e06c40]]}
  e2e__fused_op_156_0:                               {input: _fused_op_156, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7dea540], [4, 0x7f93860]]}
  e2e__fused_op_166_0:                               {input: _fused_op_166, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x80012c0], [6, 0x7ee13e0]]}
  e2e__fused_op_176_0:                               {input: _fused_op_176, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7f243a0], [1, 0x7e0c840]]}
  e2e__fused_op_186_0:                               {input: _fused_op_186, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7e06c40], [3, 0x7dea540]]}
  e2e__fused_op_196_0:                               {input: _fused_op_196, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7f93860], [5, 0x80012c0]]}
  e2e__fused_op_206_0:                               {input: _fused_op_206, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7ee13e0], [7, 0x7f243a0]]}
  e2e__fused_op_216_0:                               {input: _fused_op_216, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7e0c840], [2, 0x7e06c40]]}
  e2e__fused_op_226_0:                               {input: _fused_op_226, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7dea540], [4, 0x7f93860]]}
  e2e__fused_op_236_0:                               {input: _fused_op_236, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x80012c0], [6, 0x7ee13e0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 2], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_8, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_14: {type: matmul, grid_loc: [0, 8], grid_size: [2, 1], inputs: [_fused_op_0, _fused_op_1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_2: {type: fused_op, grid_loc: [0, 9], grid_size: [2, 3], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [_fused_op_2, lc.input_tensor.softmax_18.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_22: {type: matmul, grid_loc: [0, 4], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_2], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_4: {type: fused_op, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_22, layer.0.attention.self.value.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_29: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_4], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_33: {type: matmul, grid_loc: [2, 8], grid_size: [2, 2], inputs: [matmul_29, layer.0.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_5: {type: fused_op, grid_loc: [2, 10], grid_size: [2, 1], inputs: [matmul_33, layer.0.attention.output.dense.bias, hidden_states],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [_fused_op_5, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [3, 0], grid_size: [2, 1], inputs: [_fused_op_5, layernorm_38.dc.reduce_avg.0.lc1], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [3, 2], grid_size: [2, 1], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_6: {type: fused_op, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4, layernorm_38.dc.subtract.1, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_41: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [_fused_op_6, layer.0.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_7: {type: fused_op, grid_loc: [4, 8], grid_size: [3, 2], inputs: [matmul_41, layer.0.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_47: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_7, layer.0.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    buffer_1__fused_op_6__fused_op_8: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_6], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [8, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_6__fused_op_8], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_8: {type: fused_op, grid_loc: [4, 11], grid_size: [2, 1], inputs: [matmul_47, layer.0.output.dense.bias, buffer_0__fused_op_6__fused_op_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [_fused_op_8, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_8, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_9: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4, layernorm_52.dc.subtract.1, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_55: {type: matmul, grid_loc: [8, 7], grid_size: [2, 2], inputs: [_fused_op_9, layer.1.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_61: {type: matmul, grid_loc: [8, 10], grid_size: [2, 2], inputs: [_fused_op_9, layer.1.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_10: {type: fused_op, grid_loc: [8, 9], grid_size: [2, 1], inputs: [matmul_55, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}

  fwd_1:
    target_device: 0
    input_count: 128
    _fused_op_11: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_61_0, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_67: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e__fused_op_10_0, _fused_op_11],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [0, 2], grid_size: [2, 3], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_75: {type: matmul, grid_loc: [0, 7], grid_size: [2, 2], inputs: [e2e__fused_op_9_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_13: {type: fused_op, grid_loc: [0, 6], grid_size: [2, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_12],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_14: {type: fused_op, grid_loc: [0, 9], grid_size: [2, 1], inputs: [matmul_75, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_82: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [_fused_op_13, _fused_op_14],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_86: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_82, layer.1.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_15: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_86, layer.1.attention.output.dense.bias, e2e__fused_op_9_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [_fused_op_15, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [_fused_op_15, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_16: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4, layernorm_91.dc.subtract.1, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_2:
    target_device: 0
    input_count: 128
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_16_0, layer.1.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_17: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_94, layer.1.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_100: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_17, layer.1.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_18: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_100, layer.1.output.dense.bias, e2e__fused_op_16_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_18, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_18, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_19: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4, layernorm_105.dc.subtract.1, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_108: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_19, layer.2.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_114: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_19, layer.2.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_108, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_21: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_114, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_120: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_20, _fused_op_21],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_22: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_22, lc.input_tensor.softmax_124.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_128: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_19, layer.2.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_23: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_22], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_24: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_128, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_135: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_23, _fused_op_24],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_139: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_135, layer.2.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_19__fused_op_25: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_19], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_19__fused_op_25: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_19__fused_op_25],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_25: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_139, layer.2.attention.output.dense.bias, buffer_0__fused_op_19__fused_op_25],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_25, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_25, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_26: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4, layernorm_144.dc.subtract.1, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_3:
    target_device: 0
    input_count: 128
    matmul_147: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_26_0, layer.2.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_27: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_147, layer.2.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_153: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_27, layer.2.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_28: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_153, layer.2.output.dense.bias, e2e__fused_op_26_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_28, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_28, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_29: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4, layernorm_158.dc.subtract.1, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_161: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_29, layer.3.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_167: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_29, layer.3.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_30: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_161, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_31: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_167, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_173: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_30, _fused_op_31],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_32: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_177.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_181: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_29, layer.3.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_33: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_32], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_34: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_181, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_188: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_33, _fused_op_34],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_192: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_188, layer.3.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_29__fused_op_35: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_29], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_29__fused_op_35: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_29__fused_op_35],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_35: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_192, layer.3.attention.output.dense.bias, buffer_0__fused_op_29__fused_op_35],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_35, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_35, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_36: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4, layernorm_197.dc.subtract.1, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_200: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_36_0, layer.3.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_37: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_200, layer.3.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_206: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_37, layer.3.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_38: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_206, layer.3.output.dense.bias, e2e__fused_op_36_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_38, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_38, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_39: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4, layernorm_211.dc.subtract.1, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_214: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_39, layer.4.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_220: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_39, layer.4.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_40: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_214, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_41: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_220, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_226: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_40, _fused_op_41],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_42: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_42, lc.input_tensor.softmax_230.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_234: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_39, layer.4.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_43: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_42], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_44: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_234, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_241: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_43, _fused_op_44],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_245: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_241, layer.4.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_39__fused_op_45: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_39], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_39__fused_op_45: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_39__fused_op_45],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_45: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_245, layer.4.attention.output.dense.bias, buffer_0__fused_op_39__fused_op_45],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_45, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_45, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_46: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4, layernorm_250.dc.subtract.1, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_5:
    target_device: 0
    input_count: 128
    matmul_253: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_46_0, layer.4.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_47: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_253, layer.4.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_259: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_47, layer.4.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_48: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_259, layer.4.output.dense.bias, e2e__fused_op_46_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_48, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_48, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_49: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4, layernorm_264.dc.subtract.1, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_267: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_49, layer.5.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_273: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_49, layer.5.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_50: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_267, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_51: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_273, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_279: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_50, _fused_op_51],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_52: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_52, lc.input_tensor.softmax_283.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_287: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_49, layer.5.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_53: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_52], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_54: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_287, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_294: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_53, _fused_op_54],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_298: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_294, layer.5.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_49__fused_op_55: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_49], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_49__fused_op_55: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_49__fused_op_55],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_55: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_298, layer.5.attention.output.dense.bias, buffer_0__fused_op_49__fused_op_55],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_55, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_55, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_56: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4, layernorm_303.dc.subtract.1, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_6:
    target_device: 0
    input_count: 128
    matmul_306: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_56_0, layer.5.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_57: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_306, layer.5.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_312: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_57, layer.5.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_58: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_312, layer.5.output.dense.bias, e2e__fused_op_56_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_58, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_58, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_59: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4, layernorm_317.dc.subtract.1, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_320: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_59, layer.6.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_326: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_59, layer.6.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_60: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_320, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_61: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_326, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_332: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_60, _fused_op_61],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_62: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_62, lc.input_tensor.softmax_336.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_340: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_59, layer.6.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_63: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_62], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_64: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_340, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_347: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_63, _fused_op_64],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_351: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_347, layer.6.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_59__fused_op_65: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_59], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_59__fused_op_65: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_59__fused_op_65],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_65: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_351, layer.6.attention.output.dense.bias, buffer_0__fused_op_59__fused_op_65],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_65, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_65, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_66: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4, layernorm_356.dc.subtract.1, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_7:
    target_device: 0
    input_count: 128
    matmul_359: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_66_0, layer.6.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_67: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_359, layer.6.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_365: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_67, layer.6.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_68: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_365, layer.6.output.dense.bias, e2e__fused_op_66_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_68, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_68, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_69: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4, layernorm_370.dc.subtract.1, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_373: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_69, layer.7.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_379: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_69, layer.7.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_70: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_373, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_71: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_379, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_385: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_70, _fused_op_71],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_72: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_72, lc.input_tensor.softmax_389.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_393: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_69, layer.7.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_73: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_72], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_74: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_393, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_400: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_73, _fused_op_74],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_404: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_400, layer.7.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_69__fused_op_75: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_69], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_69__fused_op_75: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_69__fused_op_75],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_75: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_404, layer.7.attention.output.dense.bias, buffer_0__fused_op_69__fused_op_75],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_75, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_75, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_76: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4, layernorm_409.dc.subtract.1, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_8:
    target_device: 0
    input_count: 128
    matmul_412: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_76_0, layer.7.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_77: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_412, layer.7.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_418: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_77, layer.7.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_78: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_418, layer.7.output.dense.bias, e2e__fused_op_76_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_78, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_78, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_79: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4, layernorm_423.dc.subtract.1, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_426: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_79, layer.8.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_432: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_79, layer.8.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_80: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_426, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_81: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_432, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_438: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_80, _fused_op_81],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_82: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_82, lc.input_tensor.softmax_442.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_446: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_79, layer.8.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_83: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_82], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_84: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_446, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_453: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_83, _fused_op_84],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_457: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_453, layer.8.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_79__fused_op_85: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_79], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_79__fused_op_85: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_79__fused_op_85],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_85: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_457, layer.8.attention.output.dense.bias, buffer_0__fused_op_79__fused_op_85],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_85, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_85, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_86: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4, layernorm_462.dc.subtract.1, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_9:
    target_device: 0
    input_count: 128
    matmul_465: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_86_0, layer.8.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_87: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_465, layer.8.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_471: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_87, layer.8.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_88: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_471, layer.8.output.dense.bias, e2e__fused_op_86_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_88, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_88, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_89: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4, layernorm_476.dc.subtract.1, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_479: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_89, layer.9.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_485: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_89, layer.9.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_90: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_479, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_91: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_485, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_491: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_90, _fused_op_91],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_92: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_92, lc.input_tensor.softmax_495.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_499: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_89, layer.9.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_93: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_92], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_94: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_499, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_506: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_93, _fused_op_94],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_510: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_506, layer.9.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_89__fused_op_95: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_89], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_89__fused_op_95: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_89__fused_op_95],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_95: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_510, layer.9.attention.output.dense.bias, buffer_0__fused_op_89__fused_op_95],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_95, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_95, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_96: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4, layernorm_515.dc.subtract.1, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_10:
    target_device: 0
    input_count: 128
    matmul_518: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_96_0, layer.9.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_97: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_518, layer.9.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_524: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_97, layer.9.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_98: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_524, layer.9.output.dense.bias, e2e__fused_op_96_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_98, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_98, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_99: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4, layernorm_529.dc.subtract.1, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_532: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_99, layer.10.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_538: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_99, layer.10.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_100: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_532, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_101: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_538, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_544: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_100, _fused_op_101],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_102: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_102, lc.input_tensor.softmax_548.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_552: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_99, layer.10.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_103: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_102], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_104: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_552, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_559: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_103, _fused_op_104],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_563: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_559, layer.10.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_99__fused_op_105: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_99], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_99__fused_op_105: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_99__fused_op_105],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_105: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_563, layer.10.attention.output.dense.bias, buffer_0__fused_op_99__fused_op_105],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_105, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_105, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_106: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4, layernorm_568.dc.subtract.1, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_11:
    target_device: 0
    input_count: 128
    matmul_571: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_106_0, layer.10.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_107: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_571, layer.10.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_577: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_107, layer.10.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_108: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_577, layer.10.output.dense.bias, e2e__fused_op_106_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_108, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_108, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_109: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4, layernorm_582.dc.subtract.1, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_585: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_109, layer.11.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_591: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_109, layer.11.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_110: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_585, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_111: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_591, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_597: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_110, _fused_op_111],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_112: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_112, lc.input_tensor.softmax_601.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_605: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_109, layer.11.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_113: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_112], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_114: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_605, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_612: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_113, _fused_op_114],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_616: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_612, layer.11.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_109__fused_op_115: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_109], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_109__fused_op_115: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_109__fused_op_115],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_115: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_616, layer.11.attention.output.dense.bias, buffer_0__fused_op_109__fused_op_115],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_115, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_115, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_116: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4, layernorm_621.dc.subtract.1, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_12:
    target_device: 0
    input_count: 128
    matmul_624: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_116_0, layer.11.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_117: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_624, layer.11.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_630: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_117, layer.11.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_118: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_630, layer.11.output.dense.bias, e2e__fused_op_116_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_118, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_118, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_119: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4, layernorm_635.dc.subtract.1, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_638: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_119, layer.12.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_644: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_119, layer.12.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_120: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_638, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_121: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_644, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_650: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_120, _fused_op_121],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_122: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_650, input_1_multiply_652_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_654.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_122, lc.input_tensor.softmax_654.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_658: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_119, layer.12.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_123: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_654.dc.reduce_sum.1.lc1, _fused_op_122], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_124: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_658, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_665: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_123, _fused_op_124],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_669: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_665, layer.12.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_119__fused_op_125: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_119], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_119__fused_op_125: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_119__fused_op_125],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_125: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_669, layer.12.attention.output.dense.bias, buffer_0__fused_op_119__fused_op_125],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_674.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_125, lc.input_tensor.layernorm_674.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_674.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_125, layernorm_674.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_674.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_674.dc.subtract.1, layernorm_674.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_674.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_674.dc.multiply.2, lc.input_tensor.layernorm_674.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_126: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_674.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_674.4, layernorm_674.dc.subtract.1, layer.12.attention.output.LayerNorm.weight, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_13:
    target_device: 0
    input_count: 128
    matmul_677: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_126_0, layer.12.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_127: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_677, layer.12.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_683: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_127, layer.12.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_128: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_683, layer.12.output.dense.bias, e2e__fused_op_126_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_128, lc.input_tensor.layernorm_688.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_688.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_128, layernorm_688.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_688.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_688.dc.subtract.1, layernorm_688.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_688.dc.multiply.2, lc.input_tensor.layernorm_688.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_129: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_688.4, layernorm_688.dc.subtract.1, layer.12.output.LayerNorm.weight, layer.12.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_691: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_129, layer.13.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_697: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_129, layer.13.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_130: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_691, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_131: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_697, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_703: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_130, _fused_op_131],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_132: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_703, input_1_multiply_705_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_707.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_132, lc.input_tensor.softmax_707.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_711: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_129, layer.13.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_133: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_707.dc.reduce_sum.1.lc1, _fused_op_132], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_134: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_711, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_718: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_133, _fused_op_134],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_722: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_718, layer.13.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_129__fused_op_135: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_129], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_129__fused_op_135: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_129__fused_op_135],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_135: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_722, layer.13.attention.output.dense.bias, buffer_0__fused_op_129__fused_op_135],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_727.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_135, lc.input_tensor.layernorm_727.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_727.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_135, layernorm_727.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_727.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_727.dc.subtract.1, layernorm_727.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_727.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_727.dc.multiply.2, lc.input_tensor.layernorm_727.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_136: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_727.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_727.4, layernorm_727.dc.subtract.1, layer.13.attention.output.LayerNorm.weight, layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_14:
    target_device: 0
    input_count: 128
    matmul_730: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_136_0, layer.13.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_137: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_730, layer.13.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_736: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_137, layer.13.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_138: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_736, layer.13.output.dense.bias, e2e__fused_op_136_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_138, lc.input_tensor.layernorm_741.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_741.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_138, layernorm_741.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_741.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_741.dc.subtract.1, layernorm_741.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_741.dc.multiply.2, lc.input_tensor.layernorm_741.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_139: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_741.4, layernorm_741.dc.subtract.1, layer.13.output.LayerNorm.weight, layer.13.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_744: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_139, layer.14.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_750: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_139, layer.14.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_140: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_744, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_141: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_750, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_756: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_140, _fused_op_141],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_142: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_756, input_1_multiply_758_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_760.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_142, lc.input_tensor.softmax_760.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_764: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_139, layer.14.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_143: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_760.dc.reduce_sum.1.lc1, _fused_op_142], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_144: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_764, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_771: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_143, _fused_op_144],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_775: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_771, layer.14.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_139__fused_op_145: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_139], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_139__fused_op_145: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_139__fused_op_145],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_145: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_775, layer.14.attention.output.dense.bias, buffer_0__fused_op_139__fused_op_145],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_780.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_145, lc.input_tensor.layernorm_780.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_780.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_145, layernorm_780.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_780.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_780.dc.subtract.1, layernorm_780.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_780.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_780.dc.multiply.2, lc.input_tensor.layernorm_780.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_146: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_780.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_780.4, layernorm_780.dc.subtract.1, layer.14.attention.output.LayerNorm.weight, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_15:
    target_device: 0
    input_count: 128
    matmul_783: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_146_0, layer.14.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_147: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_783, layer.14.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_789: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_147, layer.14.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_148: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_789, layer.14.output.dense.bias, e2e__fused_op_146_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_148, lc.input_tensor.layernorm_794.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_794.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_148, layernorm_794.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_794.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_794.dc.subtract.1, layernorm_794.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_794.dc.multiply.2, lc.input_tensor.layernorm_794.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_149: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_794.4, layernorm_794.dc.subtract.1, layer.14.output.LayerNorm.weight, layer.14.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_797: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_149, layer.15.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_803: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_149, layer.15.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_150: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_797, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_151: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_803, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_809: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_150, _fused_op_151],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_152: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_809, input_1_multiply_811_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_813.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_152, lc.input_tensor.softmax_813.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_817: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_149, layer.15.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_153: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_813.dc.reduce_sum.1.lc1, _fused_op_152], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_154: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_817, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_824: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_153, _fused_op_154],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_828: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_824, layer.15.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_149__fused_op_155: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_149], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_149__fused_op_155: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_149__fused_op_155],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_155: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_828, layer.15.attention.output.dense.bias, buffer_0__fused_op_149__fused_op_155],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_833.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_155, lc.input_tensor.layernorm_833.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_833.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_155, layernorm_833.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_833.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_833.dc.subtract.1, layernorm_833.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_833.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_833.dc.multiply.2, lc.input_tensor.layernorm_833.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_156: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_833.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_833.4, layernorm_833.dc.subtract.1, layer.15.attention.output.LayerNorm.weight, layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_16:
    target_device: 0
    input_count: 128
    matmul_836: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_156_0, layer.15.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_157: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_836, layer.15.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_842: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_157, layer.15.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_158: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_842, layer.15.output.dense.bias, e2e__fused_op_156_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_158, lc.input_tensor.layernorm_847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_847.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_158, layernorm_847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_847.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_847.dc.subtract.1, layernorm_847.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_847.dc.multiply.2, lc.input_tensor.layernorm_847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_159: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_847.4, layernorm_847.dc.subtract.1, layer.15.output.LayerNorm.weight, layer.15.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_850: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_159, layer.16.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_856: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_159, layer.16.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_160: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_850, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_161: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_856, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_862: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_160, _fused_op_161],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_162: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_862, input_1_multiply_864_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_866.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_162, lc.input_tensor.softmax_866.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_870: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_159, layer.16.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_163: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_866.dc.reduce_sum.1.lc1, _fused_op_162], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_164: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_870, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_877: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_163, _fused_op_164],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_881: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_877, layer.16.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_159__fused_op_165: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_159], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_159__fused_op_165: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_159__fused_op_165],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_165: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_881, layer.16.attention.output.dense.bias, buffer_0__fused_op_159__fused_op_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_886.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_165, lc.input_tensor.layernorm_886.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_886.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_165, layernorm_886.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_886.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_886.dc.subtract.1, layernorm_886.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_886.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_886.dc.multiply.2, lc.input_tensor.layernorm_886.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_166: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_886.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_886.4, layernorm_886.dc.subtract.1, layer.16.attention.output.LayerNorm.weight, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_17:
    target_device: 0
    input_count: 128
    matmul_889: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_166_0, layer.16.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_167: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_889, layer.16.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_895: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_167, layer.16.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_168: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_895, layer.16.output.dense.bias, e2e__fused_op_166_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_168, lc.input_tensor.layernorm_900.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_900.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_168, layernorm_900.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_900.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_900.dc.subtract.1, layernorm_900.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_900.dc.multiply.2, lc.input_tensor.layernorm_900.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_169: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_900.4, layernorm_900.dc.subtract.1, layer.16.output.LayerNorm.weight, layer.16.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_903: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_169, layer.17.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_909: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_169, layer.17.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_170: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_903, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_171: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_909, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_915: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_170, _fused_op_171],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_172: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_915, input_1_multiply_917_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_919.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_172, lc.input_tensor.softmax_919.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_923: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_169, layer.17.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_173: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_919.dc.reduce_sum.1.lc1, _fused_op_172], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_174: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_923, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_930: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_173, _fused_op_174],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_934: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_930, layer.17.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_169__fused_op_175: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_169], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_169__fused_op_175: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_169__fused_op_175],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_175: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_934, layer.17.attention.output.dense.bias, buffer_0__fused_op_169__fused_op_175],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_939.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_175, lc.input_tensor.layernorm_939.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_939.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_175, layernorm_939.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_939.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_939.dc.subtract.1, layernorm_939.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_939.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_939.dc.multiply.2, lc.input_tensor.layernorm_939.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_176: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_939.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_939.4, layernorm_939.dc.subtract.1, layer.17.attention.output.LayerNorm.weight, layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_18:
    target_device: 0
    input_count: 128
    matmul_942: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_176_0, layer.17.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_177: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_942, layer.17.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_948: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_177, layer.17.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_178: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_948, layer.17.output.dense.bias, e2e__fused_op_176_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_178, lc.input_tensor.layernorm_953.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_953.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_178, layernorm_953.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_953.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_953.dc.subtract.1, layernorm_953.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_953.dc.multiply.2, lc.input_tensor.layernorm_953.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_179: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_953.4, layernorm_953.dc.subtract.1, layer.17.output.LayerNorm.weight, layer.17.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_956: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_179, layer.18.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_962: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_179, layer.18.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_180: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_956, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_181: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_962, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_968: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_180, _fused_op_181],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_182: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_968, input_1_multiply_970_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_972.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_182, lc.input_tensor.softmax_972.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_976: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_179, layer.18.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_183: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_972.dc.reduce_sum.1.lc1, _fused_op_182], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_184: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_976, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_983: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_183, _fused_op_184],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_987: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_983, layer.18.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_179__fused_op_185: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_179], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_179__fused_op_185: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_179__fused_op_185],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_185: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_987, layer.18.attention.output.dense.bias, buffer_0__fused_op_179__fused_op_185],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_992.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_185, lc.input_tensor.layernorm_992.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_992.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_185, layernorm_992.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_992.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_992.dc.subtract.1, layernorm_992.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_992.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_992.dc.multiply.2, lc.input_tensor.layernorm_992.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_186: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_992.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_992.4, layernorm_992.dc.subtract.1, layer.18.attention.output.LayerNorm.weight, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_19:
    target_device: 0
    input_count: 128
    matmul_995: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_186_0, layer.18.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_187: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_995, layer.18.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1001: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_187, layer.18.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_188: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1001, layer.18.output.dense.bias, e2e__fused_op_186_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_188, lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1006.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_188, layernorm_1006.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1006.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1006.dc.subtract.1, layernorm_1006.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1006.dc.multiply.2, lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_189: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1006.4, layernorm_1006.dc.subtract.1, layer.18.output.LayerNorm.weight, layer.18.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_1009: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_189, layer.19.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_1015: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_189, layer.19.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_190: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_1009, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_191: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1015, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1021: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_190, _fused_op_191],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_192: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_1021, input_1_multiply_1023_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_1025.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_192, lc.input_tensor.softmax_1025.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1029: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_189, layer.19.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_193: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_1025.dc.reduce_sum.1.lc1, _fused_op_192], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_194: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_1029, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1036: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_193, _fused_op_194],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1040: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_1036, layer.19.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_189__fused_op_195: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_189], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_189__fused_op_195: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_189__fused_op_195],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_195: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_1040, layer.19.attention.output.dense.bias, buffer_0__fused_op_189__fused_op_195],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1045.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_195, lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1045.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_195, layernorm_1045.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1045.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1045.dc.subtract.1, layernorm_1045.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1045.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1045.dc.multiply.2, lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_196: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1045.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1045.4, layernorm_1045.dc.subtract.1, layer.19.attention.output.LayerNorm.weight, layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_20:
    target_device: 0
    input_count: 128
    matmul_1048: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_196_0, layer.19.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_197: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_1048, layer.19.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1054: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_197, layer.19.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_198: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1054, layer.19.output.dense.bias, e2e__fused_op_196_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_198, lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1059.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_198, layernorm_1059.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1059.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1059.dc.subtract.1, layernorm_1059.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1059.dc.multiply.2, lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_199: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1059.4, layernorm_1059.dc.subtract.1, layer.19.output.LayerNorm.weight, layer.19.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_1062: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_199, layer.20.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_1068: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_199, layer.20.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_200: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_1062, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_201: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1068, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1074: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_200, _fused_op_201],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_202: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_1074, input_1_multiply_1076_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_1078.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_202, lc.input_tensor.softmax_1078.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1082: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_199, layer.20.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_203: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_1078.dc.reduce_sum.1.lc1, _fused_op_202], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_204: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_1082, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1089: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_203, _fused_op_204],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1093: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_1089, layer.20.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_199__fused_op_205: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_199], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_199__fused_op_205: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_199__fused_op_205],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_205: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_1093, layer.20.attention.output.dense.bias, buffer_0__fused_op_199__fused_op_205],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1098.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_205, lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1098.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_205, layernorm_1098.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1098.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1098.dc.subtract.1, layernorm_1098.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1098.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1098.dc.multiply.2, lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_206: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1098.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1098.4, layernorm_1098.dc.subtract.1, layer.20.attention.output.LayerNorm.weight, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_21:
    target_device: 0
    input_count: 128
    matmul_1101: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_206_0, layer.20.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_207: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_1101, layer.20.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1107: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_207, layer.20.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_208: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1107, layer.20.output.dense.bias, e2e__fused_op_206_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_208, lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1112.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_208, layernorm_1112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1112.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1112.dc.subtract.1, layernorm_1112.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1112.dc.multiply.2, lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_209: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1112.4, layernorm_1112.dc.subtract.1, layer.20.output.LayerNorm.weight, layer.20.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_1115: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_209, layer.21.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_1121: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_209, layer.21.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_210: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_1115, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_211: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1121, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1127: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_210, _fused_op_211],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_212: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_1127, input_1_multiply_1129_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_1131.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_212, lc.input_tensor.softmax_1131.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1135: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_209, layer.21.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_213: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_1131.dc.reduce_sum.1.lc1, _fused_op_212], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_214: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_1135, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1142: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_213, _fused_op_214],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1146: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_1142, layer.21.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_209__fused_op_215: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_209], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_209__fused_op_215: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_209__fused_op_215],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_215: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_1146, layer.21.attention.output.dense.bias, buffer_0__fused_op_209__fused_op_215],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1151.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_215, lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1151.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_215, layernorm_1151.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1151.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1151.dc.subtract.1, layernorm_1151.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1151.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1151.dc.multiply.2, lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_216: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1151.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1151.4, layernorm_1151.dc.subtract.1, layer.21.attention.output.LayerNorm.weight, layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_22:
    target_device: 0
    input_count: 128
    matmul_1154: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_216_0, layer.21.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_217: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_1154, layer.21.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1160: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_217, layer.21.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_218: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1160, layer.21.output.dense.bias, e2e__fused_op_216_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_218, lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1165.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_218, layernorm_1165.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1165.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1165.dc.subtract.1, layernorm_1165.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1165.dc.multiply.2, lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_219: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1165.4, layernorm_1165.dc.subtract.1, layer.21.output.LayerNorm.weight, layer.21.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_1168: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_219, layer.22.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_1174: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_219, layer.22.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_220: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_1168, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_221: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1174, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1180: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_220, _fused_op_221],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_222: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_1180, input_1_multiply_1182_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_1184.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_222, lc.input_tensor.softmax_1184.dc.reduce_sum.1.0], grid_transpose: true,
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1188: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [_fused_op_219, layer.22.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_223: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_1184.dc.reduce_sum.1.lc1, _fused_op_222], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_224: {type: fused_op, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_1188, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1195: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_223, _fused_op_224],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1199: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [matmul_1195, layer.22.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_219__fused_op_225: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [_fused_op_219], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_219__fused_op_225: {type: nop, grid_loc: [7, 0], grid_size: [3, 2], inputs: [buffer_1__fused_op_219__fused_op_225],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_225: {type: fused_op, grid_loc: [6, 11], grid_size: [2, 1], inputs: [matmul_1199, layer.22.attention.output.dense.bias, buffer_0__fused_op_219__fused_op_225],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1204.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [_fused_op_225, lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1204.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [_fused_op_225, layernorm_1204.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1204.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1204.dc.subtract.1, layernorm_1204.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1204.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1204.dc.multiply.2, lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_226: {type: fused_op, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1204.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1204.4, layernorm_1204.dc.subtract.1, layer.22.attention.output.LayerNorm.weight, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_23:
    target_device: 0
    input_count: 128
    matmul_1207: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_226_0, layer.22.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_227: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_1207, layer.22.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1213: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_227, layer.22.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_228: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1213, layer.22.output.dense.bias, e2e__fused_op_226_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_228, lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1218.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_228, layernorm_1218.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1218.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1218.dc.subtract.1, layernorm_1218.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1218.dc.multiply.2, lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_229: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1218.4, layernorm_1218.dc.subtract.1, layer.22.output.LayerNorm.weight, layer.22.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_1221: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_229, layer.23.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    matmul_1227: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [_fused_op_229, layer.23.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_230: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_1221, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_231: {type: fused_op, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1227, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1233: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [_fused_op_230, _fused_op_231],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_232: {type: fused_op, grid_loc: [4, 9], grid_size: [2, 3], inputs: [matmul_1233, input_1_multiply_1235_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    softmax_1237.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_232, lc.input_tensor.softmax_1237.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1241: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [_fused_op_229, layer.23.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_233: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_1237.dc.reduce_sum.1.lc1, _fused_op_232],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 216], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_234: {type: fused_op, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_1241, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    matmul_1248: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [_fused_op_233, _fused_op_234],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1252: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [matmul_1248, layer.23.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    buffer_1__fused_op_229__fused_op_235: {type: nop, grid_loc: [6, 2], grid_size: [3, 2], inputs: [_fused_op_229], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_229__fused_op_235: {type: nop, grid_loc: [6, 8], grid_size: [3, 2], inputs: [buffer_1__fused_op_229__fused_op_235], grid_transpose: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_235: {type: fused_op, grid_loc: [8, 2], grid_size: [2, 1], inputs: [matmul_1252, layer.23.attention.output.dense.bias, buffer_0__fused_op_229__fused_op_235], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1257.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_235, lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1257.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [_fused_op_235, layernorm_1257.dc.reduce_avg.0.lc1], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1257.dc.multiply.2: {type: multiply, grid_loc: [8, 8], grid_size: [2, 1], inputs: [layernorm_1257.dc.subtract.1, layernorm_1257.dc.subtract.1], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1257.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_1257.dc.multiply.2, lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_236: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [layernorm_1257.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1257.4, layernorm_1257.dc.subtract.1, layer.23.attention.output.LayerNorm.weight, layer.23.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_24:
    target_device: 0
    input_count: 128
    matmul_1260: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_236_0, layer.23.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 2}}
    _fused_op_237: {type: fused_op, grid_loc: [0, 8], grid_size: [3, 2], inputs: [matmul_1260, layer.23.intermediate.dense.bias], grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    matmul_1266: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [_fused_op_237, layer.23.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, u_kt: 8}}
    _fused_op_238: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [matmul_1266, layer.23.output.dense.bias, e2e__fused_op_236_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    layernorm_1271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_238, lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1271.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 1], inputs: [_fused_op_238, layernorm_1271.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [144, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1271.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1271.dc.subtract.1, layernorm_1271.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1271.dc.multiply.2, lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_239: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1271.4, layernorm_1271.dc.subtract.1, layer.23.output.LayerNorm.weight, layer.23.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    _fused_op_239_output_nop_0: {type: nop, grid_loc: [4, 2], grid_size: [2, 2], inputs: [_fused_op_239], untilize_output: true,
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_zero: 0, $lptr_q25: 0, $gptr_q23: 0, $gptr_q19: 0, $lptr_q19: 0, $c_microbatch_size: 128, $gptr_q15: 0, $lptr_q13: 0, $gptr_q11: 0, $lptr_q15: 0, $lptr_q11: 0, $gptr_q9: 0, $gptr_q25: 0, $lptr_q9: 0, $lptr_q41: 0, $gptr_q43: 0, $lptr_q23: 0, $lptr_q43: 0, $lptr_q3: 0, $lptr_q45: 0, $gptr_q21: 0, $lptr_q5: 0, $gptr_q47: 0, $lptr_q48: 0, $lptr_q29: 0, $lptr_q47: 0, $gptr_q48: 0, $lptr_q27: 0, $c_one: 1, $gptr_q39: 0, $lptr_q37: 0, $gptr_q41: 0, $gptr_q37: 0, $gptr_q17: 0, $gptr_q45: 0, $lptr_q35: 0, $gptr_q13: 0, $gptr_q29: 0, $gptr_q33: 0, $lptr_q39: 0, $lptr_q31: 0, $gptr_q5: 0, $gptr_q35: 0, $gptr_q31: 0, $gptr_q27: 0, $lptr_q21: 0, $gptr_q3: 0, $lptr_q17: 0, $gptr_q7: 0, $lptr_q33: 0, $lptr_q7: 0}
    - staticvar: {$lptr_q24: 0, $gptr_q20: 0, $gptr_q18_shadow: 0, $gptr_q18: 0, $gptr_q24: 0, $lptr_q20: 0, $lptr_q18: 0, $lptr_q28: 0, $lptr_q30: 0, $gptr_q8_shadow: 0, $gptr_q32: 0, $gptr_q30: 0, $lptr_q32: 0, $gptr_q16: 0, $gptr_q30_shadow: 0, $gptr_q34_shadow: 0, $lptr_q26: 0, $lptr_q16: 0, $lptr_q34: 0, $gptr_q20_shadow: 0, $gptr_q44_shadow: 0, $gptr_q36_shadow: 0, $gptr_q28: 0, $gptr_q28_shadow: 0, $gptr_q36: 0, $lptr_q4: 0, $lptr_q36: 0, $gptr_q26_shadow: 0, $gptr_q4_shadow: 0, $gptr_q24_shadow: 0, $gptr_q10: 0, $lptr_q46: 0, $gptr_q16_shadow: 0, $gptr_q32_shadow: 0, $gptr_q34: 0, $gptr_q44: 0, $gptr_q1_shadow: 0, $gptr_q42: 0, $lptr_q14: 0, $gptr_q40_shadow: 0, $lptr_q42: 0, $gptr_q38: 0, $gptr_q38_shadow: 0, $gptr_q40: 0, $lptr_q2: 0, $gptr_q42_shadow: 0, $lptr_q38: 0, $gptr_q0: 0, $gptr_q22: 0, $lptr_q1: 0, $gptr_q1: 0, $lptr_q0: 0, $gptr_q26: 0, $lptr_q10: 0, $gptr_q2: 0, $gptr_q2_shadow: 0, $lptr_q44: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q12_shadow: 0, $gptr_q6_shadow: 0, $lptr_q8: 0, $gptr_q22_shadow: 0, $lptr_q40: 0, $gptr_q8: 0, $gptr_q14: 0, $gptr_q10_shadow: 0, $lptr_q22: 0, $gptr_q46: 0, $lptr_q12: 0, $gptr_q12: 0, $lptr_q6: 0, $gptr_q14_shadow: 0}
    - varinst: [$gptr_q44, set, $gptr_q44_shadow]
    - varinst: [$gptr_q42, set, $gptr_q42_shadow]
    - varinst: [$gptr_q40, set, $gptr_q40_shadow]
    - varinst: [$gptr_q38, set, $gptr_q38_shadow]
    - varinst: [$gptr_q36, set, $gptr_q36_shadow]
    - varinst: [$gptr_q34, set, $gptr_q34_shadow]
    - varinst: [$gptr_q32, set, $gptr_q32_shadow]
    - varinst: [$gptr_q30, set, $gptr_q30_shadow]
    - varinst: [$gptr_q28, set, $gptr_q28_shadow]
    - varinst: [$gptr_q26, set, $gptr_q26_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q8, set, $gptr_q8_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q14, set, $gptr_q14_shadow]
    - varinst: [$gptr_q16, set, $gptr_q16_shadow]
    - varinst: [$gptr_q18, set, $gptr_q18_shadow]
    - varinst: [$gptr_q20, set, $gptr_q20_shadow]
    - varinst: [$gptr_q22, set, $gptr_q22_shadow]
    - varinst: [$gptr_q24, set, $gptr_q24_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_9_0, e2e_matmul_61_0, e2e__fused_op_10_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_16_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_9_0, e2e_matmul_61_0, e2e__fused_op_10_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_26_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_16_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_16_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_36_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_26_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_46_0]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_36_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_36_0]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_56_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_46_0]
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_66_0]
    -   execute: {graph_name: fwd_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_56_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_56_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_76_0]
    -   execute: {graph_name: fwd_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_66_0]
    -   varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_86_0]
    -   execute: {graph_name: fwd_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_76_0]
    -   varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_96_0]
    -   execute: {graph_name: fwd_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_86_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_86_0]
    -   varinst: [$gptr_q18_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_106_0]
    -   execute: {graph_name: fwd_10, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_96_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_96_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_116_0]
    -   execute: {graph_name: fwd_11, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_106_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_106_0]
    -   varinst: [$gptr_q22_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_126_0]
    -   execute: {graph_name: fwd_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_116_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_652_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_116_0]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_136_0]
    -   execute: {graph_name: fwd_13, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_126_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_126_0]
    -   varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_146_0]
    -   execute: {graph_name: fwd_14, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_136_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_758_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_136_0]
    -   varinst: [$gptr_q28_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_156_0]
    -   execute: {graph_name: fwd_15, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_146_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_146_0]
    -   varinst: [$gptr_q30_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_166_0]
    -   execute: {graph_name: fwd_16, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_156_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_864_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_156_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_176_0]
    -   execute: {graph_name: fwd_17, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e__fused_op_166_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_166_0]
    -   varinst: [$gptr_q34_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_186_0]
    -   execute: {graph_name: fwd_18, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e__fused_op_176_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_970_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_176_0]
    -   varinst: [$gptr_q36_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_196_0]
    -   execute: {graph_name: fwd_19, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e__fused_op_186_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_186_0]
    -   varinst: [$gptr_q38_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_206_0]
    -   execute: {graph_name: fwd_20, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e__fused_op_196_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1076_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_196_0]
    -   varinst: [$gptr_q40_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_216_0]
    -   execute: {graph_name: fwd_21, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e__fused_op_206_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_206_0]
    -   varinst: [$gptr_q42_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_226_0]
    -   execute: {graph_name: fwd_22, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e__fused_op_216_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1182_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_216_0]
    -   varinst: [$gptr_q44_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q45, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_236_0]
    -   execute: {graph_name: fwd_23, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e__fused_op_226_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_226_0]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_24, queue_settings: {
               e2e__fused_op_236_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_236_0]
    -   varinst: [$gptr_q48, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - add_4: { type: add, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [2, 4], output: output}
  2: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [3, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [3, 1], ublock: [2, 4], output: output}
  3: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 12}], pop_last: [intermed0], mblock: [3, 3], ublock: [2, 4], output: output}
  5: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - add_35: { type: add, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [2, 4], output: dest}
        - add_37: { type: add, inputs: [dest, input2], mblock: [3, 8], ublock: [2, 4], output: output}
  6: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 32}, tile_broadcast: c], pop_last: [intermed0], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input3], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input4], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [2, 4], output: output}
  7: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - add_43: { type: add, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 16], ublock: [2, 4], output: dest}
        - gelu_44: { type: gelu, inputs: [dest], mblock: [2, 16], ublock: [2, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_1271]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.04
