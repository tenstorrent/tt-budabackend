# git checkout 5259b590
# pybuda/test/benchmark/benchmark.py -m bert -c large -opt 3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=85000 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                     {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x36900020]]}

  # output
  bert_encoders.output_layernorm_1271:               {input: _fused_op_95_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x552ce20], [3, 0x552f580], [4, 0x553fc00], [5, 0x5550fa0], [6, 0x5553b60], [7, 0x5548c60], [0, 0x5538a60], [1, 0x552fe60]]}
  layer.0.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6a86300], [7, 0x6a776c0], [0, 0x6a31280], [1, 0x6a32cc0]]}
  layer.0.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6a414a0], [3, 0x6a3f1a0], [4, 0x6a6c380], [5, 0x6a83ba0], [6, 0x6a88620], [7, 0x6a799e0], [0, 0x6a335a0], [1, 0x6a34fe0]]}
  layer.0.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6a644c0], [3, 0x6a621c0], [4, 0x6a8f3a0], [5, 0x6aa6bc0]]}
  layer.0.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6a58000], [2, 0x6a667e0], [3, 0x6a644e0], [4, 0x6a916c0], [5, 0x6aa8ee0], [6, 0x6aabac0], [7, 0x6a9ca00], [0, 0x6a56a40]]}
  layer.0.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6a309a0], [2, 0x6a3f180], [3, 0x6a3ce80], [4, 0x6a6a060]]}
  layer.0.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6a89800], [3, 0x6a87500], [4, 0x6ab46e0], [5, 0x6acbf00], [6, 0x6aceae0], [7, 0x6abfa20], [0, 0x6a79a60], [1, 0x6a7b4a0]]}
  layer.0.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6aac820], [3, 0x6aaa520], [4, 0x6ad7700], [5, 0x6aeef20]]}
  layer.0.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6ad9a20]]}
  layer.0.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6af1240]]}
  layer.0.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6af1f80], [7, 0x6ae2ec0], [0, 0x6a9d7c0], [1, 0x6a9f200], [2, 0x6aaf880], [3, 0x6aad580], [4, 0x6ae2640], [5, 0x6af9e60], [6, 0x6b14fa0], [7, 0x6b05ee0], [0, 0x6ac07e0], [1, 0x6ac2220], [2, 0x6ad28a0], [3, 0x6ad05a0], [4, 0x6b05660], [5, 0x6b1ce80], [6, 0x6b37fc0], [7, 0x6b28f00], [0, 0x6ae3800], [1, 0x6ae5240], [2, 0x6af58c0], [3, 0x6af35c0], [4, 0x6b28680], [5, 0x6b3fea0], [6, 0x6b5afe0], [7, 0x6b4bf20], [0, 0x6b06820], [1, 0x6b08260], [2, 0x6b188e0], [3, 0x6b165e0], [4, 0x6b4b6a0], [5, 0x6b62ec0]]}
  layer.0.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69cb1c0], [4, 0x69f83a0], [5, 0x6a11ee0], [6, 0x6a14640], [7, 0x6a078a0], [0, 0x69c1460], [1, 0x69c1000], [2, 0x69cf7e0]]}
  layer.0.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x691ab00], [4, 0x6947ce0], [5, 0x6960f60], [6, 0x69636c0], [7, 0x6956920], [0, 0x69104e0], [1, 0x69081a0], [2, 0x6916980], [3, 0x693db20], [4, 0x696ad00], [5, 0x6983f80], [6, 0x69866e0], [7, 0x6979940], [0, 0x6933500], [1, 0x692b1c0], [2, 0x69399a0], [3, 0x6960b40], [4, 0x698dd20], [5, 0x69a6fa0], [6, 0x69a9700], [7, 0x699c960], [0, 0x6956520], [1, 0x694e1e0], [2, 0x695c9c0], [3, 0x6983b60], [4, 0x69b0d40], [5, 0x69c9fc0], [6, 0x69cc720], [7, 0x69bf980], [0, 0x6979540], [1, 0x6971200], [2, 0x697f9e0]]}
  layer.0.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69a6b80], [4, 0x69d3d60], [5, 0x69ecfe0], [6, 0x69ef740], [7, 0x69e29a0], [0, 0x699c560], [1, 0x6994220], [2, 0x69a2a00]]}
  layer.0.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69953c0]]}
  layer.0.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x69a3ba0]]}
  layer.1.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69a81a0], [4, 0x69d5380], [5, 0x69eeec0], [6, 0x69f1620], [7, 0x69e4880], [0, 0x699e440], [1, 0x699dfe0], [2, 0x69ac7c0]]}
  layer.1.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6954600], [0, 0x690e1c0], [1, 0x6905e80], [2, 0x6914660]]}
  layer.1.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69cf7e0], [4, 0x69fc9c0], [5, 0x6a16500], [6, 0x6a18c60], [7, 0x6a0bec0], [0, 0x69c5a80], [1, 0x69c5620], [2, 0x69d3e00]]}
  layer.1.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69f2800], [4, 0x6a1f9e0], [5, 0x6a39520], [6, 0x6a3bc80]]}
  layer.1.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69e8640], [2, 0x69f6e20], [3, 0x69f4b20], [4, 0x6a21d00], [5, 0x6a3b840], [6, 0x6a3dfa0], [7, 0x6a2f360], [0, 0x69e8f20]]}
  layer.1.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6a0b660], [2, 0x6a19e40], [3, 0x6a17b40], [4, 0x6a44d20]]}
  layer.1.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6a5e860], [6, 0x6a60fc0], [7, 0x6a52380], [0, 0x6a0bf40], [1, 0x6a0d980], [2, 0x6a1c160], [3, 0x6a19e60], [4, 0x6a47040]]}
  layer.1.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6a81880], [6, 0x6a83fe0], [7, 0x6a753a0], [0, 0x6a2ef60]]}
  layer.1.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6cd5d60]]}
  layer.1.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6ced580]]}
  layer.1.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6ced140], [7, 0x6cde080], [0, 0x6c90aa0], [1, 0x6c924e0], [2, 0x6ca3ce0], [3, 0x6ca19e0], [4, 0x6cde980], [5, 0x6cf61a0], [6, 0x6d10160], [7, 0x6d010a0], [0, 0x6cb3ac0], [1, 0x6cb5500], [2, 0x6cc6d00], [3, 0x6cc4a00], [4, 0x6d019a0], [5, 0x6d191c0], [6, 0x6d33180], [7, 0x6d240c0], [0, 0x6cd6ae0], [1, 0x6cd8520], [2, 0x6ce9d20], [3, 0x6ce7a20], [4, 0x6d249c0], [5, 0x6d3c1e0], [6, 0x6d561a0], [7, 0x6d470e0], [0, 0x6cf9b00], [1, 0x6cfb540], [2, 0x6d0cd40], [3, 0x6d0aa40], [4, 0x6d479e0], [5, 0x6d5f200]]}
  layer.1.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6d791c0], [7, 0x6d6a100], [0, 0x6d1cb20], [1, 0x6d1e560], [2, 0x6d2fd60], [3, 0x6d2da60], [4, 0x6d6aa00], [5, 0x6d82220]]}
  layer.1.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6d7d7e0], [7, 0x6d6e720], [0, 0x6d21140], [1, 0x6d22b80], [2, 0x6d34380], [3, 0x6d32080], [4, 0x6d6f020], [5, 0x6d86840], [6, 0x6da0800], [7, 0x6d91740], [0, 0x6d44160], [1, 0x6d45ba0], [2, 0x6d573a0], [3, 0x6d550a0], [4, 0x6d92040], [5, 0x6da9860], [6, 0x6dc3820], [7, 0x6db4760], [0, 0x6d67180], [1, 0x6d68bc0], [2, 0x6d7a3c0], [3, 0x6d780c0], [4, 0x6db5060], [5, 0x6dcc880], [6, 0x6de6840], [7, 0x6dd7780], [0, 0x6d8a1a0], [1, 0x6d8bbe0], [2, 0x6d9d3e0], [3, 0x6d9b0e0], [4, 0x6dd8080], [5, 0x6def8a0]]}
  layer.1.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e09860], [7, 0x6dfa7a0], [0, 0x6dad1c0], [1, 0x6daec00], [2, 0x6dc0400], [3, 0x6dbe100], [4, 0x6dfb0a0], [5, 0x6e128c0]]}
  layer.1.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6dfc240]]}
  layer.1.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6e13a60]]}
  layer.2.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e13620], [7, 0x6dfbdc0], [0, 0x6daf0a0], [1, 0x6db0ae0], [2, 0x6dc22e0], [3, 0x6dbffe0], [4, 0x6e04e60], [5, 0x6e1c680]]}
  layer.2.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e36640], [7, 0x6e1ede0], [0, 0x6dd20c0], [1, 0x6dd3b00]]}
  layer.2.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6de5300], [3, 0x6de3000], [4, 0x6e27e80], [5, 0x6e3f6a0], [6, 0x6e38960], [7, 0x6e21100], [0, 0x6dd43e0], [1, 0x6dd5e20]]}
  layer.2.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e08320], [3, 0x6e06020], [4, 0x6e4aea0], [5, 0x6e626c0]]}
  layer.2.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6b29840], [1, 0x6b2b280], [2, 0x6b3b900], [3, 0x6b39600], [4, 0x6b6e6c0], [5, 0x6b85ee0], [6, 0x6b86c20], [7, 0x6b6f3c0]]}
  layer.2.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6b4c860], [1, 0x6b4e2a0], [2, 0x6b5e920], [3, 0x6b5c620]]}
  layer.2.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6b916e0], [5, 0x6ba8f00], [6, 0x6ba9c40], [7, 0x6b923e0], [0, 0x6b4eb80], [1, 0x6b505c0], [2, 0x6b60c40], [3, 0x6b5e940]]}
  layer.2.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6bb4700], [5, 0x6bcbf20], [6, 0x6bccc60], [7, 0x6bb5400]]}
  layer.2.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6b7e000]]}
  layer.2.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6bb7720]]}
  layer.2.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6b72020], [1, 0x6b73a60], [2, 0x6b849a0], [3, 0x6b826a0], [4, 0x6bb7760], [5, 0x6bcef80], [6, 0x6bcf400], [7, 0x6bc0340], [0, 0x6b95040], [1, 0x6b96a80], [2, 0x6ba79c0], [3, 0x6ba56c0], [4, 0x6bda780], [5, 0x6bf1fa0], [6, 0x6bf2420], [7, 0x6be3360], [0, 0x6bb8060], [1, 0x6bb9aa0], [2, 0x6bca9e0], [3, 0x6bc86e0], [4, 0x6bfd7a0], [5, 0x6c14fc0], [6, 0x6c15440], [7, 0x6c06380], [0, 0x6bdb080], [1, 0x6bdcac0], [2, 0x6beda00], [3, 0x6beb700], [4, 0x6c207c0], [5, 0x6c37fe0], [6, 0x6c38460], [7, 0x6c293a0]]}
  layer.2.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6bfe0a0], [1, 0x6bffae0], [2, 0x6c10a20], [3, 0x6c0e720], [4, 0x6c437e0], [5, 0x6c5b000], [6, 0x6c5b480], [7, 0x6c4c3c0]]}
  layer.2.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c026c0], [1, 0x6c04100], [2, 0x6c15040], [3, 0x6c12d40], [4, 0x6c47e00], [5, 0x6c5f620], [6, 0x6c5faa0], [7, 0x6c509e0], [0, 0x6c256e0], [1, 0x6c27120], [2, 0x6c38060], [3, 0x6c35d60], [4, 0x6c6ae20], [5, 0x6c82640], [6, 0x6c82ac0], [7, 0x6c73a00], [0, 0x6c48700], [1, 0x6c4a140], [2, 0x6c5b080], [3, 0x6c58d80], [4, 0x6c8de40], [5, 0x6ca5660], [6, 0x6ca5ae0], [7, 0x6c96a20], [0, 0x6c6b720], [1, 0x6c6d160], [2, 0x6c7e0a0], [3, 0x6c7bda0], [4, 0x6cb0e60], [5, 0x6cc8680], [6, 0x6cc8b00], [7, 0x6cb9a40]]}
  layer.2.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c8e740], [1, 0x6c90180], [2, 0x6ca10c0], [3, 0x6c9edc0], [4, 0x6cd3e80], [5, 0x6ceb6a0], [6, 0x6cebb20], [7, 0x6cdca60]]}
  layer.2.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6903680]]}
  layer.2.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x65537c0]]}
  layer.3.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65258e0], [1, 0x651cce0], [2, 0x652a340], [3, 0x652e940], [4, 0x654bd60], [5, 0x655d100], [6, 0x65609e0], [7, 0x655c3e0]]}
  layer.3.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6548900], [1, 0x653fd00], [2, 0x654d360], [3, 0x6551960]]}
  layer.3.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x656ed80], [5, 0x6580120], [6, 0x6583a00], [7, 0x657f400], [0, 0x654ac20], [1, 0x6542020], [2, 0x654f680], [3, 0x6553c80]]}
  layer.3.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6591da0], [5, 0x65a3140], [6, 0x65a6a20], [7, 0x65a2420]]}
  layer.3.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x65726a0], [3, 0x6576ca0], [4, 0x65940c0], [5, 0x65a5460], [6, 0x65a8d40], [7, 0x65a4740], [0, 0x656e0c0], [1, 0x65654c0]]}
  layer.3.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x652c620], [4, 0x6549a40], [5, 0x655ade0], [6, 0x655e6c0]]}
  layer.3.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6599cc0], [4, 0x65b70e0], [5, 0x65c8480], [6, 0x65cbd60], [7, 0x65c7760], [0, 0x65910e0], [1, 0x65884e0], [2, 0x6595b40]]}
  layer.3.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x65bcce0], [4, 0x65da100], [5, 0x65eb4a0], [6, 0x65eed80]]}
  layer.3.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x65ed7c0]]}
  layer.3.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x65f10a0]]}
  layer.3.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x65eac00], [0, 0x65b4580], [1, 0x65ac240], [2, 0x65b98a0], [3, 0x65bfd40], [4, 0x65dd160], [5, 0x65f63e0], [6, 0x65f9cc0], [7, 0x660dc20], [0, 0x65d75a0], [1, 0x65cf260], [2, 0x65dc8c0], [3, 0x65e2d60], [4, 0x6600180], [5, 0x6619400], [6, 0x661cce0], [7, 0x6630c40], [0, 0x65fa5c0], [1, 0x65f2280], [2, 0x65ff8e0], [3, 0x6605d80], [4, 0x66231a0], [5, 0x663c420], [6, 0x663fd00], [7, 0x6653c60], [0, 0x661d5e0], [1, 0x66152a0], [2, 0x6622900], [3, 0x6628da0], [4, 0x66461c0], [5, 0x665f440], [6, 0x6662d20]]}
  layer.3.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64fada0], [5, 0x650c140], [6, 0x650fa20], [7, 0x6504b20], [0, 0x64d8ae0], [1, 0x64cfee0], [2, 0x64dd540], [3, 0x64dfca0]]}
  layer.3.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6426100], [1, 0x641d500], [2, 0x642a2a0], [3, 0x642ca00], [4, 0x6449e20], [5, 0x645b1c0], [6, 0x6456bc0], [7, 0x644bcc0], [0, 0x6449120], [1, 0x6440520], [2, 0x644d2c0], [3, 0x644fa20], [4, 0x646ce40], [5, 0x647e1e0], [6, 0x6479be0], [7, 0x646ece0], [0, 0x646c140], [1, 0x6463540], [2, 0x64702e0], [3, 0x6472a40], [4, 0x648fe60], [5, 0x64a1200], [6, 0x649cc00], [7, 0x6491d00], [0, 0x648f160], [1, 0x6486560], [2, 0x6493300], [3, 0x6495a60], [4, 0x64b2e80], [5, 0x64c4220], [6, 0x64bfc20], [7, 0x64b4d20]]}
  layer.3.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64b2180], [1, 0x64a9580], [2, 0x64b6320], [3, 0x64b8a80], [4, 0x64d5ea0], [5, 0x64e7240], [6, 0x64e2c40], [7, 0x64d7d40]]}
  layer.3.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x64e3de0]]}
  layer.3.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x64d8ee0]]}
  layer.4.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64b37a0], [1, 0x64aaba0], [2, 0x64b8200], [3, 0x64ba960], [4, 0x64d7d80], [5, 0x64e9120], [6, 0x64eca00], [7, 0x64e1b00]]}
  layer.4.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64d67c0], [1, 0x64cdbc0], [2, 0x64db220], [3, 0x64dd980]]}
  layer.4.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64030e0], [1, 0x63fa4e0], [2, 0x6407280], [3, 0x64099e0], [4, 0x6426e00], [5, 0x64381a0], [6, 0x6433ba0], [7, 0x6428ca0]]}
  layer.4.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64ff3c0], [5, 0x6510760], [6, 0x6514040], [7, 0x6509140]]}
  layer.4.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x64e1b60], [3, 0x64e42c0], [4, 0x65016e0], [5, 0x6512a80], [6, 0x6516360], [7, 0x650b460], [0, 0x64dd580], [1, 0x64d4980]]}
  layer.4.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6504b80], [3, 0x65072e0], [4, 0x6524700], [5, 0x6535aa0]]}
  layer.4.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6539380], [7, 0x652e480], [0, 0x65005a0], [1, 0x64f79a0], [2, 0x6506ea0], [3, 0x6509600], [4, 0x6526a20], [5, 0x6537dc0]]}
  layer.4.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x655c3a0], [7, 0x65514a0], [0, 0x65235c0], [1, 0x651a9c0]]}
  layer.4.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x67d0880]]}
  layer.4.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x67e9b00]]}
  layer.4.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x67fd1a0], [7, 0x67e5dc0], [0, 0x67a7860], [1, 0x679f520], [2, 0x67add00], [3, 0x67b41a0], [4, 0x67d94a0], [5, 0x67f2720], [6, 0x68201c0], [7, 0x6808de0], [0, 0x67ca880], [1, 0x67c2540], [2, 0x67d0d20], [3, 0x67d71c0], [4, 0x67fc4c0], [5, 0x6815740], [6, 0x68431e0], [7, 0x682be00], [0, 0x67ed8a0], [1, 0x67e5560], [2, 0x67f3d40], [3, 0x67fa1e0], [4, 0x681f4e0], [5, 0x6838760], [6, 0x6866200], [7, 0x684ee20], [0, 0x68108c0], [1, 0x6808580], [2, 0x6816d60], [3, 0x681d200], [4, 0x6842500], [5, 0x685b780]]}
  layer.4.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6889220], [7, 0x6871e40], [0, 0x68338e0], [1, 0x682b5a0], [2, 0x6839d80], [3, 0x6840220], [4, 0x6865520], [5, 0x687e7a0]]}
  layer.4.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x688d840], [7, 0x6876460], [0, 0x6837f00], [1, 0x682fbc0], [2, 0x683e3a0], [3, 0x6844840], [4, 0x6869b40], [5, 0x6882dc0], [6, 0x68b0860], [7, 0x6899480], [0, 0x685af20], [1, 0x6852be0], [2, 0x68613c0], [3, 0x6867860], [4, 0x688cb60], [5, 0x68a5de0], [6, 0x68d3880], [7, 0x68bc4a0], [0, 0x687df40], [1, 0x6875c00], [2, 0x68843e0], [3, 0x688a880], [4, 0x68afb80], [5, 0x68c8e00], [6, 0x68f68a0], [7, 0x68df4c0], [0, 0x68a0f60], [1, 0x6898c20], [2, 0x68a7400], [3, 0x68ad8a0], [4, 0x68d2ba0], [5, 0x68ebe20]]}
  layer.4.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x69198c0], [7, 0x69024e0], [0, 0x68c3f80], [1, 0x68bbc40], [2, 0x68ca420], [3, 0x68d08c0], [4, 0x68f5bc0], [5, 0x690ee40]]}
  layer.4.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x68f6d60]]}
  layer.4.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x690ffe0]]}
  layer.5.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x691aee0], [7, 0x690c2a0], [0, 0x68c5e60], [1, 0x68bdb20], [2, 0x68cc300], [3, 0x68d27a0], [4, 0x68ff980], [5, 0x6918c00]]}
  layer.5.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x693df00], [7, 0x692f2c0], [0, 0x68e8e80], [1, 0x68e0b40]]}
  layer.5.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68ef320], [3, 0x68f57c0], [4, 0x69229a0], [5, 0x693bc20], [6, 0x6940220], [7, 0x69315e0], [0, 0x68eb1a0], [1, 0x68e2e60]]}
  layer.5.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6912340], [3, 0x69187e0], [4, 0x69459c0], [5, 0x695ec40]]}
  layer.5.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6640600], [1, 0x66382c0], [2, 0x6645920], [3, 0x664bdc0], [4, 0x66691e0], [5, 0x6682460], [6, 0x6685d40], [7, 0x667f8a0]]}
  layer.5.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6663620], [1, 0x665b2e0], [2, 0x6668940], [3, 0x666ede0]]}
  layer.5.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x668c200], [5, 0x66a5480], [6, 0x66a8d60], [7, 0x66a28c0], [0, 0x6665940], [1, 0x665d600], [2, 0x666ac60], [3, 0x6671100]]}
  layer.5.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x66af220], [5, 0x66c84a0], [6, 0x66cbd80], [7, 0x66c58e0]]}
  layer.5.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x66ce0a0]]}
  layer.5.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6676c80]]}
  layer.5.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6688de0], [1, 0x6680aa0], [2, 0x668e9c0], [3, 0x6694e60], [4, 0x66b2280], [5, 0x66cb500], [6, 0x66d6cc0], [7, 0x66c8080], [0, 0x66abe00], [1, 0x66a3ac0], [2, 0x66b19e0], [3, 0x66b7e80], [4, 0x66d52a0], [5, 0x66ee520], [6, 0x66f9ce0], [7, 0x66eb0a0], [0, 0x66cee20], [1, 0x66c6ae0], [2, 0x66d4a00], [3, 0x66daea0], [4, 0x66f82c0], [5, 0x6711540], [6, 0x671cd00], [7, 0x670e0c0], [0, 0x66f1e40], [1, 0x66e9b00], [2, 0x66f7a20], [3, 0x66fdec0], [4, 0x671b2e0], [5, 0x6734560], [6, 0x673fd20], [7, 0x67310e0]]}
  layer.5.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6714e60], [1, 0x670cb20], [2, 0x671aa40], [3, 0x6720ee0], [4, 0x673e300], [5, 0x6757580], [6, 0x6762d40], [7, 0x6754100]]}
  layer.5.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6719480], [1, 0x6711140], [2, 0x671f060], [3, 0x6725500], [4, 0x6742920], [5, 0x675bba0], [6, 0x6767360], [7, 0x6758720], [0, 0x673c4a0], [1, 0x6734160], [2, 0x6742080], [3, 0x6748520], [4, 0x6765940], [5, 0x677ebc0], [6, 0x678a380], [7, 0x677b740], [0, 0x675f4c0], [1, 0x6757180], [2, 0x67650a0], [3, 0x676b540], [4, 0x6788960], [5, 0x67a1be0], [6, 0x67ad3a0], [7, 0x679e760], [0, 0x67824e0], [1, 0x677a1a0], [2, 0x67880c0], [3, 0x678e560], [4, 0x67ab980], [5, 0x67c4c00], [6, 0x67d03c0], [7, 0x67c1780]]}
  layer.5.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x67a5500], [1, 0x679d1c0], [2, 0x67ab0e0], [3, 0x67b1580], [4, 0x67ce9a0], [5, 0x67e7c20], [6, 0x67f33e0], [7, 0x67e47a0]]}
  layer.5.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x67f4580]]}
  layer.5.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x750ac40]]}
  layer.6.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x733e6c0], [0, 0x72e54c0], [1, 0x72e6f00], [2, 0x72f19a0], [3, 0x72f7580], [4, 0x7340140], [5, 0x734f1c0], [6, 0x735e280]]}
  layer.6.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x74f3ca0], [0, 0x749aaa0], [1, 0x749bc20], [2, 0x74a66c0]]}
  layer.6.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x74a3b00], [4, 0x74ec6c0], [5, 0x74fc000], [6, 0x7513860], [7, 0x74f5fc0], [0, 0x749cdc0], [1, 0x749df40], [2, 0x74a89e0]]}
  layer.6.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x74c6b20], [4, 0x750f6e0], [5, 0x751f020], [6, 0x7536880]]}
  layer.6.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74c0f60], [2, 0x74cba00], [3, 0x74c8e40], [4, 0x7511a00], [5, 0x7521340], [6, 0x7538ba0], [7, 0x7519460], [0, 0x74c0260]]}
  layer.6.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74e3f80], [2, 0x74eea20], [3, 0x74ebe60], [4, 0x7534a20]]}
  layer.6.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x74e7c20], [7, 0x74d0c80], [0, 0x7477a80], [1, 0x7478c00], [2, 0x74836a0], [3, 0x7480ae0], [4, 0x74c96a0], [5, 0x74d8fe0]]}
  layer.6.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x755bbc0], [7, 0x753c480], [0, 0x74e3280], [1, 0x74e62a0]]}
  layer.6.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x74e55a0]]}
  layer.6.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74e85c0]]}
  layer.6.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74f11c0], [3, 0x74ee600], [4, 0x7537a80], [5, 0x754dcc0], [6, 0x755ec20], [7, 0x753f4e0], [0, 0x74ee1c0], [1, 0x74f11e0], [2, 0x75141e0], [3, 0x7511620], [4, 0x755aaa0], [5, 0x7570ce0], [6, 0x7581c40], [7, 0x7562500], [0, 0x75111e0], [1, 0x7514200], [2, 0x7537200], [3, 0x7534640], [4, 0x757dac0], [5, 0x7593d00], [6, 0x75a4c60], [7, 0x7585520], [0, 0x7534200], [1, 0x7537220], [2, 0x755a220], [3, 0x7557660], [4, 0x75a0ae0], [5, 0x75b6d20], [6, 0x75c7c80], [7, 0x75a8540], [0, 0x7557220], [1, 0x755a240]]}
  layer.6.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x757d240], [3, 0x757a680], [4, 0x75c3b00], [5, 0x75d9d40], [6, 0x75eaca0], [7, 0x75cb560], [0, 0x757a240], [1, 0x757d260]]}
  layer.6.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x73fa580], [0, 0x73a1380], [1, 0x73a2500], [2, 0x73acfa0], [3, 0x73ac280], [4, 0x73f4e40], [5, 0x7404780], [6, 0x7413840], [7, 0x741d5a0], [0, 0x73c43a0], [1, 0x73c5520], [2, 0x73cffc0], [3, 0x73cf2a0], [4, 0x7417e60], [5, 0x74277a0], [6, 0x7436860], [7, 0x74405c0], [0, 0x73e73c0], [1, 0x73e8540], [2, 0x73f2fe0], [3, 0x73f22c0], [4, 0x743ae80], [5, 0x744a7c0], [6, 0x7459880], [7, 0x74635e0], [0, 0x740a3e0], [1, 0x740b560], [2, 0x7416000], [3, 0x74152e0], [4, 0x745dea0], [5, 0x746d7e0], [6, 0x747c8a0]]}
  layer.6.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7385920], [4, 0x73ce4e0], [5, 0x73dd560], [6, 0x73ec620], [7, 0x73d5680], [0, 0x737c480], [1, 0x7375720], [2, 0x73801c0]]}
  layer.6.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x73768c0]]}
  layer.6.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7381360]]}
  layer.7.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7386f40], [4, 0x73cfb00], [5, 0x73df440], [6, 0x73ee500], [7, 0x73d7560], [0, 0x737e360], [1, 0x737f4e0], [2, 0x7389f80]]}
  layer.7.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x73a9f60], [4, 0x73f2b20], [5, 0x7402460], [6, 0x7411520]]}
  layer.7.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7362900], [4, 0x73ab4c0], [5, 0x73ba540], [6, 0x73c9600], [7, 0x73b2660], [0, 0x7359460], [1, 0x7352700], [2, 0x735d1a0]]}
  layer.7.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7486600], [0, 0x742d400], [1, 0x742e580], [2, 0x7439020]]}
  layer.7.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7490800], [6, 0x749f8c0], [7, 0x7488920], [0, 0x742f720], [1, 0x74308a0], [2, 0x743b340], [3, 0x7438780], [4, 0x7481340]]}
  layer.7.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x74b3820], [6, 0x74c28e0], [7, 0x74ab940], [0, 0x7452740]]}
  layer.7.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74538c0], [2, 0x745e360], [3, 0x745b7a0], [4, 0x74a4360], [5, 0x74b5b40], [6, 0x74c4c00], [7, 0x74adc60], [0, 0x7454a60]]}
  layer.7.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x74768e0], [2, 0x7481380], [3, 0x747e7c0], [4, 0x74c7380]]}
  layer.7.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x76e7d80]]}
  layer.7.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x76f5cc0]]}
  layer.7.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x76f0540], [2, 0x76f09a0], [3, 0x76e5f00], [4, 0x7730500], [5, 0x7746740], [6, 0x77576a0], [7, 0x77376a0], [0, 0x76fe8e0], [1, 0x7713560], [2, 0x77139c0], [3, 0x7708f20], [4, 0x7753520], [5, 0x7769760], [6, 0x777a6c0], [7, 0x775a6c0], [0, 0x7721900], [1, 0x7736580], [2, 0x77369e0], [3, 0x772bf40], [4, 0x7776540], [5, 0x778c780], [6, 0x779d6e0], [7, 0x777d6e0], [0, 0x7744920], [1, 0x77595a0], [2, 0x7759a00], [3, 0x774ef60], [4, 0x7799560], [5, 0x77af7a0], [6, 0x77c0700], [7, 0x77a0700], [0, 0x7767940]]}
  layer.7.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x777c5c0], [2, 0x777ca20], [3, 0x7771f80], [4, 0x77bc580], [5, 0x77d27c0], [6, 0x77e3720], [7, 0x77c3720], [0, 0x778a960]]}
  layer.7.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7780be0], [2, 0x7781040], [3, 0x77765a0], [4, 0x77c0ba0], [5, 0x77d6de0], [6, 0x77e7d40], [7, 0x77c7d40], [0, 0x778ef80], [1, 0x77a3c00], [2, 0x77a4060], [3, 0x77995c0], [4, 0x77e3bc0], [5, 0x77f9e00], [6, 0x780ad60], [7, 0x77ead60], [0, 0x77b1fa0], [1, 0x77c6c20], [2, 0x77c7080], [3, 0x77bc5e0], [4, 0x7806be0], [5, 0x781ce20], [6, 0x782dd80], [7, 0x780dd80], [0, 0x77d4fc0], [1, 0x77e9c40], [2, 0x77ea0a0], [3, 0x77df600], [4, 0x7829c00], [5, 0x783fe40], [6, 0x7850da0], [7, 0x7830da0], [0, 0x77f7fe0]]}
  layer.7.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x780cc60], [2, 0x780d0c0], [3, 0x7802620], [4, 0x784cc20], [5, 0x7862e60], [6, 0x7873dc0], [7, 0x7853dc0], [0, 0x781b000]]}
  layer.7.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x784ddc0]]}
  layer.7.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7864000]]}
  layer.8.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7874f60], [7, 0x7854f60], [0, 0x781c1a0], [1, 0x780e280], [2, 0x780e6e0], [3, 0x7803c40], [4, 0x78569e0], [5, 0x786cc20]]}
  layer.8.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7897f80], [7, 0x7877f80], [0, 0x783f1c0], [1, 0x78312a0]]}
  layer.8.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7831700], [3, 0x7826c60], [4, 0x7879a00], [5, 0x788fc40], [6, 0x789a2a0], [7, 0x787a2a0], [0, 0x78414e0], [1, 0x78335c0]]}
  layer.8.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7854720], [3, 0x7849c80], [4, 0x789ca20], [5, 0x78b2c60]]}
  layer.8.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x760d8e0], [3, 0x760ad20], [4, 0x76541a0], [5, 0x766a3e0], [6, 0x767b340], [7, 0x765bc00], [0, 0x760a8e0], [1, 0x760d900]]}
  layer.8.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7630900], [3, 0x762dd40], [4, 0x76771c0], [5, 0x768d400]]}
  layer.8.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x769e360], [7, 0x767ec20], [0, 0x762d900], [1, 0x7630920], [2, 0x7632c20], [3, 0x7630060], [4, 0x76794e0], [5, 0x768f720]]}
  layer.8.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x76c1380], [7, 0x76a1c40], [0, 0x7650920], [1, 0x7653940]]}
  layer.8.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7655c60]]}
  layer.8.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7652c40]]}
  layer.8.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7581860], [3, 0x757eca0], [4, 0x75c8120], [5, 0x75de360], [6, 0x75ef2c0], [7, 0x75cfb80], [0, 0x757e860], [1, 0x7581880], [2, 0x75a4880], [3, 0x75a1cc0], [4, 0x75eb140], [5, 0x7601380], [6, 0x76122e0], [7, 0x75f2ba0], [0, 0x75a1880], [1, 0x75a48a0], [2, 0x75c78a0], [3, 0x75c4ce0], [4, 0x760e160], [5, 0x76243a0], [6, 0x7635300], [7, 0x7615bc0], [0, 0x75c48a0], [1, 0x75c78c0], [2, 0x75ea8c0], [3, 0x75e7d00], [4, 0x7631180], [5, 0x76473c0], [6, 0x7658320], [7, 0x7638be0], [0, 0x75e78c0], [1, 0x75ea8e0]]}
  layer.8.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x76560c0], [3, 0x7653500], [4, 0x769d240], [5, 0x76b3480], [6, 0x76c43e0], [7, 0x76a4ca0], [0, 0x765b860], [1, 0x765e880]]}
  layer.8.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x765a6e0], [3, 0x7657b20], [4, 0x76a1860], [5, 0x76b7aa0], [6, 0x76c8a00], [7, 0x76a92c0], [0, 0x765fe80], [1, 0x7662ea0], [2, 0x767d700], [3, 0x767ab40], [4, 0x76c4880], [5, 0x76daac0], [6, 0x76eba20], [7, 0x76cc2e0], [0, 0x7682ea0], [1, 0x7685ec0], [2, 0x76a0720], [3, 0x769db60], [4, 0x76e78a0], [5, 0x76fdae0], [6, 0x770ea40], [7, 0x76ef300], [0, 0x76a5ec0], [1, 0x76a8ee0], [2, 0x76c3740], [3, 0x76c0b80], [4, 0x770a8c0], [5, 0x7720b00], [6, 0x7731a60], [7, 0x7712320], [0, 0x76c8ee0], [1, 0x76cbf00]]}
  layer.8.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x76e6760], [3, 0x76e3ba0], [4, 0x772d8e0], [5, 0x7743b20], [6, 0x7754a80], [7, 0x7735340], [0, 0x76ebf00], [1, 0x76eef20]]}
  layer.8.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7544360]]}
  layer.8.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x76ed0a0]]}
  layer.9.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f4b3c0], [3, 0x6f490c0], [4, 0x6f8e800], [5, 0x6fa6020], [6, 0x6f9d440], [7, 0x6f85be0], [0, 0x6f40da0], [1, 0x6f427e0]]}
  layer.9.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f6e3e0], [3, 0x6f6c0e0], [4, 0x6fb1820], [5, 0x6fc9040]]}
  layer.9.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6fc0460], [7, 0x6fa8c00], [0, 0x6f63dc0], [1, 0x6f65800], [2, 0x6f70700], [3, 0x6f6e400], [4, 0x6fb3b40], [5, 0x6fcb360]]}
  layer.9.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6fe3480], [7, 0x6fcbc20], [0, 0x6f86de0], [1, 0x6f88820]]}
  layer.9.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6fd6b60], [5, 0x6fee380], [6, 0x6fe57a0], [7, 0x6fcdf40], [0, 0x6f89100], [1, 0x6f8ab40], [2, 0x6f93ba0], [3, 0x6f918a0]]}
  layer.9.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6ff9b80], [5, 0x70113a0], [6, 0x70087c0], [7, 0x6ff0f60]]}
  layer.9.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f283a0], [3, 0x6f260a0], [4, 0x6f6b7e0], [5, 0x6f83000], [6, 0x6f7a420], [7, 0x6f62bc0], [0, 0x6f1dd80], [1, 0x6f1f7c0]]}
  layer.9.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6ffbea0], [5, 0x70136c0], [6, 0x700aae0], [7, 0x6ff3280]]}
  layer.9.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x700ce00]]}
  layer.9.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6ff55a0]]}
  layer.9.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6fad2e0], [1, 0x6faed20], [2, 0x6fb8640], [3, 0x6fb6340], [4, 0x6ffef00], [5, 0x7016720], [6, 0x7015a20], [7, 0x6ffe1c0], [0, 0x6fd0300], [1, 0x6fd1d40], [2, 0x6fdb660], [3, 0x6fd9360], [4, 0x7021f20], [5, 0x7039740], [6, 0x7038a40], [7, 0x70211e0], [0, 0x6ff3320], [1, 0x6ff4d60], [2, 0x6ffe680], [3, 0x6ffc380], [4, 0x7044f40], [5, 0x705c760], [6, 0x705ba60], [7, 0x7044200], [0, 0x7016340], [1, 0x7017d80], [2, 0x70216a0], [3, 0x701f3a0], [4, 0x7067f60], [5, 0x707f780], [6, 0x707ea80], [7, 0x7067220]]}
  layer.9.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7039360], [1, 0x703ada0], [2, 0x70446c0], [3, 0x70423c0], [4, 0x708af80], [5, 0x70a27a0], [6, 0x70a1aa0], [7, 0x708a240]]}
  layer.9.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6ea5bc0], [7, 0x6e8e360], [0, 0x6e49520], [1, 0x6e4af60], [2, 0x6e53fc0], [3, 0x6e51cc0], [4, 0x6e97400], [5, 0x6eaec20], [6, 0x6ec8be0], [7, 0x6eb1380], [0, 0x6e6c540], [1, 0x6e6df80], [2, 0x6e76fe0], [3, 0x6e74ce0], [4, 0x6eba420], [5, 0x6ed1c40], [6, 0x6eebc00], [7, 0x6ed43a0], [0, 0x6e8f560], [1, 0x6e90fa0], [2, 0x6e9a000], [3, 0x6e97d00], [4, 0x6edd440], [5, 0x6ef4c60], [6, 0x6f0ec20], [7, 0x6ef73c0], [0, 0x6eb2580], [1, 0x6eb3fc0], [2, 0x6ebd020], [3, 0x6ebad20], [4, 0x6f00460], [5, 0x6f17c80]]}
  layer.9.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e0a640], [3, 0x6e08340], [4, 0x6e4d1c0], [5, 0x6e649e0], [6, 0x6e5dca0], [7, 0x6e46440], [0, 0x6df9720], [1, 0x6dfb160]]}
  layer.9.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6dfa8c0]]}
  layer.9.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6dfc300]]}
  layer.10.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e0bc60], [3, 0x6e09960], [4, 0x6e4f0a0], [5, 0x6e668c0], [6, 0x6e5fb80], [7, 0x6e48320], [0, 0x6e034e0], [1, 0x6e04f20]]}
  layer.10.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e2ec80], [3, 0x6e2c980], [4, 0x6e720c0], [5, 0x6e898e0]]}
  layer.10.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e82ba0], [7, 0x6e6b340], [0, 0x6e26500], [1, 0x6e27f40], [2, 0x6e30fa0], [3, 0x6e2eca0], [4, 0x6e743e0], [5, 0x6e8bc00]]}
  layer.10.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e5b980], [7, 0x6e44120], [0, 0x6df7400], [1, 0x6df8e40]]}
  layer.10.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ed55a0], [1, 0x6ed6fe0], [2, 0x6ee0040], [3, 0x6eddd40], [4, 0x6f23480], [5, 0x6f3aca0], [6, 0x6f320c0], [7, 0x6f1a860]]}
  layer.10.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ef85c0], [1, 0x6efa000], [2, 0x6f03060], [3, 0x6f00d60]]}
  layer.10.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6f464a0], [5, 0x6f5dcc0], [6, 0x6f550e0], [7, 0x6f3d880], [0, 0x6efa8e0], [1, 0x6efc320], [2, 0x6f05380], [3, 0x6f03080]]}
  layer.10.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6f694c0], [5, 0x6f80ce0], [6, 0x6f78100], [7, 0x6f608a0]]}
  layer.10.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x71d10a0]]}
  layer.10.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7219c60]]}
  layer.10.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7231480], [6, 0x7240540], [7, 0x7220e00], [0, 0x71c7c00], [1, 0x71c9640], [2, 0x71d40e0], [3, 0x71d9cc0], [4, 0x7222880], [5, 0x72544a0], [6, 0x7263560], [7, 0x7243e20], [0, 0x71eac20], [1, 0x71ec660], [2, 0x71f7100], [3, 0x71fcce0], [4, 0x72458a0], [5, 0x72774c0], [6, 0x7286580], [7, 0x7266e40], [0, 0x720dc40], [1, 0x720f680], [2, 0x721a120], [3, 0x721fd00], [4, 0x72688c0], [5, 0x729a4e0], [6, 0x72a95a0], [7, 0x7289e60], [0, 0x7230c60], [1, 0x72326a0], [2, 0x723d140], [3, 0x7242d20], [4, 0x728b8e0]]}
  layer.10.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x72bd500], [6, 0x72cc5c0], [7, 0x72ace80], [0, 0x7253c80], [1, 0x72556c0], [2, 0x7260160], [3, 0x7265d40], [4, 0x72ae900]]}
  layer.10.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x72c1b20], [6, 0x72d0be0], [7, 0x72b14a0], [0, 0x72582a0], [1, 0x7259ce0], [2, 0x7264780], [3, 0x726a360], [4, 0x72b2f20], [5, 0x72e4b40], [6, 0x72f3c00], [7, 0x72d44c0], [0, 0x727b2c0], [1, 0x727cd00], [2, 0x72877a0], [3, 0x728d380], [4, 0x72d5f40], [5, 0x7307b60], [6, 0x7316c20], [7, 0x72f74e0], [0, 0x729e2e0], [1, 0x729fd20], [2, 0x72aa7c0], [3, 0x72b03a0], [4, 0x72f8f60], [5, 0x732ab80], [6, 0x7339c40], [7, 0x731a500], [0, 0x72c1300], [1, 0x72c2d40], [2, 0x72cd7e0], [3, 0x72d33c0], [4, 0x731bf80]]}
  layer.10.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x734dba0], [6, 0x735cc60], [7, 0x733d520], [0, 0x72e4320], [1, 0x72e5d60], [2, 0x72f0800], [3, 0x72f63e0], [4, 0x733efa0]]}
  layer.10.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x73616e0]]}
  layer.10.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x73084e0]]}
  layer.11.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7309f20], [2, 0x73149c0], [3, 0x731a5a0], [4, 0x7363160], [5, 0x73721e0], [6, 0x73812a0], [7, 0x736a300], [0, 0x7311100]]}
  layer.11.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x732cf40], [2, 0x73379e0], [3, 0x733d5c0], [4, 0x7386180]]}
  layer.11.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x7395200], [6, 0x73a42c0], [7, 0x738d320], [0, 0x7334120], [1, 0x732f260], [2, 0x7339d00], [3, 0x733f8e0], [4, 0x73884a0]]}
  layer.11.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x73b8220], [6, 0x73c72e0], [7, 0x73b0340], [0, 0x7357140]]}
  layer.11.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x70ef1c0], [1, 0x70f0c00], [2, 0x70fade0], [3, 0x70f8ae0], [4, 0x71416a0], [5, 0x7158ec0], [6, 0x71600a0], [7, 0x7148840]]}
  layer.11.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x70c9a00], [1, 0x70cb440], [2, 0x70d4d60], [3, 0x70d2a60]]}
  layer.11.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x711b620], [5, 0x7132e40], [6, 0x7132140], [7, 0x711a8e0], [0, 0x70cbd20], [1, 0x70cd760], [2, 0x70d7080], [3, 0x70d4d80]]}
  layer.11.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x713e640], [5, 0x7155e60], [6, 0x7155160], [7, 0x713d900]]}
  layer.11.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7157480]]}
  layer.11.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x713fc20]]}
  layer.11.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x703d980], [1, 0x703f3c0], [2, 0x7048ce0], [3, 0x70469e0], [4, 0x708f5a0], [5, 0x70a6dc0], [6, 0x70a60c0], [7, 0x708e860], [0, 0x70609a0], [1, 0x70623e0], [2, 0x706bd00], [3, 0x7069a00], [4, 0x70b25c0], [5, 0x70c9de0], [6, 0x70c90e0], [7, 0x70b1880], [0, 0x70839c0], [1, 0x7085400], [2, 0x708ed20], [3, 0x708ca20], [4, 0x70d55e0], [5, 0x70ece00], [6, 0x70ec100], [7, 0x70d48a0], [0, 0x70a69e0], [1, 0x70a8420], [2, 0x70b1d40], [3, 0x70afa40], [4, 0x70f8600], [5, 0x710fe20], [6, 0x710f120], [7, 0x70f78c0]]}
  layer.11.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x71121e0], [1, 0x7113c20], [2, 0x711de00], [3, 0x711bb00], [4, 0x71646c0], [5, 0x717bee0], [6, 0x71830c0], [7, 0x716b860]]}
  layer.11.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7116800], [1, 0x7118240], [2, 0x7122420], [3, 0x7120120], [4, 0x7168ce0], [5, 0x7180500], [6, 0x71876e0], [7, 0x716fe80], [0, 0x7139820], [1, 0x713b260], [2, 0x7145440], [3, 0x7143140], [4, 0x718bd00], [5, 0x71a3520], [6, 0x71aa700], [7, 0x7192ea0], [0, 0x715c840], [1, 0x715e280], [2, 0x7168460], [3, 0x7166160], [4, 0x71aed20], [5, 0x71c6540], [6, 0x71cd720], [7, 0x71b5ec0], [0, 0x717f860], [1, 0x71812a0], [2, 0x718b480], [3, 0x7189180], [4, 0x71d1d40], [5, 0x71e9560], [6, 0x71f0740], [7, 0x71d8ee0]]}
  layer.11.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x71a2880], [1, 0x71a42c0], [2, 0x71ae4a0], [3, 0x71ac1a0], [4, 0x71f4d60], [5, 0x720c580], [6, 0x7213760], [7, 0x71fbf00]]}
  layer.11.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7214900]]}
  layer.11.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6e0aa00]]}
  layer.12.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x71fd0a0], [0, 0x71a3ea0], [1, 0x71a58e0], [2, 0x71b0380], [3, 0x71ae080], [4, 0x71f6c40], [5, 0x720e460], [6, 0x721d520]]}
  layer.12.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x53463e0], [7, 0x533b4e0], [0, 0x533b960], [1, 0x5332d60]]}
  layer.12.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x554fe40], [3, 0x55525a0], [4, 0x5562c20], [5, 0x5573fc0], [6, 0x5576b80], [7, 0x556bc80], [0, 0x555ba80], [1, 0x5552e80]]}
  layer.12.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5572e60], [3, 0x55755c0], [4, 0x5585c40], [5, 0x5596fe0]]}
  layer.12.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x557eaa0], [1, 0x5575ea0], [2, 0x5575180], [3, 0x55778e0], [4, 0x5587f60], [5, 0x5599300], [6, 0x559a020], [7, 0x558f120]]}
  layer.12.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x55a1ac0], [1, 0x5598ec0], [2, 0x55981a0], [3, 0x559a900]]}
  layer.12.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x55aaf80], [5, 0x55bc320], [6, 0x55bd040], [7, 0x55b2140], [0, 0x55a3de0], [1, 0x559b1e0], [2, 0x559a4c0], [3, 0x559cc20]]}
  layer.12.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5551840], [7, 0x5546940], [0, 0x5536740], [1, 0x552db40]]}
  layer.12.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x55e0da0]]}
  layer.12.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x55d5ea0]]}
  layer.12.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x55c7280], [1, 0x55be680], [2, 0x55be220], [3, 0x55c0980], [4, 0x55cfa20], [5, 0x55e0dc0], [6, 0x55e99c0], [7, 0x55deac0], [0, 0x55ea2a0], [1, 0x55e16a0], [2, 0x55e1240], [3, 0x55e39a0], [4, 0x55f2a40], [5, 0x5603de0], [6, 0x560c9e0], [7, 0x5601ae0], [0, 0x560d2c0], [1, 0x56046c0], [2, 0x5604260], [3, 0x56069c0], [4, 0x5615a60], [5, 0x5626e00], [6, 0x562fa00], [7, 0x5624b00], [0, 0x56302e0], [1, 0x56276e0], [2, 0x5627280], [3, 0x56299e0], [4, 0x5638a80], [5, 0x5649e20], [6, 0x5652a20], [7, 0x5647b20]]}
  layer.12.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5653300], [1, 0x564a700], [2, 0x564a2a0], [3, 0x564ca00], [4, 0x565baa0], [5, 0x566ce40], [6, 0x5675a40], [7, 0x566ab40]]}
  layer.12.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5657920], [1, 0x564ed20], [2, 0x564e8c0], [3, 0x5651020], [4, 0x56600c0], [5, 0x5671460], [6, 0x567a060], [7, 0x566f160], [0, 0x567a940], [1, 0x5671d40], [2, 0x56718e0], [3, 0x5674040], [4, 0x56830e0], [5, 0x5694480], [6, 0x569d080], [7, 0x5692180], [0, 0x569d960], [1, 0x5694d60], [2, 0x5694900], [3, 0x5697060], [4, 0x56a6100], [5, 0x56b74a0], [6, 0x56c00a0], [7, 0x56b51a0], [0, 0x56c0980], [1, 0x56b7d80], [2, 0x56b7920], [3, 0x56ba080], [4, 0x56c9120], [5, 0x56da4c0], [6, 0x56e30c0], [7, 0x56d81c0]]}
  layer.12.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x54e34a0], [3, 0x54e5c00], [4, 0x54f6280], [5, 0x5507620], [6, 0x5508340], [7, 0x54fd440], [0, 0x54ed240], [1, 0x54e4640]]}
  layer.12.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x54a7620]]}
  layer.12.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x54b89c0]]}
  layer.13.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x54bffe0], [7, 0x54b50e0], [0, 0x54a4ee0], [1, 0x549c2e0], [2, 0x549d460], [3, 0x549fbc0], [4, 0x54b0240], [5, 0x54c15e0]]}
  layer.13.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x54e3000], [7, 0x54d8100], [0, 0x54c7f00], [1, 0x54bf300]]}
  layer.13.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x54c0480], [3, 0x54c2be0], [4, 0x54d3260], [5, 0x54e4600], [6, 0x54e5320], [7, 0x54da420], [0, 0x54ca220], [1, 0x54c1620]]}
  layer.13.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x549a400], [3, 0x549cb60], [4, 0x54a5300], [5, 0x54b66a0]]}
  layer.13.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x54f7420], [5, 0x55087c0], [6, 0x55094e0], [7, 0x54fe5e0], [0, 0x54ee3e0], [1, 0x54e57e0], [2, 0x54e4ac0], [3, 0x54e7220]]}
  layer.13.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x551a440], [5, 0x552b7e0], [6, 0x552c500], [7, 0x5521600]]}
  layer.13.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5511400], [1, 0x5508800], [2, 0x5507ae0], [3, 0x550a240], [4, 0x551c760], [5, 0x552db00], [6, 0x552e820], [7, 0x5523920]]}
  layer.13.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5534420], [1, 0x552b820], [2, 0x552ab00], [3, 0x552d260]]}
  layer.13.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x58537e0]]}
  layer.13.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5864b80]]}
  layer.13.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x587d540], [7, 0x5872640], [0, 0x584ac00], [1, 0x5842000], [2, 0x5842d20], [3, 0x5845480], [4, 0x585c400], [5, 0x586d7a0], [6, 0x58a0560], [7, 0x5895660], [0, 0x586dc20], [1, 0x5865020], [2, 0x5865d40], [3, 0x58684a0], [4, 0x587f420], [5, 0x58907c0], [6, 0x58c3580], [7, 0x58b8680], [0, 0x5890c40], [1, 0x5888040], [2, 0x5888d60], [3, 0x588b4c0], [4, 0x58a2440], [5, 0x58b37e0], [6, 0x58e65a0], [7, 0x58db6a0], [0, 0x58b3c60], [1, 0x58ab060], [2, 0x58abd80], [3, 0x58ae4e0], [4, 0x58c5460], [5, 0x58d6800]]}
  layer.13.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x59095c0], [7, 0x58fe6c0], [0, 0x58d6c80], [1, 0x58ce080], [2, 0x58ceda0], [3, 0x58d1500], [4, 0x58e8480], [5, 0x58f9820]]}
  layer.13.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x590dbe0], [7, 0x5902ce0], [0, 0x58db2a0], [1, 0x58d26a0], [2, 0x58d33c0], [3, 0x58d5b20], [4, 0x58ecaa0], [5, 0x58fde40], [6, 0x5930c00], [7, 0x5925d00], [0, 0x58fe2c0], [1, 0x58f56c0], [2, 0x58f63e0], [3, 0x58f8b40], [4, 0x590fac0], [5, 0x5920e60], [6, 0x5953c20], [7, 0x5948d20], [0, 0x59212e0], [1, 0x59186e0], [2, 0x5919400], [3, 0x591bb60], [4, 0x5932ae0], [5, 0x5943e80], [6, 0x5976c40], [7, 0x596bd40], [0, 0x5944300], [1, 0x593b700], [2, 0x593c420], [3, 0x593eb80], [4, 0x5955b00], [5, 0x5966ea0]]}
  layer.13.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5999c60], [7, 0x598ed60], [0, 0x5967320], [1, 0x595e720], [2, 0x595f440], [3, 0x5961ba0], [4, 0x5978b20], [5, 0x5989ec0]]}
  layer.13.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x598b4e0]]}
  layer.13.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x59828e0]]}
  layer.14.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5983600], [3, 0x5985d60], [4, 0x599cce0], [5, 0x59ae080], [6, 0x59be2a0], [7, 0x59b33a0], [0, 0x5994100], [1, 0x598b500]]}
  layer.14.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59a6620], [3, 0x59a8d80], [4, 0x59bfd00], [5, 0x59d10a0]]}
  layer.14.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x59e12c0], [7, 0x59d63c0], [0, 0x59b7120], [1, 0x59ae520], [2, 0x59a8940], [3, 0x59ab0a0], [4, 0x59c2020], [5, 0x59d33c0]]}
  layer.14.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a042e0], [7, 0x59f93e0], [0, 0x59da140], [1, 0x59d1540]]}
  layer.14.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x57951e0], [1, 0x578c5e0], [2, 0x578ca40], [3, 0x578f1a0], [4, 0x579e240], [5, 0x57af5e0], [6, 0x57c00c0], [7, 0x57b51c0]]}
  layer.14.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x576fa20], [1, 0x5766e20], [2, 0x57669c0], [3, 0x5769120]]}
  layer.14.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x57781c0], [5, 0x5789560], [6, 0x5792160], [7, 0x5787260], [0, 0x5771d40], [1, 0x5769140], [2, 0x5768ce0], [3, 0x576b440]]}
  layer.14.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x579b1e0], [5, 0x57ac580], [6, 0x57b5180], [7, 0x57aa280]]}
  layer.14.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x57b74a0]]}
  layer.14.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57ac5a0]]}
  layer.14.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x56e39a0], [1, 0x56dada0], [2, 0x56da940], [3, 0x56dd0a0], [4, 0x56ec140], [5, 0x56fd4e0], [6, 0x57060e0], [7, 0x56fb1e0], [0, 0x57069c0], [1, 0x56fddc0], [2, 0x56fd960], [3, 0x57000c0], [4, 0x570f160], [5, 0x5720500], [6, 0x5729100], [7, 0x571e200], [0, 0x57299e0], [1, 0x5720de0], [2, 0x5720980], [3, 0x57230e0], [4, 0x5732180], [5, 0x5743520], [6, 0x574c120], [7, 0x5741220], [0, 0x574ca00], [1, 0x5743e00], [2, 0x57439a0], [3, 0x5746100], [4, 0x57551a0], [5, 0x5766540], [6, 0x576f140], [7, 0x5764240]]}
  layer.14.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x57b8200], [1, 0x57af600], [2, 0x57afa60], [3, 0x57b21c0], [4, 0x57c1260], [5, 0x57d2600], [6, 0x57e30e0], [7, 0x57d81e0]]}
  layer.14.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x57bc820], [1, 0x57b3c20], [2, 0x57b4080], [3, 0x57b67e0], [4, 0x57c5880], [5, 0x57d6c20], [6, 0x57e7700], [7, 0x57dc800], [0, 0x57df840], [1, 0x57d6c40], [2, 0x57d70a0], [3, 0x57d9800], [4, 0x57e88a0], [5, 0x57f9c40], [6, 0x580a720], [7, 0x57ff820], [0, 0x5802860], [1, 0x57f9c60], [2, 0x57fa0c0], [3, 0x57fc820], [4, 0x580b8c0], [5, 0x581cc60], [6, 0x582d740], [7, 0x5822840], [0, 0x5825880], [1, 0x581cc80], [2, 0x581d0e0], [3, 0x581f840], [4, 0x582e8e0], [5, 0x583fc80], [6, 0x5850760], [7, 0x5845860]]}
  layer.14.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x58488a0], [1, 0x583fca0], [2, 0x5840100], [3, 0x5842860], [4, 0x5851900], [5, 0x5862ca0], [6, 0x5873780], [7, 0x5868880]]}
  layer.14.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5874920]]}
  layer.14.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5869a20]]}
  layer.15.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x59684c0], [1, 0x595f8c0], [2, 0x59605e0], [3, 0x5962d40], [4, 0x5979cc0], [5, 0x598b060], [6, 0x599b280], [7, 0x5990380]]}
  layer.15.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5041d00], [5, 0x5041d00], [6, 0x5041d00], [7, 0x5049be0]]}
  layer.15.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5044020], [6, 0x5044020], [7, 0x504bf00], [0, 0x504c380], [1, 0x5043be0], [2, 0x5043be0], [3, 0x50444a0], [4, 0x504cc40]]}
  layer.15.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5067040], [6, 0x5067040], [7, 0x506ef20], [0, 0x506f3a0]]}
  layer.15.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x50674c0], [4, 0x506fc60], [5, 0x5069360], [6, 0x5069360], [7, 0x5071240], [0, 0x50716c0], [1, 0x5067080], [2, 0x5067080]]}
  layer.15.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x508a4e0], [4, 0x5092c80], [5, 0x508c380], [6, 0x508c380]]}
  layer.15.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5094260], [0, 0x50946e0], [1, 0x508a0a0], [2, 0x508a0a0], [3, 0x508c800], [4, 0x5094fa0], [5, 0x508e6a0], [6, 0x508e6a0]]}
  layer.15.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x504a060], [1, 0x50418c0], [2, 0x50418c0], [3, 0x5042180]]}
  layer.15.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x50b2860]]}
  layer.15.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x50b2860]]}
  layer.15.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50b88a0], [0, 0x50b8d20], [1, 0x50aefa0], [2, 0x50aefa0], [3, 0x50b1700], [4, 0x50b9ea0], [5, 0x50bb480], [6, 0x50bb480], [7, 0x50db8c0], [0, 0x50dbd40], [1, 0x50d1fc0], [2, 0x50d1fc0], [3, 0x50d4720], [4, 0x50dcec0], [5, 0x50de4a0], [6, 0x50de4a0], [7, 0x50fe8e0], [0, 0x50fed60], [1, 0x50f4fe0], [2, 0x50f4fe0], [3, 0x50f7740], [4, 0x50ffee0], [5, 0x51014c0], [6, 0x51014c0], [7, 0x5121900], [0, 0x5121d80], [1, 0x5118000], [2, 0x5118000], [3, 0x511a760], [4, 0x5122f00], [5, 0x51244e0], [6, 0x51244e0]]}
  layer.15.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5144920], [0, 0x5144da0], [1, 0x513b020], [2, 0x513b020], [3, 0x513d780], [4, 0x5145f20], [5, 0x5147500], [6, 0x5147500]]}
  layer.15.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5148f40], [0, 0x51493c0], [1, 0x513f640], [2, 0x513f640], [3, 0x5141da0], [4, 0x514a540], [5, 0x514bb20], [6, 0x514bb20], [7, 0x516bf60], [0, 0x516c3e0], [1, 0x5162660], [2, 0x5162660], [3, 0x5164dc0], [4, 0x516d560], [5, 0x516eb40], [6, 0x516eb40], [7, 0x518ef80], [0, 0x518f400], [1, 0x5185680], [2, 0x5185680], [3, 0x5187de0], [4, 0x5190580], [5, 0x5191b60], [6, 0x5191b60], [7, 0x51b1fa0], [0, 0x51b2420], [1, 0x51a86a0], [2, 0x51a86a0], [3, 0x51aae00], [4, 0x51b35a0], [5, 0x51b4b80], [6, 0x51b4b80]]}
  layer.15.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50b7280], [0, 0x50b7700], [1, 0x50ad0c0], [2, 0x50ad0c0], [3, 0x50af820], [4, 0x50b7fc0], [5, 0x50b16c0], [6, 0x50b16c0]]}
  layer.15.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  layer.15.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0d80]]}
  layer.16.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0d80], [2, 0x4fb0d80], [3, 0x4fb1640], [4, 0x4fb1640], [5, 0x4fb1640], [6, 0x4fb1640], [7, 0x4fb9520], [0, 0x4fb99a0]]}
  layer.16.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fd3da0], [2, 0x4fd3da0], [3, 0x4fd4660], [4, 0x4fd4660]]}
  layer.16.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fd4660], [6, 0x4fd4660], [7, 0x4fdc540], [0, 0x4fdc9c0], [1, 0x4fd60c0], [2, 0x4fd60c0], [3, 0x4fd6980], [4, 0x4fd6980]]}
  layer.16.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4ff7680], [6, 0x4ff7680], [7, 0x4fff560], [0, 0x4fff9e0]]}
  layer.16.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4ff99a0], [4, 0x4ff99a0], [5, 0x4ff99a0], [6, 0x4ff99a0], [7, 0x5001880], [0, 0x5001d00], [1, 0x4ff9560], [2, 0x4ff9560]]}
  layer.16.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x501c9c0], [4, 0x501c9c0], [5, 0x501c9c0], [6, 0x501c9c0]]}
  layer.16.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50248a0], [0, 0x5024d20], [1, 0x501c580], [2, 0x501c580], [3, 0x501ece0], [4, 0x501ece0], [5, 0x501ece0], [6, 0x501ece0]]}
  layer.16.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50478c0], [0, 0x5047d40], [1, 0x503f5a0], [2, 0x503f5a0]]}
  layer.16.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5348700]]}
  layer.16.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x533d800]]}
  layer.16.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x533dc80], [1, 0x5335080], [2, 0x5333aa0], [3, 0x5336200], [4, 0x533e9a0], [5, 0x534fd40], [6, 0x5351320], [7, 0x5346420], [0, 0x5360ca0], [1, 0x53580a0], [2, 0x5356ac0], [3, 0x5359220], [4, 0x53619c0], [5, 0x5372d60], [6, 0x5374340], [7, 0x5369440], [0, 0x5383cc0], [1, 0x537b0c0], [2, 0x5379ae0], [3, 0x537c240], [4, 0x53849e0], [5, 0x5395d80], [6, 0x5397360], [7, 0x538c460], [0, 0x53a6ce0], [1, 0x539e0e0], [2, 0x539cb00], [3, 0x539f260], [4, 0x53a7a00], [5, 0x53b8da0], [6, 0x53ba380], [7, 0x53af480]]}
  layer.16.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x53c9d00], [1, 0x53c1100], [2, 0x53bfb20], [3, 0x53c2280], [4, 0x53caa20], [5, 0x53dbdc0], [6, 0x53dd3a0], [7, 0x53d24a0]]}
  layer.16.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x53ce320], [1, 0x53c5720], [2, 0x53c4140], [3, 0x53c68a0], [4, 0x53cf040], [5, 0x53e03e0], [6, 0x53e19c0], [7, 0x53d6ac0], [0, 0x53f1340], [1, 0x53e8740], [2, 0x53e7160], [3, 0x53e98c0], [4, 0x53f2060], [5, 0x5403400], [6, 0x54049e0], [7, 0x53f9ae0], [0, 0x5414360], [1, 0x540b760], [2, 0x540a180], [3, 0x540c8e0], [4, 0x5415080], [5, 0x5426420], [6, 0x5427a00], [7, 0x541cb00], [0, 0x5437380], [1, 0x542e780], [2, 0x542d1a0], [3, 0x542f900], [4, 0x54380a0], [5, 0x5449440], [6, 0x544aa20], [7, 0x543fb20]]}
  layer.16.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x545a3a0], [1, 0x54517a0], [2, 0x54501c0], [3, 0x5452920], [4, 0x545b0c0], [5, 0x546c460], [6, 0x546da40], [7, 0x5462b40]]}
  layer.16.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x546ebe0]]}
  layer.16.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5463ce0]]}
  layer.17.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x545b9c0], [1, 0x5452dc0], [2, 0x54520a0], [3, 0x5454800], [4, 0x545cfa0], [5, 0x546e340], [6, 0x5477800], [7, 0x546c900]]}
  layer.17.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x547e9e0], [1, 0x5475de0], [2, 0x54750c0], [3, 0x5477820]]}
  layer.17.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x547ffc0], [5, 0x5491360], [6, 0x549a820], [7, 0x548f920], [0, 0x5480d00], [1, 0x5478100], [2, 0x54773e0], [3, 0x5479b40]]}
  layer.17.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x54a2fe0], [5, 0x54b4380], [6, 0x54bd840], [7, 0x54b2940]]}
  layer.17.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5286800], [0, 0x5286c80], [1, 0x527d7c0], [2, 0x527d7c0], [3, 0x527ff20], [4, 0x52886c0], [5, 0x5291b80], [6, 0x5291b80]]}
  layer.17.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5261040], [0, 0x52614c0], [1, 0x5257740], [2, 0x5257740]]}
  layer.17.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5259ea0], [4, 0x5262640], [5, 0x5263c20], [6, 0x5263c20], [7, 0x5263360], [0, 0x52637e0], [1, 0x5259a60], [2, 0x5259a60]]}
  layer.17.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x527cec0], [4, 0x5285660], [5, 0x5286c40], [6, 0x5286c40]]}
  layer.17.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5288f60]]}
  layer.17.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5288f60]]}
  layer.17.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x51d4fc0], [0, 0x51d5440], [1, 0x51cb6c0], [2, 0x51cb6c0], [3, 0x51cde20], [4, 0x51d65c0], [5, 0x51d7ba0], [6, 0x51d7ba0], [7, 0x51f7fe0], [0, 0x51f8460], [1, 0x51ee6e0], [2, 0x51ee6e0], [3, 0x51f0e40], [4, 0x51f95e0], [5, 0x51fabc0], [6, 0x51fabc0], [7, 0x521b000], [0, 0x521b480], [1, 0x5211700], [2, 0x5211700], [3, 0x5213e60], [4, 0x521c600], [5, 0x521dbe0], [6, 0x521dbe0], [7, 0x523e020], [0, 0x523e4a0], [1, 0x5234720], [2, 0x5234720], [3, 0x5236e80], [4, 0x523f620], [5, 0x5240c00], [6, 0x5240c00]]}
  layer.17.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x52a9820], [0, 0x52a9ca0], [1, 0x52a07e0], [2, 0x52a07e0], [3, 0x52a2f40], [4, 0x52ab6e0], [5, 0x52b4ba0], [6, 0x52b4ba0]]}
  layer.17.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x52ade40], [0, 0x52ae2c0], [1, 0x52a4e00], [2, 0x52a4e00], [3, 0x52a7560], [4, 0x52afd00], [5, 0x52b91c0], [6, 0x52b91c0], [7, 0x52d0e60], [0, 0x52d12e0], [1, 0x52c7e20], [2, 0x52c7e20], [3, 0x52ca580], [4, 0x52d2d20], [5, 0x52dc1e0], [6, 0x52dc1e0], [7, 0x52f3e80], [0, 0x52f4300], [1, 0x52eae40], [2, 0x52eae40], [3, 0x52ed5a0], [4, 0x52f5d40], [5, 0x52ff200], [6, 0x52ff200], [7, 0x5316ea0], [0, 0x5317320], [1, 0x530de60], [2, 0x530de60], [3, 0x53105c0], [4, 0x5318d60], [5, 0x5322220], [6, 0x5322220]]}
  layer.17.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5339ec0], [0, 0x533a340], [1, 0x5330e80], [2, 0x5330e80], [3, 0x53335e0], [4, 0x533bd80], [5, 0x5345240], [6, 0x5345240]]}
  layer.17.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x53463e0]]}
  layer.17.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5044020]]}
  layer.18.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x63b2e60], [5, 0x63c4200], [6, 0x63c83a0], [7, 0x63bd4a0], [0, 0x6397d60], [1, 0x638f160], [2, 0x639bf00], [3, 0x639e660]]}
  layer.18.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f754e0], [3, 0x5f77c40], [4, 0x5fa4100], [5, 0x5fb54a0]]}
  layer.18.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fb7c00], [7, 0x5facd00], [0, 0x5f80cc0], [1, 0x5f780c0], [2, 0x5f77800], [3, 0x5f79f60], [4, 0x5fa6420], [5, 0x5fb77c0]]}
  layer.18.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fdac20], [7, 0x5fcfd20], [0, 0x5fa3ce0], [1, 0x5f9b0e0]]}
  layer.18.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5fc9440], [5, 0x5fda7e0], [6, 0x5fdcf40], [7, 0x5fd2040], [0, 0x5fa6000], [1, 0x5f9d400], [2, 0x5f9aca0], [3, 0x5f9d400]]}
  layer.18.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5fec460], [5, 0x5ffd800], [6, 0x5ffff60], [7, 0x5ff5060]]}
  layer.18.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5fc9020], [1, 0x5fc0420], [2, 0x5fbdcc0], [3, 0x5fc0420], [4, 0x5fee780], [5, 0x5fffb20], [6, 0x6002280], [7, 0x5ff7380]]}
  layer.18.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fb58e0], [7, 0x5faa9e0], [0, 0x5f7e9a0], [1, 0x5f75da0]]}
  layer.18.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fe1a20]]}
  layer.18.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5fe4180]]}
  layer.18.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6011c20], [5, 0x6022fc0], [6, 0x6025fe0], [7, 0x601b0e0], [0, 0x5fedac0], [1, 0x5fe4ec0], [2, 0x5fea640], [3, 0x5fecda0], [4, 0x6034c40], [5, 0x6045fe0], [6, 0x6049000], [7, 0x603e100], [0, 0x6010ae0], [1, 0x6007ee0], [2, 0x600d660], [3, 0x600fdc0], [4, 0x6057c60], [5, 0x6069000], [6, 0x606c020], [7, 0x6061120], [0, 0x6033b00], [1, 0x602af00], [2, 0x6030680], [3, 0x6032de0], [4, 0x607ac80], [5, 0x608c020], [6, 0x608f040], [7, 0x6084140], [0, 0x6056b20], [1, 0x604df20], [2, 0x60536a0], [3, 0x6055e00]]}
  layer.18.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x609dca0], [5, 0x60af040], [6, 0x60b2060], [7, 0x60a7160], [0, 0x6079b40], [1, 0x6070f40], [2, 0x60766c0], [3, 0x6078e20]]}
  layer.18.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x60a22c0], [5, 0x60b3660], [6, 0x60b6680], [7, 0x60ab780], [0, 0x607e160], [1, 0x6075560], [2, 0x607ace0], [3, 0x607d440], [4, 0x60c52e0], [5, 0x60d6680], [6, 0x60d96a0], [7, 0x60ce7a0], [0, 0x60a1180], [1, 0x6098580], [2, 0x609dd00], [3, 0x60a0460], [4, 0x60e8300], [5, 0x60f96a0], [6, 0x60fc6c0], [7, 0x60f17c0], [0, 0x60c41a0], [1, 0x60bb5a0], [2, 0x60c0d20], [3, 0x60c3480], [4, 0x610b320], [5, 0x611c6c0], [6, 0x611f6e0], [7, 0x61147e0], [0, 0x60e71c0], [1, 0x60de5c0], [2, 0x60e3d40], [3, 0x60e64a0]]}
  layer.18.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f2bb60], [3, 0x5f2e2c0], [4, 0x5f5a780], [5, 0x5f6bb20], [6, 0x5f6c3e0], [7, 0x5f614e0], [0, 0x5f354a0], [1, 0x5f2c8a0]]}
  layer.18.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f0bb20]]}
  layer.18.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5f1cec0]]}
  layer.19.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f24080], [7, 0x5f19180], [0, 0x5eed140], [1, 0x5ee4540], [2, 0x5ee5b20], [3, 0x5ee8280], [4, 0x5f14740], [5, 0x5f25ae0]]}
  layer.19.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f470a0], [7, 0x5f3c1a0], [0, 0x5f10160], [1, 0x5f07560]]}
  layer.19.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f08b40], [3, 0x5f0b2a0], [4, 0x5f37760], [5, 0x5f48b00], [6, 0x5f493c0], [7, 0x5f3e4c0], [0, 0x5f12480], [1, 0x5f09880]]}
  layer.19.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ee2ac0], [3, 0x5ee5220], [4, 0x5f09800], [5, 0x5f1aba0]]}
  layer.19.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f5b920], [5, 0x5f6ccc0], [6, 0x5f6d580], [7, 0x5f62680], [0, 0x5f36640], [1, 0x5f2da40], [2, 0x5f2d180], [3, 0x5f2f8e0]]}
  layer.19.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f7e940], [5, 0x5f8fce0], [6, 0x5f905a0], [7, 0x5f856a0]]}
  layer.19.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5f59660], [1, 0x5f50a60], [2, 0x5f501a0], [3, 0x5f52900], [4, 0x5f80c60], [5, 0x5f92000], [6, 0x5f928c0], [7, 0x5f879c0]]}
  layer.19.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5f7c680], [1, 0x5f73a80], [2, 0x5f731c0], [3, 0x5f75920]]}
  layer.19.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6271880]]}
  layer.19.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6268c80]]}
  layer.19.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x627e1c0], [3, 0x6280920], [4, 0x62955a0], [5, 0x62a6940], [6, 0x62aaae0], [7, 0x629fbe0], [0, 0x627a4a0], [1, 0x62718a0], [2, 0x62a11e0], [3, 0x62a3940], [4, 0x62b85c0], [5, 0x62c9960], [6, 0x62cdb00], [7, 0x62c2c00], [0, 0x629d4c0], [1, 0x62948c0], [2, 0x62c4200], [3, 0x62c6960], [4, 0x62db5e0], [5, 0x62ec980], [6, 0x62f0b20], [7, 0x62e5c20], [0, 0x62c04e0], [1, 0x62b78e0], [2, 0x62e7220], [3, 0x62e9980], [4, 0x62fe600], [5, 0x630f9a0], [6, 0x6313b40], [7, 0x6308c40], [0, 0x62e3500], [1, 0x62da900]]}
  layer.19.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x630a240], [3, 0x630c9a0], [4, 0x6321620], [5, 0x63329c0], [6, 0x6336b60], [7, 0x632bc60], [0, 0x6306520], [1, 0x62fd920]]}
  layer.19.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x630e860], [3, 0x6310fc0], [4, 0x6325c40], [5, 0x6336fe0], [6, 0x633b180], [7, 0x6330280], [0, 0x630ab40], [1, 0x6301f40], [2, 0x6331880], [3, 0x6333fe0], [4, 0x6348c60], [5, 0x635a000], [6, 0x635e1a0], [7, 0x63532a0], [0, 0x632db60], [1, 0x6324f60], [2, 0x63548a0], [3, 0x6357000], [4, 0x636bc80], [5, 0x637d020], [6, 0x63811c0], [7, 0x63762c0], [0, 0x6350b80], [1, 0x6347f80], [2, 0x63778c0], [3, 0x637a020], [4, 0x638eca0], [5, 0x63a0040], [6, 0x63a41e0], [7, 0x63992e0], [0, 0x6373ba0], [1, 0x636afa0]]}
  layer.19.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x639a8e0], [3, 0x639d040], [4, 0x63b1cc0], [5, 0x63c3060], [6, 0x63c7200], [7, 0x63bc300], [0, 0x6396bc0], [1, 0x638dfc0]]}
  layer.19.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x63d5e80]]}
  layer.19.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x63e7220]]}
  layer.20.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x63eb3c0], [7, 0x63e04c0], [0, 0x63bad80], [1, 0x63b2180], [2, 0x63bef20], [3, 0x63c1680], [4, 0x63deaa0], [5, 0x63efe40]]}
  layer.20.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x640e3e0], [7, 0x64034e0], [0, 0x63ddda0], [1, 0x63d51a0]]}
  layer.20.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x63e1f40], [3, 0x63e46a0], [4, 0x6401ac0], [5, 0x6412e60], [6, 0x6410700], [7, 0x6405800], [0, 0x63e00c0], [1, 0x63d74c0]]}
  layer.20.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6404f60], [3, 0x64076c0], [4, 0x6424ae0], [5, 0x6435e80]]}
  layer.20.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x61dfb80], [5, 0x61f0f20], [6, 0x61f4800], [7, 0x61e9900], [0, 0x61bc2e0], [1, 0x61b36e0], [2, 0x61c0d40], [3, 0x61c34a0]]}
  layer.20.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x61ba3c0], [5, 0x61cb760], [6, 0x61ce780], [7, 0x61c3880]]}
  layer.20.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6196260], [1, 0x618d660], [2, 0x6192de0], [3, 0x6195540], [4, 0x61bc6e0], [5, 0x61cda80], [6, 0x61d0aa0], [7, 0x61c5ba0]]}
  layer.20.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x61b9280], [1, 0x61b0680], [2, 0x61b5e00], [3, 0x61b8560]]}
  layer.20.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x61b8120]]}
  layer.20.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x61ba880]]}
  layer.20.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x612e340], [5, 0x613f6e0], [6, 0x6142700], [7, 0x6137800], [0, 0x610a1e0], [1, 0x61015e0], [2, 0x6106d60], [3, 0x61094c0], [4, 0x6151360], [5, 0x6162700], [6, 0x6165720], [7, 0x615a820], [0, 0x612d200], [1, 0x6124600], [2, 0x6129d80], [3, 0x612c4e0], [4, 0x6174380], [5, 0x6185720], [6, 0x6188740], [7, 0x617d840], [0, 0x6150220], [1, 0x6147620], [2, 0x614cda0], [3, 0x614f500], [4, 0x61973a0], [5, 0x61a8740], [6, 0x61ab760], [7, 0x61a0860], [0, 0x6173240], [1, 0x616a640], [2, 0x616fdc0], [3, 0x6172520]]}
  layer.20.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6202ba0], [5, 0x6213f40], [6, 0x6217820], [7, 0x620c920], [0, 0x61df300], [1, 0x61d6700], [2, 0x61e3d60], [3, 0x61e64c0]]}
  layer.20.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62071c0], [5, 0x6218560], [6, 0x621be40], [7, 0x6210f40], [0, 0x61e3920], [1, 0x61dad20], [2, 0x61e8380], [3, 0x61eaae0], [4, 0x622a1e0], [5, 0x623b580], [6, 0x623ee60], [7, 0x6233f60], [0, 0x6206940], [1, 0x61fdd40], [2, 0x620b3a0], [3, 0x620db00], [4, 0x624d200], [5, 0x625e5a0], [6, 0x6261e80], [7, 0x6256f80], [0, 0x6229960], [1, 0x6220d60], [2, 0x622e3c0], [3, 0x6230b20], [4, 0x6270220], [5, 0x62815c0], [6, 0x6284ea0], [7, 0x6279fa0], [0, 0x624c980], [1, 0x6243d80], [2, 0x62513e0], [3, 0x6253b40]]}
  layer.20.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6293240], [5, 0x62a45e0], [6, 0x62a7ec0], [7, 0x629cfc0], [0, 0x626f9a0], [1, 0x6266da0], [2, 0x6274400], [3, 0x6276b60]]}
  layer.20.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x62755a0]]}
  layer.20.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6277d00]]}
  layer.21.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5eafc60], [7, 0x5ea4d60], [0, 0x5e80c00], [1, 0x5e78000], [2, 0x5e77740], [3, 0x5e79ea0], [4, 0x5e9e480], [5, 0x5eaf820]]}
  layer.21.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a77a60], [5, 0x5a88e00], [6, 0x5aa0f00], [7, 0x5a96000]]}
  layer.21.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5a70460], [1, 0x5a67860], [2, 0x5a606a0], [3, 0x5a62e00], [4, 0x5a79d80], [5, 0x5a8b120], [6, 0x5aa3220], [7, 0x5a98320]]}
  layer.21.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5a93480], [1, 0x5a8a880], [2, 0x5a836c0], [3, 0x5a85e20]]}
  layer.21.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5ac6240], [7, 0x5abb340], [0, 0x5a957a0], [1, 0x5a8cba0], [2, 0x5a859e0], [3, 0x5a88140], [4, 0x5a9d220], [5, 0x5aae5c0]]}
  layer.21.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5ae9260], [7, 0x5ade360], [0, 0x5ab87c0], [1, 0x5aafbc0]]}
  layer.21.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5aa8a00], [3, 0x5aab160], [4, 0x5ac0240], [5, 0x5ad15e0], [6, 0x5aeb580], [7, 0x5ae0680], [0, 0x5abaae0], [1, 0x5ab1ee0]]}
  layer.21.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5a6e140], [1, 0x5a65540], [2, 0x5a5e380], [3, 0x5a60ae0]]}
  layer.21.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ae3fa0]]}
  layer.21.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5af5340]]}
  layer.21.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b0ea20], [7, 0x5b03b20], [0, 0x5ade840], [1, 0x5ad5c40], [2, 0x5acd4a0], [3, 0x5acfc00], [4, 0x5aecbc0], [5, 0x5afdf60], [6, 0x5b31a40], [7, 0x5b26b40], [0, 0x5b01860], [1, 0x5af8c60], [2, 0x5af04c0], [3, 0x5af2c20], [4, 0x5b0fbe0], [5, 0x5b20f80], [6, 0x5b54a60], [7, 0x5b49b60], [0, 0x5b24880], [1, 0x5b1bc80], [2, 0x5b134e0], [3, 0x5b15c40], [4, 0x5b32c00], [5, 0x5b43fa0], [6, 0x5b77a80], [7, 0x5b6cb80], [0, 0x5b478a0], [1, 0x5b3eca0], [2, 0x5b36500], [3, 0x5b38c60], [4, 0x5b55c20], [5, 0x5b66fc0]]}
  layer.21.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b9aaa0], [7, 0x5b8fba0], [0, 0x5b6a8c0], [1, 0x5b61cc0], [2, 0x5b59520], [3, 0x5b5bc80], [4, 0x5b78c40], [5, 0x5b89fe0]]}
  layer.21.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b9f0c0], [7, 0x5b941c0], [0, 0x5b6eee0], [1, 0x5b662e0], [2, 0x5b5db40], [3, 0x5b602a0], [4, 0x5b7d260], [5, 0x5b8e600], [6, 0x5bc20e0], [7, 0x5bb71e0], [0, 0x5b91f00], [1, 0x5b89300], [2, 0x5b80b60], [3, 0x5b832c0], [4, 0x5ba0280], [5, 0x5bb1620], [6, 0x5be5100], [7, 0x5bda200], [0, 0x5bb4f20], [1, 0x5bac320], [2, 0x5ba3b80], [3, 0x5ba62e0], [4, 0x5bc32a0], [5, 0x5bd4640], [6, 0x5c08120], [7, 0x5bfd220], [0, 0x5bd7f40], [1, 0x5bcf340], [2, 0x5bc6ba0], [3, 0x5bc9300], [4, 0x5be62c0], [5, 0x5bf7660]]}
  layer.21.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a2e0e0], [5, 0x5a3f480], [6, 0x5a57580], [7, 0x5a4c680], [0, 0x5a24c40], [1, 0x5a1c040], [2, 0x5a14e80], [3, 0x5a175e0]]}
  layer.21.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a08920]]}
  layer.21.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x59fda20]]}
  layer.22.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x59dc8e0], [1, 0x59d3ce0], [2, 0x59ccb20], [3, 0x59cf280], [4, 0x59e80a0], [5, 0x59f9440], [6, 0x5a11540], [7, 0x5a06640]]}
  layer.22.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x59ff900], [1, 0x59f6d00], [2, 0x59efb40], [3, 0x59f22a0]]}
  layer.22.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a0b0c0], [5, 0x5a1c460], [6, 0x5a34560], [7, 0x5a29660], [0, 0x5a01c20], [1, 0x59f9020], [2, 0x59f1e60], [3, 0x59f45c0]]}
  layer.22.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x59e5040], [5, 0x59f63e0], [6, 0x5a06600], [7, 0x59fb700]]}
  layer.22.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a58720], [7, 0x5a4d820], [0, 0x5a25de0], [1, 0x5a1d1e0], [2, 0x5a16020], [3, 0x5a18780], [4, 0x5a2f700], [5, 0x5a40aa0]]}
  layer.22.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a7b740], [7, 0x5a70840], [0, 0x5a48e00], [1, 0x5a40200]]}
  layer.22.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5a39040], [3, 0x5a3b7a0], [4, 0x5a52720], [5, 0x5a63ac0], [6, 0x5a7da60], [7, 0x5a72b60], [0, 0x5a4b120], [1, 0x5a42520]]}
  layer.22.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5a5c060], [3, 0x5a5e7c0], [4, 0x5a75740], [5, 0x5a86ae0]]}
  layer.22.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5d51260]]}
  layer.22.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d539c0]]}
  layer.22.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d80740], [5, 0x5d91ae0], [6, 0x5d923a0], [7, 0x5d874a0], [0, 0x5d63340], [1, 0x5d5a740], [2, 0x5d59e80], [3, 0x5d5c5e0], [4, 0x5da3760], [5, 0x5db4b00], [6, 0x5db53c0], [7, 0x5daa4c0], [0, 0x5d86360], [1, 0x5d7d760], [2, 0x5d7cea0], [3, 0x5d7f600], [4, 0x5dc6780], [5, 0x5dd7b20], [6, 0x5dd83e0], [7, 0x5dcd4e0], [0, 0x5da9380], [1, 0x5da0780], [2, 0x5d9fec0], [3, 0x5da2620], [4, 0x5de97a0], [5, 0x5dfab40], [6, 0x5dfb400], [7, 0x5df0500], [0, 0x5dcc3a0], [1, 0x5dc37a0], [2, 0x5dc2ee0], [3, 0x5dc5640]]}
  layer.22.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e0c7c0], [5, 0x5e1db60], [6, 0x5e1e420], [7, 0x5e13520], [0, 0x5def3c0], [1, 0x5de67c0], [2, 0x5de5f00], [3, 0x5de8660]]}
  layer.22.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e10de0], [5, 0x5e22180], [6, 0x5e22a40], [7, 0x5e17b40], [0, 0x5df39e0], [1, 0x5deade0], [2, 0x5dea520], [3, 0x5decc80], [4, 0x5e33e00], [5, 0x5e451a0], [6, 0x5e45a60], [7, 0x5e3ab60], [0, 0x5e16a00], [1, 0x5e0de00], [2, 0x5e0d540], [3, 0x5e0fca0], [4, 0x5e56e20], [5, 0x5e681c0], [6, 0x5e68a80], [7, 0x5e5db80], [0, 0x5e39a20], [1, 0x5e30e20], [2, 0x5e30560], [3, 0x5e32cc0], [4, 0x5e79e40], [5, 0x5e8b1e0], [6, 0x5e8baa0], [7, 0x5e80ba0], [0, 0x5e5ca40], [1, 0x5e53e40], [2, 0x5e53580], [3, 0x5e55ce0]]}
  layer.22.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e9ce60], [5, 0x5eae200], [6, 0x5eaeac0], [7, 0x5ea3bc0], [0, 0x5e7fa60], [1, 0x5e76e60], [2, 0x5e765a0], [3, 0x5e78d00]]}
  layer.22.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5ed2c80]]}
  layer.22.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5ec7d80]]}
  layer.23.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5ea3c20], [1, 0x5e9b020], [2, 0x5e9a760], [3, 0x5e9cec0], [4, 0x5ec14a0], [5, 0x5ed2840], [6, 0x5edb8a0], [7, 0x5ed09a0]]}
  layer.23.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5ec6c40], [1, 0x5ebe040], [2, 0x5ebd780], [3, 0x5ebfee0]]}
  layer.23.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ee44c0], [5, 0x5ef5860], [6, 0x5efe8c0], [7, 0x5ef39c0], [0, 0x5ec8f60], [1, 0x5ec0360], [2, 0x5ebfaa0], [3, 0x5ec2200]]}
  layer.23.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f074e0], [5, 0x5f18880], [6, 0x5f218e0], [7, 0x5f169e0]]}
  layer.23.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cdc980], [7, 0x5cd1a80], [0, 0x5cad060], [1, 0x5ca4460], [2, 0x5c9bcc0], [3, 0x5c9e420], [4, 0x5cc32c0], [5, 0x5cd4660]]}
  layer.23.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cb71c0], [7, 0x5cac2c0], [0, 0x5c86fe0], [1, 0x5c7e3e0]]}
  layer.23.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c75c40], [3, 0x5c783a0], [4, 0x5c95360], [5, 0x5ca6700], [6, 0x5cb94e0], [7, 0x5cae5e0], [0, 0x5c89300], [1, 0x5c80700]]}
  layer.23.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c98c60], [3, 0x5c9b3c0], [4, 0x5cb8380], [5, 0x5cc9720]]}
  layer.23.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5cba6a0]]}
  layer.23.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ccba40]]}
  layer.23.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c2b140], [7, 0x5c20240], [0, 0x5bfaf60], [1, 0x5bf2360], [2, 0x5be9bc0], [3, 0x5bec320], [4, 0x5c092e0], [5, 0x5c1a680], [6, 0x5c4e160], [7, 0x5c43260], [0, 0x5c1df80], [1, 0x5c15380], [2, 0x5c0cbe0], [3, 0x5c0f340], [4, 0x5c2c300], [5, 0x5c3d6a0], [6, 0x5c71180], [7, 0x5c66280], [0, 0x5c40fa0], [1, 0x5c383a0], [2, 0x5c2fc00], [3, 0x5c32360], [4, 0x5c4f320], [5, 0x5c606c0], [6, 0x5c941a0], [7, 0x5c892a0], [0, 0x5c63fc0], [1, 0x5c5b3c0], [2, 0x5c52c20], [3, 0x5c55380], [4, 0x5c72340], [5, 0x5c836e0]]}
  layer.23.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cff9a0], [7, 0x5cf4aa0], [0, 0x5cd0080], [1, 0x5cc7480], [2, 0x5cbece0], [3, 0x5cc1440], [4, 0x5ce62e0], [5, 0x5cf7680]]}
  layer.23.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [2, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d03fc0], [7, 0x5cf90c0], [0, 0x5cd46a0], [1, 0x5ccbaa0], [2, 0x5cc3300], [3, 0x5cc5a60], [4, 0x5cea900], [5, 0x5cfbca0], [6, 0x5d26fe0], [7, 0x5d1c0e0], [0, 0x5cf76c0], [1, 0x5ceeac0], [2, 0x5ce6320], [3, 0x5ce8a80], [4, 0x5d0d920], [5, 0x5d1ecc0], [6, 0x5d4a000], [7, 0x5d3f100], [0, 0x5d1a6e0], [1, 0x5d11ae0], [2, 0x5d09340], [3, 0x5d0baa0], [4, 0x5d30940], [5, 0x5d41ce0], [6, 0x5d6d020], [7, 0x5d62120], [0, 0x5d3d700], [1, 0x5d34b00], [2, 0x5d2c360], [3, 0x5d2eac0], [4, 0x5d53960], [5, 0x5d64d00]]}
  layer.23.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d90040], [7, 0x5d85140], [0, 0x5d60720], [1, 0x5d57b20], [2, 0x5d4f380], [3, 0x5d51ae0], [4, 0x5d76980], [5, 0x5d87d20]]}
  layer.23.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d77b20]]}
  layer.23.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5d88ec0]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6aab640]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6a565c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6af1b00]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6ae2a40]]}
  dc.input_tensor.layernorm_38.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6a9ca80], [1, 0x6a9e4c0], [2, 0x6aaeb40], [3, 0x6aac840]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x69a7d20]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x69d4f00]]}
  dc.input_tensor.layernorm_52.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x69ee180], [6, 0x69f08e0], [7, 0x69e3b40], [0, 0x699d700]]}
  input_1_multiply_69_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6a2eee0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x69e8aa0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6a7b020]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6cddc00]]}
  dc.input_tensor.layernorm_91.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c8fd60], [1, 0x6c917a0], [2, 0x6ca2fa0], [3, 0x6ca0ca0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6ceccc0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6dfb940]]}
  dc.input_tensor.layernorm_105.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6dae360], [1, 0x6dafda0], [2, 0x6dc15a0], [3, 0x6dbf2a0]]}
  input_1_multiply_122_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6bcef80]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6b6ef40]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6b71ba0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6b735e0]]}
  dc.input_tensor.layernorm_144.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b83c60], [3, 0x6b81960], [4, 0x6bb6a20], [5, 0x6bce240]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c8f8e0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6c91320]]}
  dc.input_tensor.layernorm_158.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ca2260], [3, 0x6c9ff60], [4, 0x6cd5020], [5, 0x6cec840]]}
  input_1_multiply_175_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x656dc40]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6565040]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x65ea780]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65b4100]]}
  dc.input_tensor.layernorm_197.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65ab500], [2, 0x65b8b60], [3, 0x65bf000], [4, 0x65dc420]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64b3320]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64aa720]]}
  dc.input_tensor.layernorm_211.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x64b74c0], [3, 0x64b9c20], [4, 0x64d7040], [5, 0x64e83e0]]}
  input_1_multiply_228_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64dd100]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64d4500]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6529ec0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x65956c0]]}
  dc.input_tensor.layernorm_250.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x67a6b20], [1, 0x679e7e0], [2, 0x67acfc0], [3, 0x67b3460]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x691aa60]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x67e5940]]}
  dc.input_tensor.layernorm_264.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x68c5120], [1, 0x68bcde0], [2, 0x68cb5c0], [3, 0x68d1a60]]}
  input_1_multiply_281_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6963240]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x66c7c00]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6688960]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6680620]]}
  dc.input_tensor.layernorm_303.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x668dc80], [3, 0x6694120], [4, 0x66b1540], [5, 0x66ca7c0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x67a66a0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x679e360]]}
  dc.input_tensor.layernorm_317.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67ac280], [3, 0x67b2720], [4, 0x67cfb40], [5, 0x67e8dc0]]}
  input_1_multiply_334_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7518fe0]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x74bfde0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x74f0d40]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x74ee180]]}
  dc.input_tensor.layernorm_356.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7536d40], [5, 0x754cf80], [6, 0x755dee0], [7, 0x753e7a0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7386ac0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x73cf680]]}
  dc.input_tensor.layernorm_370.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x73de700], [6, 0x73ed7c0], [7, 0x73d6820], [0, 0x737d620]]}
  input_1_multiply_387_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7438300]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7480ec0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x74d8b60]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x76f00c0]]}
  dc.input_tensor.layernorm_409.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x76e51c0], [4, 0x772f7c0], [5, 0x7745a00], [6, 0x7756960]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x780de00]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x780e260]]}
  dc.input_tensor.layernorm_423.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78bd2c0], [7, 0x789d2c0], [0, 0x7864500], [1, 0x78565e0]]}
  input_1_multiply_440_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x78037c0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7737220]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7655c40]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7653080]]}
  dc.input_tensor.layernorm_462.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x769c500], [5, 0x76b2740], [6, 0x76c36a0], [7, 0x76a3f60]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x76e7900]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x76e4d40]]}
  dc.input_tensor.layernorm_476.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x772ea80], [5, 0x7744cc0], [6, 0x7755c20], [7, 0x77364e0]]}
  input_1_multiply_493_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6f93720]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6f91420]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6face60]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6fae8a0]]}
  dc.input_tensor.layernorm_515.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6fb7900], [3, 0x6fb5600], [4, 0x6ffe1c0], [5, 0x70159e0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e0b7e0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6e094e0]]}
  dc.input_tensor.layernorm_529.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6e4e360], [5, 0x6e65b80], [6, 0x6e5ee40], [7, 0x6e475e0]]}
  input_1_multiply_546_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6f31c40]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6f1a3e0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6f1d900]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6f1f340]]}
  dc.input_tensor.layernorm_568.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6fac120], [1, 0x6fadb60], [2, 0x6fb6bc0], [3, 0x6fb48c0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x734ed40]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x735de00]]}
  dc.input_tensor.layernorm_582.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x72200c0], [0, 0x71c6ec0], [1, 0x71c8900], [2, 0x71d33a0]]}
  input_1_multiply_599_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7352280]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x735cd20]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x70eed40]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x70f0780]]}
  dc.input_tensor.layernorm_621.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x70fa0a0], [3, 0x70f7da0], [4, 0x7140960], [5, 0x7158180]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x71a3a20]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x71a5460]]}
  dc.input_tensor.layernorm_635.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x71af640], [3, 0x71ad340], [4, 0x71f5f00], [5, 0x720d720]]}
  input_1_multiply_652_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5599ba0]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x558eca0]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x55c6e00]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x55be200]]}
  dc.input_tensor.layernorm_674.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x55bd4e0], [3, 0x55bfc40], [4, 0x55cece0], [5, 0x55e0080]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x54bfb60]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x54b4c60]]}
  dc.input_tensor.layernorm_688.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x54a41a0], [1, 0x549b5a0], [2, 0x549c720], [3, 0x549ee80]]}
  input_1_multiply_705_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x54e4640]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x54e6da0]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x553f780]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5550b20]]}
  dc.input_tensor.layernorm_727.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x55cdfa0], [5, 0x55df340], [6, 0x55e0060], [7, 0x55d5160]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x599ae00]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x598ff00]]}
  dc.input_tensor.layernorm_741.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5849ec0], [1, 0x58412c0], [2, 0x5841fe0], [3, 0x5844740]]}
  input_1_multiply_758_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59cb960]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x59ce0c0]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5794d60]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x578c160]]}
  dc.input_tensor.layernorm_780.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x578bd00], [3, 0x578e460], [4, 0x579d500], [5, 0x57ae8a0]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5849a40]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5840e40]]}
  dc.input_tensor.layernorm_794.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x58412a0], [3, 0x5843a00], [4, 0x5852aa0], [5, 0x5863e40]]}
  input_1_multiply_811_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5066c00]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5066c00]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50b8420]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x50b88a0]]}
  dc.input_tensor.layernorm_833.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x50ae260], [2, 0x50ae260], [3, 0x50b09c0], [4, 0x50b9160]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  dc.input_tensor.layernorm_847.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900], [4, 0x4fb0900], [5, 0x4fb0900], [6, 0x4fb0900]]}
  input_1_multiply_864_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4ff90e0]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4ff90e0]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5041d00]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  dc.input_tensor.layernorm_886.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5451360], [3, 0x5453ac0], [4, 0x545c260], [5, 0x546d600]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x545b540]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5452940]]}
  dc.input_tensor.layernorm_900.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5332d60], [3, 0x53354c0], [4, 0x533dc60], [5, 0x534f000]]}
  input_1_multiply_917_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x54a3d20]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x549b120]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5286380]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5286800]]}
  dc.input_tensor.layernorm_939.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x527ca80], [2, 0x527ca80], [3, 0x527f1e0], [4, 0x5287980]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x533b060]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x533b4e0]]}
  dc.input_tensor.layernorm_953.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5332020], [2, 0x5332020], [3, 0x5334780], [4, 0x533cf20]]}
  input_1_multiply_970_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f9a820]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f9cf80]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x60117a0]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6022b40]]}
  dc.input_tensor.layernorm_992.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x60252a0], [7, 0x601a3a0], [0, 0x5fecd80], [1, 0x5fe4180]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f23c00]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f18d00]]}
  dc.input_tensor.layernorm_1006.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5eec400], [1, 0x5ee3800], [2, 0x5ee4de0], [3, 0x5ee7540]]}
  input_1_multiply_1023_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f2cd00]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f2f460]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5fa3c80]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5fb5020]]}
  dc.input_tensor.layernorm_1045.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5fec040], [1, 0x5fe3440], [2, 0x5fe0ce0], [3, 0x5fe3440]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x639ba80]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x639e1e0]]}
  dc.input_tensor.layernorm_1059.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6294860], [5, 0x62a5c00], [6, 0x62a9da0], [7, 0x629eea0]]}
  input_1_multiply_1076_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6433720]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6428820]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x61df700]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x61f0aa0]]}
  dc.input_tensor.layernorm_1098.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x61f3ac0], [7, 0x61e8bc0], [0, 0x61bb5a0], [1, 0x61b29a0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62943e0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62a5780]]}
  dc.input_tensor.layernorm_1112.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x62a9060], [7, 0x629e160], [0, 0x6270b40], [1, 0x6267f40]]}
  input_1_multiply_1129_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a9cda0]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5aae140]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b0e5a0]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b036a0]]}
  dc.input_tensor.layernorm_1151.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5addb00], [1, 0x5ad4f00], [2, 0x5acc760], [3, 0x5aceec0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x59dc460]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x59d3860]]}
  dc.input_tensor.layernorm_1165.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59cbde0], [3, 0x59ce540], [4, 0x59e7360], [5, 0x59f8700]]}
  input_1_multiply_1182_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5a2f280]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a40620]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5aa0a80]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a95b80]]}
  dc.input_tensor.layernorm_1204.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5acba20], [3, 0x5ace180], [4, 0x5ae3260], [5, 0x5af4600]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e9e000]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5eaf3a0]]}
  dc.input_tensor.layernorm_1218.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d91660], [7, 0x5d86760], [0, 0x5d62600], [1, 0x5d59a00]]}
  input_1_multiply_1235_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5eebf80]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5ee3380]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cdc500]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5cd1600]]}
  dc.input_tensor.layernorm_1257.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5cac320], [1, 0x5ca3720], [2, 0x5c9af80], [3, 0x5c9d6e0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d911e0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5d862e0]]}
  dc.input_tensor.layernorm_1271.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5d618c0], [1, 0x5d58cc0], [2, 0x5d50520], [3, 0x5d52c80]]}

  # epoch_to_epoch
  e2e_gelu_44_0:                                     {input: gelu_44, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000], [0, 0x7865240], [1, 0x7857320]]}
  e2e__fused_op_2_0:                                 {input: _fused_op_2, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x92ded60], [5, 0x92f4fa0]]}
  e2e_layernorm_91.dc.reduce_avg.3.lc1_0:            {input: layernorm_91.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_layernorm_91.dc.subtract.1_0:                  {input: layernorm_91.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92fe020], [7, 0x92de020], [0, 0x92a5260], [1, 0x9297340]]}
  e2e_matmul_120_0:                                  {input: matmul_120, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9fc5280], [1, 0x9fb7360], [2, 0xa088aa0], [3, 0xa07e000]]}
  e2e__fused_op_7_0:                                 {input: _fused_op_7, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_gelu_150_0:                                    {input: gelu_150, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320], [2, 0x7856a40], [3, 0x784bfa0], [4, 0x85bed60], [5, 0x85d4fa0], [6, 0x85de020], [7, 0x85be020]]}
  e2e__fused_op_10_0:                                {input: _fused_op_10, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000]]}
  e2e_layernorm_197.dc.reduce_avg.3.lc1_0:           {input: layernorm_197.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0]]}
  e2e_layernorm_197.dc.subtract.1_0:                 {input: layernorm_197.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x92a5260], [1, 0x9297340], [2, 0x9296a60], [3, 0x928bfc0]]}
  e2e_matmul_226_0:                                  {input: matmul_226, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0], [4, 0xa0d0da0], [5, 0xa0e6fe0]]}
  e2e__fused_op_15_0:                                {input: _fused_op_15, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040], [0, 0x9fc5280], [1, 0x9fb7360]]}
  e2e_gelu_256_0:                                    {input: gelu_256, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80], [6, 0x85de020], [7, 0x85be020], [0, 0x8585260], [1, 0x8577340]]}
  e2e__fused_op_18_0:                                {input: _fused_op_18, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78be000], [7, 0x789e000], [0, 0x7865240], [1, 0x7857320]]}
  e2e_layernorm_303.dc.reduce_avg.3.lc1_0:           {input: layernorm_303.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_layernorm_303.dc.subtract.1_0:                 {input: layernorm_303.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x92ded60], [5, 0x92f4fa0]]}
  e2e_matmul_332_0:                                  {input: matmul_332, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa0f0060], [7, 0xa0d0060]]}
  e2e__fused_op_23_0:                                {input: _fused_op_23, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9fc5280], [1, 0x9fb7360], [2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_gelu_362_0:                                    {input: gelu_362, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000], [0, 0x8585260], [1, 0x8577340], [2, 0x8576a60], [3, 0x856bfc0]]}
  e2e__fused_op_26_0:                                {input: _fused_op_26, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320], [2, 0x7856a40], [3, 0x784bfa0]]}
  e2e_layernorm_409.dc.reduce_avg.3.lc1_0:           {input: layernorm_409.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9fc5280], [1, 0x9fb7360]]}
  e2e_layernorm_409.dc.subtract.1_0:                 {input: layernorm_409.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92ded60], [5, 0x92f4fa0], [6, 0x92fe020], [7, 0x92de020]]}
  e2e_matmul_438_0:                                  {input: matmul_438, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040], [0, 0xa0972a0], [1, 0xa089380]]}
  e2e__fused_op_31_0:                                {input: _fused_op_31, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0], [4, 0x9ffed80], [5, 0xa014fc0]]}
  e2e_gelu_468_0:                                    {input: gelu_468, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78be000], [7, 0x789e000], [0, 0x7865240], [1, 0x7857320], [2, 0x8576a60], [3, 0x856bfc0], [4, 0x85bed60], [5, 0x85d4fa0]]}
  e2e__fused_op_34_0:                                {input: _fused_op_34, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80]]}
  e2e_layernorm_515.dc.reduce_avg.3.lc1_0:           {input: layernorm_515.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_layernorm_515.dc.subtract.1_0:                 {input: layernorm_515.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92fe020], [7, 0x92de020], [0, 0x92a5260], [1, 0x9297340]]}
  e2e_matmul_544_0:                                  {input: matmul_544, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9fc5280], [1, 0x9fb7360], [2, 0xa088aa0], [3, 0xa07e000]]}
  e2e__fused_op_39_0:                                {input: _fused_op_39, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_gelu_574_0:                                    {input: gelu_574, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320], [2, 0x7856a40], [3, 0x784bfa0], [4, 0x85bed60], [5, 0x85d4fa0], [6, 0x85de020], [7, 0x85be020]]}
  e2e__fused_op_42_0:                                {input: _fused_op_42, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000]]}
  e2e_layernorm_621.dc.reduce_avg.3.lc1_0:           {input: layernorm_621.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x92a5260], [1, 0x9297340]]}
  e2e_layernorm_621.dc.subtract.1_0:                 {input: layernorm_621.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x9ffed80], [5, 0xa014fc0]]}
  e2e_matmul_650_0:                                  {input: matmul_650, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0], [4, 0xad1eda0], [5, 0xad34fe0]]}
  e2e__fused_op_47_0:                                {input: _fused_op_47, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040], [0, 0x9377280], [1, 0x9369360]]}
  e2e_gelu_680_0:                                    {input: gelu_680, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80], [6, 0x85de020], [7, 0x85be020], [0, 0xa0972a0], [1, 0xa089380]]}
  e2e__fused_op_50_0:                                {input: _fused_op_50, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78be000], [7, 0x789e000], [0, 0x7865240], [1, 0x7857320]]}
  e2e_layernorm_727.dc.reduce_avg.3.lc1_0:           {input: layernorm_727.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_layernorm_727.dc.subtract.1_0:                 {input: layernorm_727.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x92ded60], [5, 0x92f4fa0]]}
  e2e_matmul_756_0:                                  {input: matmul_756, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa0f0060], [7, 0xa0d0060]]}
  e2e__fused_op_55_0:                                {input: _fused_op_55, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8585260], [1, 0x8577340], [2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_gelu_786_0:                                    {input: gelu_786, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000], [0, 0x92a5280], [1, 0x9297360], [2, 0x8576a60], [3, 0x856bfc0]]}
  e2e__fused_op_58_0:                                {input: _fused_op_58, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320], [2, 0x7856a40], [3, 0x784bfa0]]}
  e2e_layernorm_833.dc.reduce_avg.3.lc1_0:           {input: layernorm_833.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8585260], [1, 0x8577340]]}
  e2e_layernorm_833.dc.subtract.1_0:                 {input: layernorm_833.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92ded60], [5, 0x92f4fa0], [6, 0x92fe020], [7, 0x92de020]]}
  e2e_matmul_862_0:                                  {input: matmul_862, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040], [0, 0x8657280], [1, 0x8649360]]}
  e2e__fused_op_63_0:                                {input: _fused_op_63, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0], [4, 0x9ffed80], [5, 0xa014fc0]]}
  e2e_gelu_892_0:                                    {input: gelu_892, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78be000], [7, 0x789e000], [0, 0xd5172a0], [1, 0xd509380], [2, 0x8576a60], [3, 0x856bfc0], [4, 0x85bed60], [5, 0x85d4fa0]]}
  e2e__fused_op_66_0:                                {input: _fused_op_66, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80]]}
  e2e_layernorm_939.dc.reduce_avg.3.lc1_0:           {input: layernorm_939.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_layernorm_939.dc.subtract.1_0:                 {input: layernorm_939.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92fe020], [7, 0x92de020], [0, 0x7865240], [1, 0x7857320]]}
  e2e_matmul_968_0:                                  {input: matmul_968, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8585260], [1, 0x8577340], [2, 0xa088aa0], [3, 0xa07e000]]}
  e2e__fused_op_71_0:                                {input: _fused_op_71, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_gelu_998_0:                                    {input: gelu_998, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xd445280], [1, 0xd437360], [2, 0x7856a40], [3, 0x784bfa0], [4, 0x85bed60], [5, 0x85d4fa0], [6, 0x85de020], [7, 0x85be020]]}
  e2e__fused_op_74_0:                                {input: _fused_op_74, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000]]}
  e2e_layernorm_1045.dc.reduce_avg.3.lc1_0:          {input: layernorm_1045.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320]]}
  e2e_layernorm_1045.dc.subtract.1_0:                {input: layernorm_1045.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x9ffed80], [5, 0xa014fc0]]}
  e2e_matmul_1074_0:                                 {input: matmul_1074, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9fb6a80], [3, 0x9fabfe0], [4, 0xad1eda0], [5, 0xad34fe0]]}
  e2e__fused_op_79_0:                                {input: _fused_op_79, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040], [0, 0x7937260], [1, 0x7929340]]}
  e2e_gelu_1104_0:                                   {input: gelu_1104, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x7856a40], [3, 0x784bfa0], [4, 0x789ed40], [5, 0x78b4f80], [6, 0x85de020], [7, 0x85be020], [0, 0x93772a0], [1, 0x9369380]]}
  e2e__fused_op_82_0:                                {input: _fused_op_82, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x78be000], [7, 0x789e000], [0, 0x8657280], [1, 0x8649360]]}
  e2e_layernorm_1151.dc.reduce_avg.3.lc1_0:          {input: layernorm_1151.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xa01e040], [7, 0x9ffe040]]}
  e2e_layernorm_1151.dc.subtract.1_0:                {input: layernorm_1151.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9296a60], [3, 0x928bfc0], [4, 0x92ded60], [5, 0x92f4fa0]]}
  e2e_matmul_1180_0:                                 {input: matmul_1180, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9ffed80], [5, 0xa014fc0], [6, 0xa0f0060], [7, 0xa0d0060]]}
  e2e__fused_op_87_0:                                {input: _fused_op_87, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320], [2, 0x9fb6a80], [3, 0x9fabfe0]]}
  e2e_gelu_1210_0:                                   {input: gelu_1210, type: queue, entries: 128, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x789ed40], [5, 0x78b4f80], [6, 0x78be000], [7, 0x789e000], [0, 0x92a5280], [1, 0x9297360], [2, 0x8576a60], [3, 0x856bfc0]]}
  e2e__fused_op_90_0:                                {input: _fused_op_90, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8585260], [1, 0x8577340], [2, 0x7856a40], [3, 0x784bfa0]]}
  e2e_layernorm_1257.dc.reduce_avg.3.lc1_0:          {input: layernorm_1257.dc.reduce_avg.3.lc1, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x7865240], [1, 0x7857320]]}
  e2e_layernorm_1257.dc.subtract.1_0:                {input: layernorm_1257.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92ded60], [5, 0x92f4fa0], [6, 0x92fe020], [7, 0x92de020]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_14: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 6], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_1: {type: fused_op, grid_loc: [2, 9], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [2, 10], grid_size: [2, 2], inputs: [_fused_op_1, matmul_22],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_33: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_37: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 2], inputs: [add_37, layernorm_38.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [4, 8], grid_size: [2, 1], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_2: {type: fused_op, grid_loc: [4, 10], grid_size: [4, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4, layernorm_38.dc.subtract.1, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_41: {type: matmul, grid_loc: [6, 0], grid_size: [4, 8], inputs: [_fused_op_2, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_44: {type: gelu, grid_loc: [8, 8], grid_size: [2, 4], inputs: [matmul_41],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_1:
    target_device: 0
    input_count: 128
    matmul_47: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_44_0, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_51: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_47, e2e__fused_op_2_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4, layernorm_52.dc.subtract.1, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_55: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_61: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_67: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_4: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_4, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_75: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_5: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_5, matmul_75],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_86: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_90: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_86, _fused_op_3],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_2:
    target_device: 0
    input_count: 128
    _fused_op_6: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_91.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_91.4, e2e_layernorm_91.dc.subtract.1_0, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_94: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_6, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_97: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_94],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_104: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_100, _fused_op_6],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_7: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4, layernorm_105.dc.subtract.1, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_108: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_7, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_114: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_7, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_120: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_3:
    target_device: 0
    input_count: 128
    _fused_op_8: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_120_0, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_128: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_7_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_9: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_8],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_9, matmul_128],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_139: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_143: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_139, e2e__fused_op_7_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_10: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4, layernorm_144.dc.subtract.1, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_147: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_150: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_147],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_157: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_153, e2e__fused_op_10_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_11: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4, layernorm_158.dc.subtract.1, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_161: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_167: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_173: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_161, matmul_167],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_181: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_13: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_12],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_13, matmul_181],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_192: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_196: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_192, _fused_op_11],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_5:
    target_device: 0
    input_count: 128
    _fused_op_14: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_197.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_197.4, e2e_layernorm_197.dc.subtract.1_0, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_200: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_14, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_203: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_200],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_210: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_206, _fused_op_14],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_15: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4, layernorm_211.dc.subtract.1, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_214: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_15, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_220: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_15, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_226: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_214, matmul_220],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_6:
    target_device: 0
    input_count: 128
    _fused_op_16: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_226_0, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_234: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_15_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_17: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_16],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_17, matmul_234],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_245: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_249: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_245, e2e__fused_op_15_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_18: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4, layernorm_250.dc.subtract.1, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_253: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_18, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_256: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_253],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_7:
    target_device: 0
    input_count: 128
    matmul_259: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_256_0, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_263: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_259, e2e__fused_op_18_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_19: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4, layernorm_264.dc.subtract.1, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_267: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_273: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_279: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_267, matmul_273],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_287: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_21: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_20],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_21, matmul_287],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_298: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_302: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_298, _fused_op_19],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_8:
    target_device: 0
    input_count: 128
    _fused_op_22: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_303.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_303.4, e2e_layernorm_303.dc.subtract.1_0, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_306: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_22, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_309: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_306],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_316: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_312, _fused_op_22],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_23: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4, layernorm_317.dc.subtract.1, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_320: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_23, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_326: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_23, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_332: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_320, matmul_326],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_9:
    target_device: 0
    input_count: 128
    _fused_op_24: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_332_0, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_340: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_23_0, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_25: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_24],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_25, matmul_340],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_351: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_355: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_351, e2e__fused_op_23_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_26: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4, layernorm_356.dc.subtract.1, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_359: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_26, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_362: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_359],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_10:
    target_device: 0
    input_count: 128
    matmul_365: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_362_0, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_369: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_365, e2e__fused_op_26_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_27: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4, layernorm_370.dc.subtract.1, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_373: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_379: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_385: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_373, matmul_379],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_28: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_393: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_29: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_28],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_29, matmul_393],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_404: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_408: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_404, _fused_op_27],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_11:
    target_device: 0
    input_count: 128
    _fused_op_30: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_409.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_409.4, e2e_layernorm_409.dc.subtract.1_0, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_412: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_30, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_415: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_412],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_422: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_418, _fused_op_30],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_31: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4, layernorm_423.dc.subtract.1, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_426: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_31, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_432: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_31, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_438: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_426, matmul_432],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_12:
    target_device: 0
    input_count: 128
    _fused_op_32: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_438_0, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_446: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_31_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_32],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_33, matmul_446],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_457: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_461: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_457, e2e__fused_op_31_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_34: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4, layernorm_462.dc.subtract.1, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_465: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_34, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_468: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_465],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_13:
    target_device: 0
    input_count: 128
    matmul_471: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_468_0, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_475: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_471, e2e__fused_op_34_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_35: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4, layernorm_476.dc.subtract.1, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_479: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_485: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_491: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_479, matmul_485],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_499: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_37: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_36],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_37, matmul_499],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_510: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_514: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_510, _fused_op_35],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_14:
    target_device: 0
    input_count: 128
    _fused_op_38: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_515.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_515.4, e2e_layernorm_515.dc.subtract.1_0, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_518: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_38, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_521: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_518],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_528: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_524, _fused_op_38],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_39: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4, layernorm_529.dc.subtract.1, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_532: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_39, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_538: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_39, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_544: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_532, matmul_538],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_15:
    target_device: 0
    input_count: 128
    _fused_op_40: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_544_0, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_552: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_39_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_41: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_40],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_41, matmul_552],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_563: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_567: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_563, e2e__fused_op_39_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_42: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4, layernorm_568.dc.subtract.1, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_571: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_42, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_574: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_571],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_16:
    target_device: 0
    input_count: 128
    matmul_577: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_574_0, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_581: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_577, e2e__fused_op_42_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_43: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4, layernorm_582.dc.subtract.1, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_585: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_591: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_597: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_585, matmul_591],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_44, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_605: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_45: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_44],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_45, matmul_605],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_616: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_620: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_616, _fused_op_43],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_17:
    target_device: 0
    input_count: 128
    _fused_op_46: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_621.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_621.4, e2e_layernorm_621.dc.subtract.1_0, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_624: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_46, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_627: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_624],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_634: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_630, _fused_op_46],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_47: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4, layernorm_635.dc.subtract.1, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_638: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_47, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_644: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_47, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_650: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_638, matmul_644],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_18:
    target_device: 0
    input_count: 128
    _fused_op_48: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_650_0, input_1_multiply_652_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_654.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_48, lc.input_tensor.softmax_654.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_658: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_47_0, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_49: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_654.dc.reduce_sum.1.lc1, _fused_op_48],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_665: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_49, matmul_658],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_669: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_665, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_673: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_669, e2e__fused_op_47_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_674.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_673, lc.input_tensor.layernorm_674.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_674.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_673, layernorm_674.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_674.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_674.dc.subtract.1, layernorm_674.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_674.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_674.dc.multiply.2, lc.input_tensor.layernorm_674.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_50: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_674.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_674.4, layernorm_674.dc.subtract.1, layer.12.attention.output.LayerNorm.weight, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_677: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_50, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_680: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_677],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_19:
    target_device: 0
    input_count: 128
    matmul_683: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_680_0, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_687: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_683, e2e__fused_op_50_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_687, lc.input_tensor.layernorm_688.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_688.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_687, layernorm_688.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_688.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_688.dc.subtract.1, layernorm_688.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_688.dc.multiply.2, lc.input_tensor.layernorm_688.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_51: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_688.4, layernorm_688.dc.subtract.1, layer.12.output.LayerNorm.weight, layer.12.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_691: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_697: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_703: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_691, matmul_697],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_52: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_703, input_1_multiply_705_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_707.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_52, lc.input_tensor.softmax_707.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_711: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_53: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_707.dc.reduce_sum.1.lc1, _fused_op_52],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_718: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_53, matmul_711],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_722: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_718, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_726: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_722, _fused_op_51],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_727.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_726, lc.input_tensor.layernorm_727.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_727.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_726, layernorm_727.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_727.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_727.dc.subtract.1, layernorm_727.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_727.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_727.dc.multiply.2, lc.input_tensor.layernorm_727.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_20:
    target_device: 0
    input_count: 128
    _fused_op_54: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_727.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_727.4, e2e_layernorm_727.dc.subtract.1_0, layer.13.attention.output.LayerNorm.weight, layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_730: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_54, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_733: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_730],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_736: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_733, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_740: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_736, _fused_op_54],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_740, lc.input_tensor.layernorm_741.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_741.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_740, layernorm_741.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_741.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_741.dc.subtract.1, layernorm_741.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_741.dc.multiply.2, lc.input_tensor.layernorm_741.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_55: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_741.4, layernorm_741.dc.subtract.1, layer.13.output.LayerNorm.weight, layer.13.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_744: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_55, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_750: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_55, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_756: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_744, matmul_750],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_21:
    target_device: 0
    input_count: 128
    _fused_op_56: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_756_0, input_1_multiply_758_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_760.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_56, lc.input_tensor.softmax_760.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_764: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_55_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_57: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_760.dc.reduce_sum.1.lc1, _fused_op_56],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_771: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_57, matmul_764],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_775: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_771, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_779: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_775, e2e__fused_op_55_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_780.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_779, lc.input_tensor.layernorm_780.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_780.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_779, layernorm_780.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_780.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_780.dc.subtract.1, layernorm_780.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_780.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_780.dc.multiply.2, lc.input_tensor.layernorm_780.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_58: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_780.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_780.4, layernorm_780.dc.subtract.1, layer.14.attention.output.LayerNorm.weight, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_783: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_58, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_786: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_783],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_22:
    target_device: 0
    input_count: 128
    matmul_789: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_786_0, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_793: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_789, e2e__fused_op_58_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_793, lc.input_tensor.layernorm_794.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_794.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_793, layernorm_794.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_794.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_794.dc.subtract.1, layernorm_794.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_794.dc.multiply.2, lc.input_tensor.layernorm_794.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_59: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_794.4, layernorm_794.dc.subtract.1, layer.14.output.LayerNorm.weight, layer.14.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_797: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_803: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_809: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_797, matmul_803],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_60: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_809, input_1_multiply_811_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_813.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_60, lc.input_tensor.softmax_813.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_817: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_61: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_813.dc.reduce_sum.1.lc1, _fused_op_60],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_824: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_61, matmul_817],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_828: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_824, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_832: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_828, _fused_op_59],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_833.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_832, lc.input_tensor.layernorm_833.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_833.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_832, layernorm_833.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_833.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_833.dc.subtract.1, layernorm_833.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_833.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_833.dc.multiply.2, lc.input_tensor.layernorm_833.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_23:
    target_device: 0
    input_count: 128
    _fused_op_62: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_833.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_833.4, e2e_layernorm_833.dc.subtract.1_0, layer.15.attention.output.LayerNorm.weight, layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_836: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_62, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_839: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_836],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_842: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_839, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_846: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_842, _fused_op_62],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_846, lc.input_tensor.layernorm_847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_847.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_846, layernorm_847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_847.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_847.dc.subtract.1, layernorm_847.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_847.dc.multiply.2, lc.input_tensor.layernorm_847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_63: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_847.4, layernorm_847.dc.subtract.1, layer.15.output.LayerNorm.weight, layer.15.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_850: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_63, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_856: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_63, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_862: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_850, matmul_856],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_24:
    target_device: 0
    input_count: 128
    _fused_op_64: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_862_0, input_1_multiply_864_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_866.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_64, lc.input_tensor.softmax_866.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_870: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_63_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_65: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_866.dc.reduce_sum.1.lc1, _fused_op_64],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_877: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_65, matmul_870],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_881: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_877, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_885: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_881, e2e__fused_op_63_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_886.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_885, lc.input_tensor.layernorm_886.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_886.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_885, layernorm_886.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_886.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_886.dc.subtract.1, layernorm_886.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_886.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_886.dc.multiply.2, lc.input_tensor.layernorm_886.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_66: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_886.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_886.4, layernorm_886.dc.subtract.1, layer.16.attention.output.LayerNorm.weight, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_889: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_66, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_892: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_889],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_25:
    target_device: 0
    input_count: 128
    matmul_895: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_892_0, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_899: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_895, e2e__fused_op_66_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_899, lc.input_tensor.layernorm_900.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_900.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_899, layernorm_900.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_900.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_900.dc.subtract.1, layernorm_900.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_900.dc.multiply.2, lc.input_tensor.layernorm_900.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_67: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_900.4, layernorm_900.dc.subtract.1, layer.16.output.LayerNorm.weight, layer.16.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_903: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_909: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_915: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_903, matmul_909],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_68: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_915, input_1_multiply_917_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_919.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_68, lc.input_tensor.softmax_919.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_923: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_69: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_919.dc.reduce_sum.1.lc1, _fused_op_68],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_930: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_69, matmul_923],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_934: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_930, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_938: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_934, _fused_op_67],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_939.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_938, lc.input_tensor.layernorm_939.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_939.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_938, layernorm_939.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_939.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_939.dc.subtract.1, layernorm_939.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_939.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_939.dc.multiply.2, lc.input_tensor.layernorm_939.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_26:
    target_device: 0
    input_count: 128
    _fused_op_70: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_939.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_939.4, e2e_layernorm_939.dc.subtract.1_0, layer.17.attention.output.LayerNorm.weight, layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_942: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_70, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_945: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_942],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_948: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_945, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_952: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_948, _fused_op_70],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_952, lc.input_tensor.layernorm_953.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_953.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_952, layernorm_953.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_953.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_953.dc.subtract.1, layernorm_953.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_953.dc.multiply.2, lc.input_tensor.layernorm_953.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_71: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_953.4, layernorm_953.dc.subtract.1, layer.17.output.LayerNorm.weight, layer.17.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_956: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_71, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_962: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_71, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_968: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_956, matmul_962],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_27:
    target_device: 0
    input_count: 128
    _fused_op_72: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_968_0, input_1_multiply_970_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_972.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_72, lc.input_tensor.softmax_972.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_976: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_71_0, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_73: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_972.dc.reduce_sum.1.lc1, _fused_op_72],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_983: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_73, matmul_976],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_987: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_983, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_991: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_987, e2e__fused_op_71_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_992.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_991, lc.input_tensor.layernorm_992.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_992.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_991, layernorm_992.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_992.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_992.dc.subtract.1, layernorm_992.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_992.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_992.dc.multiply.2, lc.input_tensor.layernorm_992.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_74: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_992.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_992.4, layernorm_992.dc.subtract.1, layer.18.attention.output.LayerNorm.weight, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_995: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_74, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_998: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_995],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_28:
    target_device: 0
    input_count: 128
    matmul_1001: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_998_0, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1005: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1001, e2e__fused_op_74_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1005, lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1006.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1005, layernorm_1006.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1006.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1006.dc.subtract.1, layernorm_1006.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1006.dc.multiply.2, lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_75: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_1006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1006.4, layernorm_1006.dc.subtract.1, layer.18.output.LayerNorm.weight, layer.18.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1009: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1015: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1021: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_1009, matmul_1015],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_76: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_1021, input_1_multiply_1023_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1025.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_76, lc.input_tensor.softmax_1025.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1029: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_77: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_1025.dc.reduce_sum.1.lc1, _fused_op_76],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1036: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_77, matmul_1029],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1040: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1036, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1044: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_1040, _fused_op_75],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1045.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_1044, lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1045.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_1044, layernorm_1045.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1045.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_1045.dc.subtract.1, layernorm_1045.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1045.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_1045.dc.multiply.2, lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_29:
    target_device: 0
    input_count: 128
    _fused_op_78: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_1045.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_1045.4, e2e_layernorm_1045.dc.subtract.1_0, layer.19.attention.output.LayerNorm.weight, layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1048: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_78, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_1051: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_1048],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_1054: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_1051, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1058: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_1054, _fused_op_78],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_1058, lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1059.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_1058, layernorm_1059.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1059.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1059.dc.subtract.1, layernorm_1059.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1059.dc.multiply.2, lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_79: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_1059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1059.4, layernorm_1059.dc.subtract.1, layer.19.output.LayerNorm.weight, layer.19.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1062: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_79, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1068: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_79, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1074: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_1062, matmul_1068],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_30:
    target_device: 0
    input_count: 128
    _fused_op_80: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_1074_0, input_1_multiply_1076_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1078.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_80, lc.input_tensor.softmax_1078.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1082: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_79_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_81: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_1078.dc.reduce_sum.1.lc1, _fused_op_80],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1089: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_81, matmul_1082],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1093: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_1089, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1097: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_1093, e2e__fused_op_79_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1098.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1097, lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1098.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_1097, layernorm_1098.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1098.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1098.dc.subtract.1, layernorm_1098.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1098.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1098.dc.multiply.2, lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_82: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_1098.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1098.4, layernorm_1098.dc.subtract.1, layer.20.attention.output.LayerNorm.weight, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1101: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_82, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_1104: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_1101],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_31:
    target_device: 0
    input_count: 128
    matmul_1107: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_1104_0, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1111: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1107, e2e__fused_op_82_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1111, lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1112.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1111, layernorm_1112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1112.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1112.dc.subtract.1, layernorm_1112.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1112.dc.multiply.2, lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_83: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_1112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1112.4, layernorm_1112.dc.subtract.1, layer.20.output.LayerNorm.weight, layer.20.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1115: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1121: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1127: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_1115, matmul_1121],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_84: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_1127, input_1_multiply_1129_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1131.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_84, lc.input_tensor.softmax_1131.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1135: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_85: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_1131.dc.reduce_sum.1.lc1, _fused_op_84],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1142: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_85, matmul_1135],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1146: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1142, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1150: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_1146, _fused_op_83],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1151.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_1150, lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1151.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_1150, layernorm_1151.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1151.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_1151.dc.subtract.1, layernorm_1151.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1151.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_1151.dc.multiply.2, lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_32:
    target_device: 0
    input_count: 128
    _fused_op_86: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_1151.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_1151.4, e2e_layernorm_1151.dc.subtract.1_0, layer.21.attention.output.LayerNorm.weight, layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1154: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_86, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_1157: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_1154],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_1160: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_1157, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1164: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_1160, _fused_op_86],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_1164, lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1165.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_1164, layernorm_1165.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1165.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1165.dc.subtract.1, layernorm_1165.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1165.dc.multiply.2, lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_87: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_1165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1165.4, layernorm_1165.dc.subtract.1, layer.21.output.LayerNorm.weight, layer.21.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1168: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_87, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1174: {type: matmul, grid_loc: [8, 6], grid_size: [2, 4], inputs: [_fused_op_87, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1180: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_1168, matmul_1174],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_33:
    target_device: 0
    input_count: 128
    _fused_op_88: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 6], inputs: [e2e_matmul_1180_0, input_1_multiply_1182_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1184.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_88, lc.input_tensor.softmax_1184.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1188: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [e2e__fused_op_87_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_89: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_1184.dc.reduce_sum.1.lc1, _fused_op_88],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1195: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [_fused_op_89, matmul_1188],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1199: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_1195, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1203: {type: add, grid_loc: [2, 6], grid_size: [2, 2], inputs: [matmul_1199, e2e__fused_op_87_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1204.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [2, 1], inputs: [add_1203, lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1204.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_1203, layernorm_1204.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1204.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1204.dc.subtract.1, layernorm_1204.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1204.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1204.dc.multiply.2, lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_90: {type: fused_op, grid_loc: [4, 1], grid_size: [4, 1], inputs: [layernorm_1204.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1204.4, layernorm_1204.dc.subtract.1, layer.22.attention.output.LayerNorm.weight, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1207: {type: matmul, grid_loc: [4, 2], grid_size: [4, 8], inputs: [_fused_op_90, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_1210: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_1207],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}

  fwd_34:
    target_device: 0
    input_count: 128
    matmul_1213: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_1210_0, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1217: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1213, e2e__fused_op_90_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1217, lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1218.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1217, layernorm_1218.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1218.dc.multiply.2: {type: multiply, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1218.dc.subtract.1, layernorm_1218.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1218.dc.multiply.2, lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_91: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_1218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1218.4, layernorm_1218.dc.subtract.1, layer.22.output.LayerNorm.weight, layer.22.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1221: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1227: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1233: {type: matmul, grid_loc: [4, 8], grid_size: [2, 2], inputs: [matmul_1221, matmul_1227],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_92: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 6], inputs: [matmul_1233, input_1_multiply_1235_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1237.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [2, 1], inputs: [_fused_op_92, lc.input_tensor.softmax_1237.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1241: {type: matmul, grid_loc: [6, 6], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    _fused_op_93: {type: fused_op, grid_loc: [6, 10], grid_size: [2, 1], inputs: [softmax_1237.dc.reduce_sum.1.lc1, _fused_op_92],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1248: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_93, matmul_1241],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, input_buf_min_size_tiles: [0, 24], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1252: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1248, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_1256: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_1252, _fused_op_91],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1257.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_1256, lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1257.dc.subtract.1: {type: subtract, grid_loc: [8, 8], grid_size: [2, 2], inputs: [add_1256, layernorm_1257.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1257.dc.multiply.2: {type: multiply, grid_loc: [8, 10], grid_size: [2, 1], inputs: [layernorm_1257.dc.subtract.1, layernorm_1257.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1257.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [2, 1], inputs: [layernorm_1257.dc.multiply.2, lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}

  fwd_35:
    target_device: 0
    input_count: 128
    _fused_op_94: {type: fused_op, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_layernorm_1257.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_1257.4, e2e_layernorm_1257.dc.subtract.1_0, layer.23.attention.output.LayerNorm.weight, layer.23.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1260: {type: matmul, grid_loc: [0, 1], grid_size: [4, 8], inputs: [_fused_op_94, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 4}}
    gelu_1263: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_1260],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_1266: {type: matmul, grid_loc: [4, 4], grid_size: [4, 8], inputs: [gelu_1263, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 8, u_kt: 16}}
    add_1270: {type: add, grid_loc: [0, 9], grid_size: [2, 2], inputs: [matmul_1266, _fused_op_94],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_1270, lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_1271.dc.subtract.1: {type: subtract, grid_loc: [2, 9], grid_size: [2, 2], inputs: [add_1270, layernorm_1271.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1271.dc.multiply.2: {type: multiply, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1271.dc.subtract.1, layernorm_1271.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_1271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1271.dc.multiply.2, lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    _fused_op_95: {type: fused_op, grid_loc: [6, 1], grid_size: [4, 1], inputs: [layernorm_1271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1271.4, layernorm_1271.dc.subtract.1, layer.23.output.LayerNorm.weight, layer.23.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 192, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_95_output_nop_0: {type: nop, grid_loc: [8, 2], grid_size: [2, 4], inputs: [_fused_op_95], untilize_output: true,
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_zero: 0, $lptr_q48: 0, $gptr_q46: 0, $gptr_q44: 0, $gptr_q43: 0, $gptr_q41: 0, $lptr_q58: 0, $gptr_q11: 0, $gptr_q29: 0, $gptr_q33: 0, $lptr_q13: 0, $gptr_q16: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q18: 0, $lptr_q41: 0, $gptr_q13: 0, $lptr_q14: 0, $gptr_q19: 0, $lptr_q46: 0, $gptr_q21: 0, $lptr_q21: 0, $gptr_q34: 0, $gptr_q49: 0, $lptr_q31: 0, $gptr_q3: 0, $gptr_q59: 0, $lptr_q59: 0, $lptr_q19: 0, $gptr_q58: 0, $lptr_q3: 0, $gptr_q56: 0, $gptr_q23: 0, $lptr_q53: 0, $lptr_q23: 0, $lptr_q54: 0, $gptr_q54: 0, $gptr_q53: 0, $lptr_q16: 0, $lptr_q49: 0, $lptr_q51: 0, $lptr_q26: 0, $lptr_q39: 0, $gptr_q51: 0, $lptr_q34: 0, $c_one: 1, $gptr_q9: 0, $lptr_q18: 0, $gptr_q24: 0, $lptr_q8: 0, $lptr_q6: 0, $lptr_q56: 0, $gptr_q6: 0, $lptr_q44: 0, $lptr_q43: 0, $gptr_q4: 0, $lptr_q4: 0, $gptr_q36: 0, $lptr_q24: 0, $lptr_q9: 0, $gptr_q26: 0, $lptr_q11: 0, $lptr_q28: 0, $gptr_q28: 0, $lptr_q29: 0, $gptr_q39: 0, $gptr_q31: 0, $lptr_q33: 0, $lptr_q36: 0, $gptr_q48: 0, $lptr_q38: 0, $gptr_q38: 0}
    - staticvar: {$gptr_q47_shadow: 0, $lptr_q47: 0, $gptr_q45: 0, $lptr_q45: 0, $gptr_q42_shadow: 0, $gptr_q40_shadow: 0, $gptr_q40: 0, $gptr_q37_shadow: 0, $gptr_q37: 0, $gptr_q7_shadow: 0, $gptr_q30_shadow: 0, $lptr_q42: 0, $gptr_q7: 0, $gptr_q42: 0, $lptr_q17: 0, $lptr_q7: 0, $gptr_q10_shadow: 0, $gptr_q47: 0, $gptr_q20_shadow: 0, $gptr_q12_shadow: 0, $gptr_q17: 0, $lptr_q35: 0, $lptr_q10: 0, $lptr_q0: 0, $gptr_q25_shadow: 0, $gptr_q12: 0, $lptr_q27: 0, $lptr_q37: 0, $gptr_q15: 0, $gptr_q57: 0, $gptr_q17_shadow: 0, $lptr_q15: 0, $lptr_q52: 0, $lptr_q40: 0, $gptr_q22_shadow: 0, $gptr_q55: 0, $gptr_q45_shadow: 0, $gptr_q30: 0, $lptr_q25: 0, $gptr_q52: 0, $gptr_q10: 0, $gptr_q5: 0, $lptr_q57: 0, $gptr_q20: 0, $lptr_q50: 0, $gptr_q0: 0, $gptr_q50: 0, $lptr_q20: 0, $lptr_q5: 0, $gptr_q5_shadow: 0, $gptr_q55_shadow: 0, $lptr_q2: 0, $gptr_q2: 0, $lptr_q55: 0, $gptr_q2_shadow: 0, $gptr_q1: 0, $gptr_q25: 0, $gptr_q1_shadow: 0, $lptr_q12: 0, $gptr_q15_shadow: 0, $lptr_q22: 0, $lptr_q1: 0, $gptr_q22: 0, $gptr_q52_shadow: 0, $gptr_q27: 0, $gptr_q27_shadow: 0, $lptr_q30: 0, $lptr_q32: 0, $gptr_q35_shadow: 0, $gptr_q32: 0, $gptr_q35: 0, $gptr_q50_shadow: 0, $gptr_q32_shadow: 0}
    - varinst: [$gptr_q55, set, $gptr_q55_shadow]
    - varinst: [$gptr_q52, set, $gptr_q52_shadow]
    - varinst: [$gptr_q50, set, $gptr_q50_shadow]
    - varinst: [$gptr_q20, set, $gptr_q20_shadow]
    - varinst: [$gptr_q17, set, $gptr_q17_shadow]
    - varinst: [$gptr_q15, set, $gptr_q15_shadow]
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q7, set, $gptr_q7_shadow]
    - varinst: [$gptr_q5, set, $gptr_q5_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q22, set, $gptr_q22_shadow]
    - varinst: [$gptr_q25, set, $gptr_q25_shadow]
    - varinst: [$gptr_q27, set, $gptr_q27_shadow]
    - varinst: [$gptr_q30, set, $gptr_q30_shadow]
    - varinst: [$gptr_q32, set, $gptr_q32_shadow]
    - varinst: [$gptr_q35, set, $gptr_q35_shadow]
    - varinst: [$gptr_q37, set, $gptr_q37_shadow]
    - varinst: [$gptr_q40, set, $gptr_q40_shadow]
    - varinst: [$gptr_q42, set, $gptr_q42_shadow]
    - varinst: [$gptr_q45, set, $gptr_q45_shadow]
    - varinst: [$gptr_q47, set, $gptr_q47_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_2_0, e2e_gelu_44_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_91.dc.subtract.1_0, e2e_layernorm_91.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_2_0, e2e_gelu_44_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_7_0, e2e_matmul_120_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               e2e_layernorm_91.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_91.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_91.dc.subtract.1_0, e2e_layernorm_91.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_10_0, e2e_gelu_150_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_120_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_7_0, e2e_matmul_120_0]
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_197.dc.subtract.1_0, e2e_layernorm_197.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_10_0, e2e_gelu_150_0]
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_15_0, e2e_matmul_226_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               e2e_layernorm_197.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_layernorm_197.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_197.dc.subtract.1_0, e2e_layernorm_197.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_18_0, e2e_gelu_256_0]
    -   execute: {graph_name: fwd_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_matmul_226_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_15_0, e2e_matmul_226_0]
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_303.dc.subtract.1_0, e2e_layernorm_303.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_gelu_256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_18_0, e2e_gelu_256_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_23_0, e2e_matmul_332_0]
    -   execute: {graph_name: fwd_8, queue_settings: {
               e2e_layernorm_303.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_layernorm_303.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_303.dc.subtract.1_0, e2e_layernorm_303.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_26_0, e2e_gelu_362_0]
    -   execute: {graph_name: fwd_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_332_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_23_0, e2e_matmul_332_0]
    -   varinst: [$gptr_q15_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_409.dc.subtract.1_0, e2e_layernorm_409.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_10, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_gelu_362_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_26_0, e2e_gelu_362_0]
    -   varinst: [$gptr_q17_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_31_0, e2e_matmul_438_0]
    -   execute: {graph_name: fwd_11, queue_settings: {
               e2e_layernorm_409.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_layernorm_409.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_409.dc.subtract.1_0, e2e_layernorm_409.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_34_0, e2e_gelu_468_0]
    -   execute: {graph_name: fwd_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_matmul_438_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_31_0, e2e_matmul_438_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_515.dc.subtract.1_0, e2e_layernorm_515.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_13, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_34_0, e2e_gelu_468_0]
    -   varinst: [$gptr_q22_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_39_0, e2e_matmul_544_0]
    -   execute: {graph_name: fwd_14, queue_settings: {
               e2e_layernorm_515.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_layernorm_515.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_515.dc.subtract.1_0, e2e_layernorm_515.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q24, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_42_0, e2e_gelu_574_0]
    -   execute: {graph_name: fwd_15, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_matmul_544_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_39_0, e2e_matmul_544_0]
    -   varinst: [$gptr_q25_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q26, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_621.dc.subtract.1_0, e2e_layernorm_621.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_16, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e__fused_op_42_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_gelu_574_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_42_0, e2e_gelu_574_0]
    -   varinst: [$gptr_q27_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q28, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_47_0, e2e_matmul_650_0]
    -   execute: {graph_name: fwd_17, queue_settings: {
               e2e_layernorm_621.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_layernorm_621.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_621.dc.subtract.1_0, e2e_layernorm_621.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_50_0, e2e_gelu_680_0]
    -   execute: {graph_name: fwd_18, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_47_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_matmul_650_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               input_1_multiply_652_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_47_0, e2e_matmul_650_0]
    -   varinst: [$gptr_q30_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_727.dc.subtract.1_0, e2e_layernorm_727.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_19, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_gelu_680_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_50_0, e2e_gelu_680_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_55_0, e2e_matmul_756_0]
    -   execute: {graph_name: fwd_20, queue_settings: {
               e2e_layernorm_727.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_layernorm_727.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               dc.input_tensor.layernorm_727.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_727.dc.subtract.1_0, e2e_layernorm_727.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q34, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_58_0, e2e_gelu_786_0]
    -   execute: {graph_name: fwd_21, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e_matmul_756_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               input_1_multiply_758_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_55_0, e2e_matmul_756_0]
    -   varinst: [$gptr_q35_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q36, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_833.dc.subtract.1_0, e2e_layernorm_833.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_22, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_gelu_786_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_58_0, e2e_gelu_786_0]
    -   varinst: [$gptr_q37_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q38, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_63_0, e2e_matmul_862_0]
    -   execute: {graph_name: fwd_23, queue_settings: {
               e2e_layernorm_833.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               e2e_layernorm_833.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               dc.input_tensor.layernorm_833.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_833.dc.subtract.1_0, e2e_layernorm_833.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_66_0, e2e_gelu_892_0]
    -   execute: {graph_name: fwd_24, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e__fused_op_63_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               e2e_matmul_862_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               input_1_multiply_864_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_63_0, e2e_matmul_862_0]
    -   varinst: [$gptr_q40_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_939.dc.subtract.1_0, e2e_layernorm_939.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_25, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_gelu_892_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_66_0, e2e_gelu_892_0]
    -   varinst: [$gptr_q42_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_71_0, e2e_matmul_968_0]
    -   execute: {graph_name: fwd_26, queue_settings: {
               e2e_layernorm_939.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e_layernorm_939.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               dc.input_tensor.layernorm_939.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_939.dc.subtract.1_0, e2e_layernorm_939.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q44, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_74_0, e2e_gelu_998_0]
    -   execute: {graph_name: fwd_27, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               e2e__fused_op_71_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_matmul_968_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               input_1_multiply_970_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_71_0, e2e_matmul_968_0]
    -   varinst: [$gptr_q45_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1045.dc.subtract.1_0, e2e_layernorm_1045.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_28, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               e2e__fused_op_74_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               e2e_gelu_998_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_74_0, e2e_gelu_998_0]
    -   varinst: [$gptr_q47_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q48, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_79_0, e2e_matmul_1074_0]
    -   execute: {graph_name: fwd_29, queue_settings: {
               e2e_layernorm_1045.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_layernorm_1045.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               dc.input_tensor.layernorm_1045.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1045.dc.subtract.1_0, e2e_layernorm_1045.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_82_0, e2e_gelu_1104_0]
    -   execute: {graph_name: fwd_30, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e__fused_op_79_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               e2e_matmul_1074_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               input_1_multiply_1076_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_79_0, e2e_matmul_1074_0]
    -   varinst: [$gptr_q50_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1151.dc.subtract.1_0, e2e_layernorm_1151.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_31, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               e2e__fused_op_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               e2e_gelu_1104_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_82_0, e2e_gelu_1104_0]
    -   varinst: [$gptr_q52_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q53, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q52, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q53, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_87_0, e2e_matmul_1180_0]
    -   execute: {graph_name: fwd_32, queue_settings: {
               e2e_layernorm_1151.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               e2e_layernorm_1151.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               dc.input_tensor.layernorm_1151.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1151.dc.subtract.1_0, e2e_layernorm_1151.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q54, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q54, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_90_0, e2e_gelu_1210_0]
    -   execute: {graph_name: fwd_33, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e__fused_op_87_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               e2e_matmul_1180_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               input_1_multiply_1182_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_87_0, e2e_matmul_1180_0]
    -   varinst: [$gptr_q55_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q56, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q55, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q56, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1257.dc.subtract.1_0, e2e_layernorm_1257.dc.reduce_avg.3.lc1_0]
    -   execute: {graph_name: fwd_34, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               e2e__fused_op_90_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               e2e_gelu_1210_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_90_0, e2e_gelu_1210_0]
    -   varinst: [$gptr_q57, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q58, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q57, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q58, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_35, queue_settings: {
               e2e_layernorm_1257.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               e2e_layernorm_1257.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               dc.input_tensor.layernorm_1257.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1257.dc.subtract.1_0, e2e_layernorm_1257.dc.reduce_avg.3.lc1_0]
    -   varinst: [$gptr_q59, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q59, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 2], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [3, 1], ublock: [2, 2], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [3, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 12}], pop_last: [intermed0], mblock: [3, 3], ublock: [2, 4], output: output}
  2: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [1, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [3, 1], ublock: [1, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [1, 1], output: intermed0}
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 32}, tile_broadcast: c], pop_last: [intermed0], mblock: [3, 8], ublock: [1, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input3], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [1, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input4], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [1, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_1271]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.05
