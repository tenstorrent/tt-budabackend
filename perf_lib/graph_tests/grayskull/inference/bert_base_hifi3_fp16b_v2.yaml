# git checkout 5259b590
# pybuda/test/benchmark/benchmark.py -m bert -c base -opt 3 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=85000 PYBUDA_DISABLE_DYNAMIC_DRAM=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1 --loop_count 100

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30208020]]}
  attention_mask:                                   {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}

  # output
  bert_encoders.output_layernorm_635:               {input: _fused_op_47_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x54c13a0], [3, 0x54feac0], [4, 0x5473060], [5, 0x5472840]]}
  layer.0.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5e25360], [5, 0x5e1c120], [6, 0x5dd2f20], [7, 0x5eabc80]]}
  layer.0.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b16b00], [1, 0x6b08780], [2, 0x5f2aba0], [3, 0x5e5e160]]}
  layer.0.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5e22280], [5, 0x5e19040], [6, 0x5dcfe40], [7, 0x5ea8ba0]]}
  layer.0.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dcfe20], [6, 0x5d86c20], [7, 0x5e5f980], [0, 0x6acd8e0]]}
  layer.0.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5d78800], [4, 0x5d3d160], [5, 0x5d31680], [6, 0x5ce8480]]}
  layer.0.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ab8b80], [2, 0x5edb7e0], [3, 0x5e0e560], [4, 0x5dd2ec0]]}
  layer.0.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dc6ba0], [6, 0x5d7d9a0], [7, 0x5e56700], [0, 0x6ac4660]]}
  layer.0.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6ab8340]]}
  layer.0.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5e4a3e0]]}
  layer.0.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5db7fa0], [0, 0x6a25f00], [1, 0x6a256e0], [2, 0x5e48340], [3, 0x5d7b8e0], [4, 0x5d40240], [5, 0x5d34760], [6, 0x5ceb560], [7, 0x5e011c0], [0, 0x6a6f120], [1, 0x6a6e900], [2, 0x5e91560], [3, 0x5dc4b00], [4, 0x5d89460], [5, 0x5d7d980], [6, 0x5d34780]]}
  layer.0.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dc9c80], [6, 0x5d80a80], [7, 0x5e597e0], [0, 0x6ac7740], [1, 0x6b01da0], [2, 0x5f24a00], [3, 0x5e57780], [4, 0x5e1c0e0]]}
  layer.0.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5e26c80], [7, 0x5ec5380], [0, 0x6bb2a20], [1, 0x6c33a00], [2, 0x60586c0], [3, 0x5fd1dc0], [4, 0x5f15f80], [5, 0x5e7e220], [6, 0x5e6fea0], [7, 0x5f0e5a0], [0, 0x6bfbc40], [1, 0x6c7cc20], [2, 0x60a18e0], [3, 0x601afe0], [4, 0x5f5f1a0], [5, 0x5ec7440]]}
  layer.0.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60555e0], [3, 0x5fcece0], [4, 0x5f12ea0], [5, 0x5e7b140]]}
  layer.0.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e6ee20]]}
  layer.0.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5f06b80]]}
  layer.1.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b68fc0], [1, 0x6be9fa0], [2, 0x600c3c0], [3, 0x5f85ac0]]}
  layer.1.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6b519a0], [2, 0x5f73dc0], [3, 0x5ea7380], [4, 0x5e28440]]}
  layer.1.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5f3c8a0], [4, 0x5ebd960], [5, 0x5e25c00], [6, 0x5ddca00]]}
  layer.1.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5eb4f20], [0, 0x6b65ee0], [1, 0x6be6ec0], [2, 0x60092e0]]}
  layer.1.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6b9dca0], [2, 0x5fc00c0], [3, 0x5ef3680], [4, 0x5e74740]]}
  layer.1.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e222e0], [6, 0x5dd90e0], [7, 0x5eb1e40], [0, 0x6b62e00]]}
  layer.1.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6b54a80], [2, 0x5f76ea0], [3, 0x5eaa460], [4, 0x5e2b520]]}
  layer.1.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e1f200], [6, 0x5dd6000], [7, 0x5eaed60], [0, 0x6b5fd20]]}
  layer.1.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5bed580]]}
  layer.1.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b9e200]]}
  layer.1.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b0bdc0], [7, 0x5b5b140], [0, 0x67d4360], [1, 0x67d1ac0], [2, 0x5bf4f40], [3, 0x5ba63e0], [4, 0x5b6ad40], [5, 0x5b6cdc0], [6, 0x5b54fe0], [7, 0x5ba4360], [0, 0x681d580], [1, 0x681ace0], [2, 0x5c3e160], [3, 0x5bef600], [4, 0x5bb3f60], [5, 0x5bb5fe0]]}
  layer.1.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b05c20], [7, 0x5b54fa0], [0, 0x67ce1c0], [1, 0x67cb920], [2, 0x5beeda0], [3, 0x5ba0240], [4, 0x5b64ba0], [5, 0x5b66c20]]}
  layer.1.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a737e0], [7, 0x5ac2b60], [0, 0x673bd80], [1, 0x67394e0], [2, 0x5b5c960], [3, 0x5b0de00], [4, 0x5ad2760], [5, 0x5ad47e0], [6, 0x5abca00], [7, 0x5b0bd80], [0, 0x6784fa0], [1, 0x6782700], [2, 0x5ba5b80], [3, 0x5b57020], [4, 0x5b1b980], [5, 0x5b1da00]]}
  layer.1.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5b59880], [3, 0x5b0ad20], [4, 0x5acf680], [5, 0x5ad1700]]}
  layer.1.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ac53e0]]}
  layer.1.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5ac3360]]}
  layer.2.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x66f2320], [1, 0x66efa80], [2, 0x5b10660], [3, 0x5ac1b00]]}
  layer.2.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5ac0280], [5, 0x5ac2300], [6, 0x5a6f6a0], [7, 0x5abea20]]}
  layer.2.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x66a9100], [1, 0x66a6860], [2, 0x5ac7440], [3, 0x5a788e0]]}
  layer.2.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5abd1a0], [5, 0x5abf220], [6, 0x5a6c5c0], [7, 0x5abb940]]}
  layer.2.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5d6ed80], [0, 0x69dcce0], [1, 0x69dc4c0], [2, 0x5dff120]]}
  layer.2.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5d75720], [4, 0x5d3a080], [5, 0x5d2e5a0], [6, 0x5ce53a0]]}
  layer.2.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5d25b60], [0, 0x6993ac0], [1, 0x69932a0], [2, 0x5db5f00]]}
  layer.2.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5d72640], [4, 0x5d36fa0], [5, 0x5d2b4c0], [6, 0x5ce22c0]]}
  layer.2.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5cd5fa0]]}
  layer.2.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5c38820]]}
  layer.2.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c98840], [6, 0x5c43b60], [7, 0x5c926c0], [0, 0x6900620], [1, 0x6900620], [2, 0x5d23280], [3, 0x5ce0200], [4, 0x5ca4b60], [5, 0x5ce1a60], [6, 0x5c8cd80], [7, 0x5cdb8e0], [0, 0x6949840], [1, 0x6949840], [2, 0x5d6c4a0], [3, 0x5d29420], [4, 0x5cedd80]]}
  layer.2.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c926a0], [6, 0x5c3d9c0], [7, 0x5c8c520], [0, 0x68fa480], [1, 0x68fa480], [2, 0x5d1d0e0], [3, 0x5cda060], [4, 0x5c9e9c0]]}
  layer.2.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c00260], [6, 0x5bab580], [7, 0x5bfa0e0], [0, 0x6868040], [1, 0x6868040], [2, 0x5c8aca0], [3, 0x5c47c20], [4, 0x5c0c580], [5, 0x5c49480], [6, 0x5bf47a0], [7, 0x5c43300], [0, 0x68b1260], [1, 0x68b1260], [2, 0x5cd3ec0], [3, 0x5c90e40], [4, 0x5c557a0]]}
  layer.2.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6864f60], [2, 0x5c87bc0], [3, 0x5c44b40], [4, 0x5c094a0]]}
  layer.2.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5bfd180]]}
  layer.2.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6380920]]}
  layer.3.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6eb08c0], [1, 0x6f318a0], [2, 0x6353cc0], [3, 0x62cd3c0]]}
  layer.3.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x648b2a0], [3, 0x6407240], [4, 0x63d7680], [5, 0x63839e0]]}
  layer.3.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6337700], [7, 0x63c7a80], [0, 0x70296c0], [1, 0x70665e0]]}
  layer.3.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x64881c0], [3, 0x6404160], [4, 0x63d45a0], [5, 0x6380900]]}
  layer.3.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x638b380], [5, 0x63376e0], [6, 0x62ee4e0], [7, 0x637e860]]}
  layer.3.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7025da0], [1, 0x7062cc0], [2, 0x64850e0], [3, 0x6401080]]}
  layer.3.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f80c80], [2, 0x63a30a0], [3, 0x631c7a0], [4, 0x62a6aa0]]}
  layer.3.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x63fdfa0], [4, 0x63882a0], [5, 0x6334600], [6, 0x62eb400]]}
  layer.3.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x62df0e0]]}
  layer.3.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x63282e0]]}
  layer.3.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6295ea0], [6, 0x624cca0], [7, 0x62eab80], [0, 0x6f92900], [1, 0x6fd0040], [2, 0x63f2460], [3, 0x636bb60], [4, 0x62f5e60], [5, 0x62df0c0], [6, 0x6295ec0], [7, 0x6333da0], [0, 0x6fdbb20], [1, 0x7019260], [2, 0x643b680], [3, 0x63b4d80], [4, 0x633f080]]}
  layer.3.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x628fd00], [6, 0x6246b00], [7, 0x62e49e0], [0, 0x6f8c760], [1, 0x6fc9ea0], [2, 0x63ec2c0], [3, 0x63659c0], [4, 0x62efcc0]]}
  layer.3.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7154100], [1, 0x7191020], [2, 0x6532ca0], [3, 0x64ac3a0], [4, 0x63e52a0], [5, 0x6391600], [6, 0x63df940], [7, 0x6466a80], [0, 0x719d320], [1, 0x71da240], [2, 0x657bec0], [3, 0x64f55c0], [4, 0x642e4c0], [5, 0x63da820], [6, 0x6428b60], [7, 0x64afca0]]}
  layer.3.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6410ca0], [0, 0x70728e0], [1, 0x70af800], [2, 0x648e380]]}
  layer.3.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x64a0080]]}
  layer.3.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6526980]]}
  layer.4.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6395ee0], [7, 0x641d020], [0, 0x710aee0], [1, 0x7147e00]]}
  layer.4.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x65238a0], [3, 0x649cfa0], [4, 0x63e1160], [5, 0x638d4c0]]}
  layer.4.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x71e6540], [1, 0x7223460], [2, 0x65c50e0], [3, 0x653e7e0]]}
  layer.4.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6392e00], [7, 0x6419f40], [0, 0x7107e00], [1, 0x7144d20]]}
  layer.4.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70bebe0], [1, 0x70fbb00], [2, 0x64da680], [3, 0x6453d80]]}
  layer.4.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x63dd840], [5, 0x6389ba0], [6, 0x638fd20], [7, 0x6416e60]]}
  layer.4.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70759c0], [1, 0x70b28e0], [2, 0x6491460], [3, 0x640ab60]]}
  layer.4.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x63da760], [5, 0x6386ac0], [6, 0x638cc40], [7, 0x6413d80]]}
  layer.4.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6195e40]]}
  layer.4.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x621c740]]}
  layer.4.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x618a300], [3, 0x6103a00], [4, 0x60d3620], [5, 0x603b8c0], [6, 0x5fe7400], [7, 0x6085b00], [0, 0x6cf2a00], [1, 0x6d739e0], [2, 0x61d3520], [3, 0x614cc20], [4, 0x611c840], [5, 0x6084ae0], [6, 0x6030620], [7, 0x60ced20], [0, 0x6d3bc20], [1, 0x6dbcc00]]}
  layer.4.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6184160], [3, 0x60fd860], [4, 0x60cd480], [5, 0x6035720], [6, 0x5fe1260], [7, 0x607f960], [0, 0x6cec860], [1, 0x6d6d840]]}
  layer.4.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60f1d20], [3, 0x606b420], [4, 0x603b040], [5, 0x5fa32e0], [6, 0x5f4ee20], [7, 0x5fed520], [0, 0x6c5a420], [1, 0x6cdb400], [2, 0x613af40], [3, 0x60b4640], [4, 0x6084260], [5, 0x5fec500], [6, 0x5f98040], [7, 0x6036740], [0, 0x6ca3640], [1, 0x6d24620]]}
  layer.4.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5f4bd40], [7, 0x5fea440], [0, 0x6c57340], [1, 0x6cd8320]]}
  layer.4.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ccc000]]}
  layer.4.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c4b020]]}
  layer.5.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5ff15e0], [5, 0x5f59880], [6, 0x5f02b20], [7, 0x5fa1220]]}
  layer.5.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c47f40], [1, 0x6cc8f20], [2, 0x60edbe0], [3, 0x60672e0]]}
  layer.5.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5fa83c0], [5, 0x5f10660], [6, 0x5eb9900], [7, 0x5f58000]]}
  layer.5.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c44e60], [1, 0x6cc5e40], [2, 0x60eab00], [3, 0x6064200]]}
  layer.5.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6246ae0], [6, 0x61fd8e0], [7, 0x629b7c0], [0, 0x6f43540]]}
  layer.5.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f7dba0], [2, 0x639ffc0], [3, 0x63196c0], [4, 0x62a39c0]]}
  layer.5.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61fd8c0], [6, 0x61b46c0], [7, 0x62525a0], [0, 0x6efa320]]}
  layer.5.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f7aac0], [2, 0x639cee0], [3, 0x63165e0], [4, 0x62a08e0]]}
  layer.5.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x62945c0]]}
  layer.5.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6079840]]}
  layer.5.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e1e480], [1, 0x6e9f460], [2, 0x62c1880], [3, 0x623af80], [4, 0x6202180], [5, 0x616a420], [6, 0x6121220], [7, 0x61bf920], [0, 0x6e676a0], [1, 0x6ee8680], [2, 0x630aaa0], [3, 0x62841a0], [4, 0x624b3a0], [5, 0x61b3640], [6, 0x616a440], [7, 0x6208b40]]}
  layer.5.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e182e0], [1, 0x6e992c0], [2, 0x62bb6e0], [3, 0x6234de0], [4, 0x61fbfe0], [5, 0x6164280], [6, 0x611b080], [7, 0x61b9780]]}
  layer.5.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6d85ea0], [1, 0x6e06e80], [2, 0x62292a0], [3, 0x61a29a0], [4, 0x6169ba0], [5, 0x60d1e40], [6, 0x6088c40], [7, 0x6127340], [0, 0x6dcf0c0], [1, 0x6e500a0], [2, 0x62724c0], [3, 0x61ebbc0], [4, 0x61b2dc0], [5, 0x611b060], [6, 0x60d1e60], [7, 0x6170560]]}
  layer.5.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6166ac0], [5, 0x60ced60], [6, 0x6085b60], [7, 0x6124260]]}
  layer.5.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5eb8000]]}
  layer.5.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6117f40]]}
  layer.6.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5201440], [4, 0x5203ce0], [5, 0x52034c0], [6, 0x5182d20]]}
  layer.6.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x542bec0], [7, 0x53eefc0], [0, 0x609ffa0], [1, 0x609ffa0]]}
  layer.6.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5478180], [3, 0x54b58a0], [4, 0x5429e40], [5, 0x5429620]]}
  layer.6.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5428de0], [7, 0x53ebee0], [0, 0x609cec0], [1, 0x609cec0]]}
  layer.6.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6053ca0], [1, 0x6053ca0], [2, 0x542ef60], [3, 0x546c680]]}
  layer.6.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5426520], [5, 0x5425d00], [6, 0x5425d00], [7, 0x53e8e00]]}
  layer.6.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x52b1880], [7, 0x5274980], [0, 0x5edcf80], [1, 0x5edcf80]]}
  layer.6.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fbe780], [1, 0x5fbe780], [2, 0x5399a40], [3, 0x53d7160]]}
  layer.6.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x53cae40]]}
  layer.6.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x538d720]]}
  layer.6.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x52fb2e0], [3, 0x5338a00], [4, 0x5300c40], [5, 0x5300420], [6, 0x5300c40], [7, 0x52c3d40], [0, 0x5f2c340], [1, 0x5f2c340], [2, 0x5344500], [3, 0x5381c20], [4, 0x5349e60], [5, 0x5349640], [6, 0x5349e60], [7, 0x530cf60], [0, 0x5f75560], [1, 0x5f75560]]}
  layer.6.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x52f5140], [3, 0x5332860], [4, 0x52faaa0], [5, 0x52fa280], [6, 0x52faaa0], [7, 0x52bdba0], [0, 0x5f261a0], [1, 0x5f261a0]]}
  layer.6.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x53940e0], [5, 0x53938c0], [6, 0x53938c0], [7, 0x53569c0], [0, 0x5fc1860], [1, 0x5fc1860], [2, 0x539cb20], [3, 0x53da240], [4, 0x53dd300], [5, 0x53dcae0], [6, 0x53dcae0], [7, 0x539fbe0], [0, 0x600aa80], [1, 0x600aa80], [2, 0x53e5d40], [3, 0x5423460]]}
  layer.6.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61ca9e0], [1, 0x61c8140], [2, 0x55ac640], [3, 0x555dae0]]}
  layer.6.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55517c0]]}
  layer.6.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55a0320]]}
  layer.7.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5481460], [7, 0x54d07e0], [0, 0x61817c0], [1, 0x617ef20]]}
  layer.7.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x559d240], [3, 0x554e6e0], [4, 0x54c2c80], [5, 0x54c4d00]]}
  layer.7.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x542efa0], [7, 0x53f20a0], [0, 0x60a3080], [1, 0x60a3080]]}
  layer.7.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x54c1c20], [6, 0x547e380], [7, 0x54cd700], [0, 0x617e6e0]]}
  layer.7.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x54844e0], [0, 0x61354c0], [1, 0x61354c0], [2, 0x5554020]]}
  layer.7.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x554adc0], [4, 0x54bf360], [5, 0x54beb40], [6, 0x547b2a0]]}
  layer.7.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x543b2c0], [0, 0x60ec2a0], [1, 0x60ec2a0], [2, 0x550ae00]]}
  layer.7.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5547ce0], [4, 0x54bc280], [5, 0x54bba60], [6, 0x54781c0]]}
  layer.7.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d12560]]}
  layer.7.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d12560]]}
  layer.7.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5c80120], [1, 0x5c80120], [2, 0x50dc3a0], [3, 0x50dcbc0], [4, 0x50df460], [5, 0x50dec40], [6, 0x505e4a0], [7, 0x505e4a0], [0, 0x5cc9340], [1, 0x5cc9340], [2, 0x51255c0], [3, 0x5125de0], [4, 0x5128680], [5, 0x5127e60], [6, 0x50a76c0], [7, 0x50a76c0]]}
  layer.7.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5c79f80], [1, 0x5c79f80], [2, 0x50d6200], [3, 0x50d6a20], [4, 0x50d92c0], [5, 0x50d8aa0], [6, 0x5058300], [7, 0x5058300]]}
  layer.7.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5be7b40], [1, 0x5be7b40], [2, 0x5043dc0], [3, 0x50445e0], [4, 0x5046e80], [5, 0x5046660], [6, 0x4fc5ec0], [7, 0x4fc5ec0], [0, 0x5c30d60], [1, 0x5c30d60], [2, 0x508cfe0], [3, 0x508d800], [4, 0x50900a0], [5, 0x508f880], [6, 0x500f0e0], [7, 0x500f0e0]]}
  layer.7.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5043da0], [5, 0x5043580], [6, 0x4fc2de0], [7, 0x4fc2de0]]}
  layer.7.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x4fb6ac0]]}
  layer.7.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x4fb6ac0]]}
  layer.8.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4ffa360], [3, 0x4ffab80], [4, 0x4ffab80], [5, 0x4ffa360]]}
  layer.8.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x4fb39e0], [7, 0x4fb39e0], [0, 0x5be3a00], [1, 0x5be3a00]]}
  layer.8.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4fb1140], [3, 0x4fb1960], [4, 0x4fb1960], [5, 0x4fb1140]]}
  layer.8.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900], [7, 0x4fb0900], [0, 0x5be0920], [1, 0x5be0920]]}
  layer.8.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e4a300], [1, 0x5e4a300], [2, 0x52a5d60], [3, 0x52e3480]]}
  layer.8.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x52f2060], [3, 0x532f780], [4, 0x52f79c0], [5, 0x52f71a0]]}
  layer.8.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5268660], [7, 0x522b760], [0, 0x5e93d60], [1, 0x5e93d60]]}
  layer.8.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x52eef80], [3, 0x532c6a0], [4, 0x52f48e0], [5, 0x52f40c0]]}
  layer.8.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52e7da0]]}
  layer.8.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x52e85c0]]}
  layer.8.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x516f000], [4, 0x51718a0], [5, 0x5171080], [6, 0x50f08e0], [7, 0x50f08e0], [0, 0x5d1e880], [1, 0x5d1e880], [2, 0x517ab00], [3, 0x51b8220], [4, 0x51baac0], [5, 0x51ba2a0], [6, 0x5139b00], [7, 0x5139b00], [0, 0x5d67aa0], [1, 0x5d67aa0], [2, 0x51c3d20]]}
  layer.8.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e44160], [1, 0x5e44160], [2, 0x529fbc0], [3, 0x52dd2e0], [4, 0x52e2420], [5, 0x52e1c00], [6, 0x5261460], [7, 0x5224560]]}
  layer.8.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5db1d20], [1, 0x5db1d20], [2, 0x520d780], [3, 0x524aea0], [4, 0x524ffe0], [5, 0x524f7c0], [6, 0x51cf020], [7, 0x5192120], [0, 0x5dfaf40], [1, 0x5dfaf40], [2, 0x52569a0], [3, 0x52940c0], [4, 0x5299200], [5, 0x52989e0], [6, 0x5218240], [7, 0x51db340]]}
  layer.8.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x524cf00], [5, 0x524c6e0], [6, 0x51cbf40], [7, 0x518f040]]}
  layer.8.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5182d20]]}
  layer.8.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x516e7e0]]}
  layer.9.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a64b80], [5, 0x5a66c00], [6, 0x5a1f260], [7, 0x5a6e5e0]]}
  layer.9.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6580720], [1, 0x657de80], [2, 0x5a2ace0], [3, 0x59dc180]]}
  layer.9.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a126c0], [5, 0x5a14740], [6, 0x5940b20], [7, 0x598fea0]]}
  layer.9.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x657d640], [1, 0x657ada0], [2, 0x5a27c00], [3, 0x59d90a0]]}
  layer.9.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59de9e0], [3, 0x598fe80], [4, 0x59c94a0], [5, 0x59cb520]]}
  layer.9.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x593d200], [7, 0x598c580], [0, 0x657a560], [1, 0x6577cc0]]}
  layer.9.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59957c0], [3, 0x5946c60], [4, 0x5980280], [5, 0x5982300]]}
  layer.9.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5851f40], [5, 0x5853fc0], [6, 0x580c620], [7, 0x585b9a0]]}
  layer.9.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x65688c0]]}
  layer.9.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x656b160]]}
  layer.9.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x64d8d20], [1, 0x64d6480], [2, 0x5902320], [3, 0x58b37c0], [4, 0x58ed600], [5, 0x58ef680], [6, 0x58a7ce0], [7, 0x58f7060], [0, 0x6521f40], [1, 0x651f6a0], [2, 0x594b540], [3, 0x58fc9e0], [4, 0x5936820], [5, 0x59388a0], [6, 0x58f0f00], [7, 0x5940280]]}
  layer.9.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x64d2b80], [1, 0x64d02e0], [2, 0x58fc180], [3, 0x58ad620], [4, 0x58e7460], [5, 0x58e94e0], [6, 0x58a1b40], [7, 0x58f0ec0]]}
  layer.9.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6440740], [1, 0x643dea0], [2, 0x5869d40], [3, 0x581b1e0], [4, 0x5855020], [5, 0x58570a0], [6, 0x580f700], [7, 0x585ea80], [0, 0x6489960], [1, 0x64870c0], [2, 0x58b2f60], [3, 0x5864400], [4, 0x589e240], [5, 0x58a02c0], [6, 0x5858920], [7, 0x58a7ca0]]}
  layer.9.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x593a120], [7, 0x59894a0], [0, 0x6577480], [1, 0x6574be0]]}
  layer.9.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ab2f00]]}
  layer.9.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5ab0e80]]}
  layer.10.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x665f6a0], [1, 0x665ce00], [2, 0x5a7d9e0], [3, 0x5a2ee80]]}
  layer.10.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5aadda0], [5, 0x5aafe20], [6, 0x5a68480], [7, 0x5ab7800]]}
  layer.10.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6616480], [1, 0x6613be0], [2, 0x5a347c0], [3, 0x59e5c60]]}
  layer.10.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a5b8e0], [5, 0x5a5d960], [6, 0x5989d40], [7, 0x59d90c0]]}
  layer.10.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x59d6040], [7, 0x5a253c0], [0, 0x65cd260], [1, 0x65ca9c0]]}
  layer.10.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a30ea0], [3, 0x59e2340], [4, 0x5a61aa0], [5, 0x5a63b20]]}
  layer.10.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x598ce20], [7, 0x59dc1a0], [0, 0x6584040], [1, 0x65817a0]]}
  layer.10.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a2ddc0], [3, 0x59df260], [4, 0x5a5e9c0], [5, 0x5a60a40]]}
  layer.10.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x56cff20]]}
  layer.10.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x56cdea0]]}
  layer.10.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x563ba60], [5, 0x563dae0], [6, 0x55f6960], [7, 0x5645ce0], [0, 0x627b660], [1, 0x6278dc0], [2, 0x569a1c0], [3, 0x564b660], [4, 0x5684c80], [5, 0x5686d00], [6, 0x563fb80], [7, 0x568ef00], [0, 0x62c4880], [1, 0x62c1fe0], [2, 0x56e33e0], [3, 0x5694880]]}
  layer.10.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x56358c0], [5, 0x5637940], [6, 0x55f07c0], [7, 0x563fb40], [0, 0x62754c0], [1, 0x6272c20], [2, 0x5694020], [3, 0x56454c0]]}
  layer.10.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x55a3480], [5, 0x55a5500], [6, 0x555e380], [7, 0x55ad700], [0, 0x61e3080], [1, 0x61e07e0], [2, 0x5601be0], [3, 0x55b3080], [4, 0x55ec6a0], [5, 0x55ee720], [6, 0x55a75a0], [7, 0x55f6920], [0, 0x622c2a0], [1, 0x6229a00], [2, 0x564ae00], [3, 0x55fc2a0]]}
  layer.10.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61dffa0], [1, 0x61dd700], [2, 0x55feb00], [3, 0x55affa0]]}
  layer.10.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x61d13e0]]}
  layer.10.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61d3c80]]}
  layer.11.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5511040], [5, 0x55130c0], [6, 0x5514920], [7, 0x5563ca0]]}
  layer.11.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61d0ba0], [1, 0x61ce300], [2, 0x55b2800], [3, 0x5563ca0]]}
  layer.11.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x54c7e20], [5, 0x54c9ea0], [6, 0x54cb700], [7, 0x551aa80]]}
  layer.11.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61cdac0], [1, 0x61cb220], [2, 0x55af720], [3, 0x5560bc0]]}
  layer.11.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55b58e0], [3, 0x5566d80], [4, 0x555a260], [5, 0x555c2e0]]}
  layer.11.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x643d660], [1, 0x643adc0], [2, 0x5866c60], [3, 0x5818100]]}
  layer.11.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5808d20], [5, 0x580ada0], [6, 0x57c3400], [7, 0x5812780]]}
  layer.11.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x643a580], [1, 0x6437ce0], [2, 0x5863b80], [3, 0x5815020]]}
  layer.11.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5808d00]]}
  layer.11.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5857860]]}
  layer.11.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [6, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5688da0], [7, 0x56d8120], [0, 0x630daa0], [1, 0x630b200], [2, 0x572c600], [3, 0x56ddaa0], [4, 0x56da1c0], [5, 0x56dc240], [6, 0x56d1fc0], [7, 0x5721340], [0, 0x6356cc0], [1, 0x6354420], [2, 0x5775820], [3, 0x5726cc0], [4, 0x57233e0], [5, 0x5725460]]}
  layer.11.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6433380], [1, 0x6430ae0], [2, 0x58516c0], [3, 0x5802b60], [4, 0x5801b20], [5, 0x5803ba0], [6, 0x57bca20], [7, 0x580bda0]]}
  layer.11.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [3, 3], ublock: [8, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x63a0f40], [1, 0x639e6a0], [2, 0x57bf280], [3, 0x5770720], [4, 0x576f6e0], [5, 0x5771760], [6, 0x572a5e0], [7, 0x5779960], [0, 0x63ea160], [1, 0x63e78c0], [2, 0x58084a0], [3, 0x57b9940], [4, 0x57b8900], [5, 0x57ba980], [6, 0x5773800], [7, 0x57c2b80]]}
  layer.11.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x576c600], [5, 0x576e680], [6, 0x5727500], [7, 0x5776880]]}
  layer.11.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x576a560]]}
  layer.11.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x571b1e0]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5e5d920]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6b07f40]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5dd2680]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5e0dd20]]}
  dc.input_tensor.layernorm_38.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ab7b20], [2, 0x5eda780]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6c331c0]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6bb21e0]]}
  dc.input_tensor.layernorm_52.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5e25c20], [7, 0x5ec4320]]}
  input_1_multiply_69_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5ddc1c0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e253c0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d2ac80]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c87380]]}
  dc.input_tensor.layernorm_91.4:                   {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x68667a0], [1, 0x6863f00]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5ac6c00]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x673b540]]}
  dc.input_tensor.layernorm_105.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a72780], [7, 0x5ac1b00]]}
  input_1_multiply_122_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5a780a0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6738ca0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5db56c0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6992a60]]}
  dc.input_tensor.layernorm_144.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5d24b00], [0, 0x6992a60]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6867800]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5bf98a0]]}
  dc.input_tensor.layernorm_158.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bff200], [6, 0x5baa520]]}
  input_1_multiply_175_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7065da0]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7028e80]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x64848a0]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7062480]]}
  dc.input_tensor.layernorm_197.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x637cfc0], [0, 0x7024d40]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6466240]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x63df100]]}
  dc.input_tensor.layernorm_211.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x63e4240], [5, 0x63905a0]]}
  input_1_multiply_228_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x638cc80]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x63e0920]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x637e020]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x640a320]]}
  dc.input_tensor.layernorm_250.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6165a60], [5, 0x60cdd00]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fa2aa0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5eb90c0]]}
  dc.input_tensor.layernorm_264.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60f0cc0], [3, 0x606a3c0]]}
  input_1_multiply_281_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5f577c0]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x603a800]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6ef9ae0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6251d60]]}
  dc.input_tensor.layernorm_303.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61fc860], [6, 0x61b3660]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x61a2160]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6228a60]]}
  dc.input_tensor.layernorm_317.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6d84e40], [1, 0x6e05e20]]}
  input_1_multiply_334_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5428de0]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5429600]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5356180]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5393080]]}
  dc.input_tensor.layernorm_356.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5393080], [5, 0x5392860]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5519a00]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x54ca680]]}
  dc.input_tensor.layernorm_370.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x54c5d60], [5, 0x54c7de0]]}
  input_1_multiply_387_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x54c2440]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x554dea0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x550a5c0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x617e6e0]]}
  dc.input_tensor.layernorm_409.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900], [4, 0x4fb0900]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5043da0]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5043580]]}
  dc.input_tensor.layernorm_423.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5be6ae0], [1, 0x5be6ae0]]}
  input_1_multiply_440_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e93520]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e93520]]}
  dc.input_tensor.layernorm_462.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5267600], [7, 0x522a700]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x524a660]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x520cf40]]}
  dc.input_tensor.layernorm_476.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5db0cc0], [1, 0x5db0cc0]]}
  input_1_multiply_493_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x598f660]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x59402e0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5981ac0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x597fa40]]}
  dc.input_tensor.layernorm_515.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5994760], [3, 0x5945c00]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x66a6020]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x66a88c0]]}
  dc.input_tensor.layernorm_529.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a6b560], [7, 0x5aba8e0]]}
  input_1_multiply_546_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x59e5420]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a33f80]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6580f60]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6583800]]}
  dc.input_tensor.layernorm_568.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6439520], [1, 0x6436c80]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x55acec0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x555db40]]}
  dc.input_tensor.layernorm_582.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x54c6dc0], [5, 0x54c8e40]]}
  input_1_multiply_599_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x551a240]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x54caec0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5811f40]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x57c2bc0]]}
  dc.input_tensor.layernorm_621.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5807cc0], [5, 0x5809d40]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x576fee0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x57bea40]]}
  dc.input_tensor.layernorm_635.4:                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x639fee0], [1, 0x639d640]]}

  # epoch_to_epoch
  e2e__fused_op_6_0:                                {input: _fused_op_6, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900], [1, 0x4fb0900]]}
  e2e_gelu_150_0:                                   {input: gelu_150, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6471d80], [7, 0x64f8ec0], [0, 0x722f760]]}
  e2e__fused_op_10_0:                               {input: _fused_op_10, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x64776e0], [5, 0x6423a40]]}
  e2e_matmul_241_0:                                 {input: matmul_241, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6587a00]]}
  e2e__fused_op_15_0:                               {input: _fused_op_15, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x726c680], [2, 0x660e300]]}
  e2e_gelu_309_0:                                   {input: gelu_309, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x84f1da0], [7, 0x8578ee0], [0, 0x92af780]]}
  e2e__fused_op_22_0:                               {input: _fused_op_22, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70a7700], [5, 0x7053a60]]}
  e2e_matmul_400_0:                                 {input: matmul_400, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7de7a20]]}
  e2e__fused_op_27_0:                               {input: _fused_op_27, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7e9c6a0], [2, 0x723e320]]}
  e2e_gelu_468_0:                                   {input: gelu_468, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa571dc0], [7, 0xa5f8f00], [0, 0xb32f7a0]]}
  e2e__fused_op_34_0:                               {input: _fused_op_34, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7cd7720], [5, 0x7c83a80]]}
  e2e_matmul_559_0:                                 {input: matmul_559, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9647a40]]}
  e2e__fused_op_39_0:                               {input: _fused_op_39, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8acc6c0], [2, 0x7e6e340]]}
  e2e_gelu_627_0:                                   {input: gelu_627, type: queue, entries: 128, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc5f1de0], [7, 0xc678f20], [0, 0xd3af7c0]]}
  e2e__fused_op_46_0:                               {input: _fused_op_46, type: queue, entries: 128, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8907740], [5, 0x88b3aa0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_1, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 5], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_37: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [1, 2], inputs: [add_37, layernorm_38.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_2: {type: fused_op, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4, layernorm_38.dc.subtract.1, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_41: {type: matmul, grid_loc: [2, 4], grid_size: [2, 8], inputs: [_fused_op_2, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_44: {type: gelu, grid_loc: [3, 0], grid_size: [1, 3], inputs: [matmul_41],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_51: {type: add, grid_loc: [4, 4], grid_size: [1, 2], inputs: [matmul_47, _fused_op_2],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [1, 2], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_3: {type: fused_op, grid_loc: [4, 11], grid_size: [2, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4, layernorm_52.dc.subtract.1, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_55: {type: matmul, grid_loc: [5, 4], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_61: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_67: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_4: {type: fused_op, grid_loc: [5, 9], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [_fused_op_4, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_75: {type: matmul, grid_loc: [7, 4], grid_size: [1, 4], inputs: [_fused_op_3, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_4],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_5, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [7, 8], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_90: {type: add, grid_loc: [6, 10], grid_size: [1, 2], inputs: [matmul_86, _fused_op_3],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [1, 2], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_6: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4, layernorm_91.dc.subtract.1, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_1:
    target_device: 0
    input_count: 128
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e__fused_op_6_0, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_97: {type: gelu, grid_loc: [0, 8], grid_size: [1, 3], inputs: [matmul_94],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [1, 8], grid_size: [4, 4], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_104: {type: add, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_100, e2e__fused_op_6_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [1, 2], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_7: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4, layernorm_105.dc.subtract.1, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_108: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_114: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_120: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_8: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_128: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [_fused_op_7, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_9: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_9, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [5, 1], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_143: {type: add, grid_loc: [5, 5], grid_size: [1, 2], inputs: [matmul_139, _fused_op_7],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [5, 8], grid_size: [1, 2], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_10: {type: fused_op, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4, layernorm_144.dc.subtract.1, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_147: {type: matmul, grid_loc: [6, 1], grid_size: [2, 8], inputs: [_fused_op_10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_150: {type: gelu, grid_loc: [6, 9], grid_size: [1, 3], inputs: [matmul_147],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_2:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_157: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_153, e2e__fused_op_10_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_11: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4, layernorm_158.dc.subtract.1, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_161: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_167: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_173: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_181: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_11, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_13: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_12],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_13, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_196: {type: add, grid_loc: [2, 10], grid_size: [1, 2], inputs: [matmul_192, _fused_op_11],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 2], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_14: {type: fused_op, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4, layernorm_197.dc.subtract.1, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_200: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_14, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_203: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_200],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_210: {type: add, grid_loc: [4, 9], grid_size: [1, 2], inputs: [matmul_206, _fused_op_14],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [5, 0], grid_size: [1, 2], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_15: {type: fused_op, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4, layernorm_211.dc.subtract.1, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_214: {type: matmul, grid_loc: [8, 1], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_220: {type: matmul, grid_loc: [9, 1], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_226: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_16: {type: fused_op, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_234: {type: matmul, grid_loc: [9, 5], grid_size: [1, 4], inputs: [_fused_op_15, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_16],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_17, matmul_234],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}

  fwd_3:
    target_device: 0
    input_count: 128
    matmul_245: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_matmul_241_0, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_249: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_245, e2e__fused_op_15_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_18: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4, layernorm_250.dc.subtract.1, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_253: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_18, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_256: {type: gelu, grid_loc: [1, 8], grid_size: [1, 3], inputs: [matmul_253],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [2, 8], grid_size: [4, 4], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_263: {type: add, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_259, _fused_op_18],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [3, 3], grid_size: [1, 2], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_19: {type: fused_op, grid_loc: [3, 7], grid_size: [2, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4, layernorm_264.dc.subtract.1, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_267: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_273: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_279: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_287: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [_fused_op_19, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_21: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_20],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_21, matmul_287],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_302: {type: add, grid_loc: [5, 6], grid_size: [1, 2], inputs: [matmul_298, _fused_op_19],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [6, 9], grid_size: [1, 2], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_22: {type: fused_op, grid_loc: [7, 1], grid_size: [2, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4, layernorm_303.dc.subtract.1, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_306: {type: matmul, grid_loc: [7, 2], grid_size: [2, 8], inputs: [_fused_op_22, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_309: {type: gelu, grid_loc: [9, 0], grid_size: [1, 3], inputs: [matmul_306],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_4:
    target_device: 0
    input_count: 128
    matmul_312: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_309_0, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_316: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_312, e2e__fused_op_22_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_23: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4, layernorm_317.dc.subtract.1, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_320: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_326: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_332: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_24: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_340: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_24],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_25, matmul_340],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_355: {type: add, grid_loc: [2, 10], grid_size: [1, 2], inputs: [matmul_351, _fused_op_23],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 2], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_26: {type: fused_op, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4, layernorm_356.dc.subtract.1, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_359: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_26, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_362: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_359],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_369: {type: add, grid_loc: [4, 9], grid_size: [1, 2], inputs: [matmul_365, _fused_op_26],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [5, 0], grid_size: [1, 2], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_27: {type: fused_op, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4, layernorm_370.dc.subtract.1, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_373: {type: matmul, grid_loc: [8, 1], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_379: {type: matmul, grid_loc: [9, 1], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_385: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_28: {type: fused_op, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_393: {type: matmul, grid_loc: [9, 5], grid_size: [1, 4], inputs: [_fused_op_27, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_28],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_29, matmul_393],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}

  fwd_5:
    target_device: 0
    input_count: 128
    matmul_404: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_matmul_400_0, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_408: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_404, e2e__fused_op_27_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_30: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4, layernorm_409.dc.subtract.1, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_412: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_30, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_415: {type: gelu, grid_loc: [1, 8], grid_size: [1, 3], inputs: [matmul_412],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [2, 8], grid_size: [4, 4], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_422: {type: add, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_418, _fused_op_30],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [3, 3], grid_size: [1, 2], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_31: {type: fused_op, grid_loc: [3, 7], grid_size: [2, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4, layernorm_423.dc.subtract.1, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_426: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_432: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_438: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_32: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_446: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_33: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_32],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_33, matmul_446],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_457: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_461: {type: add, grid_loc: [5, 6], grid_size: [1, 2], inputs: [matmul_457, _fused_op_31],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [6, 9], grid_size: [1, 2], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_34: {type: fused_op, grid_loc: [7, 1], grid_size: [2, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4, layernorm_462.dc.subtract.1, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_465: {type: matmul, grid_loc: [7, 2], grid_size: [2, 8], inputs: [_fused_op_34, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_468: {type: gelu, grid_loc: [9, 0], grid_size: [1, 3], inputs: [matmul_465],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_6:
    target_device: 0
    input_count: 128
    matmul_471: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_468_0, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_475: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_471, e2e__fused_op_34_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_35: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4, layernorm_476.dc.subtract.1, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_479: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_485: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_491: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_499: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_35, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_37: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_36],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_37, matmul_499],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_514: {type: add, grid_loc: [2, 10], grid_size: [1, 2], inputs: [matmul_510, _fused_op_35],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [1, 2], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_38: {type: fused_op, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4, layernorm_515.dc.subtract.1, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_518: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_38, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_521: {type: gelu, grid_loc: [4, 6], grid_size: [1, 3], inputs: [matmul_518],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [5, 8], grid_size: [4, 4], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_528: {type: add, grid_loc: [4, 9], grid_size: [1, 2], inputs: [matmul_524, _fused_op_38],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [5, 0], grid_size: [1, 2], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_39: {type: fused_op, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4, layernorm_529.dc.subtract.1, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_532: {type: matmul, grid_loc: [8, 1], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_538: {type: matmul, grid_loc: [9, 1], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_544: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_40: {type: fused_op, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_552: {type: matmul, grid_loc: [9, 5], grid_size: [1, 4], inputs: [_fused_op_39, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_40],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_41, matmul_552],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}

  fwd_7:
    target_device: 0
    input_count: 128
    matmul_563: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_matmul_559_0, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_567: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_563, e2e__fused_op_39_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_42: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4, layernorm_568.dc.subtract.1, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_571: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_42, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_574: {type: gelu, grid_loc: [1, 8], grid_size: [1, 3], inputs: [matmul_571],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [2, 8], grid_size: [4, 4], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_581: {type: add, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_577, _fused_op_42],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [3, 3], grid_size: [1, 2], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_43: {type: fused_op, grid_loc: [3, 7], grid_size: [2, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4, layernorm_582.dc.subtract.1, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_585: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_591: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_597: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_44, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_605: {type: matmul, grid_loc: [5, 4], grid_size: [1, 4], inputs: [_fused_op_43, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_45: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_44],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_45, matmul_605],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_620: {type: add, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_616, _fused_op_43],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [6, 9], grid_size: [1, 2], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_46: {type: fused_op, grid_loc: [7, 1], grid_size: [2, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4, layernorm_621.dc.subtract.1, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_624: {type: matmul, grid_loc: [7, 2], grid_size: [2, 8], inputs: [_fused_op_46, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    gelu_627: {type: gelu, grid_loc: [9, 0], grid_size: [1, 3], inputs: [matmul_624],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_8:
    target_device: 0
    input_count: 128
    matmul_630: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_gelu_627_0, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 12, u_kt: 8}}
    add_634: {type: add, grid_loc: [0, 4], grid_size: [1, 2], inputs: [matmul_630, e2e__fused_op_46_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_47: {type: fused_op, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4, layernorm_635.dc.subtract.1, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 96, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 4}], input_3_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_47_output_nop_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_47], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $gptr_q1_shadow: 0, $lptr_q1: 0, $gptr_q2_shadow: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $gptr_q13: 0, $gptr_q12: 0, $lptr_q0: 0, $lptr_q3: 0, $lptr_q10: 0, $lptr_q14: 0, $gptr_q11: 0, $gptr_q1: 0, $lptr_q9: 0, $lptr_q11: 0, $lptr_q15: 0, $gptr_q6_shadow: 0, $lptr_q13: 0, $gptr_q16: 0, $lptr_q8: 0, $gptr_q15: 0, $lptr_q12: 0, $gptr_q8_shadow: 0, $gptr_q4: 0, $gptr_q12_shadow: 0, $gptr_q6: 0, $lptr_q16: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q4_shadow: 0, $gptr_q5: 0, $gptr_q10: 0, $gptr_q10_shadow: 0, $lptr_q5: 0, $gptr_q9: 0, $lptr_q7: 0, $gptr_q7: 0, $lptr_q6: 0, $lptr_q4: 0}
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q8, set, $gptr_q8_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_241_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_309_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_27_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_matmul_400_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_matmul_559_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_8, queue_settings: {
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_gelu_627_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 4}], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [2, 1], output: intermed0}
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 24}, tile_broadcast: c], pop_last: [intermed0], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input3], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input4], input_1_tms: [tile_broadcast: r], mblock: [1, 6], ublock: [2, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_635]

performance-check:
  host:
    backend-samples-per-second:
      expected: 0
      rtol: 0.05
