# git checkout eedd464b6
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole_B0-chip2-enc12-base]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                          {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ff1c7c0]]}
  attention_mask:                                   {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ffdf7e0]]}

  # output
  bert_encoder.output_layernorm_635:                {input: _fused_op_83_output_nop_0, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x20]]}

  # parameter
  layer.0.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7ff4d360], [3, 0x3fec7260]]}
  layer.0.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f86d840], [0, 0x2f97baa0]]}
  layer.0.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fbb6720], [1, 0x3fa264a0]]}
  layer.0.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fa4fee0], [2, 0x3fb6ac80]]}
  layer.0.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fb87ba0], [4, 0x7fae7420]]}
  layer.0.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fa09da0], [5, 0x7f8676a0]]}
  layer.0.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2f8e9680], [0, 0x7fb24300]]}
  layer.0.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fa20300], [1, 0x7fa49d40]]}
  layer.0.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fadb100]]}
  layer.0.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f9fda80]]}
  layer.0.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f7d5280], [0, 0x2f857260], [0, 0x7fa91ee0], [1, 0x3f98dee0], [1, 0x7f9b7920], [2, 0x3fad8020], [2, 0x7faa8c80], [3, 0x3fa2b5a0]]}
  layer.0.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fb21220], [4, 0x3fb7f960], [4, 0x7fad4f60], [5, 0x3f9f78e0], [5, 0x7f7cf0e0], [0, 0x2f8510c0], [0, 0x7fa8bd40], [1, 0x3f987d40]]}
  layer.0.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f8f4900], [2, 0x3fa15000], [2, 0x7f9e5c60], [3, 0x3f968580], [3, 0x7fa5e200], [4, 0x3fabc940]]}
  layer.0.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fad2ec0], [5, 0x3f9f5840], [5, 0x7f7cd040], [0, 0x2f84f020], [0, 0x7fa89ca0], [1, 0x3f985ca0]]}
  layer.0.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fce5240]]}
  layer.0.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fd91cc0]]}
  layer.1.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fc283c0], [5, 0x7f9cefc0]]}
  layer.1.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fadd220], [0, 0x7fd13da0]]}
  layer.1.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fb83b20], [1, 0x7fa6a5e0]]}
  layer.1.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fb85380], [2, 0x7fbd6780]]}
  layer.1.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fcff8a0], [5, 0x3fb95fa0]]}
  layer.1.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f9c8e20], [0, 0x2fad7080]]}
  layer.1.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f9edf00], [1, 0x3f988e20]]}
  layer.1.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa16900], [2, 0x3fad47c0]]}
  layer.1.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fb7f1c0]]}
  layer.1.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fb84300]]}
  layer.1.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fada960], [2, 0x7fa7fb00], [3, 0x3fbd0e00], [3, 0x7fb65380], [4, 0x3fc77700], [4, 0x7fc85a80], [5, 0x3fb8b4e0], [5, 0x7fb90620]]}
  layer.1.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f9e7d60], [1, 0x3f982c80], [1, 0x7fa10760], [2, 0x3face620], [2, 0x7fa79120], [3, 0x3fb9a040], [3, 0x7fb5e9a0], [4, 0x3fc6f4c0]]}
  layer.1.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fbc09c0], [5, 0x3fabc1a0], [5, 0x7fac12e0], [0, 0x2f8fec00], [0, 0x7f924d40], [1, 0x3f8bfc60]]}
  layer.1.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa0e6c0], [2, 0x3facc580], [2, 0x7fa77080], [3, 0x3fb97fa0], [3, 0x7fb5c900], [4, 0x3fc6d420]]}
  layer.1.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f8b3940]]}
  layer.1.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fd0bbc0]]}
  layer.2.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fb73f80], [2, 0x3fc35f40]]}
  layer.2.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fbdb0e0], [3, 0x3fcf76e0]]}
  layer.2.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fc8bc60], [4, 0x3fd17ee0]]}
  layer.2.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fd1a780], [5, 0x3fc505c0]]}
  layer.2.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fadeaa0], [1, 0x7fae1b60]]}
  layer.2.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fc2fda0], [2, 0x7fbd4f40]]}
  layer.2.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fc652c0], [3, 0x7fbf9840]]}
  layer.2.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fb8c580], [1, 0x3fb70ec0]]}
  layer.2.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fa5e2c0]]}
  layer.2.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fb79060]]}
  layer.2.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fb44360], [3, 0x3fac6c80], [3, 0x7fb49cc0], [4, 0x3fc22200], [4, 0x7fc6d480], [5, 0x3fb03b80], [5, 0x7f936a00], [0, 0x2fa44c60]]}
  layer.2.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fd0bb60], [1, 0x3fb7b8e0], [1, 0x7fa58120], [2, 0x3fb72ec0], [2, 0x7fb3e1c0], [3, 0x3fac0ae0], [3, 0x7fb43b20], [4, 0x3fc1c060]]}
  layer.2.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fbaa460], [5, 0x3fa40b60], [5, 0x7f8739e0], [0, 0x2f981c40], [0, 0x7fc48b40], [1, 0x3fab88c0]]}
  layer.2.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fa56080], [2, 0x3fb70e20], [2, 0x7fb3c120], [3, 0x3fabea40], [3, 0x7fb41a80], [4, 0x3fc19fc0]]}
  layer.2.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7f92eec0]]}
  layer.2.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f8bba60]]}
  layer.3.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f8fca60], [4, 0x3f979920]]}
  layer.3.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f90ff20], [5, 0x3f8beb20]]}
  layer.3.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f696320], [0, 0x2f71dc60]]}
  layer.3.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7f9efe20], [1, 0x3f8ea5c0]]}
  layer.3.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f829640], [3, 0x7f86a640]]}
  layer.3.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3f973780], [4, 0x7f909d80]]}
  layer.3.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f82c700], [5, 0x7f603f00]]}
  layer.3.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2f717ac0], [0, 0x7f9e9c80]]}
  layer.3.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fa9e160]]}
  layer.3.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fab0620]]}
  layer.3.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fa40aa0], [5, 0x3f963420], [5, 0x7f73ac20], [0, 0x2f7bcc00], [0, 0x7f9f7880], [1, 0x3f8f3880], [1, 0x7f861ca0], [2, 0x3f97ca40]]}
  layer.3.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7f9d9920], [3, 0x3f960340], [3, 0x7fa27440], [4, 0x3faaa480], [4, 0x7fa3a900], [5, 0x3f95d280], [5, 0x7f734a80], [0, 0x2f7b6a60]]}
  layer.3.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f9c4500], [0, 0x7fab0f40], [1, 0x3fa1ba80], [1, 0x7fa1eb40], [2, 0x3fb6cd80], [2, 0x7fb11f20]]}
  layer.3.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fc63220], [3, 0x7fbf77a0], [4, 0x3fd09b20], [4, 0x7fd17ea0], [5, 0x3fc1d900], [5, 0x7fc22a40]]}
  layer.3.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fb8fce0]]}
  layer.3.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fa86ba0]]}
  layer.4.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f9568c0], [5, 0x7f98bde0]]}
  layer.4.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f849ea0], [0, 0x7f7b49c0]]}
  layer.4.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f602f00], [1, 0x7f787ec0]]}
  layer.4.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8ba220], [2, 0x7f7eaf20]]}
  layer.4.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f9f4780], [5, 0x3f8c44a0]]}
  layer.4.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f911fe0], [3, 0x7fab5720]]}
  layer.4.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7b7a80], [0, 0x7f7225a0]]}
  layer.4.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f5fcd60], [1, 0x7f781d20]]}
  layer.4.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f9e8460]]}
  layer.4.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f8b8180]]}
  layer.4.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fa26440], [0, 0x2f862500], [0, 0x7f7cd020], [1, 0x3f72d8e0], [1, 0x7f8b28a0], [2, 0x3f958980], [2, 0x7f889680], [3, 0x3f9aa5a0]]}
  layer.4.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa70ee0], [3, 0x3fb91e00], [3, 0x7fb56760], [4, 0x3fc67280], [4, 0x7fbb9fe0], [5, 0x3fa853e0], [5, 0x7faba900], [0, 0x2f8f69c0]]}
  layer.4.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f85fc80], [1, 0x3f7f0920], [1, 0x7f945500], [2, 0x3fa033c0], [2, 0x7f9adec0], [3, 0x3facede0]]}
  layer.4.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fb546c0], [4, 0x3fc651e0], [4, 0x7fbb7f40], [5, 0x3fa83340], [5, 0x7fab8860], [0, 0x2f8f4920]]}
  layer.4.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f970720]]}
  layer.4.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7f9cd600]]}
  layer.5.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f8cdf20], [3, 0x7f995020]]}
  layer.5.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fa0ee60], [2, 0x7f9dfac0]]}
  layer.5.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f9a84e0], [5, 0x3f8cae60]]}
  layer.5.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f72e8e0], [0, 0x2f7b08c0]]}
  layer.5.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f8de300], [2, 0x7f93b1e0]]}
  layer.5.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f8c7d80], [3, 0x7f98ee80]]}
  layer.5.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fa0bd40], [4, 0x7f9160c0]]}
  layer.5.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f8c4cc0], [5, 0x7f728740]]}
  layer.5.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ff534e0]]}
  layer.5.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fec7260]]}
  layer.5.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7fd85b40], [0, 0x2fd801c0], [0, 0x7fe0dca0], [1, 0x3fe15680], [1, 0x7fea00a0], [2, 0x3fe95e20], [2, 0x7febaf40], [3, 0x3fe34e40]]}
  layer.5.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fecb340], [4, 0x3ff575c0], [4, 0x7ff4d340], [5, 0x3fec10c0], [5, 0x7fd7f9a0], [0, 0x2fd7a020], [0, 0x7fe07b00], [1, 0x3fe0f4e0]]}
  layer.5.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fddd080], [2, 0x3fdd2e00], [2, 0x7fdf7f20], [3, 0x3fd71e20], [3, 0x7fe08320], [4, 0x3fe945a0]]}
  layer.5.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ff4b2a0], [5, 0x3febf020], [5, 0x7fd7d900], [0, 0x2fd77f80], [0, 0x7fe05a60], [1, 0x3fe0d440]]}
  layer.5.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ff3ef80]]}
  layer.5.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3feb2d00]]}
  layer.6.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7fceb4e0], [0, 0x2fce5b60]]}
  layer.6.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fdff8c0], [1, 0x3fe072a0]]}
  layer.6.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fd4a420], [2, 0x3fd0fdc0]]}
  layer.6.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fd5f960], [3, 0x3fcd9860]]}
  layer.6.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f91baa0], [3, 0x3fa3c9c0]]}
  layer.6.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fb4e520], [4, 0x3fc5f040]]}
  layer.6.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fb25b20], [5, 0x3f9f0f20]]}
  layer.6.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa08520], [2, 0x3fac63e0]]}
  layer.6.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f8561e0]]}
  layer.6.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7c0d00]]}
  layer.6.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f69b4c0], [1, 0x7f820480], [2, 0x3f8c6560], [2, 0x7f7f7260], [3, 0x3f918180], [3, 0x7fabb8c0], [4, 0x3fb9c000], [4, 0x7fa92ec0]]}
  layer.6.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f9e8ce0], [5, 0x7fa1e200], [0, 0x2f850040], [0, 0x7f7bab60], [1, 0x3f695320], [1, 0x7f81a2e0], [2, 0x3f8c03c0], [2, 0x7f7f10c0]]}
  layer.6.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fc736a0], [3, 0x3fd03a20], [3, 0x7fd24220], [4, 0x3fdaab40], [4, 0x7fd21160], [5, 0x3fc6ed80]]}
  layer.6.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7ffd7560], [1, 0x3fea2140], [1, 0x7fe71d60], [2, 0x3ff2e3c0], [2, 0x7ff2cb60], [3, 0x3fea08e0]]}
  layer.6.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2ff52ca0]]}
  layer.6.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7ffcb240]]}
  layer.7.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fe0fd20], [1, 0x7fddf940]]}
  layer.7.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ff28220], [2, 0x7ff269c0]]}
  layer.7.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ff65160], [0, 0x2ff5efc0]]}
  layer.7.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fe05aa0], [4, 0x3ff228c0]]}
  layer.7.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fec0880], [0, 0x7ff38e20]]}
  layer.7.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fe09b80], [1, 0x7fdd97a0]]}
  layer.7.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fe95e00], [2, 0x7fe945a0]]}
  layer.7.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fe69b20], [3, 0x7fdff900]]}
  layer.7.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fff3ce0]]}
  layer.7.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fff3ce0]]}
  layer.7.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ff6dbe0], [4, 0x3ff6dbe0], [4, 0x7ff6dbe0], [5, 0x3ff6dbe0], [5, 0x7ff6dbe0], [0, 0x2ff0c3c0], [0, 0x7ff69ac0], [1, 0x3ff6bb40]]}
  layer.7.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fff7dc0], [2, 0x3fff7dc0], [2, 0x7ffedb40], [3, 0x3ffedb40], [3, 0x7ff67a40], [4, 0x3ff67a40], [4, 0x7ff67a40], [5, 0x3ff67a40]]}
  layer.7.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7feaabc0], [0, 0x2fe493a0], [0, 0x7fea6aa0], [1, 0x3fea8b20], [1, 0x7ff34da0], [2, 0x3ff34da0]]}
  layer.7.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7ffebaa0], [3, 0x3ffebaa0], [3, 0x7ff659a0], [4, 0x3ff659a0], [4, 0x7ff659a0], [5, 0x3ff659a0]]}
  layer.7.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3ff28a80]]}
  layer.7.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7ffdf780]]}
  layer.8.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3ff59680], [3, 0x7fed3580]]}
  layer.8.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3ff5f800], [4, 0x7ff5f800]]}
  layer.8.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fed3580], [5, 0x7fe17f60]]}
  layer.8.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fe125e0], [0, 0x7fea00c0]]}
  layer.8.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fd65b00], [3, 0x3fcdfa00]]}
  layer.8.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fb76040], [3, 0x7fca2220]]}
  layer.8.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fde51a0], [4, 0x7fe61840]]}
  layer.8.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fd7f8a0], [5, 0x7fb264a0]]}
  layer.8.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fbdf200]]}
  layer.8.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fba7c80]]}
  layer.8.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fe904a0], [4, 0x7fecc360], [5, 0x3fed0460], [5, 0x7feb8680], [0, 0x2fe2e460], [0, 0x7fea6a00], [1, 0x3fd77760], [1, 0x7fd47380]]}
  layer.8.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fff7dc0], [0, 0x2fff1c20], [0, 0x7fff1c20], [1, 0x3ff36600], [1, 0x7ff06220], [2, 0x3ff36600], [2, 0x7ff34da0], [3, 0x3ff34da0]]}
  layer.8.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ff3cfe0], [1, 0x7ff3cfe0], [2, 0x3ff3cfe0], [2, 0x7ff3cfe0], [3, 0x3ff3cfe0], [3, 0x7ff3cfe0]]}
  layer.8.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fffdf60], [4, 0x7fffdf60], [5, 0x3fffdf60], [5, 0x7fffdf60], [0, 0x2fff7dc0], [0, 0x7fff7dc0]]}
  layer.8.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ff30cc0]]}
  layer.8.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fff1c40]]}
  layer.9.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ff6bb40], [5, 0x3ff6bb40]]}
  layer.9.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fff9e60], [0, 0x7fff9e60]]}
  layer.9.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fe9e8a0], [4, 0x3ff5f820]]}
  layer.9.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ff659a0], [5, 0x3ff659a0]]}
  layer.9.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fea41e0], [1, 0x7fe73e00]]}
  layer.9.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ff30460], [2, 0x7ff2ec00]]}
  layer.9.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fea2980], [3, 0x7fe0c480]]}
  layer.9.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3ff59680], [4, 0x7ff5f800]]}
  layer.9.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fe73d00]]}
  layer.9.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fdea320]]}
  layer.9.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fd37f40], [5, 0x7fcca440], [0, 0x2fbb2740], [0, 0x7fc2ace0], [1, 0x3fb7da40], [1, 0x7fc0e5e0], [2, 0x3fd5c820], [2, 0x7fd366c0]]}
  layer.9.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fdc6a40], [3, 0x7fde7240], [4, 0x3fe6db60], [4, 0x7fde4180], [5, 0x3fd31da0], [5, 0x7fcc42a0], [0, 0x2fbac5a0], [0, 0x7fc24b40]]}
  layer.9.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fbdf200], [4, 0x3fd22180], [4, 0x7fd9e820], [5, 0x3fcbc880], [5, 0x7fa63480], [0, 0x2faef6e0]]}
  layer.9.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fd26260], [1, 0x3fca8360], [1, 0x7fb8ee20], [2, 0x3fba3b40], [2, 0x7fbdd160], [3, 0x3fb5b980]]}
  layer.9.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fae33c0]]}
  layer.9.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fd19f40]]}
  layer.10.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fc15f40], [1, 0x7fafca00]]}
  layer.10.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fd093e0], [2, 0x7fd411a0]]}
  layer.10.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7fc590c0], [0, 0x2fc53740]]}
  layer.10.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fdf9720], [1, 0x3fe01100]]}
  layer.10.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fc47440], [3, 0x7fd73620]]}
  layer.10.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fe8bb20], [4, 0x7ff081c0]]}
  layer.10.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fe200a0], [5, 0x7fbc6ca0]]}
  layer.10.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fc4d5a0], [0, 0x7fdf3580]]}
  layer.10.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fe7f800]]}
  layer.10.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fefbea0]]}
  layer.10.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fd8dc80], [5, 0x7fb34880], [0, 0x2fbbb180], [0, 0x7fd61160], [1, 0x3fd6e4a0], [1, 0x7fc86ba0], [2, 0x3fc76fc0], [2, 0x7fcaed80]]}
  layer.10.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fc3f200], [3, 0x7fd6b3e0], [4, 0x3fe79660], [4, 0x7fef5d00], [5, 0x3fd87ae0], [5, 0x7fb2e6e0], [0, 0x2fbb4fe0], [0, 0x7fd5afc0]]}
  layer.10.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fcab480], [1, 0x7fbc3b80], [2, 0x3fbb3fa0], [2, 0x7fbebd60], [3, 0x3fb7c1e0], [3, 0x7fca83c0]]}
  layer.10.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fe775c0], [4, 0x7fef3c60], [5, 0x3fd85a40], [5, 0x7fb2c640], [0, 0x2fbb2f40], [0, 0x7fd58f20]]}
  layer.10.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fdeec40]]}
  layer.10.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fcb7f80]]}
  layer.11.attention.self.query.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fb1a180], [0, 0x7fb92720]]}
  layer.11.attention.self.query.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fb77060], [1, 0x7fc063a0]]}
  layer.11.attention.self.key.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fcc8360], [2, 0x7fbe1280]]}
  layer.11.attention.self.key.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fcfd880], [3, 0x7fd1e080]]}
  layer.11.attention.self.value.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fc25b60], [0, 0x2fa87d60]]}
  layer.11.attention.self.value.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fdf1520], [4, 0x3fe820c0]]}
  layer.11.attention.output.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fd9c040], [0, 0x7fe145e0]]}
  layer.11.attention.output.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fd715c0], [1, 0x7fd411e0]]}
  layer.11.attention.output.LayerNorm.weight:       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7feb9ea0]]}
  layer.11.attention.output.LayerNorm.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3febdfa0]]}
  layer.11.intermediate.dense.weight:               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [12, 6], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fe200c0], [0, 0x2fd09c20], [0, 0x7fd821c0], [1, 0x3fcdf1a0], [1, 0x7fcaedc0], [2, 0x3fdfd000], [2, 0x7fdcb3c0], [3, 0x3fdd0d20]]}
  layer.11.intermediate.dense.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 6], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fe8fc60], [2, 0x7fe8e400], [3, 0x3fe63980], [3, 0x7fdf9760], [4, 0x3fe8a300], [4, 0x7fec61c0], [5, 0x3feca2c0], [5, 0x7feb24e0]]}
  layer.11.output.dense.weight:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 1], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fdf6e80], [5, 0x3fdfaf80], [5, 0x7fd5d0a0], [0, 0x2fc46c00], [0, 0x7fcbf1a0], [1, 0x3fc1c180]]}
  layer.11.output.dense.bias:                       {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fcacd20], [2, 0x3fdfaf60], [2, 0x7fdc9320], [3, 0x3fdcec80], [3, 0x7fdef480], [4, 0x3fe80020]]}
  layer.11.output.LayerNorm.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fc0fe60]]}
  layer.11.output.LayerNorm.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fca0a00]]}

  # constant
  input_1_multiply_16:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fb3b8e0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fabe200]]}
  dc.input_tensor.softmax_18.4:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fb29460]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fb6a440]]}
  dc.input_tensor.layernorm_38.1:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fa0ff40]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fabd9c0]]}
  dc.input_tensor.layernorm_38.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fb273c0]]}
  dc.input_tensor.layernorm_38.8:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fb85b00]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f8f40c0]]}
  dc.input_tensor.layernorm_52.1:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fcb4620]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fbdc920]]}
  dc.input_tensor.layernorm_52.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fb598e0]]}
  dc.input_tensor.layernorm_52.8:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fbdc920]]}
  input_1_multiply_69:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fb590a0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fbdc0e0]]}
  dc.input_tensor.softmax_71.4:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fb8b520]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa7f2c0]]}
  dc.input_tensor.layernorm_91.1:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fba01e0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fb64b40]]}
  dc.input_tensor.layernorm_91.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fc75660]]}
  dc.input_tensor.layernorm_91.8:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fc839e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fbc0180]]}
  dc.input_tensor.layernorm_105.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fa8b580]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fac0aa0]]}
  dc.input_tensor.layernorm_105.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f8fcb60]]}
  dc.input_tensor.layernorm_105.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f922ca0]]}
  input_1_multiply_122:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fc25320]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fa87520]]}
  dc.input_tensor.softmax_124.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fb73f60]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fd19f40]]}
  dc.input_tensor.layernorm_144.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fc1f9a0]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fc24ae0]]}
  dc.input_tensor.layernorm_144.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fd11d00]]}
  dc.input_tensor.layernorm_144.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fb81a80]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fb3b0a0]]}
  dc.input_tensor.layernorm_158.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fa2d5e0]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f8f0760]]}
  dc.input_tensor.layernorm_158.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f845540]]}
  dc.input_tensor.layernorm_158.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f8dc260]]}
  input_1_multiply_175:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f844d00]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f8dba20]]}
  dc.input_tensor.softmax_177.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7f9168a0]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f8e9d80]]}
  dc.input_tensor.layernorm_197.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f8140e0]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7f9f5fc0]]}
  dc.input_tensor.layernorm_197.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f8d9980]]}
  dc.input_tensor.layernorm_197.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f9664e0]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f9c3cc0]]}
  dc.input_tensor.layernorm_211.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fa80320]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fa1b240]]}
  dc.input_tensor.layernorm_211.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa1caa0]]}
  dc.input_tensor.layernorm_211.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f9c1c20]]}
  input_1_multiply_228:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f9117a0]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fab4ee0]]}
  dc.input_tensor.softmax_230.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fb776c0]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8b99e0]]}
  dc.input_tensor.layernorm_250.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7ba300]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f910f60]]}
  dc.input_tensor.layernorm_250.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fab2e40]]}
  dc.input_tensor.layernorm_250.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fb75620]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f85f440]]}
  dc.input_tensor.layernorm_264.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7bfd00]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7f9f7040]]}
  dc.input_tensor.layernorm_264.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f8f17e0]]}
  dc.input_tensor.layernorm_264.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f85fc00]]}
  input_1_multiply_281:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7f9f6800]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f8f0fa0]]}
  dc.input_tensor.softmax_283.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7f8475e0]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2f7b0080]]}
  dc.input_tensor.layernorm_303.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fb79840]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fbeb520]]}
  dc.input_tensor.layernorm_303.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fed14e0]]}
  dc.input_tensor.layernorm_303.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3ff5d760]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fddc840]]}
  dc.input_tensor.layernorm_317.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fda21e0]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3ff28240]]}
  dc.input_tensor.layernorm_317.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fe06280]]}
  dc.input_tensor.layernorm_317.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fe92500]]}
  input_1_multiply_334:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fe05a40]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f944cc0]]}
  dc.input_tensor.softmax_336.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f9eada0]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fb4dce0]]}
  dc.input_tensor.layernorm_356.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fc2e420]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fb252e0]]}
  dc.input_tensor.layernorm_356.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f9eee80]]}
  dc.input_tensor.layernorm_356.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fa243a0]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fe0bc40]]}
  dc.input_tensor.layernorm_370.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3ff28a60]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ff5efc0]]}
  dc.input_tensor.layernorm_370.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ff630c0]]}
  dc.input_tensor.layernorm_370.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ff630c0]]}
  input_1_multiply_387:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ff5e780]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ff62880]]}
  dc.input_tensor.softmax_389.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7ff4aaa0]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fe91cc0]]}
  dc.input_tensor.layernorm_409.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ff0e360]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fea82e0]]}
  dc.input_tensor.layernorm_409.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fffdf60]]}
  dc.input_tensor.layernorm_409.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fffdf60]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7feaa380]]}
  dc.input_tensor.layernorm_423.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fe18780]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fea6260]]}
  dc.input_tensor.layernorm_423.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fffdf60]]}
  dc.input_tensor.layernorm_423.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7ff32d00]]}
  input_1_multiply_440:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fea7aa0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7ff324c0]]}
  dc.input_tensor.softmax_442.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fb5da20]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2fbb2700]]}
  dc.input_tensor.layernorm_462.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fd28300]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fcaa400]]}
  dc.input_tensor.layernorm_462.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fb90ec0]]}
  dc.input_tensor.layernorm_462.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fba5be0]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ff3c7a0]]}
  dc.input_tensor.layernorm_476.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7ff0c3c0]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ff3c7a0]]}
  dc.input_tensor.layernorm_476.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7ff3af40]]}
  dc.input_tensor.layernorm_476.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3ff3af40]]}
  input_1_multiply_493:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fff7580]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fff13e0]]}
  dc.input_tensor.softmax_495.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7ffd9600]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ff65160]]}
  dc.input_tensor.layernorm_515.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fe6fcc0]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fdc8ae0]]}
  dc.input_tensor.layernorm_515.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fdccbe0]]}
  dc.input_tensor.layernorm_515.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fded3e0]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fbde9c0]]}
  dc.input_tensor.layernorm_529.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3fcf1560]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7fd9dfe0]]}
  dc.input_tensor.layernorm_529.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fcba7e0]]}
  dc.input_tensor.layernorm_529.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7fa613e0]]}
  input_1_multiply_546:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fd49be0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fd0f580]]}
  dc.input_tensor.softmax_548.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7fd47340]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fe008c0]]}
  dc.input_tensor.layernorm_568.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fd18fc0]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3feb24c0]]}
  dc.input_tensor.layernorm_568.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3fc453a0]]}
  dc.input_tensor.layernorm_568.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7fd71580]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3fcaac40]]}
  dc.input_tensor.layernorm_582.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x7fb92f60]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fb7d200]]}
  dc.input_tensor.layernorm_582.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fc0c540]]}
  dc.input_tensor.layernorm_582.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fd5a780]]}
  input_1_multiply_599:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fdaa300]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fd20920]]}
  dc.input_tensor.softmax_601.4:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fc56760]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fe8f420]]}
  dc.input_tensor.layernorm_621.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fe5d7e0]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fe63140]]}
  dc.input_tensor.layernorm_621.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fdf76c0]]}
  dc.input_tensor.layernorm_621.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fe88260]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fdf6640]]}
  dc.input_tensor.layernorm_635.1:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fdca360]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.5.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fd5c860]]}
  dc.input_tensor.layernorm_635.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fc44b60]]}
  dc.input_tensor.layernorm_635.8:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fcbd100]]}

  # epoch_to_epoch
  e2e_layernorm_144.dc.reduce_sum.5.lc1_0:          {input: layernorm_144.dc.reduce_sum.5.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7fffbee0]]}
  e2e__fused_op_17_0:                               {input: _fused_op_17, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x2ff9e7e0]]}
  e2e__fused_op_33_0:                               {input: _fused_op_33, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x7f8b5080]]}
  e2e_matmul_404_0:                                 {input: matmul_404, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x3f7f8a20], [3, 0x7f839a20]]}
  e2e__fused_op_48_0:                               {input: _fused_op_48, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x3f911f60]]}
  e2e_gelu_521_0:                                   {input: gelu_521, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [4, 6], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f846d60], [5, 0x3f7696e0]]}
  e2e__fused_op_67_0:                               {input: _fused_op_67, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7f5a26e0]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 1
    input_count: 2
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 2], inputs: [_fused_op_0, softmax_18.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_22: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_2: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4, _fused_op_1],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_2, matmul_22],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_33: {type: matmul, grid_loc: [1, 6], grid_size: [1, 2], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2132_3334: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_37],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_3: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, buffer_0_2132_3334],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_3, _fused_op_3],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3334_3335: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_3],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3334_3335: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_1_3334_3335],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_4: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8, buffer_0_3334_3335, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_41: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_4, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_44: {type: gelu, grid_loc: [3, 1], grid_size: [1, 2], inputs: [matmul_41],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [5, 0], grid_size: [1, 6], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3335_2164: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_4],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_47, buffer_0_3335_2164],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2164_3336: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_51],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_5: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, buffer_0_2164_3336],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_5, _fused_op_5],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3336_3337: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_5],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3336_3337: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [buffer_1_3336_3337],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_6: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8, buffer_0_3336_3337, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_55: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [_fused_op_6, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_61: {type: matmul, grid_loc: [6, 5], grid_size: [1, 2], inputs: [_fused_op_6, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_67: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [7, 1], grid_size: [1, 1], inputs: [_fused_op_7],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [7, 2], grid_size: [1, 2], inputs: [_fused_op_7, softmax_71.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_75: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [_fused_op_6, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_9: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4, _fused_op_8],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}

  fwd_0_1_temporal_epoch_0:
    target_device: 0
    input_count: 2
    matmul_82: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [_fused_op_9, matmul_75],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_86: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3337_2236: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3337_2236: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [buffer_1_3337_2236],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_86, buffer_0_3337_2236],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2236_3341: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_90],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_10: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, buffer_0_2236_3341],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [_fused_op_10, _fused_op_10],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3341_3342: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_10],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3341_3342: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [buffer_1_3341_3342],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_11: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8, buffer_0_3341_3342, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_94: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [_fused_op_11, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_97: {type: gelu, grid_loc: [1, 6], grid_size: [1, 2], inputs: [matmul_94],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [3, 0], grid_size: [1, 6], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3342_2268: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [_fused_op_11],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_104: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_100, buffer_0_3342_2268],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2268_3343: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_104],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_12: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, buffer_0_2268_3343],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_12, _fused_op_12],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3343_3344: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [_fused_op_12],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3343_3344: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [buffer_1_3343_3344],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_13: {type: fused_op, grid_loc: [4, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8, buffer_0_3343_3344, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_108: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [_fused_op_13, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_114: {type: matmul, grid_loc: [5, 2], grid_size: [1, 2], inputs: [_fused_op_13, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_120: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_14: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_14],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_15: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 2], inputs: [_fused_op_14, softmax_124.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_15, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_128: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [_fused_op_13, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_16: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4, _fused_op_15],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_135: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_16, matmul_128],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_139: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3344_2340: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [_fused_op_13],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3344_2340: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [buffer_1_3344_2340],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [matmul_139, buffer_0_3344_2340],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2340_3348: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_143],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_17: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1, buffer_0_2340_3348],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [_fused_op_17, _fused_op_17],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}

  fwd_0_2_temporal_epoch_1:
    target_device: 1
    input_count: 2
    _fused_op_18: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, e2e_layernorm_144.dc.reduce_sum.5.lc1_0, dc.input_tensor.layernorm_144.8, e2e__fused_op_17_0, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0, 24, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_147: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [_fused_op_18, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_150: {type: gelu, grid_loc: [0, 1], grid_size: [1, 2], inputs: [matmul_147],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [2, 0], grid_size: [1, 6], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3349_2372: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_18],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [matmul_153, buffer_0_3349_2372],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2372_3350: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_157],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_19: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, buffer_0_2372_3350],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_19, _fused_op_19],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3350_3351: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_19],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3350_3351: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [buffer_1_3350_3351],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_20: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8, buffer_0_3350_3351, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_161: {type: matmul, grid_loc: [3, 3], grid_size: [1, 2], inputs: [_fused_op_20, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_167: {type: matmul, grid_loc: [3, 5], grid_size: [1, 2], inputs: [_fused_op_20, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_173: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_177.dc.reduce_max.0: {type: reduce, grid_loc: [4, 1], grid_size: [1, 1], inputs: [_fused_op_21],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_22: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 2], inputs: [_fused_op_21, softmax_177.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_177.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_22, lc.input_tensor.softmax_177.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_181: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [_fused_op_20, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_23: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_177.4, _fused_op_22],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_188: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_23, matmul_181],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_192: {type: matmul, grid_loc: [5, 1], grid_size: [1, 2], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3351_2444: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_20],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3351_2444: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_1_3351_2444],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_196: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_192, buffer_0_3351_2444],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2444_3355: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_196],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_24: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.1, layernorm_197.dc.reduce_sum.0.lc1, buffer_0_2444_3355],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_24, _fused_op_24],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.4, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3355_3356: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [_fused_op_24],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3355_3356: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [buffer_1_3355_3356],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_25: {type: fused_op, grid_loc: [6, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_197.8, buffer_0_3355_3356, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_200: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [_fused_op_25, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_203: {type: gelu, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_200],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_0_3_temporal_epoch_1:
    target_device: 0
    input_count: 2
    matmul_206: {type: matmul, grid_loc: [0, 0], grid_size: [1, 6], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3356_2476: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_25],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_206, buffer_0_3356_2476],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2476_3357: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_210],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_26: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1, buffer_0_2476_3357],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_26, _fused_op_26],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3357_3358: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_26],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3357_3358: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_1_3357_3358],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_27: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_211.8, buffer_0_3357_3358, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_214: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [_fused_op_27, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_220: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [_fused_op_27, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_226: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_28: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_226, input_1_multiply_228, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_230.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_28],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 2], inputs: [_fused_op_28, softmax_230.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_230.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_29, lc.input_tensor.softmax_230.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_234: {type: matmul, grid_loc: [3, 3], grid_size: [1, 2], inputs: [_fused_op_27, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_30: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_230.4, _fused_op_29],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_241: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [_fused_op_30, matmul_234],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_245: {type: matmul, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3358_2548: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [_fused_op_27],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3358_2548: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [buffer_1_3358_2548],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_249: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [matmul_245, buffer_0_3358_2548],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2548_3362: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [add_249],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_31: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.1, layernorm_250.dc.reduce_sum.0.lc1, buffer_0_2548_3362],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_250.dc.multiply.4: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_31, _fused_op_31],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.4, lc.input_tensor.layernorm_250.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3362_3363: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_31],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3362_3363: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_1_3362_3363],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_32: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.6, layernorm_250.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_250.8, buffer_0_3362_3363, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_253: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [_fused_op_32, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_256: {type: gelu, grid_loc: [5, 3], grid_size: [1, 2], inputs: [matmul_253],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [7, 0], grid_size: [1, 6], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3363_2580: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_32],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_263: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_259, buffer_0_3363_2580],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2580_3364: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_263],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_33: {type: fused_op, grid_loc: [7, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.1, layernorm_264.dc.reduce_sum.0.lc1, buffer_0_2580_3364],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}

  fwd_0_4_temporal_epoch_2:
    target_device: 1
    input_count: 2
    layernorm_264.dc.multiply.4: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_33_0, e2e__fused_op_33_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.4, lc.input_tensor.layernorm_264.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_34: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.6, layernorm_264.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_264.8, e2e__fused_op_33_0, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_267: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [_fused_op_34, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_273: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [_fused_op_34, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_279: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_35: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_279, input_1_multiply_281, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_283.dc.reduce_max.0: {type: reduce, grid_loc: [1, 1], grid_size: [1, 1], inputs: [_fused_op_35],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_36: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_35, softmax_283.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_283.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_283.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_287: {type: matmul, grid_loc: [1, 6], grid_size: [1, 2], inputs: [_fused_op_34, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_37: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_283.4, _fused_op_36],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_294: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_37, matmul_287],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_298: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3365_2652: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_34],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3365_2652: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1_3365_2652],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_302: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_298, buffer_0_3365_2652],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2652_3369: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [add_302],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_38: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.1, layernorm_303.dc.reduce_sum.0.lc1, buffer_0_2652_3369],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_303.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_38, _fused_op_38],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.4, lc.input_tensor.layernorm_303.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3369_3370: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_38],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3369_3370: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [buffer_1_3369_3370],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_39: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.6, layernorm_303.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_303.8, buffer_0_3369_3370, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_306: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_39, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_309: {type: gelu, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_306],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [5, 0], grid_size: [1, 6], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3370_2684: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_39],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_316: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_312, buffer_0_3370_2684],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2684_3371: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_316],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_40: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.1, layernorm_317.dc.reduce_sum.0.lc1, buffer_0_2684_3371],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_317.dc.multiply.4: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [_fused_op_40, _fused_op_40],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.4, lc.input_tensor.layernorm_317.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3371_3372: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_40],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3371_3372: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [buffer_1_3371_3372],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_41: {type: fused_op, grid_loc: [6, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.6, layernorm_317.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_317.8, buffer_0_3371_3372, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_320: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [_fused_op_41, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_326: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [_fused_op_41, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_332: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_42: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_332, input_1_multiply_334, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_336.dc.reduce_max.0: {type: reduce, grid_loc: [7, 6], grid_size: [1, 1], inputs: [_fused_op_42],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}

  fwd_0_5_temporal_epoch_2:
    target_device: 0
    input_count: 2
    _fused_op_43: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 2], inputs: [_fused_op_42, softmax_336.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_336.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [_fused_op_43, lc.input_tensor.softmax_336.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_340: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [_fused_op_41, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [softmax_336.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_336.4, _fused_op_43],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_347: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_44, matmul_340],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_351: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3372_2756: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [_fused_op_41],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3372_2756: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [buffer_1_3372_2756],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_355: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_351, buffer_0_3372_2756],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2756_3376: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_355],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_45: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_356.1, layernorm_356.dc.reduce_sum.0.lc1, buffer_0_2756_3376],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_356.dc.multiply.4: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_45, _fused_op_45],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.4, lc.input_tensor.layernorm_356.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3376_3377: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [_fused_op_45],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3376_3377: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_1_3376_3377],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_46: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_356.6, layernorm_356.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_356.8, buffer_0_3376_3377, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_359: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [_fused_op_46, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_362: {type: gelu, grid_loc: [2, 4], grid_size: [1, 2], inputs: [matmul_359],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [4, 0], grid_size: [1, 6], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3377_2788: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_46],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_369: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_365, buffer_0_3377_2788],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2788_3378: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_369],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_47: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_370.1, layernorm_370.dc.reduce_sum.0.lc1, buffer_0_2788_3378],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_370.dc.multiply.4: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_47, _fused_op_47],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.4, lc.input_tensor.layernorm_370.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3378_3379: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_47],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3378_3379: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_1_3378_3379],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_48: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_370.6, layernorm_370.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_370.8, buffer_0_3378_3379, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_373: {type: matmul, grid_loc: [5, 6], grid_size: [1, 2], inputs: [_fused_op_48, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_379: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [_fused_op_48, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_385: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [matmul_385, input_1_multiply_387, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_389.dc.reduce_max.0: {type: reduce, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_49],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_50: {type: fused_op, grid_loc: [6, 5], grid_size: [1, 2], inputs: [_fused_op_49, softmax_389.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_389.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [_fused_op_50, lc.input_tensor.softmax_389.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_393: {type: matmul, grid_loc: [7, 1], grid_size: [1, 2], inputs: [_fused_op_48, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_51: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [softmax_389.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_389.4, _fused_op_50],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_400: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [_fused_op_51, matmul_393],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_404: {type: matmul, grid_loc: [7, 4], grid_size: [1, 2], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}

  fwd_0_6_temporal_epoch_3:
    target_device: 1
    input_count: 2
    add_408: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_404_0, e2e__fused_op_48_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2860_3383: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_408],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_52: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_409.1, layernorm_409.dc.reduce_sum.0.lc1, buffer_0_2860_3383],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_409.dc.multiply.4: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [_fused_op_52, _fused_op_52],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.4, lc.input_tensor.layernorm_409.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3383_3384: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_52],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3383_3384: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [buffer_1_3383_3384],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_53: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_409.6, layernorm_409.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_409.8, buffer_0_3383_3384, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_412: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [_fused_op_53, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_415: {type: gelu, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_412],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [3, 0], grid_size: [1, 6], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3384_2892: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_53],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_422: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_418, buffer_0_3384_2892],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2892_3385: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_422],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_54: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_423.1, layernorm_423.dc.reduce_sum.0.lc1, buffer_0_2892_3385],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_423.dc.multiply.4: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [_fused_op_54, _fused_op_54],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.4, lc.input_tensor.layernorm_423.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3385_3386: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [_fused_op_54],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3385_3386: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [buffer_1_3385_3386],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_55: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_423.6, layernorm_423.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_423.8, buffer_0_3385_3386, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_426: {type: matmul, grid_loc: [4, 3], grid_size: [1, 2], inputs: [_fused_op_55, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_432: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_55, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_438: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_56: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 1], inputs: [matmul_438, input_1_multiply_440, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_442.dc.reduce_max.0: {type: reduce, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_56],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_57: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 2], inputs: [_fused_op_56, softmax_442.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_442.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [_fused_op_57, lc.input_tensor.softmax_442.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_446: {type: matmul, grid_loc: [5, 6], grid_size: [1, 2], inputs: [_fused_op_55, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_58: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_442.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_442.4, _fused_op_57],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_453: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_58, matmul_446],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_457: {type: matmul, grid_loc: [6, 1], grid_size: [1, 2], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3386_2964: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [_fused_op_55],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3386_2964: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [buffer_1_3386_2964],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_461: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_457, buffer_0_3386_2964],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2964_3390: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [add_461],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_59: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_462.1, layernorm_462.dc.reduce_sum.0.lc1, buffer_0_2964_3390],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_462.dc.multiply.4: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [_fused_op_59, _fused_op_59],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.4, lc.input_tensor.layernorm_462.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3390_3391: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [_fused_op_59],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3390_3391: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [buffer_1_3390_3391],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_60: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_462.6, layernorm_462.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_462.8, buffer_0_3390_3391, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}

  fwd_0_7_temporal_epoch_3:
    target_device: 0
    input_count: 2
    matmul_465: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [_fused_op_60, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_468: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_465],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_471: {type: matmul, grid_loc: [1, 2], grid_size: [1, 6], inputs: [gelu_468, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3391_2996: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_60],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_475: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [matmul_471, buffer_0_3391_2996],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_2996_3392: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_475],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_61: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_476.1, layernorm_476.dc.reduce_sum.0.lc1, buffer_0_2996_3392],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_476.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_61, _fused_op_61],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.4, lc.input_tensor.layernorm_476.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3392_3393: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_61],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3392_3393: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_1_3392_3393],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_62: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_476.6, layernorm_476.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_476.8, buffer_0_3392_3393, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_479: {type: matmul, grid_loc: [3, 2], grid_size: [1, 2], inputs: [_fused_op_62, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_485: {type: matmul, grid_loc: [3, 4], grid_size: [1, 2], inputs: [_fused_op_62, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_491: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_63: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_491, input_1_multiply_493, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_495.dc.reduce_max.0: {type: reduce, grid_loc: [4, 0], grid_size: [1, 1], inputs: [_fused_op_63],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_64: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 2], inputs: [_fused_op_63, softmax_495.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_495.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_64, lc.input_tensor.softmax_495.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_499: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_62, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_65: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [softmax_495.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_495.4, _fused_op_64],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_506: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_65, matmul_499],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_510: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3393_3068: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [_fused_op_62],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3393_3068: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [buffer_1_3393_3068],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_514: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_510, buffer_0_3393_3068],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3068_3397: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_514],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_66: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_515.1, layernorm_515.dc.reduce_sum.0.lc1, buffer_0_3068_3397],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_515.dc.multiply.4: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_66, _fused_op_66],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.4, lc.input_tensor.layernorm_515.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3397_3398: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [_fused_op_66],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3397_3398: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [buffer_1_3397_3398],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_67: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_515.6, layernorm_515.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_515.8, buffer_0_3397_3398, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_518: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [_fused_op_67, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_521: {type: gelu, grid_loc: [6, 5], grid_size: [1, 2], inputs: [matmul_518],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_0_8_temporal_epoch_4:
    target_device: 1
    input_count: 2
    matmul_524: {type: matmul, grid_loc: [0, 0], grid_size: [1, 6], inputs: [e2e_gelu_521_0, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    add_528: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_524, e2e__fused_op_67_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3100_3399: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_528],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_68: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_529.1, layernorm_529.dc.reduce_sum.0.lc1, buffer_0_3100_3399],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_529.dc.multiply.4: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_68, _fused_op_68],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.4, lc.input_tensor.layernorm_529.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3399_3400: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_68],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3399_3400: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [buffer_1_3399_3400],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_69: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_529.6, layernorm_529.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_529.8, buffer_0_3399_3400, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_532: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [_fused_op_69, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_538: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [_fused_op_69, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_544: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_70: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_544, input_1_multiply_546, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_548.dc.reduce_max.0: {type: reduce, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_70],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_71: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 2], inputs: [_fused_op_70, softmax_548.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_548.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_71, lc.input_tensor.softmax_548.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_552: {type: matmul, grid_loc: [3, 2], grid_size: [1, 2], inputs: [_fused_op_69, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_72: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_548.4, _fused_op_71],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_559: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_72, matmul_552],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_563: {type: matmul, grid_loc: [3, 5], grid_size: [1, 2], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3400_3172: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_69],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3400_3172: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_1_3400_3172],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_567: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [matmul_563, buffer_0_3400_3172],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3172_3404: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [add_567],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_73: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_568.1, layernorm_568.dc.reduce_sum.0.lc1, buffer_0_3172_3404],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_568.dc.multiply.4: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [_fused_op_73, _fused_op_73],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.4, lc.input_tensor.layernorm_568.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3404_3405: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_73],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3404_3405: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_1_3404_3405],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_74: {type: fused_op, grid_loc: [5, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_568.6, layernorm_568.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_568.8, buffer_0_3404_3405, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_571: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [_fused_op_74, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_574: {type: gelu, grid_loc: [5, 2], grid_size: [1, 2], inputs: [matmul_571],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [7, 0], grid_size: [1, 6], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3405_3204: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [_fused_op_74],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_581: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_577, buffer_0_3405_3204],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3204_3406: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_581],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_75: {type: fused_op, grid_loc: [7, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_582.1, layernorm_582.dc.reduce_sum.0.lc1, buffer_0_3204_3406],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_582.dc.multiply.4: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [_fused_op_75, _fused_op_75],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_9_temporal_epoch_4:
    target_device: 0
    input_count: 2
    layernorm_582.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.4, lc.input_tensor.layernorm_582.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3406_3407: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_75],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3406_3407: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [buffer_1_3406_3407],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_76: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_582.6, layernorm_582.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_582.8, buffer_0_3406_3407, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_585: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [_fused_op_76, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_591: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [_fused_op_76, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_597: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_77: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_597, input_1_multiply_599, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}], input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 16, input_1: 1}}}
    softmax_601.dc.reduce_max.0: {type: reduce, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_77],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_78: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 2], inputs: [_fused_op_77, softmax_601.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [432, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_601.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_78, lc.input_tensor.softmax_601.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_605: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_76, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    _fused_op_79: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [softmax_601.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_601.4, _fused_op_78],
         t: 12, mblock: [1, 4], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 368], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 2}}
    matmul_612: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [_fused_op_79, matmul_605],
         t: 12, mblock: [1, 1], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 24, input_buf_min_size_tiles: [0, 268], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {l1_acc: true, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_616: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_1_3407_3276: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_76],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3407_3276: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_1_3407_3276],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_620: {type: add, grid_loc: [2, 6], grid_size: [1, 1], inputs: [matmul_616, buffer_0_3407_3276],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3276_3411: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_620],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_80: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_621.1, layernorm_621.dc.reduce_sum.0.lc1, buffer_0_3276_3411],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_621.dc.multiply.4: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_80, _fused_op_80],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.4, lc.input_tensor.layernorm_621.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3411_3412: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_80],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3411_3412: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_1_3411_3412],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_81: {type: fused_op, grid_loc: [3, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_621.6, layernorm_621.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_621.8, buffer_0_3411_3412, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    matmul_624: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_81, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [4, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, l1_acc: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    gelu_627: {type: gelu, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_624],
         t: 1, mblock: [4, 6], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [5, 2], grid_size: [1, 6], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 4}}
    buffer_0_3412_3308: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_81],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_634: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_630, buffer_0_3412_3308],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 256], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_3308_3413: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_634],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_82: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_635.1, layernorm_635.dc.reduce_sum.0.lc1, buffer_0_3308_3413],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 3}}
    layernorm_635.dc.multiply.4: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_82, _fused_op_82],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.4, lc.input_tensor.layernorm_635.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_1_3413_3414: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_82],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_3413_3414: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [buffer_1_3413_3414],
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [272], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_83: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_635.6, layernorm_635.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_635.8, buffer_0_3413_3414, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 24], ublock: [4, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 48, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {r: 4}], input_4_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: false, fused_op_id: 4, kernel_broadcast: {input_5: 96, input_4: 96}}}
    _fused_op_83_output_nop_0: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [_fused_op_83], untilize_output: true,
         t: 1, mblock: [4, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q4_shadow: 0, $gptr_q6_shadow: 0, $gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $gptr_q2_shadow: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q9: 0, $lptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q1_shadow: 0, $gptr_q7: 0, $gptr_q4: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q6, set, $gptr_q6_shadow]
    -   varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_1_temporal_epoch_0, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_144.dc.reduce_sum.5.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_3_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_230.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_264.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_283.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_5_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.softmax_336.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_336.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_389.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_48_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_404_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.layernorm_409.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_442.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_7_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_495.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_67_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_521_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_548.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_9_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               lc.input_tensor.layernorm_582.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_601.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16.0: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17.0: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1.0: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [1, 1], ublock: [4, 2], output: dest}
        - softmax_18.dc.exp.2.0: { type: exp, inputs: [dest], mblock: [1, 1], ublock: [4, 2], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.add.5.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [4, 1], output: dest}
        - softmax_18.dc.reciprocal.6.0: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [4, 1], output: intermed0}
        - softmax_18.dc.multiply.7.0: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 4}, tile_broadcast: c], pop: [intermed0], mblock: [1, 4], ublock: [4, 1], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.2.0: { type: multiply, inputs: [input0, input1], mblock: [4, 3], ublock: [1, 8], output: dest}
        - layernorm_38.dc.subtract.3.0: { type: subtract, inputs: [input2, dest], mblock: [4, 3], ublock: [1, 8], output: output}
  4: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.7.0: { type: multiply, inputs: [input0, input1], mblock: [1, 1], ublock: [4, 1], output: dest}
        - layernorm_38.dc.add.9.0: { type: add, inputs: [dest, input2], mblock: [1, 1], ublock: [4, 1], output: dest}
        - layernorm_38.dc.sqrt.10.0: { type: sqrt, inputs: [dest], mblock: [1, 1], ublock: [4, 1], output: dest}
        - layernorm_38.dc.reciprocal.11.0: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [4, 1], output: intermed0}
        - layernorm_38.dc.multiply.12.0: { type: multiply, inputs: [input3, intermed0], input_1_tms: [broadcast: {c: 24}, tile_broadcast: c], pop: [intermed0], mblock: [1, 24], ublock: [4, 1], output: dest}
        - layernorm_38.dc.multiply.13.0: { type: multiply, inputs: [dest, input4], mblock: [1, 24], ublock: [4, 1], output: dest}
        - layernorm_38.dc.add.14.0: { type: add, inputs: [dest, input5], mblock: [1, 24], ublock: [4, 1], output: output}
