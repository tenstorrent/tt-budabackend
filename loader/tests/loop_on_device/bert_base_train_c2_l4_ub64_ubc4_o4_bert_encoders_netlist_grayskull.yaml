# git checkout 0db1baecc
# pytest pybuda/test/benchmark/benchmark.py -m bert -c base -opt 4 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_FUSED_OP_MULTIPLIER=2 PYBUDA_FORCE_INTERMED_TO_OUTPUT_DF=1 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1 --training --auto_transpose --layers 4 --microbatch_count 4 --microbatch 64 --chips 2

devices:
  arch: grayskull

test-config:
  stimulus-config:
    type: Normal
    normal_mean: 0.5
    normal_stddev: 0.1
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoders.output_layernorm_211]
    outputs: [bert_encoders.output_layernorm_211]
  test-args:
    sequence_lenth: 384
    head_size: 16

queues:

  # input
  input_1:                                                                {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                         {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x330c0020]]}

  # output
  bert_encoders.output_layernorm_211:                                     {input: _fused_op_39_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc291260], [7, 0xe57baa0], [0, 0x1020c2c0], [1, 0x101db6c0]]}
  layer.0.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fea480], [4, 0xff92040], [5, 0xcf1a200], [6, 0xc4d6b40]]}
  layer.0.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe7d70e0], [0, 0x1048b9e0], [1, 0x104458a0], [2, 0x134c2800]]}
  layer.0.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fed560], [4, 0xff95120], [5, 0xcf1d2e0], [6, 0xc4d9c20]]}
  layer.0.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc48d920], [7, 0xe78dec0], [0, 0x104427c0], [1, 0x103fc680]]}
  layer.0.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1048eac0], [2, 0x1350c260], [3, 0xa008c60], [4, 0xff98a40]]}
  layer.0.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcf695e0], [6, 0xc525f20], [7, 0xe869d60], [0, 0x1051e660]]}
  layer.0.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10491ba0], [2, 0x1350f340], [3, 0xa00bd40], [4, 0xff9bb20]]}
  layer.0.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13512420]]}
  layer.0.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa00ee20]]}
  layer.0.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [12, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcdf2080], [6, 0xc3ae9c0], [7, 0xe6aef60], [0, 0x1034ba80], [1, 0x10363860], [2, 0x133e1000], [3, 0x9f08c80], [4, 0xfeb0840]]}
  layer.0.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc33cd20], [7, 0xe633040], [0, 0x102cfb60], [1, 0x102aaa40]]}
  layer.0.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x132d3500], [3, 0x9dcada0], [4, 0xfda2d40], [5, 0xcd2bf40], [6, 0xc349040], [7, 0xe63f360], [0, 0x102dbe80], [1, 0x102b6d60], [2, 0x13334d20], [3, 0x9e2c5c0], [4, 0xfe04560], [5, 0xcd8d760]]}
  layer.0.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc3aa860], [7, 0xe6a0b80], [0, 0x1033d6a0], [1, 0x10318580], [2, 0x13396540], [3, 0x9e8dde0], [4, 0xfe65d80], [5, 0xcdeef80], [6, 0xc3ab8c0], [7, 0xe6a1be0], [0, 0x1033e700], [1, 0x103195e0]]}
  layer.0.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe6a2c40]]}
  layer.0.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1033f760]]}
  layer.1.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1031a640], [2, 0x13397de0], [3, 0x9ebfa60], [4, 0xfe67620]]}
  layer.1.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x132d0420], [3, 0x9dc7cc0], [4, 0xfd9fc60], [5, 0xcd28e60]]}
  layer.1.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xce844a0], [6, 0xc440de0], [7, 0xe741380], [0, 0x103ddea0]]}
  layer.1.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x103f5c80], [2, 0x13473420], [3, 0x9f9b0a0], [4, 0xff42c60]]}
  layer.1.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13476500], [3, 0x9f9e180], [4, 0xff45d40], [5, 0xcecdf00]]}
  layer.1.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc48a840], [7, 0xe78ade0], [0, 0x1043f6e0], [1, 0x103f95a0]]}
  layer.1.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcf203c0], [6, 0xc4dcd00], [7, 0xe820b40], [0, 0x104d5440]]}
  layer.1.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x134bf720], [3, 0x9fe73a0], [4, 0xff8ef60], [5, 0xcf17120]]}
  layer.1.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xffddbe0]]}
  layer.1.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcfc03e0]]}
  layer.1.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [12, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc5c4ee0], [7, 0xe8d8940], [0, 0x105827a0], [1, 0x104a5920], [2, 0x1352eba0], [3, 0xa01ca00], [4, 0xffe9f00], [5, 0xcfcc700]]}
  layer.1.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcfb40c0], [6, 0xc5b8bc0], [7, 0xe8cc620], [0, 0x10576480]]}
  layer.1.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc657300], [7, 0xe96ad60], [0, 0x10614bc0], [1, 0x10537d40], [2, 0x135c0fc0], [3, 0xa0aee20], [4, 0x1007c320], [5, 0xd05eb20], [6, 0xc6b8b20], [7, 0xe9cc580], [0, 0x106763e0], [1, 0x10599560]]}
  layer.1.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x136227e0], [3, 0xa110640], [4, 0x100ddb40], [5, 0xd0c0340], [6, 0xc71a340], [7, 0xea2dda0], [0, 0x106d7c00], [1, 0x105fad80], [2, 0x13623840], [3, 0xa1116a0], [4, 0x100deba0], [5, 0xd0c13a0]]}
  layer.1.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10497560]]}
  layer.1.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1056a160]]}
  layer.2.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x55c8c40], [4, 0x5536840], [5, 0x548c660], [6, 0x548b620]]}
  layer.2.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x54dfae0], [0, 0x54d8100], [1, 0x553b980], [2, 0x5552740]]}
  layer.2.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x5611e60], [4, 0x557fa60], [5, 0x54d5880], [6, 0x54d4840]]}
  layer.2.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x54e2bc0], [0, 0x54db1e0], [1, 0x553ea60], [2, 0x5555820]]}
  layer.2.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x54e5ca0], [0, 0x54de2c0], [1, 0x5541b40], [2, 0x5558900]]}
  layer.2.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x54dca00], [0, 0x54d5020], [1, 0x55388a0], [2, 0x554f660]]}
  layer.2.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x55c94c0], [5, 0x55370c0], [6, 0x551e2a0], [7, 0x552eec0]]}
  layer.2.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x55274e0], [1, 0x558ad60], [2, 0x55a1b20], [3, 0x5667be0]]}
  layer.2.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x558de40]]}
  layer.2.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x55a4c00]]}
  layer.2.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [12, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x53fb200], [0, 0x53f3820], [1, 0x54570a0], [2, 0x546de60], [3, 0x54e9ce0], [4, 0x54578e0], [5, 0x5395920], [6, 0x53ac6c0]]}
  layer.2.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x53e5c40], [5, 0x5319a00], [6, 0x53307a0], [7, 0x538b600]]}
  layer.2.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x532ef40], [1, 0x53623e0], [2, 0x53a9580], [3, 0x5423ba0], [4, 0x53f1f60], [5, 0x5325d20], [6, 0x533cac0], [7, 0x5397920], [0, 0x5390760], [1, 0x53c3c00], [2, 0x540ada0], [3, 0x54853c0]]}
  layer.2.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x5453780], [5, 0x5387540], [6, 0x539e2e0], [7, 0x53f9140], [0, 0x53f1f80], [1, 0x5425420], [2, 0x546c5c0], [3, 0x54e6be0], [4, 0x54547e0], [5, 0x53885a0], [6, 0x539f340], [7, 0x53fa1a0]]}
  layer.2.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5389600]]}
  layer.2.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x53a03a0]]}
  layer.3.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x52e5d20], [1, 0x53191c0], [2, 0x5360360], [3, 0x53da980]]}
  layer.3.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x548d620], [0, 0x5485c40], [1, 0x54e94c0], [2, 0x5500280]]}
  layer.3.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x557c100], [4, 0x54e9d00], [5, 0x5427d40], [6, 0x543eae0]]}
  layer.3.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x5490700], [0, 0x5488d20], [1, 0x54ec5a0], [2, 0x5503360]]}
  layer.3.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x54937e0], [0, 0x548be00], [1, 0x54ef680], [2, 0x5506440]]}
  layer.3.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x55c5b60], [4, 0x5533760], [5, 0x5489580], [6, 0x5488540]]}
  layer.3.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [24, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x57cdd00], [4, 0x5721260], [5, 0x56ab580], [6, 0x56cbd80]]}
  layer.3.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x56d3740], [0, 0x5685400], [1, 0x5708c60], [2, 0x56bea40]]}
  layer.3.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5567d00]]}
  layer.3.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x5612f20]]}
  layer.3.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [12, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x557a180], [0, 0x552c660], [1, 0x559a160], [2, 0x55b0f20], [3, 0x569b8e0], [4, 0x561f240], [5, 0x55b2fa0], [6, 0x5574020]]}
  layer.3.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x562c580], [2, 0x5643340], [3, 0x572dd00], [4, 0x56b1660]]}
  layer.3.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5606440], [7, 0x560e640], [0, 0x55c0b20], [1, 0x56388a0], [2, 0x564f660], [3, 0x573a020], [4, 0x56bd980], [5, 0x5647460], [6, 0x5667c60], [7, 0x566fe60], [0, 0x5622340], [1, 0x569a0c0]]}
  layer.3.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x571f1a0], [5, 0x56a8c80], [6, 0x56c9480], [7, 0x56d1680], [0, 0x5683b60], [1, 0x56fb8e0], [2, 0x56b16c0], [3, 0x57cc460], [4, 0x5720200], [5, 0x56a9ce0], [6, 0x56ca4e0], [7, 0x56d26e0]]}
  layer.3.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x56fc940]]}
  layer.3.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x56b2720]]}

  # constant
  input_1_multiply_16_fork_clone1522:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe820300]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x104d4c00]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1350ba20]]}
  dc.input_tensor.softmax_18.4:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ff0640]]}
  lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xff98200]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcfb2800]]}
  dc.input_tensor.layernorm_38.1:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc56f140]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe8b2f80]]}
  dc.input_tensor.layernorm_38.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10567880]]}
  dc.input_tensor.layernorm_38.8:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10494c80]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x133975a0]]}
  dc.input_tensor.layernorm_52.1:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e8ee40]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfe66de0]]}
  dc.input_tensor.layernorm_52.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcdeffe0]]}
  dc.input_tensor.layernorm_52.8:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc3ac920]]}
  input_1_multiply_69_fork_clone1544:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcecd6c0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc48a000]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe78a5a0]]}
  dc.input_tensor.softmax_71.4:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x104270c0]]}
  lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x103f8d60]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x106d8c60]]}
  dc.input_tensor.layernorm_91.1:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xea2ee00]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc71b3a0]]}
  dc.input_tensor.layernorm_91.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1352cb00]]}
  dc.input_tensor.layernorm_91.8:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x104a3880]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa01c1c0]]}
  dc.input_tensor.layernorm_105.1:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xffacfc0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa01b980]]}
  dc.input_tensor.layernorm_105.6:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xff9ec00]]}
  dc.input_tensor.layernorm_105.8:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1352aa60]]}
  input_1_multiply_122_fork_clone1562:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x565b080]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe8cbde0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x55c8c80]]}
  dc.input_tensor.softmax_124.4:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x551eaa0]]}
  lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x551da60]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x56126e0]]}
  dc.input_tensor.layernorm_144.1:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x55802e0]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x55674c0]]}
  dc.input_tensor.layernorm_144.6:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x55780e0]]}
  dc.input_tensor.layernorm_144.8:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x552a5c0]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x53f2fe0]]}
  dc.input_tensor.layernorm_158.1:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5426480]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x546d620]]}
  dc.input_tensor.layernorm_158.6:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x54e7c40]]}
  dc.input_tensor.layernorm_158.8:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x5455840]]}
  input_1_multiply_175_fork_clone1580:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x55c5320]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc5b8380]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x5532f20]]}
  dc.input_tensor.softmax_177.4:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5470f60]]}
  lc.input_tensor.softmax_177.dc.reciprocal.6_s_brcst_m1_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5487d00]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x56aad40]]}
  dc.input_tensor.layernorm_197.1:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x579b840]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x56b0e80]]}
  dc.input_tensor.layernorm_197.6:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x56453c0]]}
  dc.input_tensor.layernorm_197.8:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x560c5a0]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x56cb540]]}
  dc.input_tensor.layernorm_211.1:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x566acc0]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5684bc0]]}
  dc.input_tensor.layernorm_211.6:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x55b0f00]]}
  dc.input_tensor.layernorm_211.8:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x55bea80]]}
  lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x57cd4c0]]}
  lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x4ffaba0]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4ffaba0]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x50129a0]]}
  dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x508c7a0], [4, 0x5098280]]}
  lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x508c7a0]]}
  lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x508bf60]]}
  lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x527cac0]]}
  lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x50f7240]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x51a0c20]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4fb1140]]}
  dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x50949c0], [1, 0x507d400]]}
  lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x4fb0900]]}
  lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4fb0900]]}
  lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x4ff9b20]]}
  input_1_multiply_175:                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4ff9b20]]}
  lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4fb0900]]}
  lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x4ffa360]]}
  lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x508bf60]]}
  lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x538adc0]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x53191c0]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x53e5400]]}
  dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5347d40], [3, 0x53c2360]]}
  lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5318980]]}
  lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x52a1c20]]}
  lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x5224d20]]}
  lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x521cb20]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x521baa0]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x51ddb20]]}
  dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x51c2460], [0, 0x511ab20]]}
  lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x50f7a80]]}
  lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x51daa80]]}
  lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x51d2040]]}
  input_1_multiply_122:                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5227580]]}
  lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x51db2c0]]}
  lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x521c2e0]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcfb3880]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa01b140]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10496d20]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10569920]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc59fd60], [7, 0xe8b37c0]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcfb3040]]}
  lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xff74e40]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10058680]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x130662a0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe3d4180]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10058ec0]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xffe31c0], [2, 0x13066ae0]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c1eae0]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xfffb7e0]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12f69c00]]}
  input_1_multiply_69:                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfa509e0]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ad89e0]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ad9220]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc2da480]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9cebe20]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcc592c0]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xc99cd00]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x131f4dc0], [3, 0x9cec660]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfcdc3e0]]}
  lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe5c4cc0]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc33c4e0]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x102cf320]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c68d80]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfbf7ae0]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcb749e0], [6, 0xc1953c0]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe41dbe0]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcb8d000]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c1f320]]}
  input_1_multiply_16:                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfc41540]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcb8d840]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c695c0]]}

  # epoch_to_epoch
  e2e__fused_op_12_0:                                                     {input: _fused_op_12, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  e2e_buffer_0__fused_op_12__fused_op_14_0:                               {input: buffer_0__fused_op_12__fused_op_14, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  e2e__fused_op_19_0:                                                     {input: _fused_op_19, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x332c8060]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30104020]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}
  e2e_layernorm_197.dc.multiply.4_0:                                      {input: layernorm_197.dc.multiply.4, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x56884e0]]}
  e2e_buffer_0__fused_op_32__fused_op_34_0:                               {input: buffer_0__fused_op_32__fused_op_34, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x570bd40]]}
  e2e__fused_op_38_0:                                                     {input: _fused_op_38, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0xd0a1c00]]}
  e2e__fused_op_37_0:                                                     {input: _fused_op_37, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xc7cf000]]}
  e2e_gelu_203_0:                                                         {input: gelu_203, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0xf5468e0], [0, 0xd0685c0], [1, 0xa233e20]]}
  e2e_matmul_200_0:                                                       {input: matmul_200, type: queue, entries: 256, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x136da560], [5, 0x105a4880], [6, 0x12a55080], [7, 0x13646900], [0, 0x111685e0], [1, 0xe333e40], [2, 0x10161c20], [3, 0xc9d7020], [4, 0x1430a580], [5, 0x111d48a0], [6, 0x136850a0], [7, 0x14276920], [0, 0x11d98600], [1, 0xef63e60], [2, 0x10d91c40], [3, 0xd607040]]}
  e2e__fused_op_35_0:                                                     {input: _fused_op_35, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x14f3a5a0]]}
  e2e__fused_op_34_0:                                                     {input: _fused_op_34, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x11e048c0]]}
  e2e__fused_op_33_0:                                                     {input: _fused_op_33, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x142b50c0]]}
  e2e_matmul_188_0:                                                       {input: matmul_188, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5714fa0]]}
  e2e_matmul_181_0:                                                       {input: matmul_181, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x56c1b20], [3, 0x5816f20], [4, 0x576a480], [5, 0x56f47a0]]}
  e2e_softmax_177.dc.multiply.7_0:                                        {input: softmax_177.dc.multiply.7, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xbcfa500]]}
  e2e__fused_op_29_0:                                                     {input: _fused_op_29, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0xe135020]]}
  e2e_bw_in1_matmul_181_transpose_0_0:                                    {input: bw_in1_matmul_181_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x14ec48e0], [6, 0x144bd0e0], [7, 0x154be960], [0, 0x12fe0640]]}
  e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0:                      {input: bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0xfe9fea0]]}
  e2e_bw_in0_matmul_188_matmul_1_0:                                       {input: bw_in0_matmul_188_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x11ccdc80]]}
  e2e_matmul_167_0:                                                       {input: matmul_167, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x56d6820], [0, 0x62b8500], [1, 0x633bd60], [2, 0x62f1b40]]}
  e2e_matmul_161_0:                                                       {input: matmul_161, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x7b66880], [0, 0x8748560], [1, 0x87cbdc0], [2, 0x8781ba0]]}
  e2e_bw_in0_matmul_181_matmul_1_0:                                       {input: bw_in0_matmul_181_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0xfb93e80], [2, 0x119c1c60], [3, 0xe237060], [4, 0x17ffa5c0]]}
  e2e__fused_op_44_0:                                                     {input: _fused_op_44, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x14ea6940], [0, 0x129c8620]]}
  e2e__fused_op_28_0:                                                     {input: _fused_op_28, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0xbc84820]]}
  e2e__fused_op_27_0:                                                     {input: _fused_op_27, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xad66fa0]]}
  e2e_gelu_150_0:                                                         {input: gelu_150, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bfa4e0], [5, 0x7b84800], [6, 0xa035000]]}
  e2e_matmul_147_0:                                                       {input: matmul_147, type: queue, entries: 256, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x639a4a0], [5, 0x63247c0], [6, 0x87d4fc0], [7, 0x6306840], [0, 0x6ee8520], [1, 0x6f6bd80], [2, 0x6f21b60], [3, 0x9506f60], [4, 0x6fca4c0], [5, 0x6f547e0], [6, 0x9404fe0], [7, 0x6f36860], [0, 0x7b18540], [1, 0x7b9bda0], [2, 0x7b51b80], [3, 0xa136f80]]}
  e2e__fused_op_49_0:                                                     {input: _fused_op_49, type: queue, entries: 64, grid_size: [1, 12], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x10c55f00], [2, 0x136b3ce0], [3, 0xece10e0], [4, 0x18aa4640], [5, 0x15662960], [6, 0x14c5b160], [7, 0x15c5c9e0], [0, 0x1377e6c0], [1, 0x11065f20], [2, 0x13ac3d00], [3, 0xf0f1100], [4, 0x18eb4660]]}
  e2e__fused_op_25_0:                                                     {input: _fused_op_25, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x6446f40]]}
  e2e_bw_in1_matmul_147_transpose_0_0:                                    {input: bw_in1_matmul_147_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x15356940], [6, 0x1494f140], [7, 0x159509c0], [0, 0x134726a0]]}
  e2e__fused_op_48_0:                                                     {input: _fused_op_48, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xe6c90c0], [4, 0x1848c620]]}
  e2e_bw_in0_matmul_147_matmul_1_0:                                       {input: bw_in0_matmul_147_matmul_1, type: queue, entries: 64, grid_size: [2, 8], t: 1, mblock: [1, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xe543080], [4, 0x183065e0], [5, 0x151d0900], [6, 0x147c9100], [7, 0x157ca980], [0, 0x132ec660], [1, 0x10acfec0], [2, 0x1352dca0], [3, 0xe6060a0], [4, 0x183c9600], [5, 0x15293920], [6, 0x1488c120], [7, 0x1588d9a0], [0, 0x133af680], [1, 0x10b92ee0], [2, 0x135f0cc0]]}
  e2e__fused_op_24_0:                                                     {input: _fused_op_24, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x9fe1be0]]}
  e2e__fused_op_23_0:                                                     {input: _fused_op_23, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0xa02be00]]}
  e2e_matmul_135_0:                                                       {input: matmul_135, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x9fa85a0]]}
  e2e_matmul_128_0:                                                       {input: matmul_128, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x11e7a520], [5, 0xed44840], [6, 0x111f5040], [7, 0xe9168c0]]}
  e2e_softmax_124.dc.multiply.7_0:                                        {input: softmax_124.dc.multiply.7, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x87968a0]]}
  e2e_matmul_114_0:                                                       {input: matmul_114, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x9378580], [1, 0x93fbde0], [2, 0x93b1bc0], [3, 0xaf6efc0]]}
  e2e_matmul_108_0:                                                       {input: matmul_108, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xbb9efe0], [4, 0x12aaa540], [5, 0xf974860], [6, 0x11e25060]]}
  e2e_bw_in0_matmul_128_matmul_1_0:                                       {input: bw_in0_matmul_128_matmul_1, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x332c8040], [0, 0x338e0060], [0, 0x33ef8080], [0, 0x345100a0]]}
  e2e_bw_in0_matmul_114_matmul_1_0:                                       {input: bw_in0_matmul_114_matmul_1, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x37be8160], [0, 0x38200180], [0, 0x388181a0], [0, 0x38e301c0]]}
  e2e_bw_in0_matmul_108_matmul_1_0:                                       {input: bw_in0_matmul_108_matmul_1, type: queue, entries: 128, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x363880e0], [0, 0x369a0100], [0, 0x36fb8120], [0, 0x375d0140]]}
  e2e__fused_op_51_chip_to_chip_nop_0_0:                                  {input: _fused_op_51_chip_to_chip_nop_0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x34b280c0]]}
  e2e__fused_op_18_0:                                                     {input: _fused_op_18, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xdf7bc20]]}
  e2e__fused_op_17_0:                                                     {input: _fused_op_17, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11f394e0]]}
  e2e_gelu_97_0:                                                          {input: gelu_97, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb972740], [4, 0x1193fc40], [5, 0xe922440]]}
  e2e_matmul_94_0:                                                        {input: matmul_94, type: queue, entries: 256, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa112700], [4, 0x100dfc00], [5, 0xd0c2400], [6, 0xc71bbe0], [7, 0xea5fa20], [0, 0x106d94a0], [1, 0x136bbe00], [2, 0x1382c8c0], [3, 0xad42720], [4, 0x10d0fc20], [5, 0xdcf2420], [6, 0xd34bc00], [7, 0xf68fa40], [0, 0x113094c0], [1, 0x142ebe20], [2, 0x1445c8e0]]}
  e2e__fused_op_15_0:                                                     {input: _fused_op_15, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x105fbde0]]}
  e2e__fused_op_14_0:                                                     {input: _fused_op_14, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x102bfa60]]}
  e2e__fused_op_13_0:                                                     {input: _fused_op_13, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x136248a0]]}
  e2e_matmul_82_0:                                                        {input: matmul_82, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  e2e_matmul_75_0:                                                        {input: matmul_75, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900], [3, 0x4fb0900], [4, 0x4fb0900], [5, 0x4fb0900]]}
  e2e_softmax_71.dc.multiply.7_0:                                         {input: softmax_71.dc.multiply.7, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6810940]]}
  e2e_bw_in1_matmul_82_matmul_1_0:                                        {input: bw_in1_matmul_82_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x12141500]]}
  e2e__fused_op_9_0:                                                      {input: _fused_op_9, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  e2e_bw_in1_matmul_75_transpose_0_0:                                     {input: bw_in1_matmul_75_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1508c900], [3, 0xfa72760], [4, 0x15a3fc60], [5, 0x12a22460]]}
  e2e_bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0_0:                       {input: bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x15398920]]}
  e2e_bw_in0_matmul_82_matmul_1_0:                                        {input: bw_in0_matmul_82_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14f1be40]]}
  e2e_matmul_61_0:                                                        {input: matmul_61, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5be0920], [4, 0x5be0920], [5, 0x5be0920], [6, 0x8070920]]}
  e2e_matmul_55_0:                                                        {input: matmul_55, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8070920], [0, 0x9ce0940], [1, 0x9ce0940], [2, 0x9ce0940]]}
  e2e_bw_in0_matmul_75_matmul_1_0:                                        {input: bw_in0_matmul_75_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x11653c60], [7, 0x13997aa0], [0, 0x12d71520], [1, 0x1677be60]]}
  e2e__fused_op_58_0:                                                     {input: _fused_op_58, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1103bc40], [7, 0x1337fa80]]}
  e2e__fused_op_8_0:                                                      {input: _fused_op_8, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6810940]]}
  e2e__fused_op_7_0:                                                      {input: _fused_op_7, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6810940]]}
  e2e_gelu_44_0:                                                          {input: gelu_44, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5be0920], [1, 0x5be0920], [2, 0x5be0920]]}
  e2e_matmul_41_0:                                                        {input: matmul_41, type: queue, entries: 256, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8ca0940], [7, 0x8ca0940], [0, 0xa910960], [1, 0xa910960], [2, 0xa910960], [3, 0x6a18960], [4, 0xc990960], [5, 0x98d0960], [6, 0x98d0960], [7, 0x98d0960], [0, 0xb540980], [1, 0xb540980], [2, 0xb540980], [3, 0x7648980], [4, 0xd5c0980], [5, 0xa500980]]}
  e2e_bw_in0_matmul_47_matmul_1_0:                                        {input: bw_in0_matmul_47_matmul_1, type: queue, entries: 64, grid_size: [1, 12], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xfd7e780], [4, 0x15d4bc80], [5, 0x12d2e480], [6, 0x1195fc80], [7, 0x13ca3ac0], [0, 0x1307d540], [1, 0x16a87e80], [2, 0x15fc8940], [3, 0x1018e7a0], [4, 0x1615bca0], [5, 0x1313e4a0], [6, 0x11d6fca0]]}
  e2e__fused_op_5_0:                                                      {input: _fused_op_5, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xc1709a0]]}
  e2e__fused_op_62_0:                                                     {input: _fused_op_62, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x140b3ae0], [0, 0x1348d560]]}
  e2e__fused_op_4_0:                                                      {input: _fused_op_4, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa500980]]}
  e2e__fused_op_3_0:                                                      {input: _fused_op_3, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa500980]]}
  e2e_matmul_29_0:                                                        {input: matmul_29, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xc1709a0]]}
  e2e_matmul_22_0:                                                        {input: matmul_22, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x82789a0], [4, 0xe1f09a0], [5, 0xb1309a0], [6, 0xa7089a0]]}
  e2e_softmax_18.dc.multiply.7_0:                                         {input: softmax_18.dc.multiply.7, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc1709a0]]}
  e2e_matmul_8_0:                                                         {input: matmul_8, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xd5c09a0], [0, 0xf2309c0], [1, 0xf2309c0], [2, 0x122f09c0]]}
  e2e_matmul_2_0:                                                         {input: matmul_2, type: queue, entries: 256, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8ea89c0], [4, 0xee209c0], [5, 0xbd609c0], [6, 0xb3389c0]]}

  # loss
  loss_bert_encoders.output_layernorm_211:                                {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30208040]]}

  # grad_accumulator
  grad_acc_layer.3.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x565b8c0]]}
  grad_acc_layer.3.output.LayerNorm.weight:                               {input: bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5006680]]}
  grad_acc_layer.3.output.dense.bias:                                     {input: bw_in1_add_208_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x508c7a0]]}
  grad_acc_layer.3.output.dense.weight:                                   {input: bw_in1_matmul_206_matmul_1, type: ram, entries: 1, grid_size: [3, 3], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x4ffb3e0], [0, 0x50129a0], [1, 0x4ffb3e0], [2, 0x50131e0], [3, 0x50a4dc0], [4, 0x50b08a0], [5, 0x508cfe0], [6, 0x5098ac0], [7, 0x507d400]]}
  grad_acc_layer.3.intermediate.dense.bias:                               {input: bw_in1_add_202_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5095200], [3, 0x5126de0]]}
  grad_acc_layer.3.intermediate.dense.weight:                             {input: bw_in1_matmul_200_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x51328c0], [5, 0x510f000], [6, 0x511aae0], [7, 0x50ff420], [0, 0x50acfe0], [1, 0x5095a20], [2, 0x50ad820], [3, 0x513f400], [4, 0x51940e0], [5, 0x5170820], [6, 0x517c300], [7, 0x5160c40]]}
  grad_acc_layer.3.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x510e800]]}
  grad_acc_layer.3.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x510f040]]}
  grad_acc_layer.3.attention.output.dense.bias:                           {input: bw_in1_add_194_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x4fb0900]]}
  grad_acc_layer.3.attention.output.dense.weight:                         {input: bw_in1_matmul_192_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4fb0900], [6, 0x4fb0900], [7, 0x4fb0900], [0, 0x4fb0900]]}
  grad_acc_layer.3.attention.self.value.bias:                             {input: bw_in1_add_183_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4fb1140]]}
  grad_acc_layer.3.attention.self.value.weight:                           {input: bw_in1_matmul_181_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x4fb1140], [4, 0x4fbcc20], [5, 0x4ff9b20], [6, 0x4ff9b20]]}
  grad_acc_layer.3.attention.self.key.bias:                               {input: bw_in1_add_169_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4fbd460]]}
  grad_acc_layer.3.attention.self.key.weight:                             {input: bw_in1_matmul_167_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x4ffa360], [4, 0x5005e40], [5, 0x5042d40], [6, 0x5042d40]]}
  grad_acc_layer.3.attention.self.query.bias:                             {input: bw_in1_add_163_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4ffa360]]}
  grad_acc_layer.3.attention.self.query.weight:                           {input: bw_in1_matmul_161_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4fb1980], [2, 0x4fc9780], [3, 0x5043580], [4, 0x504f060]]}
  grad_acc_layer.2.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x51f5900]]}
  grad_acc_layer.2.output.LayerNorm.weight:                               {input: bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5324480]]}
  grad_acc_layer.2.output.dense.bias:                                     {input: bw_in1_add_155_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x52d9a00]]}
  grad_acc_layer.2.output.dense.weight:                                   {input: bw_in1_matmul_153_matmul_1, type: ram, entries: 1, grid_size: [3, 3], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x5286d80], [0, 0x52579e0], [1, 0x5296960], [2, 0x52c5d20], [3, 0x5340340], [4, 0x53633e0], [5, 0x52971a0], [6, 0x52a2460], [7, 0x5308da0]]}
  grad_acc_layer.2.intermediate.dense.bias:                               {input: bw_in1_add_149_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x534adc0], [5, 0x527eb80]]}
  grad_acc_layer.2.intermediate.dense.weight:                             {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x51949a0], [1, 0x51d3920], [2, 0x5202ce0], [3, 0x527d300], [4, 0x52e95a0], [5, 0x521d360], [6, 0x5240400], [7, 0x5225560], [0, 0x51f61c0], [1, 0x5235140], [2, 0x5264500], [3, 0x52deb20]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x52340e0]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x52dd280]]}
  grad_acc_layer.2.attention.output.dense.bias:                           {input: bw_in1_add_141_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x511b360]]}
  grad_acc_layer.2.attention.output.dense.weight:                         {input: bw_in1_matmul_139_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x51a1460], [4, 0x5201c20], [5, 0x51d2880], [6, 0x51de360]]}
  grad_acc_layer.2.attention.self.value.bias:                             {input: bw_in1_add_130_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5133140]]}
  grad_acc_layer.2.attention.self.value.weight:                           {input: bw_in1_matmul_128_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x50f82c0], [2, 0x5127680], [3, 0x51ea680], [4, 0x524ae40]]}
  grad_acc_layer.2.attention.self.key.bias:                               {input: bw_in1_add_116_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x513f460]]}
  grad_acc_layer.2.attention.self.key.weight:                             {input: bw_in1_matmul_114_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x51414e0], [2, 0x51708a0], [3, 0x52338a0], [4, 0x5294060]]}
  grad_acc_layer.2.attention.self.query.bias:                             {input: bw_in1_add_110_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[6, 0x5227dc0]]}
  grad_acc_layer.2.attention.self.query.weight:                           {input: bw_in1_matmul_108_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[7, 0x51dbb00], [0, 0x514b780], [1, 0x518a700], [2, 0x51b9ac0]]}
  grad_acc_layer.1.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xffa0ca0]]}
  grad_acc_layer.1.output.LayerNorm.weight:                               {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1351e740]]}
  grad_acc_layer.1.output.dense.bias:                                     {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe282e00]]}
  grad_acc_layer.1.output.dense.weight:                                   {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [3, 3], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xfef2e20], [1, 0xfef2e20], [2, 0x12f6a440], [3, 0x9ad9a60], [4, 0xfa5d540], [5, 0xc99d540], [6, 0xbfbdf20], [7, 0xe28f120], [0, 0xff74e40]]}
  grad_acc_layer.1.intermediate.dense.bias:                               {input: bw_in1_add_96_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12fec460], [3, 0x9b5ba80]]}
  grad_acc_layer.1.intermediate.dense.weight:                             {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfadf560], [5, 0xca1f560], [6, 0xc03ff40], [7, 0xe311140], [0, 0xfff6e60], [1, 0xff75680], [2, 0x13004a80], [3, 0x9b740a0], [4, 0xfb40d80], [5, 0xca80d80], [6, 0xc0a1760], [7, 0xe372960]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xffd6ea0]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xbfb1c00]]}
  grad_acc_layer.1.attention.output.dense.bias:                           {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfbeb7c0]]}
  grad_acc_layer.1.attention.output.dense.weight:                         {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcb2b7c0], [6, 0xc14c1a0], [7, 0xe3d49c0], [0, 0x10059700]]}
  grad_acc_layer.1.attention.self.value.bias:                             {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1307f100]]}
  grad_acc_layer.1.attention.self.value.weight:                           {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd58c0], [4, 0xfba25a0], [5, 0xcae25a0], [6, 0xc102f80]]}
  grad_acc_layer.1.attention.self.key.bias:                               {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xc9909e0]]}
  grad_acc_layer.1.attention.self.key.weight:                             {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xbf689e0], [7, 0xe239be0], [0, 0xfea9c00], [1, 0xfea9c00]]}
  grad_acc_layer.1.attention.self.query.bias:                             {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfa51220]]}
  grad_acc_layer.1.attention.self.query.weight:                           {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe1f09c0], [0, 0xfe609e0], [1, 0xfe609e0], [2, 0x12f209e0]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x131e8aa0]]}
  grad_acc_layer.0.output.LayerNorm.weight:                               {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfcd00c0]]}
  grad_acc_layer.0.output.dense.bias:                                     {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcc59b00]]}
  grad_acc_layer.0.output.dense.weight:                                   {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [3, 3], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x100d7680], [2, 0x13166a80], [3, 0x9c69e00], [4, 0xfc4e0a0], [5, 0xcbd72a0], [6, 0xc20f240], [7, 0xe4f9a80], [0, 0x1018a2a0], [1, 0x101596a0]]}
  grad_acc_layer.0.intermediate.dense.bias:                               {input: bw_in1_add_43_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x102554e0], [1, 0x102248e0]]}
  grad_acc_layer.0.intermediate.dense.weight:                             {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1320d3e0], [3, 0x9d04c80], [4, 0xfcdcc20], [5, 0xcc65e20], [6, 0xc2dacc0], [7, 0xe5c5500], [0, 0x1026db00], [1, 0x1023cf00], [2, 0x1326ec00], [3, 0x9d664a0], [4, 0xfd3e440], [5, 0xccc7640]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe626d20]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1029e720]]}
  grad_acc_layer.0.attention.output.dense.bias:                           {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x100a2920]]}
  grad_acc_layer.0.attention.output.dense.weight:                         {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xfffc020], [2, 0x1308b420], [3, 0x9c1fb60], [4, 0xfbf8320]]}
  grad_acc_layer.0.attention.self.value.bias:                             {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc1ad9e0]]}
  grad_acc_layer.0.attention.self.value.weight:                           {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe41e420], [0, 0x100aec40], [1, 0x10045240], [2, 0x130d4640]]}
  grad_acc_layer.0.attention.self.key.bias:                               {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xc1b9d00]]}
  grad_acc_layer.0.attention.self.key.weight:                             {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe467640], [0, 0x100f7e60], [1, 0x1008e460], [2, 0x1311d860]]}
  grad_acc_layer.0.attention.self.query.bias:                             {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xfc41d80]]}
  grad_acc_layer.0.attention.self.query.weight:                           {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcb8e080], [6, 0xc1c6020], [7, 0xe4b0860], [0, 0x10141080]]}

graphs:
  fwd_0_0:
    target_device: 0
    input_count: 64
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    multiply_16: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone1522],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_17: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [multiply_16, attention_mask_s_brcst_m2_3_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 8}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_17],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 2], inputs: [add_17, softmax_18.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [12, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 8], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_18.dc.multiply.7: {type: multiply, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_0, softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_29: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [softmax_18.dc.multiply.7, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_33: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_37: {type: add, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_2, _fused_op_2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_2_buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_2_buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_3: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_4: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [buffer_0__fused_op_2__fused_op_4, _fused_op_3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [_fused_op_4, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_41: {type: matmul, grid_loc: [3, 6], grid_size: [4, 4], inputs: [_fused_op_5, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 24}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    gelu_44: {type: gelu, grid_loc: [4, 0], grid_size: [1, 3], inputs: [matmul_41],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [7, 0], grid_size: [1, 12], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_5_buffer_0__fused_op_5_add_51: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_5],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_5_add_51: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_5_buffer_0__fused_op_5_add_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [matmul_47, buffer_0__fused_op_5_add_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_6, _fused_op_6],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_6_buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [4, 10], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_6_buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_7: {type: fused_op, grid_loc: [5, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_8: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_0__fused_op_6__fused_op_8, _fused_op_7],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_9: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_8, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_55: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [_fused_op_9, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_61: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_9, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_67: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    multiply_69: {type: multiply, grid_loc: [5, 11], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone1544],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_70: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [multiply_69, attention_mask_s_brcst_m2_2_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 8}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [6, 5], grid_size: [1, 1], inputs: [add_70],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_10: {type: fused_op, grid_loc: [6, 10], grid_size: [1, 2], inputs: [add_70, softmax_71.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [12, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [_fused_op_10, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_11: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_11, lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_71.dc.multiply.7: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_10, softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_75: {type: matmul, grid_loc: [8, 8], grid_size: [1, 4], inputs: [_fused_op_9, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_82: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_71.dc.multiply.7, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_86: {type: matmul, grid_loc: [9, 1], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_9_buffer_0__fused_op_9_add_90: {type: nop, grid_loc: [9, 5], grid_size: [1, 1], inputs: [_fused_op_9],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_9_add_90: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [buffer_0__fused_op_9_buffer_0__fused_op_9_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [matmul_86, buffer_0__fused_op_9_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 64], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [9, 9], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [_fused_op_12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_12__fused_op_14: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1:
    target_device: 0
    input_count: 64
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_12_0, e2e__fused_op_12_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_13: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_14: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_buffer_0__fused_op_12__fused_op_14_0, _fused_op_13],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_15: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [_fused_op_14, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_94: {type: matmul, grid_loc: [0, 5], grid_size: [4, 4], inputs: [_fused_op_15, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 24}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    gelu_97: {type: gelu, grid_loc: [0, 9], grid_size: [1, 3], inputs: [matmul_94],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_15_buffer_0__fused_op_15_add_104: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_15],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_15_add_104: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_15_buffer_0__fused_op_15_add_104],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_104: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_100, buffer_0__fused_op_15_add_104],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, add_104],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_16, _fused_op_16],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_16_buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [_fused_op_16],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_16_buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_17: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_18: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0__fused_op_16__fused_op_18, _fused_op_17],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_19: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_18, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_2:
    target_device: 1
    input_count: 64
    matmul_108: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_19_0, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_114: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_19_0, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_120: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    multiply_122: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_fork_clone1562],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    add_123: {type: add, grid_loc: [0, 10], grid_size: [1, 1], inputs: [multiply_122, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 8}}}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_123],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_20: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [add_123, softmax_124.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [12, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_21, lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_124.dc.multiply.7: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_20, softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_128: {type: matmul, grid_loc: [1, 6], grid_size: [1, 4], inputs: [e2e__fused_op_19_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_135: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [softmax_124.dc.multiply.7, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_139: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_19_buffer_0__fused_op_19_add_143: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_19_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_19_add_143: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_0__fused_op_19_buffer_0__fused_op_19_add_143],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_139, buffer_0__fused_op_19_add_143],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_22: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1, add_143],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_22, _fused_op_22],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_22_buffer_0__fused_op_22_buffer_0__fused_op_22__fused_op_24: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [_fused_op_22],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_22_buffer_0__fused_op_22__fused_op_24: {type: nop, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_22_buffer_0__fused_op_22_buffer_0__fused_op_22__fused_op_24],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_22__fused_op_24: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_22_buffer_0__fused_op_22__fused_op_24],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_23: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_24: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [buffer_0__fused_op_22__fused_op_24, _fused_op_23],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_24, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_147: {type: matmul, grid_loc: [3, 4], grid_size: [4, 4], inputs: [_fused_op_25, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 24}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    gelu_150: {type: gelu, grid_loc: [3, 8], grid_size: [1, 3], inputs: [matmul_147],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [7, 0], grid_size: [1, 12], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_25_buffer_0__fused_op_25_add_157: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [_fused_op_25],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_25_add_157: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_25_buffer_0__fused_op_25_add_157],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [matmul_153, buffer_0__fused_op_25_add_157],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_26: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, add_157],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_26, _fused_op_26],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_26_buffer_0__fused_op_26_buffer_0__fused_op_26__fused_op_28: {type: nop, grid_loc: [4, 8], grid_size: [1, 1], inputs: [_fused_op_26],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_26_buffer_0__fused_op_26__fused_op_28: {type: nop, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_26_buffer_0__fused_op_26_buffer_0__fused_op_26__fused_op_28],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_26__fused_op_28: {type: nop, grid_loc: [4, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_26_buffer_0__fused_op_26__fused_op_28],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_27: {type: fused_op, grid_loc: [5, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_28: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_0__fused_op_26__fused_op_28, _fused_op_27],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_28, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_161: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [_fused_op_29, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_167: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [_fused_op_29, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_173: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    multiply_175: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175_fork_clone1580],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    add_176: {type: add, grid_loc: [6, 10], grid_size: [1, 1], inputs: [multiply_175, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 8}}}
    softmax_177.dc.reduce_max.0: {type: reduce, grid_loc: [6, 11], grid_size: [1, 1], inputs: [add_176],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_30: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 2], inputs: [add_176, softmax_177.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [12, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [_fused_op_30, lc.input_tensor.softmax_177.dc.reduce_sum.3.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_31: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_177.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_177.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [_fused_op_31, lc.input_tensor.softmax_177.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_177.dc.multiply.7: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [_fused_op_30, softmax_177.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_181: {type: matmul, grid_loc: [8, 6], grid_size: [1, 4], inputs: [_fused_op_29, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    matmul_188: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [softmax_177.dc.multiply.7, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    matmul_192: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 12}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_29_buffer_0__fused_op_29_add_196: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [_fused_op_29],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_29_add_196: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [buffer_0__fused_op_29_buffer_0__fused_op_29_add_196],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_196: {type: add, grid_loc: [9, 5], grid_size: [1, 1], inputs: [matmul_192, buffer_0__fused_op_29_add_196],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 64], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_32: {type: fused_op, grid_loc: [9, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.1, layernorm_197.dc.reduce_sum.0.lc1, add_196],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [9, 11], grid_size: [1, 1], inputs: [_fused_op_32, _fused_op_32],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_32_buffer_0__fused_op_32_buffer_0__fused_op_32__fused_op_34: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [_fused_op_32],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_32_buffer_0__fused_op_32__fused_op_34: {type: nop, grid_loc: [9, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_32_buffer_0__fused_op_32_buffer_0__fused_op_32__fused_op_34],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_32__fused_op_34: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_32_buffer_0__fused_op_32__fused_op_34],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_3:
    target_device: 1
    input_count: 64
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_197.dc.multiply.4_0, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_197.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_34: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_buffer_0__fused_op_32__fused_op_34_0, _fused_op_33],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_35: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_34, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_200: {type: matmul, grid_loc: [0, 4], grid_size: [4, 4], inputs: [_fused_op_35, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 24}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    gelu_203: {type: gelu, grid_loc: [0, 8], grid_size: [1, 3], inputs: [matmul_200],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, u_kt: 6}}
    buffer_0__fused_op_35_buffer_0__fused_op_35_add_210: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_35],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_35_add_210: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_35_buffer_0__fused_op_35_add_210],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_206, buffer_0__fused_op_35_add_210],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_36: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1, add_210],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 40], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_36, _fused_op_36],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_36_buffer_0__fused_op_36_buffer_0__fused_op_36__fused_op_38: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [_fused_op_36],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_36_buffer_0__fused_op_36__fused_op_38: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_36_buffer_0__fused_op_36_buffer_0__fused_op_36__fused_op_38],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_36__fused_op_38: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_36_buffer_0__fused_op_36__fused_op_38],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_37: {type: fused_op, grid_loc: [2, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_211.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_38: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_0__fused_op_36__fused_op_38, _fused_op_37],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_39: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_38, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_2: 48}}}
    _fused_op_39_output_nop_0: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [_fused_op_39], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  bwd_0_4:
    target_device: 1
    input_count: 64
    bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoders.output_layernorm_211], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e__fused_op_38_0, loss_bert_encoders.output_layernorm_211],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_40: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [loss_bert_encoders.output_layernorm_211, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_40, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_40, e2e__fused_op_38_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_41: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 2], inputs: [e2e__fused_op_38_0, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6, _fused_op_40, e2e__fused_op_37_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_208_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0, _fused_op_41], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_206_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 12], inputs: [_fused_op_41, layer.3.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_206_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [3, 1], inputs: [e2e_gelu_203_0], grid_transpose: true,
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_206_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [3, 3], inputs: [bw_in1_matmul_206_transpose_0, _fused_op_41], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_42: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 12], inputs: [e2e_matmul_200_0, bw_in0_matmul_206_matmul_1],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in1_add_202_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0, _fused_op_42], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_200_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 8], inputs: [_fused_op_42, layer.3.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 3}}
    bw_in1_matmul_200_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [4, 1], inputs: [e2e__fused_op_35_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_200_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_200_transpose_0, _fused_op_42], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_41_buffer_0__fused_op_41_bw_in0_layernorm_197_combine_add_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_41],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_41_bw_in0_layernorm_197_combine_add_0: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0__fused_op_41_buffer_0__fused_op_41_bw_in0_layernorm_197_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_197_combine_add_0: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_41_bw_in0_layernorm_197_combine_add_0, bw_in0_matmul_200_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_197_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, bw_in0_layernorm_197_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_43: {type: fused_op, grid_loc: [4, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_combine_add_0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_43, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_43, e2e__fused_op_34_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_44: {type: fused_op, grid_loc: [6, 7], grid_size: [1, 2], inputs: [e2e__fused_op_34_0, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6, _fused_op_43, e2e__fused_op_33_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_194_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0, _fused_op_44], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_192_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 4], inputs: [_fused_op_44, layer.3.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_192_transpose_0: {type: nop, grid_loc: [7, 8], grid_size: [4, 1], inputs: [e2e_matmul_188_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_192_matmul_1: {type: matmul, grid_loc: [8, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_192_transpose_0, _fused_op_44], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_188_matmul_1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [bw_in0_matmul_192_matmul_1, e2e_matmul_181_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_188_transpose_0: {type: nop, grid_loc: [8, 9], grid_size: [1, 1], inputs: [e2e_softmax_177.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_188_matmul_1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [bw_in1_matmul_188_transpose_0, bw_in0_matmul_192_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_183_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0, bw_in1_matmul_188_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_188_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_181_matmul_1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 4], inputs: [bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0, layer.3.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_181_transpose_0: {type: nop, grid_loc: [9, 5], grid_size: [4, 1], inputs: [e2e__fused_op_29_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_0_5:
    target_device: 1
    input_count: 64
    bw_in1_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_181_transpose_0_0, e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_177_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_188_matmul_1_0, e2e_softmax_177.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_177_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_45: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_188_matmul_1_0, bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_177.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 45}}
    bw_in0_multiply_175_multiply_0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [_fused_op_45, input_1_multiply_175],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    buffer_0_bw_in0_multiply_175_multiply_0_bw_in0_matmul_173_matmul_1: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_multiply_175_multiply_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [240], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_matmul_173_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [buffer_0_bw_in0_multiply_175_multiply_0_bw_in0_matmul_173_matmul_1, e2e_matmul_167_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [80, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_173_transpose_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_matmul_161_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_173_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_173_transpose_0, bw_in0_multiply_175_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_169_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0, bw_in1_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [0, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_173_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_167_matmul_1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 4], inputs: [bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0, layer.3.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_167_transpose_0: {type: nop, grid_loc: [0, 10], grid_size: [4, 1], inputs: [e2e__fused_op_29_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_167_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_167_transpose_0, bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_163_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0, bw_in0_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_161_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_173_matmul_1, layer.3.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_161_transpose_0: {type: nop, grid_loc: [1, 9], grid_size: [4, 1], inputs: [e2e__fused_op_29_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_161_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_161_transpose_0, bw_in0_matmul_173_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_46: {type: fused_op, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_181_matmul_1_0, bw_in0_matmul_167_matmul_1, bw_in0_matmul_161_matmul_1, e2e__fused_op_44_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 46}}
    bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_46], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [e2e__fused_op_28_0, _fused_op_46],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_47: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_46, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_47, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_47, e2e__fused_op_28_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_48: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 2], inputs: [e2e__fused_op_28_0, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6, _fused_op_47, e2e__fused_op_27_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_155_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0, _fused_op_48], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_153_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 12], inputs: [_fused_op_48, layer.2.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_153_transpose_0: {type: nop, grid_loc: [4, 0], grid_size: [3, 1], inputs: [e2e_gelu_150_0], grid_transpose: true,
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_153_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [3, 3], inputs: [bw_in1_matmul_153_transpose_0, _fused_op_48], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 12], inputs: [e2e_matmul_147_0, bw_in0_matmul_153_matmul_1],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 8], inputs: [_fused_op_49, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 3}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [4, 1], inputs: [e2e__fused_op_25_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_0_6:
    target_device: 1
    input_count: 64
    bw_in1_add_149_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0, e2e__fused_op_49_0], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 3], inputs: [e2e_bw_in1_matmul_147_transpose_0_0, e2e__fused_op_49_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_144_combine_add_0: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e__fused_op_48_0, e2e_bw_in0_matmul_147_matmul_1_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_144_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e__fused_op_24_0, bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_50: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_combine_add_0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_50, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [_fused_op_50, e2e__fused_op_24_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_51: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_24_0, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6, _fused_op_50, e2e__fused_op_23_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_141_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0, _fused_op_51], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_139_matmul_1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 4], inputs: [_fused_op_51, layer.2.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_139_transpose_0: {type: nop, grid_loc: [1, 11], grid_size: [4, 1], inputs: [e2e_matmul_135_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_139_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_139_transpose_0, _fused_op_51], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_135_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_139_matmul_1, e2e_matmul_128_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_135_transpose_0: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [e2e_softmax_124.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_135_matmul_1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_135_transpose_0, bw_in0_matmul_139_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_130_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0, bw_in1_matmul_135_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [bw_in1_matmul_135_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [3, 8], grid_size: [4, 1], inputs: [e2e__fused_op_19_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [3, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, e2e_softmax_124.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_52: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_124.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 45}}
    bw_in0_multiply_122_multiply_0: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_52, input_1_multiply_122],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    buffer_0_bw_in0_multiply_122_multiply_0_bw_in0_matmul_120_matmul_1: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_multiply_122_multiply_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_matmul_120_matmul_1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [buffer_0_bw_in0_multiply_122_multiply_0_bw_in0_matmul_120_matmul_1, e2e_matmul_114_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_120_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e_matmul_108_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_120_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_120_transpose_0, bw_in0_multiply_122_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_116_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0, bw_in1_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [4, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_120_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_114_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_114_transpose_0: {type: nop, grid_loc: [5, 4], grid_size: [4, 1], inputs: [e2e__fused_op_19_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_114_matmul_1: {type: matmul, grid_loc: [5, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_114_transpose_0, bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_110_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_108_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_120_matmul_1, layer.2.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_108_transpose_0: {type: nop, grid_loc: [6, 4], grid_size: [4, 1], inputs: [e2e__fused_op_19_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_108_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [4, 1], inputs: [bw_in1_matmul_108_transpose_0, bw_in0_matmul_120_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_51_chip_to_chip_nop_0: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  bwd_0_7:
    target_device: 0
    input_count: 64
    buffer_0__fused_op_51_buffer_0__fused_op_51__fused_op_53: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_51_chip_to_chip_nop_0_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_51__fused_op_53: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_51_buffer_0__fused_op_51__fused_op_53],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_53: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in0_matmul_114_matmul_1_0, e2e_bw_in0_matmul_108_matmul_1_0, buffer_0__fused_op_51__fused_op_53],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 46}}
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_53], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_18_0, _fused_op_53],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_54: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_53, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_54, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [_fused_op_54, e2e__fused_op_18_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_55: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 2], inputs: [e2e__fused_op_18_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, _fused_op_54, e2e__fused_op_17_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, _fused_op_55], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 12], inputs: [_fused_op_55, layer.1.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [3, 1], inputs: [e2e_gelu_97_0], grid_transpose: true,
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [3, 3], inputs: [bw_in1_matmul_100_transpose_0, _fused_op_55], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_56: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 12], inputs: [e2e_matmul_94_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in1_add_96_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0, _fused_op_56], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [_fused_op_56, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 3}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [2, 7], grid_size: [4, 1], inputs: [e2e__fused_op_15_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [6, 8], grid_size: [4, 3], inputs: [bw_in1_matmul_94_transpose_0, _fused_op_56], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_55_buffer_0__fused_op_55_bw_in0_layernorm_91_combine_add_0: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_55],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_55_bw_in0_layernorm_91_combine_add_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [buffer_0__fused_op_55_buffer_0__fused_op_55_bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_0__fused_op_55_bw_in0_layernorm_91_combine_add_0, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [e2e__fused_op_14_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_57: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_57, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 8], grid_size: [1, 1], inputs: [_fused_op_57, e2e__fused_op_14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_58: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 2], inputs: [e2e__fused_op_14_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, _fused_op_57, e2e__fused_op_13_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, _fused_op_58], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 4], inputs: [_fused_op_58, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [4, 1], inputs: [e2e_matmul_82_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_86_transpose_0, _fused_op_58], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in0_matmul_86_matmul_1, e2e_matmul_75_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, bw_in0_matmul_86_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 4], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [9, 8], grid_size: [4, 1], inputs: [e2e__fused_op_9_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_0_8:
    target_device: 0
    input_count: 64
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_82_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [0, 1], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_75_transpose_0_0, e2e_bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_82_matmul_1_0, e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_59: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_82_matmul_1_0, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 45}}
    bw_in0_multiply_69_multiply_0: {type: multiply, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_59, input_1_multiply_69],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    buffer_0_bw_in0_multiply_69_multiply_0_bw_in0_matmul_67_matmul_1: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_multiply_69_multiply_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [240], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_bw_in0_multiply_69_multiply_0_bw_in0_matmul_67_matmul_1, e2e_matmul_61_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [80, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, bw_in0_multiply_69_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 4], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [1, 6], grid_size: [4, 1], inputs: [e2e__fused_op_9_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [1, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [4, 1], inputs: [e2e__fused_op_9_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [2, 9], grid_size: [4, 1], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_60: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_75_matmul_1_0, bw_in0_matmul_61_matmul_1, bw_in0_matmul_55_matmul_1, e2e__fused_op_58_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 46}}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_60], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [e2e__fused_op_8_0, _fused_op_60],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_61: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_60, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_61, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_61, e2e__fused_op_8_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_62: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 2], inputs: [e2e__fused_op_8_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, _fused_op_61, e2e__fused_op_7_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, _fused_op_62], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_62, layer.0.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [3, 10], grid_size: [3, 1], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [3, 3], inputs: [bw_in1_matmul_47_transpose_0, _fused_op_62], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}

  bwd_0_9:
    target_device: 0
    input_count: 64
    _fused_op_63: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 12], inputs: [e2e_matmul_41_0, e2e_bw_in0_matmul_47_matmul_1_0],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in1_add_43_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0, _fused_op_63], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [2, 8], inputs: [_fused_op_63, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 3}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_5_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [2, 8], grid_size: [4, 3], inputs: [bw_in1_matmul_41_transpose_0, _fused_op_63], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [e2e__fused_op_62_0, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [e2e__fused_op_4_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_64: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 40}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [_fused_op_64, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [256, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_64, e2e__fused_op_4_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_65: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 2], inputs: [e2e__fused_op_4_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, _fused_op_64, e2e__fused_op_3_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 40, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 41}}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, _fused_op_65], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [_fused_op_65, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [5, 0], grid_size: [4, 1], inputs: [e2e_matmul_29_0], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [5, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_33_transpose_0, _fused_op_65], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [input_1], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [6, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_22_transpose_0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_66: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 45}}
    bw_in0_multiply_16_multiply_0: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [_fused_op_66, input_1_multiply_16],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [bw_in0_multiply_16_multiply_0, e2e_matmul_8_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, bw_in0_multiply_16_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [7, 1], grid_size: [4, 1], inputs: [input_1], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [7, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_8_transpose_0, bw_in1_matmul_14_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [4, 1], inputs: [input_1], grid_transpose: true,
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_2_transpose_0, bw_in0_matmul_14_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 64, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0_shadow: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0, $gptr_q3_shadow: 0, $gptr_q3: 0, $gptr_q2: 0, $gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q1: 0}
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone1522: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone1544: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               e2e__fused_op_12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_0__fused_op_12__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_2, queue_settings: {
               e2e__fused_op_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_fork_clone1562: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_fork_clone1580: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_0_3, queue_settings: {
               e2e_layernorm_197.dc.multiply.4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_buffer_0__fused_op_32__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    - endloop

  - run_bwd_0:
    - param: [$p_loop_count, $p_zero_grad]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 64, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q1_shadow: 0, $gptr_q0: 0, $gptr_q12: 0, $gptr_q11: 0, $lptr_q11: 0, $lptr_q8: 0, $lptr_q9: 0, $gptr_q1: 0, $lptr_q12: 0, $lptr_q13: 0, $lptr_q2: 0, $gptr_q9: 0, $gptr_q13: 0, $gptr_q3: 0, $gptr_q2: 0, $lptr_q14: 0, $gptr_q14: 0, $gptr_q8: 0, $lptr_q7: 0, $gptr_q7: 0, $gptr_q6: 0, $gptr_q4: 0, $gptr_q7_shadow: 0, $lptr_q6: 0, $lptr_q5: 0, $gptr_q10: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q1: 0, $lptr_q10: 0, $lptr_q3: 0, $lptr_q0: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   varinst: [$gptr_q7, set, $gptr_q7_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: bwd_0_4, queue_settings: {
               loss_bert_encoders.output_layernorm_211: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e__fused_op_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_softmax_177.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_181_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_188_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_200_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_gelu_203_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_37_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: bwd_0_5, queue_settings: {
               e2e__fused_op_25_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_147_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_27_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_167_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_softmax_177.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_matmul_188_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_matmul_181_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in1_matmul_181_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_175: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: bwd_0_6, queue_settings: {
               e2e__fused_op_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_softmax_124.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_24_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_48_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_49_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in0_matmul_147_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in1_matmul_147_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: bwd_0_7, queue_settings: {
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_softmax_71.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_13_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_bw_in0_matmul_128_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_matmul_114_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_matmul_108_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_51_chip_to_chip_nop_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: bwd_0_8, queue_settings: {
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_softmax_71.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_matmul_82_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_82_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_matmul_75_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_75_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: bwd_0_9, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_softmax_18.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e__fused_op_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_matmul_47_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 64, $c_one: 1, $c_zero: 0}


fused_ops:
  0: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 1], ublock: [2, 2], output: dest}
        - softmax_18.dc.exp.2: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.add.5: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.6: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.2: { type: multiply, inputs: [input0, input1], mblock: [2, 6], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.subtract.3: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 6], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  4: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 6], ublock: [2, 4], output: output}
  5: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.13: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  40: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  41: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.4: { type: multiply, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_211_layernorm_bw_0.dc.add.5: { type: add, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.7: { type: multiply, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_211_layernorm_bw_0.dc.subtract.8: { type: subtract, inputs: [input4, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: dest}
        - bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.9: { type: multiply, inputs: [dest, input5], input_1_tms: [tile_broadcast: c], mblock: [2, 3], ublock: [2, 4], output: output}
  42: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_gelu_203_gelu_derivative_0: { type: gelu_derivative, inputs: [input0], mblock: [2, 2], ublock: [2, 4], output: dest}
        - bw_in0_gelu_203_multiply_1: { type: multiply, inputs: [dest, input1], mblock: [2, 2], ublock: [2, 4], output: output}
  45: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - bw_in0_softmax_177_softmax_bw_0.dc.subtract.2: { type: subtract, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_softmax_177_softmax_bw_0.dc.multiply.3: { type: multiply, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: output}
  46: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - bw_in0_reshape_159.dc.squeeze.0_combine_add_0: { type: add, inputs: [input0, input1], mblock: [2, 6], ublock: [2, 4], output: dest}
        - bw_in0_reshape_159.dc.squeeze.0_combine_add_1: { type: add, inputs: [dest, input2], mblock: [2, 6], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_158_combine_add_0: { type: add, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 6], ublock: [2, 4], output: output}
