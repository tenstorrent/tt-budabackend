# git checkout ed21b6ed8
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 4 --chips 1 --microbatch 48 --layers 3 --microbatch_count 4 --loop_count 0 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=85000 PYBUDA_FORCE_INTERMED_TO_OUTPUT_DF=1 PYBUDA_FORK_JOIN_BUF_MULTIPLE=4 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1  --training --auto_transpose

devices:
  arch: grayskull

test-config:
  stimulus-config:
    type: Normal
    normal_mean: 0.5
    normal_stddev: 0.1
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoders.output_layernorm_158]
    outputs: [bert_encoders.output_layernorm_158]
  test-args:
    sequence_lenth: 384
    head_size: 16

queues:

  # input
  input_1:                                                                                       {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                                                {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31a40020]]}

  # output
  bert_encoders.output_layernorm_158:                                                            {input: _fused_op_35_output_nop_0, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa4b2260], [1, 0xa4a9ac0], [2, 0x8b518c0], [3, 0xd31b020]]}
  layer.0.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcc95640], [5, 0x98dea60], [6, 0xb23efa0], [7, 0x84a5620]]}
  layer.0.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa4f8280], [1, 0xa4efae0], [2, 0x8b978e0], [3, 0xd361040]]}
  layer.0.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcc97960], [5, 0x98e0d80], [6, 0xb2412c0], [7, 0x84a7940]]}
  layer.0.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcc99c80], [5, 0x98e30a0], [6, 0xb2435e0], [7, 0x84a9c60]]}
  layer.0.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa53e720], [1, 0xa535b00], [2, 0x8bddd80], [3, 0xd3b8880]]}
  layer.0.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xccdfca0], [5, 0x99290c0], [6, 0xb289600], [7, 0x84efc80]]}
  layer.0.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa540a40], [1, 0xa537e20], [2, 0x8be00a0], [3, 0xd3baba0]]}
  layer.0.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcd26140]]}
  layer.0.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9992100]]}
  layer.0.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb2cfaa0], [7, 0x857bcc0], [0, 0xa588d80], [1, 0xa580160], [2, 0x8c283e0], [3, 0xd3be060], [4, 0xcd2ed60], [5, 0x999ad20], [6, 0xb315ac0], [7, 0x85c1ce0], [0, 0xa5ceda0], [1, 0xa5c6180], [2, 0x8c6e400], [3, 0xd404080], [4, 0xcd74d80], [5, 0x99e0d40]]}
  layer.0.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb35bae0], [7, 0x8607d00], [0, 0xa614dc0], [1, 0xa60c1a0]]}
  layer.0.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8cb4420], [3, 0xd44a0a0], [4, 0xcdbada0], [5, 0x9a26d60], [6, 0xb364700], [7, 0x8610920], [0, 0xa61d9e0], [1, 0xa614dc0], [2, 0x8cfa440], [3, 0xd4900c0], [4, 0xce00dc0], [5, 0x9a6cd80], [6, 0xb3aa720], [7, 0x8656940], [0, 0xa663a00], [1, 0xa65ade0]]}
  layer.0.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8d40460], [3, 0xd4d60e0], [4, 0xce46de0], [5, 0x9ab2da0]]}
  layer.0.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd4d8400]]}
  layer.0.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xce49100]]}
  layer.1.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd0e1160], [4, 0xca62080], [5, 0x96a3a20], [6, 0xb003f60]]}
  layer.1.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x823fac0], [0, 0xa2722e0], [1, 0xa246280], [2, 0x8998620]]}
  layer.1.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd096680], [4, 0xca175a0], [5, 0x9647ba0], [6, 0xaf31a80]]}
  layer.1.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8241de0], [0, 0xa274600], [1, 0xa2485a0], [2, 0x899a940]]}
  layer.1.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xaf77aa0], [7, 0x8244100], [0, 0xa276920], [1, 0xa24a8c0]]}
  layer.1.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x899cc60], [3, 0xd0dcb20], [4, 0xca5da40], [5, 0x969f3e0]]}
  layer.1.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xafbdac0], [7, 0x828a120], [0, 0xa2bc940], [1, 0xa2908e0]]}
  layer.1.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x899ef80], [3, 0xd0dee40], [4, 0xca5fd60], [5, 0x96a1700]]}
  layer.1.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xaf28e60]]}
  layer.1.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x82f3160]]}
  layer.1.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa302de0], [1, 0xa2d7aa0], [2, 0x89a2440], [3, 0xd127180], [4, 0xcaa80a0], [5, 0x96e9a40], [6, 0xb049f80], [7, 0x82fbd80], [0, 0xa348e00], [1, 0xa31dac0], [2, 0x89e8460], [3, 0xd16d1a0], [4, 0xcaee0c0], [5, 0x972fa60], [6, 0xb08ffa0], [7, 0x8341da0]]}
  layer.1.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa38ee20], [1, 0xa363ae0], [2, 0x8a2e480], [3, 0xd1b31c0]]}
  layer.1.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcb340e0], [5, 0x9775a80], [6, 0xb0d5fc0], [7, 0x8387dc0], [0, 0xa397a40], [1, 0xa36c700], [2, 0x8a370a0], [3, 0xd1bbde0], [4, 0xcb7a100], [5, 0x97bbaa0], [6, 0xb11bfe0], [7, 0x83cdde0], [0, 0xa3dda60], [1, 0xa3b2720], [2, 0x8a7d0c0], [3, 0xd201e00]]}
  layer.1.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcbc0120], [5, 0x9801ac0], [6, 0xb162000], [7, 0x8413e00]]}
  layer.1.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9803de0]]}
  layer.1.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb164320]]}
  layer.2.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8535ca0], [0, 0xa542d60], [1, 0xa53a140], [2, 0x8be23c0]]}
  layer.2.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x84a2160], [0, 0xa4aff40], [1, 0xa4a77a0], [2, 0x8b4f5a0]]}
  layer.2.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd2d5000], [4, 0xcc4f620], [5, 0x9898a40], [6, 0xb1f8f80]]}
  layer.2.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb47ef20], [7, 0x874dce0], [0, 0xa738200], [1, 0xa730300]]}
  layer.2.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd6d3d40], [4, 0xcff6b60], [5, 0x9c6bb60], [6, 0xb59a8e0]]}
  layer.2.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x888b520], [0, 0xa85b640], [1, 0xa84afa0], [2, 0x8f02720]]}
  layer.2.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd719d60], [4, 0xd03cb80], [5, 0x9cb1b80], [6, 0xb5e0900]]}
  layer.2.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x888d840], [0, 0xa85d960], [1, 0xa84d2c0], [2, 0x8f04a40]]}
  layer.2.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9cf8020]]}
  layer.2.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xd0a5bc0]]}
  layer.2.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa7cf180], [1, 0xa7beae0], [2, 0x8e64ec0], [3, 0xd647d00], [4, 0xcf6ab20], [5, 0x9bdfb20], [6, 0xb50e8a0], [7, 0x87ff4e0], [0, 0xa8151a0], [1, 0xa804b00], [2, 0x8eaaee0], [3, 0xd68dd20], [4, 0xcfb0b40], [5, 0x9c25b40], [6, 0xb5548c0], [7, 0x8845500]]}
  layer.2.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x888fb60], [0, 0xa85fc80], [1, 0xa84f5e0], [2, 0x8f06d60]]}
  layer.2.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb3f0bc0], [7, 0x86bf980], [0, 0xa6a9ea0], [1, 0xa6a1fa0], [2, 0x8d43920], [3, 0xd4e1020], [4, 0xce51d20], [5, 0x9abdce0], [6, 0xb436be0], [7, 0x87059a0], [0, 0xa6efec0], [1, 0xa6e7fc0], [2, 0x8d89940], [3, 0xd527040], [4, 0xce97d40], [5, 0x9b03d00]]}
  layer.2.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb47cc00], [7, 0x874b9c0], [0, 0xa735ee0], [1, 0xa72dfe0]]}
  layer.2.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8dcfde0]]}
  layer.2.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd590080]]}

  # constant
  input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa53e2a0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8bdd900]]}
  dc.input_tensor.softmax_18.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd3a7060]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcd25cc0]]}
  dc.input_tensor.layernorm_38.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x996f0e0]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb2cf620]]}
  dc.input_tensor.layernorm_38.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x84a4480]]}
  dc.input_tensor.layernorm_38.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd3bcec0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb3f0740]]}
  dc.input_tensor.layernorm_52.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x869c960]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa6a9a20]]}
  dc.input_tensor.layernorm_52.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa6a0e00]]}
  dc.input_tensor.layernorm_52.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8d42780]]}
  input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd0dc6a0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xca5d5c0]]}
  dc.input_tensor.softmax_71.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x968dbc0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb003ae0]]}
  dc.input_tensor.layernorm_91.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x82d0140]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa302960]]}
  dc.input_tensor.layernorm_91.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa2d6900]]}
  dc.input_tensor.layernorm_91.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x89a12a0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa423a80]]}
  dc.input_tensor.layernorm_105.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa3f8740]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8ac30e0]]}
  dc.input_tensor.layernorm_105.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd247e20]]}
  dc.input_tensor.layernorm_105.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcbc2440]]}
  input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa85b1c0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa84ab20]]}
  dc.input_tensor.softmax_124.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8ef0f00]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd75fd80]]}
  dc.input_tensor.layernorm_144.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xd082ba0]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9cf7ba0]]}
  dc.input_tensor.layernorm_144.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb626920]]}
  dc.input_tensor.layernorm_144.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd760200]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8dcf960]]}
  dc.input_tensor.layernorm_158.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd56d060]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xceddd60]]}
  dc.input_tensor.layernorm_158.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9b49d20]]}
  dc.input_tensor.layernorm_158.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb481240]]}
  lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcede1e0]]}
  lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8750000]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa732620]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8dd8a00]]}
  dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd598ca0]]}
  lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xcede660]]}
  lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb50e420]]}
  lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fe0ba0]]}
  lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x933b320]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8623660]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7fada40]]}
  dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xac1c9e0]]}
  lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xad69740]]}
  lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9ff2380]]}
  lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ff3ee0]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9ff1f00]]}
  lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fd7b00]]}
  lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ff4360]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xad692c0]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x86b8740]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x940d800]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ffd400]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9ffb420]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fe1020]]}
  lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92ae9e0]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9f88a40]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcd78a40]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc5d09a0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x933aea0]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9f88ec0]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9f91660]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x956c1a0]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcfc38a0]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc933880]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x956c620]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcfc3d20]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc933400]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xae9c9a0]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa1e5e20]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa1b9dc0]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x88e95c0]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd00a1c0]]}
  lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9647720]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcee87a0]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9499cc0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80898c0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa0aa480]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa06d4e0]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8745080]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x86b8bc0]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xae44840]]}
  input_1_multiply_16_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80cfd60]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa0cd920]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xae44cc0]]}

  # epoch_to_epoch
  e2e__fused_op_17_0:                                                                            {input: _fused_op_17, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5cd0940]]}
  e2e__fused_op_18_0:                                                                            {input: _fused_op_18, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb627ac0]]}
  e2e__fused_op_30_0:                                                                            {input: _fused_op_30, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xd0ae7e0]]}
  e2e__fused_op_34_0:                                                                            {input: _fused_op_34, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x12cb1420]]}
  e2e__fused_op_33_0:                                                                            {input: _fused_op_33, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x125fe860]]}
  e2e_gelu_150_0:                                                                                {input: gelu_150, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xca38800], [0, 0xd09a920], [1, 0xd648280], [2, 0xbcffa00]]}
  e2e__fused_op_31_0:                                                                            {input: _fused_op_31, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x11271400], [4, 0x10bbe840], [5, 0xec92ca0], [6, 0x11f27b40]]}
  e2e__fused_op_29_0:                                                                            {input: _fused_op_29, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd7613a0]]}
  e2e__fused_op_28_0:                                                                            {input: _fused_op_28, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9d00c40]]}
  e2e_matmul_135_0:                                                                              {input: matmul_135, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xd067ae0]]}
  e2e_matmul_128_0:                                                                              {input: matmul_128, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8898780], [0, 0xa8688a0], [1, 0xa858200], [2, 0x8f0f980]]}
  e2e__fused_op_26_0:                                                                            {input: _fused_op_26, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dd2c60]]}
  e2e_bw_in1_matmul_135_matmul_1_0:                                                              {input: bw_in1_matmul_135_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xe478820]]}
  e2e_bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0_0:                                             {input: bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x106d2cc0]]}
  e2e__fused_op_23_0:                                                                            {input: _fused_op_23, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f287a0]]}
  e2e_bw_in0_matmul_135_matmul_1_0:                                                              {input: bw_in0_matmul_135_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x13967b60]]}
  e2e_matmul_114_0:                                                                              {input: matmul_114, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xc3a87e0], [0, 0xca0a900], [1, 0xcfb8260], [2, 0xb66f9e0]]}
  e2e_matmul_108_0:                                                                              {input: matmul_108, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaee8220], [2, 0x959f9a0], [3, 0xf1a13c0], [4, 0xeaee800]]}
  e2e__fused_op_40_0:                                                                            {input: _fused_op_40, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xeada940]]}
  e2e__fused_op_22_0:                                                                            {input: _fused_op_22, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9c2f9c0]]}
  e2e__fused_op_21_0:                                                                            {input: _fused_op_21, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaef88c0]]}
  e2e_gelu_97_0:                                                                                 {input: gelu_97, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xf8313e0], [4, 0xf17e820], [5, 0xd252c80], [6, 0x104e7b20]]}
  e2e__fused_op_19_0:                                                                            {input: _fused_op_19, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xeaa7b00], [7, 0xa9687c0], [0, 0xafca8e0], [1, 0xb578240]]}
  e2e__fused_op_45_0:                                                                            {input: _fused_op_45, type: queue, entries: 48, grid_size: [2, 8], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xd73fa20], [3, 0x146f1440], [4, 0x126d0880], [5, 0x10d62ce0], [6, 0x14687b80], [7, 0xeb08840], [0, 0xf16a960], [1, 0xf7182c0], [2, 0xd8e3a40], [3, 0x14895460], [4, 0x128748a0], [5, 0x10f06d00], [6, 0x1482bba0], [7, 0xecac860], [0, 0xf30e980], [1, 0xf8bc2e0]]}
  e2e__fused_op_44_0:                                                                            {input: _fused_op_44, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xf0882a0]]}
  e2e__fused_op_16_0:                                                                            {input: _fused_op_16, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  e2e_matmul_82_0:                                                                               {input: matmul_82, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900]]}
  e2e_matmul_75_0:                                                                               {input: matmul_75, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900], [1, 0x4fb0900], [2, 0x4fb0900], [3, 0x4fb0900]]}
  e2e__fused_op_14_0:                                                                            {input: _fused_op_14, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900]]}
  e2e__fused_op_11_0:                                                                            {input: _fused_op_11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  e2e_matmul_61_0:                                                                               {input: matmul_61, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5640920], [1, 0x5640920], [2, 0x5640920], [3, 0x5640920]]}
  e2e_matmul_55_0:                                                                               {input: matmul_55, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8430920], [5, 0x69f0920], [6, 0x69f0920], [7, 0x5082920]]}
  e2e__fused_op_10_0:                                                                            {input: _fused_op_10, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5cd0940]]}
  e2e__fused_op_9_0:                                                                             {input: _fused_op_9, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5cd0940]]}
  e2e__fused_op_51_0:                                                                            {input: _fused_op_51, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xda87a60]]}
  e2e_gelu_44_0:                                                                                 {input: gelu_44, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5712940], [0, 0x7710960], [1, 0x7710960], [2, 0x5da2960]]}
  e2e__fused_op_7_0:                                                                             {input: _fused_op_7, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5cd0940], [4, 0x8ac0940], [5, 0x7080940], [6, 0x7080940]]}
  e2e__fused_op_6_0:                                                                             {input: _fused_op_6, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7710960]]}
  e2e__fused_op_5_0:                                                                             {input: _fused_op_5, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa500960]]}
  e2e__fused_op_4_0:                                                                             {input: _fused_op_4, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8ac0960]]}
  e2e_matmul_29_0:                                                                               {input: matmul_29, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8ac0960]]}
  e2e_matmul_22_0:                                                                               {input: matmul_22, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7152960], [0, 0x9150980], [1, 0x9150980], [2, 0x77e2980]]}
  e2e__fused_op_2_0:                                                                             {input: _fused_op_2, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97e09a0]]}
  e2e_bw_in1_matmul_29_matmul_1_0:                                                               {input: bw_in1_matmul_29_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x14a39480]]}
  e2e_matmul_8_0:                                                                                {input: matmul_8, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9150980], [4, 0xbf40980], [5, 0x8b92980], [6, 0xa500980]]}
  e2e_matmul_2_0:                                                                                {input: matmul_2, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x77e2980], [0, 0x97e09a0], [1, 0x97e09a0], [2, 0x7e729a0]]}
  e2e_bw_in1_matmul_14_matmul_1_0:                                                               {input: bw_in1_matmul_14_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x12a188c0]]}
  e2e_bw_in0_matmul_14_matmul_1_0:                                                               {input: bw_in0_matmul_14_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x110aad20]]}

  # loss
  loss_bert_encoders.output_layernorm_158:                                                       {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31b12040]]}

  # grad_accumulator
  grad_acc_layer.2.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9ab50c0]]}
  grad_acc_layer.2.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa73a520]]}
  grad_acc_layer.2.output.dense.bias:                                                            {input: bw_in1_add_155_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9b4aec0]]}
  grad_acc_layer.2.output.dense.weight:                                                          {input: bw_in1_matmul_153_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb4823e0], [7, 0x8750480], [0, 0xa743140], [1, 0xa732aa0], [2, 0x8dd8e80], [3, 0xd5bbcc0], [4, 0xcedeae0], [5, 0x9b53ae0], [6, 0xb4c8400], [7, 0x87964a0], [0, 0xa789160], [1, 0xa778ac0], [2, 0x8e1eea0], [3, 0xd601ce0], [4, 0xcf24b00], [5, 0x9b99b00]]}
  grad_acc_layer.2.intermediate.dense.bias:                                                      {input: bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x87dc4c0]]}
  grad_acc_layer.2.intermediate.dense.weight:                                                    {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8416120], [0, 0xa423f00], [1, 0xa41b760], [2, 0x8ac3560], [3, 0xd248fc0], [4, 0xcbc35e0], [5, 0x980ca00], [6, 0xb16cf40], [7, 0x845c140], [0, 0xa469f20], [1, 0xa461780], [2, 0x8b09580], [3, 0xd28efe0], [4, 0xcc09600], [5, 0x9852a20], [6, 0xb1b2f60]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xacd4660]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcd78ec0]]}
  grad_acc_layer.2.attention.output.dense.bias:                                                  {input: bw_in1_add_141_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x861aa40]]}
  grad_acc_layer.2.attention.output.dense.weight:                                                {input: bw_in1_matmul_139_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7fadec0], [0, 0x9fabee0], [1, 0x9f91ae0], [2, 0x8623ae0]]}
  grad_acc_layer.2.attention.self.value.bias:                                                    {input: bw_in1_add_130_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xaccba40]]}
  grad_acc_layer.2.attention.self.value.weight:                                                  {input: bw_in1_matmul_128_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcd81ae0], [4, 0xc6fa6e0], [5, 0x933b7a0], [6, 0xacdd280]]}
  grad_acc_layer.2.attention.self.key.bias:                                                      {input: bw_in1_add_116_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8669b00]]}
  grad_acc_layer.2.attention.self.key.weight:                                                    {input: bw_in1_matmul_114_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcdc7b00], [4, 0xc740700], [5, 0x93817c0], [6, 0xad232a0]]}
  grad_acc_layer.2.attention.self.query.bias:                                                    {input: bw_in1_add_110_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fd7f80]]}
  grad_acc_layer.2.attention.self.query.weight:                                                  {input: bw_in1_matmul_108_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8672720], [3, 0xce0db20], [4, 0xc786720], [5, 0x93c77e0]]}
  grad_acc_layer.1.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9ff2800]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xce53b40]]}
  grad_acc_layer.1.output.dense.bias:                                                            {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ff47e0]]}
  grad_acc_layer.1.output.dense.weight:                                                          {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92229a0], [6, 0xab909a0], [7, 0x7e729a0], [0, 0x9e709c0], [1, 0x9e709c0], [2, 0x85029c0], [3, 0xcc609c0], [4, 0xc5d0e20], [5, 0x92689c0], [6, 0xabd69c0], [7, 0x7eb89c0], [0, 0x9eb69e0], [1, 0x9eb69e0], [2, 0x85489e0], [3, 0xcca69e0], [4, 0xc616e40]]}
  grad_acc_layer.1.intermediate.dense.bias:                                                      {input: bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7efe9e0]]}
  grad_acc_layer.1.intermediate.dense.weight:                                                    {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9efca00], [1, 0x9efca00], [2, 0x858ea00], [3, 0xcceca00], [4, 0xc65ce60], [5, 0x92aee60], [6, 0xac3fa00], [7, 0x7f21a00], [0, 0x9f42a20], [1, 0x9f42a20], [2, 0x85d4a20], [3, 0xcd32a20], [4, 0xc6a2e80], [5, 0x92f4e80], [6, 0xac85a20], [7, 0x7f67a20]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9f88a40]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc6e8ea0]]}
  grad_acc_layer.1.attention.output.dense.bias:                                                  {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc6f1ac0]]}
  grad_acc_layer.1.attention.output.dense.weight:                                                {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa173da0], [2, 0x88a35a0], [3, 0xcfc41a0], [4, 0xc93c920]]}
  grad_acc_layer.1.attention.self.value.bias:                                                    {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xae45140]]}
  grad_acc_layer.1.attention.self.value.weight:                                                  {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80d8e00], [0, 0xa113dc0], [1, 0xa0e7d60], [2, 0x8817560]]}
  grad_acc_layer.1.attention.self.key.bias:                                                      {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xae4dd60]]}
  grad_acc_layer.1.attention.self.key.weight:                                                    {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x811ee20], [0, 0xa159de0], [1, 0xa12dd80], [2, 0x885d580]]}
  grad_acc_layer.1.attention.self.query.bias:                                                    {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc933d00]]}
  grad_acc_layer.1.attention.self.query.weight:                                                  {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x956caa0], [6, 0xae56980], [7, 0x8164e40], [0, 0xa19fe00]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x95b2ac0]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x81aae60]]}
  grad_acc_layer.0.output.dense.bias:                                                            {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc982940]]}
  grad_acc_layer.0.output.dense.weight:                                                          {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x95bb6e0], [6, 0xae9ce20], [7, 0x81b3a80], [0, 0xa1e62a0], [1, 0xa1ba240], [2, 0x890c5e0], [3, 0xd00a640], [4, 0xc98b560], [5, 0x9601700], [6, 0xaee2e40], [7, 0x81f9aa0], [0, 0xa22c2c0], [1, 0xa200260], [2, 0x8952600], [3, 0xd050660], [4, 0xc9d1580]]}
  grad_acc_layer.0.intermediate.dense.bias:                                                      {input: bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa0aa900]]}
  grad_acc_layer.0.intermediate.dense.weight:                                                    {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xce5c760], [4, 0xc7cc740], [5, 0x940dc80], [6, 0xad69bc0], [7, 0x7ffd880], [0, 0xa01e440], [1, 0x9fe14a0], [2, 0x86b9040], [3, 0xcea2780], [4, 0xc812760], [5, 0x9453ca0], [6, 0xadafbe0], [7, 0x80438a0], [0, 0xa064460], [1, 0xa0274c0], [2, 0x86ff060]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc858780]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xadf5c00]]}
  grad_acc_layer.0.attention.output.dense.bias:                                                  {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xcee8c20]]}
  grad_acc_layer.0.attention.output.dense.weight:                                                {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xc8613a0], [5, 0x949a140], [6, 0xadfe820], [7, 0x8089d40]]}
  grad_acc_layer.0.attention.self.value.bias:                                                    {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa090500]]}
  grad_acc_layer.0.attention.self.value.weight:                                                  {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8745500], [3, 0xcef1840], [4, 0xc8a73c0], [5, 0x94e0160]]}
  grad_acc_layer.0.attention.self.key.bias:                                                      {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa099120]]}
  grad_acc_layer.0.attention.self.key.weight:                                                    {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x878b520], [3, 0xcf37860], [4, 0xc8ed3e0], [5, 0x9526180]]}
  grad_acc_layer.0.attention.self.query.bias:                                                    {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80d01e0]]}
  grad_acc_layer.0.attention.self.query.weight:                                                  {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa0cdda0], [1, 0xa0a1d40], [2, 0x87d1540], [3, 0xcf7d880]]}

graphs:
  fwd_0_0:
    target_device: 0
    input_count: 48
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 2], inputs: [_fused_op_0, softmax_18.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_2: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4, _fused_op_1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_2, matmul_22],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_3, _fused_op_3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_3__fused_op_5: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_3__fused_op_5: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_3__fused_op_5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_4: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_3__fused_op_5, _fused_op_4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_6: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_5, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_41: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_6, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_7: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_41, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_44: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_6_add_51: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6_add_51: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_6_add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_47, buffer_0__fused_op_6_add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_8: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_8, _fused_op_8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_8__fused_op_10: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_8__fused_op_10: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_8__fused_op_10],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_9: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_10: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_8__fused_op_10, _fused_op_9],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_11: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_10, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_55: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [_fused_op_11, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_61: {type: matmul, grid_loc: [7, 8], grid_size: [1, 4], inputs: [_fused_op_11, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_67: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [6, 11], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [8, 0], grid_size: [1, 1], inputs: [_fused_op_12],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_13: {type: fused_op, grid_loc: [8, 1], grid_size: [1, 2], inputs: [_fused_op_12, softmax_71.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_13, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_75: {type: matmul, grid_loc: [8, 5], grid_size: [1, 4], inputs: [_fused_op_11, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_14: {type: fused_op, grid_loc: [8, 4], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4, _fused_op_13],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_82: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [_fused_op_14, matmul_75],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    buffer_0__fused_op_11_add_90: {type: nop, grid_loc: [8, 10], grid_size: [1, 1], inputs: [_fused_op_11],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [8, 11], grid_size: [1, 1], inputs: [matmul_86, buffer_0__fused_op_11_add_90],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_15: {type: fused_op, grid_loc: [9, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, add_90],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [9, 8], grid_size: [1, 1], inputs: [_fused_op_15, _fused_op_15],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_15__fused_op_17: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [_fused_op_15],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_15__fused_op_17: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [buffer_1__fused_op_15__fused_op_17],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_16: {type: fused_op, grid_loc: [9, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [9, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_15__fused_op_17, _fused_op_16],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}

  fwd_0_1:
    target_device: 0
    input_count: 48
    _fused_op_18: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_17_0, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_2:
    target_device: 0
    input_count: 48
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_18_0, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_19: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 4], inputs: [matmul_94, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_97: {type: gelu, grid_loc: [0, 8], grid_size: [1, 4], inputs: [_fused_op_19],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [1, 4], grid_size: [4, 4], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_104: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_100, e2e__fused_op_18_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_20: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, add_104],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_20, _fused_op_20],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_20__fused_op_22: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_20],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_20__fused_op_22: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_20__fused_op_22],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_21: {type: fused_op, grid_loc: [2, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_22: {type: fused_op, grid_loc: [3, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_20__fused_op_22, _fused_op_21],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_23: {type: fused_op, grid_loc: [3, 9], grid_size: [1, 1], inputs: [_fused_op_22, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_108: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_23, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_114: {type: matmul, grid_loc: [4, 8], grid_size: [1, 4], inputs: [_fused_op_23, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_120: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_24: {type: fused_op, grid_loc: [3, 11], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_24],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [5, 1], grid_size: [1, 2], inputs: [_fused_op_24, softmax_124.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [_fused_op_25, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_128: {type: matmul, grid_loc: [5, 5], grid_size: [1, 4], inputs: [_fused_op_23, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_26: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4, _fused_op_25],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_135: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_26, matmul_128],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    buffer_0__fused_op_23_add_143: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [_fused_op_23],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [5, 11], grid_size: [1, 1], inputs: [matmul_139, buffer_0__fused_op_23_add_143],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_27: {type: fused_op, grid_loc: [6, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1, add_143],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [6, 8], grid_size: [1, 1], inputs: [_fused_op_27, _fused_op_27],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_27__fused_op_29: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [_fused_op_27],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_27__fused_op_29: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [buffer_1__fused_op_27__fused_op_29],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_28: {type: fused_op, grid_loc: [6, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [6, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_27__fused_op_29, _fused_op_28],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_30: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [_fused_op_29, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_3:
    target_device: 0
    input_count: 48
    matmul_147: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_30_0, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_31: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 4], inputs: [matmul_147, layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_150: {type: gelu, grid_loc: [0, 8], grid_size: [1, 4], inputs: [_fused_op_31],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [1, 4], grid_size: [4, 4], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_157: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_153, e2e__fused_op_30_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_32: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, add_157],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_32, _fused_op_32],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_32__fused_op_34: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_32],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_32__fused_op_34: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_32__fused_op_34],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_33: {type: fused_op, grid_loc: [2, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_34: {type: fused_op, grid_loc: [3, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_32__fused_op_34, _fused_op_33],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    _fused_op_35: {type: fused_op, grid_loc: [3, 9], grid_size: [1, 1], inputs: [_fused_op_34, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6, kernel_broadcast: {input_2: 64, input_1: 64}}}
    _fused_op_35_output_nop_0: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_35], untilize_output: true,
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}

  bwd_0_4:
    target_device: 0
    input_count: 48
    bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoders.output_layernorm_158], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, loss_bert_encoders.output_layernorm_158],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_36: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [loss_bert_encoders.output_layernorm_158, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_36, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_36, e2e__fused_op_34_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_37: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6, _fused_op_36, e2e__fused_op_33_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}
    bw_in1_add_155_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0, _fused_op_37], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_153_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [4, 4], inputs: [_fused_op_37, layer.2.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_153_transpose_0: {type: nop, grid_loc: [1, 0], grid_size: [1, 4], inputs: [e2e_gelu_150_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_153_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_153_transpose_0, _fused_op_37], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_38: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e__fused_op_31_0, bw_in0_matmul_153_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 38}}
    bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0, _fused_op_38], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [4, 8], grid_size: [4, 4], inputs: [_fused_op_38, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [e2e__fused_op_30_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [8, 2], inputs: [bw_in1_matmul_147_transpose_0, _fused_op_38], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    buffer_1__fused_op_37_bw_in0_layernorm_144_combine_add_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_37],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_37_bw_in0_layernorm_144_combine_add_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_1__fused_op_37_bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_144_combine_add_0: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_37_bw_in0_layernorm_144_combine_add_0, bw_in0_matmul_147_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_144_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [e2e__fused_op_29_0, bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_39: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_combine_add_0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [_fused_op_39, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_39, e2e__fused_op_29_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_40: {type: fused_op, grid_loc: [8, 6], grid_size: [1, 1], inputs: [e2e__fused_op_29_0, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6, _fused_op_39, e2e__fused_op_28_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}
    bw_in1_add_141_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0, _fused_op_40], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_139_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [_fused_op_40, layer.2.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_139_transpose_0: {type: nop, grid_loc: [8, 10], grid_size: [1, 1], inputs: [e2e_matmul_135_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_139_matmul_1: {type: matmul, grid_loc: [9, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_139_transpose_0, _fused_op_40], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_135_matmul_1: {type: matmul, grid_loc: [9, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_139_matmul_1, e2e_matmul_128_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_135_transpose_0: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [e2e__fused_op_26_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_135_matmul_1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_135_transpose_0, bw_in0_matmul_139_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [bw_in1_matmul_135_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}

  bwd_0_5:
    target_device: 0
    input_count: 48
    bw_in1_add_130_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_135_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0_0, layer.2.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_128_transpose_0, e2e_bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_135_matmul_1_0, e2e__fused_op_26_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_135_matmul_1_0, bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_26_0, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 41, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_120_matmul_1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_41, e2e_matmul_114_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_120_transpose_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_matmul_108_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_120_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_120_transpose_0, _fused_op_41],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_116_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0, bw_in1_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_120_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_114_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 4], inputs: [bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_114_transpose_0: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_114_matmul_1: {type: matmul, grid_loc: [1, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_114_transpose_0, bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_110_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_108_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_120_matmul_1, layer.2.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_108_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_108_matmul_1: {type: matmul, grid_loc: [2, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_108_transpose_0, bw_in0_matmul_120_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_42: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_128_matmul_1, bw_in0_matmul_114_matmul_1, bw_in0_matmul_108_matmul_1, e2e__fused_op_40_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_42], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [e2e__fused_op_22_0, _fused_op_42],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_43: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_42, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_43, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_43, e2e__fused_op_22_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_44: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e__fused_op_22_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, _fused_op_43, e2e__fused_op_21_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, _fused_op_44], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_44, layer.1.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 4], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_100_transpose_0, _fused_op_44], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_45: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 8], inputs: [e2e__fused_op_19_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 38}}
    bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0, _fused_op_45], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}

  bwd_0_6:
    target_device: 0
    input_count: 48
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_45_0, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_18_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [8, 2], inputs: [bw_in1_matmul_94_transpose_0, e2e__fused_op_45_0], gradient_op: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e__fused_op_44_0, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [e2e__fused_op_17_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_46: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_46, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_46, e2e__fused_op_17_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_47: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e__fused_op_17_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, _fused_op_46, e2e__fused_op_16_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, _fused_op_47], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 4], inputs: [_fused_op_47, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_86_transpose_0, _fused_op_47], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_86_matmul_1, e2e_matmul_75_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_14_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, bw_in0_matmul_86_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 7], grid_size: [4, 1], inputs: [bw_in1_matmul_75_transpose_0, bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e__fused_op_14_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_48: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_14_0, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 41, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_48, e2e_matmul_61_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, _fused_op_48],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [4, 1], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 4], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [7, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    buffer_1__fused_op_47__fused_op_49: {type: nop, grid_loc: [7, 2], grid_size: [1, 1], inputs: [_fused_op_47],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_47__fused_op_49: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [buffer_1__fused_op_47__fused_op_49],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_49: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1, bw_in0_matmul_55_matmul_1, buffer_0__fused_op_47__fused_op_49],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 42}}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_49], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, _fused_op_49],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_50: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [_fused_op_49, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_50, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [_fused_op_50, e2e__fused_op_10_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_51: {type: fused_op, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, _fused_op_50, e2e__fused_op_9_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}

  bwd_0_7:
    target_device: 0
    input_count: 48
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, e2e__fused_op_51_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_51_0, layer.0.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_47_transpose_0, e2e__fused_op_51_0], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_52: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 8], inputs: [e2e__fused_op_7_0, bw_in0_matmul_47_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 38}}
    bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0, _fused_op_52], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_52, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e__fused_op_6_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [8, 2], inputs: [bw_in1_matmul_41_transpose_0, _fused_op_52], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [e2e__fused_op_51_0, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 9], grid_size: [1, 1], inputs: [e2e__fused_op_5_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 36}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [_fused_op_53, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_53, e2e__fused_op_5_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_54: {type: fused_op, grid_loc: [7, 8], grid_size: [1, 1], inputs: [e2e__fused_op_5_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, _fused_op_53, e2e__fused_op_4_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 37}}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, _fused_op_54], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_54, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [8, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_33_transpose_0, _fused_op_54], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [e2e__fused_op_2_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [9, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_22_transpose_0, bw_in1_matmul_29_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e__fused_op_2_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_55: {type: fused_op, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_2_0, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 41, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [_fused_op_55, e2e_matmul_8_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, _fused_op_55],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}

  bwd_0_8:
    target_device: 0
    input_count: 48
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_29_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [4, 1], inputs: [bw_in1_matmul_8_transpose_0, e2e_bw_in1_matmul_14_matmul_1_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_2_transpose_0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 48, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0_shadow: 0, $gptr_q0: 0, $lptr_q5: 0, $gptr_q5: 0, $gptr_q5_shadow: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q1_shadow: 0, $gptr_q4: 0, $gptr_q4_shadow: 0, $gptr_q3: 0, $gptr_q2: 0, $gptr_q2_shadow: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q1: 0}
    - varinst: [$gptr_q5, set, $gptr_q5_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_3, queue_settings: {
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    - endloop

  - run_bwd_0:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 48, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q2: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0, $lptr_q4: 0, $gptr_q5: 0, $lptr_q11: 0, $gptr_q10: 0, $lptr_q10: 0, $lptr_q7: 0, $gptr_q11: 0, $lptr_q9: 0, $gptr_q2_shadow: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q7_shadow: 0, $gptr_q1: 0, $lptr_q8: 0, $gptr_q4: 0, $gptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   varinst: [$gptr_q7, set, $gptr_q7_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   execute: {graph_name: bwd_0_4, queue_settings: {
               loss_bert_encoders.output_layernorm_158: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_matmul_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: bwd_0_5, queue_settings: {
               e2e__fused_op_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_40_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_matmul_135_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in1_matmul_135_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_6, queue_settings: {
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_16_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_45_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_7, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_51_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_8, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_bw_in1_matmul_29_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_matmul_14_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_14_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 96]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 48, $c_one: 1, $c_zero: 0}


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 1], ublock: [2, 2], output: dest}
        - softmax_18.dc.exp.2: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 2], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.add.5: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.6: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.7: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 4}, tile_broadcast: c], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.2: { type: multiply, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.subtract.3: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  5: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 8], ublock: [2, 4], output: output}
  6: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.13: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  7: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - add_43: { type: add, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  36: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  37: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.4: { type: multiply, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_158_layernorm_bw_0.dc.add.5: { type: add, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.7: { type: multiply, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_158_layernorm_bw_0.dc.subtract.8: { type: subtract, inputs: [input4, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: dest}
        - bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9: { type: multiply, inputs: [dest, input5], input_1_tms: [tile_broadcast: c], mblock: [2, 8], ublock: [2, 4], output: output}
  38: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_gelu_150_gelu_derivative_0: { type: gelu_derivative, inputs: [input0], mblock: [1, 4], ublock: [2, 4], output: dest}
        - bw_in0_gelu_150_multiply_1: { type: multiply, inputs: [dest, input1], mblock: [1, 4], ublock: [2, 4], output: output}
  41: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - bw_in0_softmax_124_softmax_bw_0.dc.subtract.2: { type: subtract, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_softmax_124_softmax_bw_0.dc.multiply.3: { type: multiply, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_multiply_122_multiply_0: { type: multiply, inputs: [dest, input3], mblock: [2, 1], ublock: [2, 4], output: output}
  42: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - bw_in0_reshape_106.dc.squeeze.0_combine_add_0: { type: add, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: dest}
        - bw_in0_reshape_106.dc.squeeze.0_combine_add_1: { type: add, inputs: [dest, input2], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_105_combine_add_0: { type: add, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: output}
