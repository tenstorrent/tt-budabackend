# Training NL does this with hidden_state (input_1) where two BWD epochs read (lptr update) from queue but only 2nd epoch pops (gptr update)

devices:
  arch: [grayskull, wormhole, wormhole_b0]

queues:
  hidden_state:   {type: queue, input: HOST    , entries: 80, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  out0:           {type: queue, input: unary0  , entries: 80, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x10000000]]}
  out1:           {type: queue, input: unary1  , entries: 80, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x11000000]]}

graphs:
  bwd0:
    target_device: 0
    input_count: 4
    unary0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [hidden_state], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}

  bwd1:
    target_device: 0
    input_count: 4
    unary1: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [hidden_state], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}

programs:

  - run_bwd:
      - staticvar: {$lptr_q0: 0, $gptr_q0: 0, $lptr_q1: 0, $gptr_q1: 0, $gptr_q0_shadow: 0}
      - var: {$c_microbatches: 4, $c_input_count: 4, $c_input_wrap: 160}

      - loop: $c_microbatches

      - varinst: [$gptr_q0, set, $gptr_q0_shadow] # Outside of microbatch loop.

      - execute: {graph_name: bwd0, queue_settings: {
          hidden_state: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}}}
      - varinst: [$lptr_q0,        incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      - varinst: [$gptr_q0_shadow, incwrap, $c_input_count, $c_input_wrap] # Increment shadow variable only.

      - execute: {graph_name: bwd1, queue_settings: {
          hidden_state: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}}}
      - varinst: [$lptr_q1,       incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      - varinst: [$gptr_q1,       incwrap, $c_input_count, $c_input_wrap] # incr and wrap

      - endloop


test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [hidden_state]
    outputs: [out0, out1]
