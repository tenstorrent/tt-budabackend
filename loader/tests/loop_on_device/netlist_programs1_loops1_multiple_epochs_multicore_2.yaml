# Untilize directly to queue allocated in host memory

devices:
  arch: [grayskull, wormhole, wormhole_b0]

queues:
  in0:                {type: queue, input: HOST    , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x30000000], [0, 0x31000000], [0, 0x32000000], [0, 0x33000000]]}

  e2e_graph0_graph1:  {type: queue, input: unary0  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x20000000], [0, 0x20100000], [0, 0x20200000], [0, 0x20300000]]}
  e2e_graph1_graph2:  {type: queue, input: unary1  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x20400000], [0, 0x20500000], [0, 0x20600000], [0, 0x20700000]]}
  e2e_graph2_graph3:  {type: queue, input: unary2  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x20800000], [0, 0x20900000], [0, 0x20A00000], [0, 0x20B00000]]}


  e2e_graph3_graph4:  {type: queue, input: unary3  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x20C00000], [0, 0x20D00000], [0, 0x20E00000], [0, 0x20F00000]]}
  e2e_graph4_graph5:  {type: queue, input: unary4  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x21000000], [0, 0x21100000], [0, 0x21200000], [0, 0x21300000]]}

  out0:               {type: queue, input: unary5  , entries: 80, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16, target_device: 0, loc: dram, dram: [[0, 0x10000000], [0, 0x11000000], [0, 0x12000000], [0, 0x13000000]]}

graphs:
  graph0:
    target_device: 0
    input_count: 4
    unary0: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [in0], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}
  graph1:
    target_device: 0
    input_count: 4
    unary1: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_graph0_graph1], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}
  graph2:
    target_device: 0
    input_count: 4
    unary2: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_graph1_graph2], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}
  graph3:
    target_device: 0
    input_count: 4
    unary3: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_graph2_graph3], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}
  graph4:
    target_device: 0
    input_count: 4
    unary4: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_graph3_graph4], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}
  graph5:
    target_device: 0
    input_count: 4
    unary5: {type: nop, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_graph4_graph5], in_df: [Float16], acc_df: Float16, out_df: Float16,  intermed_df: Float16, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3, untilize_output: false, t: 1, mblock: [1, 1], ublock: [1, 1]}


programs:
  - program_0:
      - staticvar: {$lptr0: 0, $gptr0: 0, $lptr1: 0, $gptr1: 0, $lptr2: 0, $gptr2: 0}
      - var: {$c_loop_count: 20, $c_input_count: 4, $c_input_count_div2: 2, $c_input_wrap: 160}
      - loop: $c_loop_count
      - execute: {graph_name: graph0, queue_settings: {
          in0:               {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr0, rd_ptr_global: $gptr0}}}
      - execute: {graph_name: graph1, queue_settings: {
          e2e_graph0_graph1: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr0, rd_ptr_global: $gptr0}}}
      - execute: {graph_name: graph2, queue_settings: {
          e2e_graph1_graph2: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr1, rd_ptr_global: $gptr1}}}
      - execute: {graph_name: graph3, queue_settings: {
          e2e_graph2_graph3: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr1, rd_ptr_global: $gptr1}}}

      - execute: {graph_name: graph4, queue_settings: {
          e2e_graph3_graph4: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr2, rd_ptr_global: $gptr2}}}
      - execute: {graph_name: graph5, queue_settings: {
          e2e_graph4_graph5: {prologue: false, epilogue: false, zero: false,  rd_ptr_local: $lptr2, rd_ptr_global: $gptr2}}}

          
      - varinst: [$lptr0, incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      - varinst: [$gptr0, incwrap, $c_input_count, $c_input_wrap] # incr and wrap

      - varinst: [$lptr1, incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      - varinst: [$gptr1, incwrap, $c_input_count_div2, $c_input_wrap] # incr and wrap
      - varinst: [$gptr1, incwrap, $c_input_count_div2, $c_input_wrap] # incr and wrap


      # These three are compressed
      # - varinst: [$lptr2, mul, $c_input_count_div2, $c_input_count_div2]
      # - varinst: [$lptr2, mul, $c_input_count_div2, $c_input_count_div2]
      # - varinst: [$lptr2, mul, 2, 3]
      #  Always | FATAL    | varinst var $lptr2 bound to a queue, must be const/immed for prog looping on device. isntrn: tt_instruction_info{ .opcode = INSTRUCTION_OPCODE::VarInst, .graph_name = , .varinst_opcode = VAR_INSTRUCTION_OPCODE::Set, .loop_count = ,, .vars = {$lptr2=0,5=0,  }}
      # - varinst: [$lptr2, set, 5]

      # Mismatches, maybe try training.

      # These are not merged
      - varinst: [$lptr2, incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      - varinst: [$gptr2, incwrap, $c_input_count, $c_input_wrap] # incr and wrap
      # - varinst: [$gptr2, incwrap, $c_input_count_div2, $c_input_wrap] # incr and wrap

      - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [in0]
    outputs: [out0]
