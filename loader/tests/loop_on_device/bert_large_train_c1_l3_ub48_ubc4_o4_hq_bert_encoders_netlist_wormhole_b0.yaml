# git checkout 7e79b5985
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 4 --chips 1 --microbatch 48 --layers 3 --microbatch_count 4 --loop_count 0 -o perf.json --env PYBUDA_EXP_APPROX=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1 PYBUDA_NO_TRIPLET_PLACEMENT=1 PYBUDA_NO_PLACER_BWD_GROUPS=1 PYBUDA_DRAM_PICK_CAPACITY=1 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1  --training

devices:
  arch: wormhole_b0

test-config:
  stimulus-config:
    type: Normal
    normal_mean: 0.5
    normal_stddev: 0.1
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoders.output_layernorm_158]
    outputs: [bert_encoders.output_layernorm_158]
  test-args:
    sequence_lenth: 384
    head_size: 16

queues:

  # input
  input_1:                                                                                       {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: host, host: [[1, 0x30000000]]}
  attention_mask:                                                                                {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: host, host: [0x33480040]}

  # output
  bert_encoders.output_layernorm_158:                                                            {input: layernorm_158.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xafb8780], [4, 0xafba4c0], [1, 0xafba8a0], [5, 0xafbd6e0]]}
  layer.0.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xafb6260], [3, 0xafb6460], [4, 0xafb81a0], [1, 0xafb8580]]}
  layer.0.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf70240], [3, 0xaf70440], [4, 0xaf72180], [5, 0xaf776c0]]}
  layer.0.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf698e0], [1, 0xaf6bc00], [1, 0xaf6df20], [4, 0xaf6fe60]]}
  layer.0.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaf1fa00], [1, 0xaf226c0], [4, 0xaf29e40], [3, 0xaf2a420]]}
  layer.0.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaf16d80], [5, 0xaf190a0], [5, 0xaf1b3c0], [5, 0xaf1d6e0]]}
  layer.0.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaed0d60], [1, 0xaedc6a0], [4, 0xaee3e20], [3, 0xaee4400]]}
  layer.0.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaec80e0], [5, 0xaeca400], [5, 0xaecc720], [5, 0xaecea40]]}
  layer.0.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb247180]]}
  layer.0.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb246140]]}
  layer.0.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [32, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb12e100], [4, 0xb12ecc0], [1, 0xb13b920], [3, 0xb13f4c0], [5, 0xb1ba120], [4, 0xb1bace0], [1, 0xb1c7940], [3, 0xb1cb4e0]]}
  layer.0.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb11c480], [5, 0xb11c8e0]]}
  layer.0.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb004440], [5, 0xb0048a0], [4, 0xb005460], [1, 0xb0238e0], [3, 0xb090460], [5, 0xb0908c0], [4, 0xb091480], [1, 0xb0af900]]}
  layer.0.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb000de0], [3, 0xb000f60], [4, 0xb001f80], [3, 0xb002100], [4, 0xb003120], [3, 0xb0032a0], [5, 0xb003700], [4, 0xb0042c0]]}
  layer.0.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xac818a0]]}
  layer.0.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac81420]]}
  layer.1.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac3b400], [5, 0xac3b400], [4, 0xac406a0], [1, 0xac43160]]}
  layer.1.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac36dc0], [5, 0xac36dc0], [3, 0xac390e0], [5, 0xac390e0]]}
  layer.1.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabf0da0], [5, 0xabf0da0], [4, 0xabfa680], [1, 0xabfd140]]}
  layer.1.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabec760], [5, 0xabec760], [3, 0xabeea80], [5, 0xabeea80]]}
  layer.1.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaba5480], [3, 0xaba5e40], [5, 0xaba5e40], [4, 0xabb4660]]}
  layer.1.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaba1800], [5, 0xaba1800], [3, 0xaba3b20], [5, 0xaba3b20]]}
  layer.1.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab5b7e0], [5, 0xab5b7e0], [1, 0xab5f460], [4, 0xab6e640]]}
  layer.1.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab52b60], [3, 0xab54e80], [3, 0xab571a0], [3, 0xab594c0]]}
  layer.1.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaebf040]]}
  layer.1.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xac86b40]]}
  layer.1.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [32, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xada6b80], [3, 0xada7000], [4, 0xada77a0], [1, 0xadc4660], [5, 0xae32ba0], [3, 0xae33020], [4, 0xae337c0], [1, 0xae50680]]}
  layer.1.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb11d4a0], [3, 0xb12dca0]]}
  layer.1.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac8eb40], [5, 0xac8eb40], [4, 0xac8f760], [1, 0xacac620], [3, 0xad1ab60], [5, 0xad1ab60], [4, 0xad1b780], [1, 0xad38640]]}
  layer.1.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac8a4c0], [5, 0xac8a4c0], [3, 0xac8b660], [5, 0xac8b660], [3, 0xac8c800], [5, 0xac8c800], [3, 0xac8d9a0], [5, 0xac8d9a0]]}
  layer.1.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb580440]]}
  layer.1.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb581340]]}
  layer.2.attention.self.query.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb539c40], [1, 0xb539d00], [5, 0xb53a420], [4, 0xb53afe0]]}
  layer.2.attention.self.query.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb589060], [1, 0xb589f60], [3, 0xb58a320], [5, 0xb58b380]]}
  layer.2.attention.self.key.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb58bb40], [1, 0xb58c280], [3, 0xb58c640], [5, 0xb58d6a0]]}
  layer.2.attention.self.key.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb5d22a0], [3, 0xb5d2660], [4, 0xb5d3180], [5, 0xb5d36c0]]}
  layer.2.attention.self.value.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb257980], [1, 0xb265a80], [5, 0xb273820], [4, 0xb274860]]}
  layer.2.attention.self.value.bias:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb29de20], [3, 0xb2a0140], [3, 0xb2a2460], [3, 0xb2a4780]]}
  layer.2.attention.output.dense.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2a6aa0], [1, 0xb2abaa0], [5, 0xb2b9840], [4, 0xb2ba880]]}
  layer.2.attention.output.dense.bias:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2ecac0], [3, 0xb2eede0], [3, 0xb2f1100], [1, 0xb2f1ac0]]}
  layer.2.attention.output.LayerNorm.weight:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb5323c0]]}
  layer.2.attention.output.LayerNorm.bias:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb531800]]}
  layer.2.intermediate.dense.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [32, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb4197c0], [4, 0xb419a80], [3, 0xb421c00], [1, 0xb421cc0], [5, 0xb4a57e0], [4, 0xb4a5aa0], [3, 0xb4adc20], [1, 0xb4adce0]]}
  layer.2.intermediate.dense.bias:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb4103e0], [1, 0xb4104a0]]}
  layer.2.output.dense.weight:                                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2f83a0], [1, 0xb2f8460], [5, 0xb2ff860], [4, 0xb3008a0], [3, 0xb3843c0], [1, 0xb384480], [5, 0xb38b880], [4, 0xb38c8c0]]}
  layer.2.output.dense.bias:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2f3d20], [1, 0xb2f3de0], [3, 0xb2f4ec0], [1, 0xb2f4f80], [3, 0xb2f6060], [1, 0xb2f6120], [3, 0xb2f7200], [1, 0xb2f72c0]]}
  layer.2.output.LayerNorm.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb581700]]}
  layer.2.output.LayerNorm.bias:                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb582f20]]}

  # constant
  input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf69460]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf68fe0]]}
  lc.input_tensor.softmax_18.dc.reduce_max.0_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf68b60]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaec0f60]]}
  dc.input_tensor.softmax_18.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaf65ea0]]}
  lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaf65a20]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaec7c60]]}
  dc.input_tensor.layernorm_38.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaec13e0]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaf686e0]]}
  dc.input_tensor.layernorm_38.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb24fda0]]}
  dc.input_tensor.layernorm_38.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb24f660]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb24f1e0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb24ed60]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb246d00]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaffe7a0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb000960]]}
  dc.input_tensor.layernorm_52.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb0008c0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb0004e0]]}
  dc.input_tensor.layernorm_52.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xafffdc0]]}
  dc.input_tensor.layernorm_52.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaffec20]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaebebc0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xac866c0]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xac81420]]}
  input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab526e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabec2e0]]}
  lc.input_tensor.softmax_71.dc.reduce_max.0_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xabebe60]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabebe60]]}
  dc.input_tensor.softmax_71.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xabeb920]]}
  lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xabeb4a0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xabec2e0]]}
  dc.input_tensor.layernorm_91.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaec0e00]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaec0ae0]]}
  dc.input_tensor.layernorm_91.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaebfc60]]}
  dc.input_tensor.layernorm_91.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaebf940]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaebf7e0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaebf4c0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaebf040]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xada6b80]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac8a040]]}
  dc.input_tensor.layernorm_105.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xac89600]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xac89180]]}
  dc.input_tensor.layernorm_105.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb5d1fe0]]}
  dc.input_tensor.layernorm_105.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb581d80]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb581280]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb581000]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb580ec0]]}
  input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb2513c0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb253960]]}
  lc.input_tensor.softmax_124.dc.reduce_max.0_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2f38a0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb253de0]]}
  dc.input_tensor.softmax_124.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb254260]]}
  lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb257500]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb2f3420]]}
  dc.input_tensor.layernorm_144.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb250800]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb4178a0]]}
  dc.input_tensor.layernorm_144.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb417d20]]}
  dc.input_tensor.layernorm_144.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb4188e0]]}
  lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_1_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb418ec0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb531f40]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb531ac0]]}
  lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb419340]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb29d9a0]]}
  dc.input_tensor.layernorm_158.1:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb251840]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb250f40]]}
  dc.input_tensor.layernorm_158.6:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb57fc60]]}
  dc.input_tensor.layernorm_158.8:                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb57fd20]]}
  lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_1_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb580e00]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb581480]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb581900]]}
  lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb5d1b60]]}
  lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa3cdb00]]}
  lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3d4180]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3d4600]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3d4a80]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3d4f00]]}
  dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3d5380]]}
  lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa3d6ba0]]}
  lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa4f3b20]]}
  lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3cb0e0]]}
  lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa516fc0]]}
  lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa519000]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa519480]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa519900]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa519d80]]}
  dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa51a200]]}
  lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa520060]]}
  lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa583ca0]]}
  lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0ca7c0]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa03e320]]}
  lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa03dea0]]}
  lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa02c1e0]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0cac40]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3cac60]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3c1bc0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3c1740]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3c12c0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3c0e40]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa3b8ac0]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa3b8640]]}
  lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa2a0180]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0ee5c0]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0e5520]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaab4100]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa9079c0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa907e40]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa9082c0]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa908740]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa90eea0]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaa352e0]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaab3380]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaab3800]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaab3c80]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaaca3a0]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab405a0]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab49640]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab4a420]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab4a8a0]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab4ad20]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab4b1a0]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab4b620]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa7e51a0]]}
  lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa6aab00]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7c5e60]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7cef00]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7d7fa0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7d8420]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7d88a0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7d8d20]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7d91a0]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa58cd40]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa7f92c0]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa872000]]}
  input_1_multiply_16_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa872480]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa872900]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa8881e0]]}

  # epoch_to_epoch
  e2e_layernorm_52.dc.add.9_0:                                                                   {input: layernorm_52.dc.add.9, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x365a100]]}
  e2e_layernorm_52.dc.subtract.3_0:                                                              {input: layernorm_52.dc.subtract.3, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x365a100]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                                       {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x365a100]]}
  e2e_layernorm_91.dc.add.14_0:                                                                  {input: layernorm_91.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xb5d45c0]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                                       {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x368e920]]}
  e2e_layernorm_144.dc.reciprocal.11_0:                                                          {input: layernorm_144.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xf775a60]]}
  e2e_buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12_0:                       {input: buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xde06a00]]}
  e2e_layernorm_158.dc.multiply.12_0:                                                            {input: layernorm_158.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x13fa4640]]}
  e2e_layernorm_158.dc.reciprocal.11_0:                                                          {input: layernorm_158.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x147075e0]]}
  e2e_gelu_150_0:                                                                                {input: gelu_150, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x147d9600], [2, 0x150f32e0]]}
  e2e_add_149_0:                                                                                 {input: add_149, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x139e7ae0], [3, 0x13ab8b00]]}
  e2e_layernorm_144.dc.add.14_0:                                                                 {input: layernorm_144.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x159e4660]]}
  e2e_bw_in1_matmul_147_transpose_0_0:                                                           {input: bw_in1_matmul_147_transpose_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x17c58b40]]}
  e2e_bw_in0_gelu_150_multiply_1_0:                                                              {input: bw_in0_gelu_150_multiply_1, type: queue, entries: 48, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x16f38b20], [1, 0x17424680]]}
  e2e_bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9_0:                                       {input: bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x182166c0]]}
  e2e_bw_in0_matmul_147_matmul_1_0:                                                              {input: bw_in0_matmul_147_matmul_1, type: queue, entries: 48, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x17c59620], [4, 0x17d2b640], [4, 0x17dfd660], [4, 0x17ecf680], [4, 0x17fa16a0], [4, 0x180736c0], [1, 0x181446a0], [4, 0x181456e0]]}
  e2e_layernorm_144.dc.multiply.12_0:                                                            {input: layernorm_144.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x16e67b00]]}
  e2e_matmul_135_0:                                                                              {input: matmul_135, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xe7f3280]]}
  e2e_matmul_128_0:                                                                              {input: matmul_128, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xeb26a40], [4, 0xeb27560], [3, 0xf1b6a60], [4, 0xf1b7580]]}
  e2e_softmax_124.dc.multiply.7_0:                                                               {input: softmax_124.dc.multiply.7, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xf8475a0]]}
  e2e_layernorm_105.dc.add.14_0:                                                                 {input: layernorm_105.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x102332a0]]}
  e2e_matmul_114_0:                                                                              {input: matmul_114, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xf847a80], [3, 0xf918aa0], [5, 0xfed7aa0], [3, 0xffa8ac0]]}
  e2e_matmul_108_0:                                                                              {input: matmul_108, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xde07520], [2, 0xe163260], [3, 0xe496a20], [4, 0xe497540]]}
  e2e_bw_in0_layernorm_105_combine_add_0_0:                                                      {input: bw_in0_layernorm_105_combine_add_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x18217700]]}
  e2e_layernorm_105.dc.multiply.12_0:                                                            {input: layernorm_105.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x12cc75c0]]}
  e2e_layernorm_105.dc.reciprocal.11_0:                                                          {input: layernorm_105.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xf846a80]]}
  e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.add.5_0:                                            {input: bw_in0_layernorm_105_layernorm_bw_0.dc.add.5, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x182e8b60]]}
  e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0_0:                                       {input: bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x18573300]]}
  e2e_layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0:                                       {input: layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x188a66e0]]}
  e2e_gelu_97_0:                                                                                 {input: gelu_97, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x10b24620], [2, 0x11c732c0]]}
  e2e_add_96_0:                                                                                  {input: add_96, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x10567ac0], [3, 0x10638ae0]]}
  e2e_bw_in1_matmul_94_transpose_0_0:                                                            {input: bw_in1_matmul_94_transpose_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x18da7360]]}
  e2e_bw_in0_gelu_97_multiply_1_0:                                                               {input: bw_in0_gelu_97_multiply_1, type: queue, entries: 48, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x188a7b20], [1, 0x188daf00]]}
  e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9_0:                                       {input: bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x188a7720]]}
  e2e_bw_in0_matmul_94_matmul_1_0:                                                               {input: bw_in0_matmul_94_matmul_1, type: queue, entries: 48, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x18978b80], [3, 0x18a4aba0], [3, 0x18b1cbc0], [3, 0x18beebe0], [2, 0x18c03320], [3, 0x18cc0c00], [2, 0x18cd5340], [3, 0x18d92c20]]}
  e2e_layernorm_91.dc.multiply.12_0:                                                             {input: layernorm_91.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb5d54a0]]}
  e2e_layernorm_91.dc.reciprocal.11_0:                                                           {input: layernorm_91.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xdd349e0]]}
  e2e_matmul_82_0:                                                                               {input: matmul_82, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xb5d59e0]]}
  e2e_matmul_75_0:                                                                               {input: matmul_75, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd6a49c0], [4, 0xd6a54e0], [5, 0xd6a5a20], [2, 0xdad3240]]}
  e2e_softmax_71.dc.multiply.7_0:                                                                {input: softmax_71.dc.multiply.7, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xd6a4600]]}
  e2e_layernorm_52.dc.add.14_0:                                                                  {input: layernorm_52.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb5d4980]]}
  e2e_matmul_61_0:                                                                               {input: matmul_61, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xc0931c0], [2, 0xc7231e0], [2, 0xcdb3200], [1, 0xd0145e0]]}
  e2e_matmul_55_0:                                                                               {input: matmul_55, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xd0149a0], [4, 0xd0154c0], [5, 0xd015a00], [2, 0xd443220]]}
  e2e_bw_in0_layernorm_52_combine_add_0_0:                                                       {input: bw_in0_layernorm_52_combine_add_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x18e99460]]}
  e2e_layernorm_52.dc.multiply.12_0:                                                             {input: layernorm_52.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xdd35a40]]}
  e2e_layernorm_52.dc.reciprocal.11_0:                                                           {input: layernorm_52.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xdd35500]]}
  e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.add.5_0:                                             {input: bw_in0_layernorm_52_layernorm_bw_0.dc.add.5, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x18f37740]]}
  e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0_0:                                        {input: bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x19437380]]}
  e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0:                                        {input: layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x18e64c40]]}
  e2e_gelu_44_0:                                                                                 {input: gelu_44, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x368e920], [2, 0x36c3140]]}
  e2e_add_43_0:                                                                                  {input: add_43, type: queue, entries: 192, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x3cea120], [5, 0x6b0e940]]}
  e2e_layernorm_38.dc.add.14_0:                                                                  {input: layernorm_38.dc.add.14, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b43160]]}
  e2e_bw_in1_matmul_41_transpose_0_0:                                                            {input: bw_in1_matmul_41_transpose_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x195faf20]]}
  e2e_bw_in0_gelu_44_multiply_1_0:                                                               {input: bw_in0_gelu_44_multiply_1, type: queue, entries: 48, grid_size: [1, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x195c7760], [5, 0x195c7b40]]}
  e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9_0:                                        {input: bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x19529480]]}
  e2e_bw_in0_matmul_41_matmul_1_0:                                                               {input: bw_in0_matmul_41_matmul_1, type: queue, entries: 48, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x19ac73a0], [2, 0x19b993c0], [3, 0x19bb94a0], [2, 0x19c6b3e0], [1, 0x19c8af40], [3, 0x19c8b4c0], [2, 0x19d3d400], [1, 0x19d5cf60]]}
  e2e_layernorm_38.dc.multiply.12_0:                                                             {input: layernorm_38.dc.multiply.12, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x723c160]]}
  e2e_layernorm_38.dc.reciprocal.11_0:                                                           {input: layernorm_38.dc.reciprocal.11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x716a140]]}
  e2e_matmul_29_0:                                                                               {input: matmul_29, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  e2e_matmul_22_0:                                                                               {input: matmul_22, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100], [4, 0x8144120], [2, 0x8583180], [4, 0x87d4140]]}
  e2e_softmax_18.dc.multiply.7_0:                                                                {input: softmax_18.dc.multiply.7, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8c131a0]]}
  e2e_matmul_8_0:                                                                                {input: matmul_8, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x8c7c180], [4, 0x8e64160], [1, 0x930c1a0], [3, 0x94f4120]]}
  e2e_matmul_2_0:                                                                                {input: matmul_2, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x94f4180], [1, 0x999c1c0], [3, 0x9b84140], [4, 0x9b841a0]]}

  # loss
  loss_bert_encoders.output_layernorm_158:                                                       {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31a40020]]}

  # grad_accumulator
  grad_acc_layer.2.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3cb560]]}
  grad_acc_layer.2.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa3cdf80]]}
  grad_acc_layer.2.output.dense.bias:                                                            {input: bw_in1_add_155_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa3d7020]]}
  grad_acc_layer.2.output.dense.weight:                                                          {input: bw_in1_matmul_153_matmul_1, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [64, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa3dbae0], [1, 0xa3dfc40], [5, 0xa3eea60], [4, 0xa3f83a0], [3, 0xa467b00], [1, 0xa46bc60], [5, 0xa47aa80], [4, 0xa4843c0]]}
  grad_acc_layer.2.intermediate.dense.bias:                                                      {input: bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa4f3fa0]]}
  grad_acc_layer.2.intermediate.dense.weight:                                                    {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [8, 1], t: 1, mblock: [2, 32], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa90f320], [4, 0xa91a3a0], [1, 0xa91d2a0], [5, 0xa92b760], [3, 0xa99b340], [4, 0xa9a63c0], [1, 0xa9a92c0], [5, 0xa9b7780]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa5103e0]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa517440]]}
  grad_acc_layer.2.attention.output.dense.bias:                                                  {input: bw_in1_add_141_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa5204e0]]}
  grad_acc_layer.2.attention.output.dense.weight:                                                {input: bw_in1_matmul_139_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa529100], [4, 0xa53d220]]}
  grad_acc_layer.2.attention.self.value.bias:                                                    {input: bw_in1_add_130_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa584120]]}
  grad_acc_layer.2.attention.self.value.weight:                                                  {input: bw_in1_matmul_128_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa4f7c80], [5, 0xa506aa0]]}
  grad_acc_layer.2.attention.self.key.bias:                                                      {input: bw_in1_add_116_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa02c660]]}
  grad_acc_layer.2.attention.self.key.weight:                                                    {input: bw_in1_matmul_114_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9f8e960], [5, 0xa01a980]]}
  grad_acc_layer.2.attention.self.query.bias:                                                    {input: bw_in1_add_110_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa035280]]}
  grad_acc_layer.2.attention.self.query.weight:                                                  {input: bw_in1_matmul_108_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa03e7a0], [5, 0xa0a69a0]]}
  grad_acc_layer.1.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3b8220]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0cb0c0]]}
  grad_acc_layer.1.output.dense.bias:                                                            {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0d3ce0]]}
  grad_acc_layer.1.output.dense.weight:                                                          {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [64, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa2a01e0], [3, 0xa2a0600], [1, 0xa2b5ac0], [5, 0xa2d6a20], [4, 0xa32c200], [3, 0xa32c620], [1, 0xa341ae0], [5, 0xa362a40]]}
  grad_acc_layer.1.intermediate.dense.bias:                                                      {input: bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa292aa0]]}
  grad_acc_layer.1.intermediate.dense.weight:                                                    {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [8, 1], t: 1, mblock: [2, 32], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0eea40], [5, 0xa1329c0], [1, 0xa17aa60], [5, 0xa1be9e0], [1, 0xa206a80], [3, 0xa214160], [4, 0xa2141c0], [5, 0xa24aa00]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0e59a0]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3c2040]]}
  grad_acc_layer.1.attention.output.dense.bias:                                                  {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0dc900]]}
  grad_acc_layer.1.attention.output.dense.weight:                                                {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaa27360], [4, 0xaa323e0]]}
  grad_acc_layer.1.attention.self.value.bias:                                                    {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaa35760]]}
  grad_acc_layer.1.attention.self.value.weight:                                                  {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaa3e380], [5, 0xaa437a0]]}
  grad_acc_layer.1.attention.self.key.bias:                                                      {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa906280]]}
  grad_acc_layer.1.attention.self.key.weight:                                                    {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaab4580], [4, 0xaabe400]]}
  grad_acc_layer.1.attention.self.query.bias:                                                    {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xaaca820]]}
  grad_acc_layer.1.attention.self.query.weight:                                                  {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xaacf7c0], [1, 0xaad3440]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                                        {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab40a20]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                                      {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab49ac0]]}
  grad_acc_layer.0.output.dense.bias:                                                            {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa58d1c0]]}
  grad_acc_layer.0.output.dense.weight:                                                          {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [64, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa592ac0], [1, 0xa595de0], [3, 0xa5b5120], [4, 0xa5c9240], [5, 0xa61eae0], [1, 0xa621e00], [3, 0xa641140], [4, 0xa655260]]}
  grad_acc_layer.0.intermediate.dense.bias:                                                      {input: bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa6aaf80]]}
  grad_acc_layer.0.intermediate.dense.weight:                                                    {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [8, 1], t: 1, mblock: [2, 32], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa6ade20], [3, 0xa6cd160], [5, 0xa6cdfa0], [4, 0xa6e1280], [1, 0xa739e40], [3, 0xa759180], [5, 0xa759fc0], [4, 0xa76d2a0]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                                              {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7c62e0]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                                            {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7cf380]]}
  grad_acc_layer.0.attention.output.dense.bias:                                                  {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa7e5620]]}
  grad_acc_layer.0.attention.output.dense.weight:                                                {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa7e5fe0], [3, 0xa7ee240]]}
  grad_acc_layer.0.attention.self.value.bias:                                                    {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa7f9740]]}
  grad_acc_layer.0.attention.self.value.weight:                                                  {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa7fc1c0], [4, 0xa802360]]}
  grad_acc_layer.0.attention.self.key.bias:                                                      {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa872d80]]}
  grad_acc_layer.0.attention.self.key.weight:                                                    {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa87a260], [5, 0xa87b9a0]]}
  grad_acc_layer.0.attention.self.query.bias:                                                    {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa888660]]}
  grad_acc_layer.0.attention.self.query.weight:                                                  {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [16, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa88e380], [1, 0xa891280]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 48
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_14: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_16: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_17: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [multiply_16, attention_mask_s_brcst_m2_2_1.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}]}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_17],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    softmax_18.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_max.0, lc.input_tensor.softmax_18.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_18.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_17, softmax_18.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    softmax_18.dc.exp.2: {type: exp, grid_loc: [2, 3], grid_size: [1, 1], inputs: [softmax_18.dc.subtract.1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_18.dc.exp.2, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    softmax_18.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_18.dc.reciprocal.6: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_18.dc.add.5],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_18.dc.reciprocal.6, lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_18.dc.multiply.7: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_18.dc.exp.2, softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_22: {type: matmul, grid_loc: [1, 0], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_29: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_18.dc.multiply.7, matmul_22],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [3, 2], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_38.dc.subtract.3: {type: subtract, grid_loc: [4, 3], grid_size: [1, 1], inputs: [add_37, layernorm_38.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3, layernorm_38.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_38.dc.multiply.7: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.add.9: {type: add, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.7, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.sqrt.10: {type: sqrt, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_38.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reciprocal.11: {type: reciprocal, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_38.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.reciprocal.11, lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.multiply.12: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12, layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [528, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.multiply.13: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.12, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.add.14: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.13, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_41: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [layernorm_38.dc.add.14, layer.0.intermediate.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_43: {type: add, grid_loc: [6, 3], grid_size: [1, 2], inputs: [matmul_41, layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_44: {type: gelu, grid_loc: [6, 5], grid_size: [1, 2], inputs: [add_43],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 8, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_38.dc.add.14_add_51: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_38.dc.add.14],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [384], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_47, buffer_0_layernorm_38.dc.add.14_add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_52.dc.subtract.3: {type: subtract, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_51, layernorm_52.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.3, layernorm_52.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_52.dc.multiply.7: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.add.9: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.7, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 48
    layernorm_52.dc.sqrt.10: {type: sqrt, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.9_0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reciprocal.11: {type: reciprocal, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_52.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_52.dc.reciprocal.11, lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.multiply.12: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.subtract.3_0, layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.multiply.13: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.12, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.add.14: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.13, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_55: {type: matmul, grid_loc: [1, 0], grid_size: [1, 4], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_61: {type: matmul, grid_loc: [1, 4], grid_size: [1, 4], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_67: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_69: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    add_70: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [multiply_69, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}]}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_70],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    softmax_71.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_max.0, lc.input_tensor.softmax_71.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_71.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [1, 1], inputs: [add_70, softmax_71.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    softmax_71.dc.exp.2: {type: exp, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_71.dc.subtract.1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_71.dc.exp.2, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    softmax_71.dc.add.5: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_71.dc.reciprocal.6: {type: reciprocal, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_71.dc.add.5],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [softmax_71.dc.reciprocal.6, lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_71.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_71.dc.exp.2, softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_75: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_82: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [softmax_71.dc.multiply.7, matmul_75],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [4, 1], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_52.dc.add.14_add_90: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.add.14],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [matmul_86, buffer_0_layernorm_52.dc.add.14_add_90],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_91.dc.subtract.3: {type: subtract, grid_loc: [5, 1], grid_size: [1, 1], inputs: [add_90, layernorm_91.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3, layernorm_91.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_91.dc.multiply.7: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.add.9: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.7, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.sqrt.10: {type: sqrt, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reciprocal.11: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_91.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_91.dc.reciprocal.11, lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.multiply.12: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12, layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [528, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.multiply.13: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.12, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.add.14: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.13, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 48
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_91.dc.add.14_0, layer.1.intermediate.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_96: {type: add, grid_loc: [1, 2], grid_size: [1, 2], inputs: [matmul_94, layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_97: {type: gelu, grid_loc: [1, 4], grid_size: [1, 2], inputs: [add_96],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 8, min_buffer_input: 0, u_kt: 16}}
    add_104: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_100, e2e_layernorm_91.dc.add.14_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_105.dc.subtract.3: {type: subtract, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_104, layernorm_105.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3, layernorm_105.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_105.dc.multiply.7: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.add.9: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.7, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.sqrt.10: {type: sqrt, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_105.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_105.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_105.dc.reciprocal.11, lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.multiply.12: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12, layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [528, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.multiply.13: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.12, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.add.14: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.13, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_108: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [layernorm_105.dc.add.14, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_114: {type: matmul, grid_loc: [5, 4], grid_size: [1, 4], inputs: [layernorm_105.dc.add.14, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_120: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_122: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    add_123: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_122, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}]}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_123],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    softmax_124.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_max.0, lc.input_tensor.softmax_124.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_124.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_123, softmax_124.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    softmax_124.dc.exp.2: {type: exp, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_124.dc.subtract.1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_124.dc.exp.2, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    softmax_124.dc.add.5: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_124.dc.reciprocal.6: {type: reciprocal, grid_loc: [7, 0], grid_size: [1, 1], inputs: [softmax_124.dc.add.5],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [softmax_124.dc.reciprocal.6, lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_124.dc.multiply.7: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_124.dc.exp.2, softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_128: {type: matmul, grid_loc: [7, 3], grid_size: [1, 4], inputs: [layernorm_105.dc.add.14, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 16}}
    matmul_135: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [softmax_124.dc.multiply.7, matmul_128],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_105.dc.add.14_add_143: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.add.14],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [matmul_139, buffer_0_layernorm_105.dc.add.14_add_143],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_144.dc.subtract.3: {type: subtract, grid_loc: [9, 0], grid_size: [1, 1], inputs: [add_143, layernorm_144.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.3, layernorm_144.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_144.dc.multiply.7: {type: multiply, grid_loc: [9, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.add.9: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.7, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.sqrt.10: {type: sqrt, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_144.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reciprocal.11: {type: reciprocal, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_144.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 48
    layernorm_144.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_144.dc.reciprocal.11_0, lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_144.dc.multiply.12: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12_0, layernorm_144.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_144.dc.multiply.13: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.12, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_144.dc.add.14: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.13, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_147: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [layernorm_144.dc.add.14, layer.2.intermediate.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_149: {type: add, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_147, layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_150: {type: gelu, grid_loc: [2, 2], grid_size: [1, 2], inputs: [add_149],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 8, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_144.dc.add.14_add_157: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_144.dc.add.14],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [384], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_153, buffer_0_layernorm_144.dc.add.14_add_157],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    layernorm_158.dc.subtract.3: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_157, layernorm_158.dc.multiply.2],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [400, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.3, layernorm_158.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    layernorm_158.dc.multiply.7: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.add.9: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.7, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.sqrt.10: {type: sqrt, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_158.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reciprocal.11: {type: reciprocal, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_158.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    layernorm_158.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_158.dc.reciprocal.11, lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.multiply.12: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12, layernorm_158.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [512, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_158.dc.multiply.13: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.12, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {kernel_broadcast: {input_1: 64}}}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_158.dc.add.14: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.13, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {kernel_broadcast: {input_1: 64}}}

  bwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 48
    bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoders.output_layernorm_158], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_layernorm_158.dc.multiply.12_0, loss_bert_encoders.output_layernorm_158],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_158.dc.reciprocal.11_0, lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [loss_bert_encoders.output_layernorm_158, layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.0, e2e_layernorm_158.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_layernorm_158.dc.multiply.12_0, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_158_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6, bw_in0_layernorm_158_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_158_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [640, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_155_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0, bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_153_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9, layer.2.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_153_transpose_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 2], inputs: [e2e_gelu_150_0],
         t: 1, mblock: [64, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_153_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [bw_in1_matmul_153_transpose_0, bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [64, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_gelu_150_gelu_derivative_0: {type: gelu_derivative, grid_loc: [5, 0], grid_size: [1, 8], inputs: [e2e_add_149_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    bw_in0_gelu_150_multiply_1: {type: multiply, grid_loc: [3, 1], grid_size: [1, 2], inputs: [bw_in0_gelu_150_gelu_derivative_0, bw_in0_matmul_153_matmul_1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0, bw_in0_gelu_150_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [bw_in0_gelu_150_multiply_1, layer.2.intermediate.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e_layernorm_144.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}

  bwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 48
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 1], inputs: [e2e_bw_in1_matmul_147_transpose_0_0, e2e_bw_in0_gelu_150_multiply_1_0], gradient_op: true,
         t: 1, mblock: [2, 32], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_144_combine_add_0: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_147_matmul_1_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_144_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [e2e_layernorm_144.dc.multiply.12_0, bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_layernorm_144.dc.reciprocal.11_0, lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_combine_add_0, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.0, e2e_layernorm_144.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_layernorm_144.dc.multiply.12_0, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_144_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6, bw_in0_layernorm_144_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_144_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [640, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_141_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0, bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_139_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 4], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9, layer.2.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_139_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [e2e_matmul_135_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_139_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_139_transpose_0, bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_135_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_139_matmul_1, e2e_matmul_128_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_135_transpose_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e_softmax_124.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_135_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_135_transpose_0, bw_in0_matmul_139_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_130_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0, bw_in1_matmul_135_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_135_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 4], inputs: [bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, e2e_softmax_124.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_124_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.subtract.2, e2e_softmax_124.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_multiply_122_multiply_0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.3, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    bw_in0_matmul_120_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [bw_in0_multiply_122_multiply_0, e2e_matmul_114_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_120_transpose_0: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [e2e_matmul_108_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_120_matmul_1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_120_transpose_0, bw_in0_multiply_122_multiply_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_116_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0, bw_in1_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_120_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_114_matmul_1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 4], inputs: [bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_114_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_114_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [bw_in1_matmul_114_transpose_0, bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_110_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_108_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 4], inputs: [bw_in0_matmul_120_matmul_1, layer.2.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_108_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_108_matmul_1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 2], inputs: [bw_in1_matmul_108_transpose_0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_106.dc.squeeze.0_combine_add_0: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_128_matmul_1, bw_in0_matmul_114_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_reshape_106.dc.squeeze.0_combine_add_1: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [bw_in0_reshape_106.dc.squeeze.0_combine_add_0, bw_in0_matmul_108_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9_bw_in0_layernorm_105_combine_add_0: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_combine_add_0: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.9_bw_in0_layernorm_105_combine_add_0, bw_in0_reshape_106.dc.squeeze.0_combine_add_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.reciprocal.11_0, lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_combine_add_0, layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, e2e_layernorm_105.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.multiply.12_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_105_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}

  bwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 48
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_layernorm_105_combine_add_0_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.multiply.12_0, e2e_bw_in0_layernorm_105_combine_add_0_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.add.5_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_105_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_105_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9, layer.1.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 2], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [64, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [bw_in1_matmul_100_transpose_0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [64, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_gelu_97_gelu_derivative_0: {type: gelu_derivative, grid_loc: [4, 0], grid_size: [1, 8], inputs: [e2e_add_96_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    bw_in0_gelu_97_multiply_1: {type: multiply, grid_loc: [2, 1], grid_size: [1, 2], inputs: [bw_in0_gelu_97_gelu_derivative_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0, bw_in0_gelu_97_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [bw_in0_gelu_97_multiply_1, layer.1.intermediate.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}

  bwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 48
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 1], inputs: [e2e_bw_in1_matmul_94_transpose_0_0, e2e_bw_in0_gelu_97_multiply_1_0], gradient_op: true,
         t: 1, mblock: [2, 32], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_94_matmul_1_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.multiply.12_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.reciprocal.11_0, lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, e2e_layernorm_91.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.multiply.12_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, bw_in0_layernorm_91_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_91_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [640, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 4], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_86_transpose_0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_86_matmul_1, e2e_matmul_75_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e_softmax_71.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, bw_in0_matmul_86_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 4], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [bw_in1_matmul_75_transpose_0, bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e_softmax_71.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_71_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.subtract.2, e2e_softmax_71.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_multiply_69_multiply_0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.3, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [bw_in0_multiply_69_multiply_0, e2e_matmul_61_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, bw_in0_multiply_69_multiply_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 4], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 4], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 2], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_53.dc.squeeze.0_combine_add_0: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_reshape_53.dc.squeeze.0_combine_add_1: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [bw_in0_reshape_53.dc.squeeze.0_combine_add_0, bw_in0_matmul_55_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_bw_in0_layernorm_52_combine_add_0: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [656], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_combine_add_0: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_bw_in0_layernorm_52_combine_add_0, bw_in0_reshape_53.dc.squeeze.0_combine_add_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.reciprocal.11_0, lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_combine_add_0, layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, e2e_layernorm_52.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.multiply.12_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_52_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}

  bwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 48
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_layernorm_52_combine_add_0_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.multiply.12_0, e2e_bw_in0_layernorm_52_combine_add_0_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.add.5_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_52_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_52_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9, layer.0.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 2], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [64, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [bw_in1_matmul_47_transpose_0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [64, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_gelu_44_gelu_derivative_0: {type: gelu_derivative, grid_loc: [4, 0], grid_size: [1, 8], inputs: [e2e_add_43_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    bw_in0_gelu_44_multiply_1: {type: multiply, grid_loc: [2, 1], grid_size: [1, 2], inputs: [bw_in0_gelu_44_gelu_derivative_0, bw_in0_matmul_47_matmul_1],
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0, bw_in0_gelu_44_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [bw_in0_gelu_44_multiply_1, layer.0.intermediate.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.add.14_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}

  bwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 48
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 1], inputs: [e2e_bw_in1_matmul_41_transpose_0_0, e2e_bw_in0_gelu_44_multiply_1_0], gradient_op: true,
         t: 1, mblock: [2, 32], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_41_matmul_1_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.multiply.12_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.reciprocal.11_0, lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [320, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, e2e_layernorm_38.dc.multiply.12_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.multiply.12_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, bw_in0_layernorm_38_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_38_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [640, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 4], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_33_transpose_0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e_softmax_18.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_22_transpose_0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e_softmax_18.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_18_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [4, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.subtract.2, e2e_softmax_18.dc.multiply.7_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_multiply_16_multiply_0: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.3, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in0_multiply_16_multiply_0, e2e_matmul_8_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, bw_in0_multiply_16_multiply_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 2], inputs: [bw_in1_matmul_8_transpose_0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [bw_in1_matmul_2_transpose_0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 48, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0_shadow: 0, $gptr_q0: 0, $gptr_q1: 0, $lptr_q6: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $gptr_q5_shadow: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0, $gptr_q3_shadow: 0, $gptr_q3: 0, $gptr_q2: 0}
    - varinst: [$gptr_q5, set, $gptr_q5_shadow]
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone1156_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e_layernorm_52.dc.subtract.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_52.dc.add.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone1178_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e_layernorm_91.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_fork_clone1197_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_layernorm_144.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 96]
    - endloop

  - run_bwd_0:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 48, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q8_shadow: 0, $gptr_q0: 0, $gptr_q12: 0, $gptr_q3_shadow: 0, $gptr_q11: 0, $lptr_q11: 0, $lptr_q8: 0, $lptr_q9: 0, $gptr_q1: 0, $lptr_q12: 0, $lptr_q13: 0, $lptr_q2: 0, $gptr_q13: 0, $gptr_q8: 0, $gptr_q14: 0, $gptr_q3: 0, $gptr_q2: 0, $lptr_q14: 0, $gptr_q9: 0, $lptr_q7: 0, $gptr_q7: 0, $gptr_q6: 0, $gptr_q4: 0, $lptr_q6: 0, $lptr_q5: 0, $gptr_q10: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q1: 0, $lptr_q10: 0, $lptr_q3: 0, $lptr_q0: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -   varinst: [$gptr_q3, set, $gptr_q3_shadow]
    -   execute: {graph_name: bwd_0_4_temporal_epoch_4, queue_settings: {
               loss_bert_encoders.output_layernorm_158: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_layernorm_144.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_add_149_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_158.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_158.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: bwd_0_5_temporal_epoch_5, queue_settings: {
               e2e_layernorm_105.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_105.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_105.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_softmax_124.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_144.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_144.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_gelu_150_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_matmul_147_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in1_matmul_147_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_layernorm_91.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_add_96_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layernorm_105.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in0_layernorm_105_combine_add_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.add.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_layernorm_52.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_layernorm_52.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_layernorm_52.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_softmax_71.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_layernorm_91.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_layernorm_91.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_gelu_97_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_matmul_94_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in1_matmul_94_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_8_temporal_epoch_8, queue_settings: {
               e2e_layernorm_38.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_add_43_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_layernorm_52.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_bw_in0_layernorm_52_combine_add_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.add.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone120_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_9_temporal_epoch_9, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_softmax_18.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_layernorm_38.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_layernorm_38.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_gelu_44_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_matmul_41_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in1_matmul_41_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 96]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 48, $c_one: 1, $c_zero: 0}


