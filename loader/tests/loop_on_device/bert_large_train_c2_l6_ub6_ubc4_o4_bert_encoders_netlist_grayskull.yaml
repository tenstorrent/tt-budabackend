# git checkout ed21b6ed8
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 4 --chips 2 --microbatch 48 --layers 6 --microbatch_count 4 --loop_count 0 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=85000 PYBUDA_FORCE_INTERMED_TO_OUTPUT_DF=1 PYBUDA_FORK_JOIN_BUF_MULTIPLE=4 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_MICROBATCH_LOOPING=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1  --training --auto_transpose

devices:
  arch: grayskull


test-config:
  stimulus-config:
    type: Normal
    normal_mean: 0.5
    normal_stddev: 0.1
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoders.output_layernorm_317]
    outputs: [bert_encoders.output_layernorm_317]
  test-args:
    sequence_lenth: 384
    head_size: 16

queues:

  # input
  input_1:                                                                                        {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                                                 {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31a40020]]}

  # output
  bert_encoders.output_layernorm_317:                                                             {input: _fused_op_71_output_nop_0, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x921af80], [6, 0x78945c0], [7, 0x7e25840], [0, 0x9942bc0]]}
  layer.0.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7edbe80], [2, 0x987baa0], [3, 0xc6b27e0], [4, 0x90ffb20]]}
  layer.0.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9260fa0], [6, 0x78da5e0], [7, 0x7e6b860], [0, 0x9988be0]]}
  layer.0.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7ede1a0], [2, 0x987ddc0], [3, 0xc6b4b00], [4, 0x9101e40]]}
  layer.0.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7ee04c0], [2, 0x98800e0], [3, 0xc6b6e20], [4, 0x9104160]]}
  layer.0.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92a7440], [6, 0x7920600], [7, 0x7eb1d00], [0, 0x99e0420]]}
  layer.0.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7f264e0], [2, 0x98c6100], [3, 0xc6fce40], [4, 0x914a180]]}
  layer.0.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92a9760], [6, 0x7922920], [7, 0x7eb4020], [0, 0x99e2740]]}
  layer.0.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7924c40]]}
  layer.0.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7eb6340]]}
  layer.0.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x99e4a60], [1, 0x7f6c980], [2, 0x992f140], [3, 0xc7432e0], [4, 0x9198dc0], [5, 0x92acc20], [6, 0x792d860], [7, 0x7ebef60], [0, 0x9a2aa80], [1, 0x7fb29a0], [2, 0x9975160], [3, 0xc789300], [4, 0x91dede0], [5, 0x92f2c40], [6, 0x7973880], [7, 0x7f04f80]]}
  layer.0.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9a70aa0], [1, 0x7ff89c0], [2, 0x99bb180], [3, 0xc7cf320]]}
  layer.0.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9224e00], [5, 0x9338c60], [6, 0x79b98a0], [7, 0x7f4afa0], [0, 0x9a796c0], [1, 0x80015e0], [2, 0x99c3da0], [3, 0xc7d7f40], [4, 0x926ae20], [5, 0x937ec80], [6, 0x79ff8c0], [7, 0x7f90fc0], [0, 0x9abf6e0], [1, 0x8047600], [2, 0x9a09dc0], [3, 0xc81df60]]}
  layer.0.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92b0e40], [5, 0x93c4ca0], [6, 0x7a458e0], [7, 0x7fd6fe0]]}
  layer.0.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x93c6fc0]]}
  layer.0.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7a47c00]]}
  layer.1.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x90b0e00], [6, 0x7730d40], [7, 0x7cba540], [0, 0x97567c0]]}
  layer.1.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7c797e0], [2, 0x963de40], [3, 0xc4512c0], [4, 0x8fd1720]]}
  layer.1.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9066320], [6, 0x76e6260], [7, 0x7c5e6c0], [0, 0x96842e0]]}
  layer.1.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7c7bb00], [2, 0x9640160], [3, 0xc4535e0], [4, 0x8fd3a40]]}
  layer.1.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x96ca300], [1, 0x7c7de20], [2, 0x9642480], [3, 0xc455900]]}
  layer.1.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8fd5d60], [5, 0x90ac7c0], [6, 0x772c700], [7, 0x7cb5f00]]}
  layer.1.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9710320], [1, 0x7cc3e40], [2, 0x96884a0], [3, 0xc49b920]]}
  layer.1.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8fd8080], [5, 0x90aeae0], [6, 0x772ea20], [7, 0x7cb8220]]}
  layer.1.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x967b6c0]]}
  layer.1.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7d2ce80]]}
  layer.1.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x96ce940], [3, 0xc4e2ae0], [4, 0x8fdb540], [5, 0x90f6e20], [6, 0x7776d60], [7, 0x7d00560], [0, 0x979c7e0], [1, 0x7d35aa0], [2, 0x9714960], [3, 0xc528b00], [4, 0x9021560], [5, 0x913ce40], [6, 0x77bcd80], [7, 0x7d46580], [0, 0x97e2800], [1, 0x7d7bac0]]}
  layer.1.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x975a980], [3, 0xc56eb20], [4, 0x9067580], [5, 0x9182e60]]}
  layer.1.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7802da0], [7, 0x7d8c5a0], [0, 0x9828820], [1, 0x7dc1ae0], [2, 0x97635a0], [3, 0xc577740], [4, 0x90701a0], [5, 0x918ba80], [6, 0x7848dc0], [7, 0x7dd25c0], [0, 0x986e840], [1, 0x7e07b00], [2, 0x97a95c0], [3, 0xc5bd760], [4, 0x90b61c0], [5, 0x91d1aa0]]}
  layer.1.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x788ede0], [7, 0x7e185e0], [0, 0x98b4860], [1, 0x7e4db20]]}
  layer.1.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7e1a900]]}
  layer.1.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x91901a0]]}
  layer.2.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x98b6b80], [1, 0x7e4fe40], [2, 0x97efa60], [3, 0xc6267a0]]}
  layer.2.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x90fc660], [5, 0x9218c60], [6, 0x78922a0], [7, 0x7e23520]]}
  layer.2.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x98fcba0], [1, 0x7e95e60], [2, 0x9835a80], [3, 0xc66c7c0]]}
  layer.2.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c1eda0], [1, 0x81c9860], [2, 0x9b70f00], [3, 0xc985dc0]]}
  layer.2.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc98a880], [4, 0x9401060], [5, 0x951c940], [6, 0x7b7a9e0]]}
  layer.2.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8127f20], [0, 0x9c23ce0], [1, 0x81d6f40], [2, 0x9b871e0]]}
  layer.2.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc9d08a0], [4, 0x9447080], [5, 0x9562960], [6, 0x7bc0a00]]}
  layer.2.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c26000], [1, 0x81d9260], [2, 0x9b89500], [3, 0xca168c0]]}
  layer.2.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9a50260]]}
  layer.2.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc865120]]}
  layer.2.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92b4300], [5, 0x93cfbe0], [6, 0x7a50820], [7, 0x7fda4a0], [0, 0x9b06d20], [1, 0x80b17e0], [2, 0x9a58e80], [3, 0xc86dd40], [4, 0x92fa320], [5, 0x9415c00], [6, 0x7a96840], [7, 0x80204c0], [0, 0x9b4cd40], [1, 0x80f7800], [2, 0x9a9eea0], [3, 0xc8b3d60]]}
  layer.2.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9340340], [5, 0x945bc20], [6, 0x7adc860], [7, 0x80664e0]]}
  layer.2.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9b92d60], [1, 0x813d820], [2, 0x9ae4ec0], [3, 0xc8f9d80], [4, 0x9348f60], [5, 0x9464840], [6, 0x7ae5480], [7, 0x806f100], [0, 0x9bd8d80], [1, 0x8183840], [2, 0x9b2aee0], [3, 0xc93fda0], [4, 0x938ef80], [5, 0x94aa860], [6, 0x7b2b4a0], [7, 0x80b5120]]}
  layer.2.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c210c0], [1, 0x81cbb80], [2, 0x9b73220], [3, 0xc9880e0]]}
  layer.2.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x93d5420]]}
  layer.2.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x95138a0]]}
  layer.3.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x56755a0], [4, 0x5651ce0], [5, 0x5797320], [6, 0x56af520]]}
  layer.3.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x5564300], [0, 0x55c5ce0], [1, 0x5478c40], [2, 0x55e8440]]}
  layer.3.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x56bb5c0], [4, 0x5697d00], [5, 0x57dd340], [6, 0x56f5540]]}
  layer.3.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x5566620], [0, 0x55c8000], [1, 0x547af60], [2, 0x55ea760]]}
  layer.3.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x573b560], [7, 0x5568940], [0, 0x55ca320], [1, 0x547d280]]}
  layer.3.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x55eca80], [3, 0x5701a60], [4, 0x56de1a0], [5, 0x5834b80]]}
  layer.3.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5781580], [7, 0x55ae960], [0, 0x5610340], [1, 0x54c32a0]]}
  layer.3.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x55eeda0], [3, 0x5703d80], [4, 0x56e04c0], [5, 0x5836ea0]]}
  layer.3.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x567d100]]}
  layer.3.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x57920e0]]}
  layer.3.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x576e820], [5, 0x58c5200], [6, 0x5853a60], [7, 0x56a39e0], [0, 0x56e3540], [1, 0x55964a0], [2, 0x5685d20], [3, 0x579ad00], [4, 0x57b4840], [5, 0x590b220], [6, 0x5899a80], [7, 0x56e9a00], [0, 0x5729560], [1, 0x55dc4c0], [2, 0x56cbd40], [3, 0x57e0d20]]}
  layer.3.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x57fa860], [5, 0x5951240], [6, 0x58dfaa0], [7, 0x572fa20]]}
  layer.3.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x576f580], [1, 0x56224e0], [2, 0x5711d60], [3, 0x5826d40], [4, 0x5803480], [5, 0x5959e60], [6, 0x58e86c0], [7, 0x5738640], [0, 0x57b55a0], [1, 0x5668500], [2, 0x5757d80], [3, 0x586cd60], [4, 0x58494a0], [5, 0x599fe80], [6, 0x592e6e0], [7, 0x577e660]]}
  layer.3.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x57fb5c0], [1, 0x56ae520], [2, 0x579dda0], [3, 0x58b2d80]]}
  layer.3.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x54c6600]]}
  layer.3.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x53d6d00]]}
  layer.4.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x54c0540], [3, 0x554db20], [4, 0x5521660], [5, 0x55e9d40]]}
  layer.4.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5544200], [7, 0x5435f80], [0, 0x549e6c0], [1, 0x53df920]]}
  layer.4.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5506560], [3, 0x5593b40], [4, 0x5567680], [5, 0x562fd60]]}
  layer.4.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5546520], [7, 0x54382a0], [0, 0x54a09e0], [1, 0x53e1c40]]}
  layer.4.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5675d80], [6, 0x5548840], [7, 0x543a5c0], [0, 0x54a2d00]]}
  layer.4.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x53e3f60], [2, 0x554ca00], [3, 0x55d9fe0], [4, 0x55beec0]]}
  layer.4.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x56bbda0], [6, 0x558e860], [7, 0x54805e0], [0, 0x54e8d20]]}
  layer.4.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x53e6280], [2, 0x554ed20], [3, 0x55dc300], [4, 0x55c11e0]]}
  layer.4.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5551040]]}
  layer.4.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x55de620]]}
  layer.4.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5656360], [1, 0x55092c0], [2, 0x55f10c0], [3, 0x57060a0], [4, 0x56e27e0], [5, 0x58391c0], [6, 0x57c7a20], [7, 0x56179a0], [0, 0x569c380], [1, 0x554f2e0], [2, 0x56370e0], [3, 0x574c0c0], [4, 0x5728800], [5, 0x587f1e0], [6, 0x580da40], [7, 0x565d9c0]]}
  layer.4.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5702240], [6, 0x55f78a0], [7, 0x54cf220], [0, 0x552fee0]]}
  layer.4.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x53e9740], [2, 0x5559c60], [3, 0x55e7240], [4, 0x55c3980], [5, 0x570ae60], [6, 0x56004c0], [7, 0x54d7e40], [0, 0x5538b00], [1, 0x542f760], [2, 0x559fc80], [3, 0x562d260], [4, 0x56099a0], [5, 0x5750e80], [6, 0x56464e0], [7, 0x551de60], [0, 0x557eb20]]}
  layer.4.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5475780], [2, 0x55e5ca0], [3, 0x5673280], [4, 0x564f9c0]]}
  layer.4.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x58030c0]]}
  layer.4.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5bdaee0]]}
  layer.5.attention.self.query.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x59283a0], [1, 0x57fc000], [2, 0x58d0300], [3, 0x5a63820]]}
  layer.5.attention.self.query.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5b1b3e0], [7, 0x58e4680], [0, 0x5926080], [1, 0x57f9ce0]]}
  layer.5.attention.self.key.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5a4e2e0], [5, 0x5be3b00], [6, 0x5b1d700], [7, 0x58e6e20]]}
  layer.5.attention.self.key.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x596e3c0], [1, 0x5842020], [2, 0x5916320], [3, 0x5aa9840]]}
  layer.5.attention.self.value.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x58b50a0], [4, 0x588f940], [5, 0x5a08ec0], [6, 0x5974b80]]}
  layer.5.attention.self.value.bias:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x57c5820], [0, 0x57fea80], [1, 0x56d3860], [2, 0x57a1260]]}
  layer.5.attention.output.dense.weight:                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x58fb0c0], [4, 0x58d5960], [5, 0x5a4eee0], [6, 0x59baba0]]}
  layer.5.attention.output.dense.bias:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x57c7b40], [0, 0x5800da0], [1, 0x56d5b80], [2, 0x57a3580]]}
  layer.5.attention.output.LayerNorm.weight:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x591be00]]}
  layer.5.attention.output.LayerNorm.bias:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x56d9040]]}
  layer.5.intermediate.dense.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x57a6a40], [3, 0x5942700], [4, 0x5924a20], [5, 0x5aba240], [6, 0x5a03360], [7, 0x57cc600], [0, 0x580e000], [1, 0x56e1c60], [2, 0x57eca60], [3, 0x5988720], [4, 0x596aa40], [5, 0x5b00260], [6, 0x5a49380], [7, 0x5812620], [0, 0x5854020], [1, 0x5727c80]]}
  layer.5.intermediate.dense.bias:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x58beac0], [3, 0x5a5a780], [4, 0x5a3caa0], [5, 0x5bd22c0]]}
  layer.5.output.dense.weight:                                                                    {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5832a80], [3, 0x59ce740], [4, 0x59b0a60], [5, 0x5b46280], [6, 0x5a8f3a0], [7, 0x5858640], [0, 0x589a040], [1, 0x576dca0], [2, 0x5878aa0], [3, 0x5a14760], [4, 0x59f6a80], [5, 0x5b8c2a0], [6, 0x5ad53c0], [7, 0x589e660], [0, 0x58e0060], [1, 0x57b3cc0]]}
  layer.5.output.dense.bias:                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5ab7f20], [6, 0x5a01040], [7, 0x57ca2e0], [0, 0x580bce0]]}
  layer.5.output.LayerNorm.weight:                                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x59706e0]]}
  layer.5.output.LayerNorm.bias:                                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5b63720]]}

  # constant
  input_1_multiply_16_fork_clone2314_tile_bcast_tile_bcast:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92a6fc0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7eb1880]]}
  dc.input_tensor.softmax_18.4:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x99cec00]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7f6c500]]}
  dc.input_tensor.layernorm_38.1:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x990c120]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc742e60]]}
  dc.input_tensor.layernorm_38.6:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x90fe980]]}
  dc.input_tensor.layernorm_38.8:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x92aba80]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9b05700]]}
  dc.input_tensor.layernorm_52.1:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x808d620]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9a4fde0]]}
  dc.input_tensor.layernorm_52.6:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc863f80]]}
  dc.input_tensor.layernorm_52.8:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x92b3160]]}
  input_1_multiply_69_fork_clone2339_tile_bcast_tile_bcast:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x90ac340]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x772c280]]}
  dc.input_tensor.softmax_71.4:                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ca46e0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9756340]]}
  dc.input_tensor.layernorm_91.1:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7d09e60]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x96ce4c0]]}
  dc.input_tensor.layernorm_91.6:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc4e1940]]}
  dc.input_tensor.layernorm_91.8:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8fda3a0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x97ef5e0]]}
  dc.input_tensor.layernorm_105.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc603780]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x90fc1e0]]}
  dc.input_tensor.layernorm_105.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9217ac0]]}
  dc.input_tensor.layernorm_105.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7891100]]}
  input_1_multiply_122_fork_clone2358_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c23860]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x81d6ac0]]}
  dc.input_tensor.softmax_124.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9b759c0]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x948d0a0]]}
  dc.input_tensor.layernorm_144.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8104f00]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x812a240]]}
  dc.input_tensor.layernorm_144.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9b05b80]]}
  dc.input_tensor.layernorm_144.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x80b0640]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x93d4fa0]]}
  dc.input_tensor.layernorm_158.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x94f0880]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7b714c0]]}
  dc.input_tensor.layernorm_158.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80fb140]]}
  dc.input_tensor.layernorm_158.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7fd9300]]}
  input_1_multiply_175_fork_clone2377_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x57015e0]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x56ddd20]]}
  dc.input_tensor.softmax_177.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5823360]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x57c75a0]]}
  dc.input_tensor.layernorm_197.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x55f4980]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x55e7fc0]]}
  dc.input_tensor.layernorm_197.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x56e23a0]]}
  dc.input_tensor.layernorm_197.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5595300]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x588f4c0]]}
  dc.input_tensor.layernorm_211.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x59e5ea0]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5974700]]}
  dc.input_tensor.layernorm_211.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x57c4680]]}
  dc.input_tensor.layernorm_211.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x57fd8e0]]}
  input_1_multiply_228_fork_clone2396_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x554c580]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x55d9b60]]}
  dc.input_tensor.softmax_230.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x55ad6a0]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5701dc0]]}
  dc.input_tensor.layernorm_250.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x55d4880]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x549e240]]}
  dc.input_tensor.layernorm_250.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x552ed40]]}
  dc.input_tensor.layernorm_250.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x53e85a0]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5796ea0]]}
  dc.input_tensor.layernorm_264.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x568c500]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x5563e80]]}
  dc.input_tensor.layernorm_264.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x55c4b40]]}
  dc.input_tensor.layernorm_264.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5477aa0]]}
  input_1_multiply_281_fork_clone2415_tile_bcast_tile_bcast:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5a94300]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5c29b20]]}
  dc.input_tensor.softmax_283.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x592ce40]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x591b980]]}
  dc.input_tensor.layernorm_303.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5a94f00]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5a00bc0]]}
  dc.input_tensor.layernorm_303.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x56d7ea0]]}
  dc.input_tensor.layernorm_303.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x57a58a0]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x5942280]]}
  dc.input_tensor.layernorm_317.1:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x56b0840]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.5.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x57c9e60]]}
  dc.input_tensor.layernorm_317.6:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x59410e0]]}
  dc.input_tensor.layernorm_317.8:                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x57a00c0]]}
  lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x58e69a0]]}
  lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x5a633a0]]}
  lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x55c3500]]}
  lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x50da640]]}
  dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5117a40]]}
  lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5117ec0]]}
  lc.input_tensor.bw_in0_reshape_310.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x51e1300]]}
  lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5252ae0]]}
  lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x51f2b40]]}
  lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x52303c0]]}
  lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x52a12a0]]}
  dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x526d7c0]]}
  lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5252f60]]}
  lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5230840]]}
  lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x5238fe0]]}
  input_1_multiply_281_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5298f80]]}
  lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5230cc0]]}
  lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x4fb0900]]}
  lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x4ff6920]]}
  lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x4fb0d80]]}
  lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x4ff6920]]}
  lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x4fff540]]}
  dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x503c940]]}
  lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x503c940]]}
  lc.input_tensor.bw_in0_reshape_257.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x508b580]]}
  lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x50da1c0]]}
  lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5117a40]]}
  lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x5154e40]]}
  lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5117a40]]}
  dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5117a40]]}
  lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5252ae0]]}
  lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x547ad40]]}
  lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5349f40]]}
  input_1_multiply_228_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5422840]]}
  lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x547b1c0]]}
  lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x534a3c0]]}
  lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x53a0ea0]]}
  lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x534a840]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x54c1660]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x54951a0]]}
  dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x553ace0]]}
  lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x54b7d40]]}
  lc.input_tensor.bw_in0_reshape_204.dc.squeeze.0_operand_commute_clone133_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x532e060]]}
  lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x52c54a0]]}
  lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x52bd180]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x53a8860]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x536b460]]}
  dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x53b1000]]}
  lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x5350c00]]}
  lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x52bd600]]}
  lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x52ce540]]}
  input_1_multiply_175_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x532e4e0]]}
  lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x52bda80]]}
  lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x52ce9c0]]}
  lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7b71940]]}
  lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c233e0]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9b75540]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc98a400]]}
  dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x93de040]]}
  lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x951c4c0]]}
  lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone198_brcst_reduce_sum_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7987340]]}
  lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc179b20]]}
  lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8cf17e0]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7964380]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x93acb40]]}
  dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7a13800]]}
  lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7650d00]]}
  lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1bffc0]]}
  lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7a7c840]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x93fb300]]}
  lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1c0440]]}
  lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7a7ccc0]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7710960]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8ac0de0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7710de0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8ac0960]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x91dce40]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x786ee40]]}
  lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone273_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7296620]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8bd92e0]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8ac0de0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x77e2980]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9150980]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7152960]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xbfd55e0]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7650880]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8f44960]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8fae220]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8ac0960]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8f44de0]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7bc95e0]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7bed320]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc3c4e00]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8f45260]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8fb72c0]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7659da0]]}
  lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone335_brcst_reduce_sum_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7b11da0]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x950b060]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x94903e0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8e2c460]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8e95d20]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x755b800]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7ae58c0]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9490860]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9514100]]}
  input_1_multiply_16_tile_bcast_tile_bcast:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7b12220]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9490ce0]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9514580]]}

  # epoch_to_epoch
  e2e__fused_op_11_0:                                                                             {input: _fused_op_11, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  e2e__fused_op_23_0:                                                                             {input: _fused_op_23, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xca18be0]]}
  e2e__fused_op_35_0:                                                                             {input: _fused_op_35, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x31aa9040]]}
  e2e_attention_mask_chip_to_chip_nop_1_0:                                                        {input: attention_mask_chip_to_chip_nop_1, type: queue, entries: 96, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}
  e2e__fused_op_47_0:                                                                             {input: _fused_op_47, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x5aabb60]]}
  e2e__fused_op_59_0:                                                                             {input: _fused_op_59, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0xae8e6e0]]}
  e2e__fused_op_70_0:                                                                             {input: _fused_op_70, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x11e1e760]]}
  e2e__fused_op_69_0:                                                                             {input: _fused_op_69, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0xb7cf400]]}
  e2e_gelu_309_0:                                                                                 {input: gelu_309, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0xca3bc60], [4, 0xb0b6880], [5, 0xf9aa0a0], [6, 0xdeac440]]}
  e2e__fused_op_67_0:                                                                             {input: _fused_op_67, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x103de740], [0, 0x9d8f3e0], [1, 0xd4f4420], [2, 0xf008740]]}
  e2e__fused_op_66_0:                                                                             {input: _fused_op_66, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0xd5c8720]]}
  e2e__fused_op_65_0:                                                                             {input: _fused_op_65, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0xef34440]]}
  e2e__fused_op_64_0:                                                                             {input: _fused_op_64, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x10a48760]]}
  e2e_matmul_294_0:                                                                               {input: matmul_294, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0xe47bc80]]}
  e2e_matmul_287_0:                                                                               {input: matmul_287, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0xc3abc40], [4, 0xaa26860], [5, 0xf31a080], [6, 0xd81c420]]}
  e2e__fused_op_62_0:                                                                             {input: _fused_op_62, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0xd1868c0]]}
  e2e_bw_in1_matmul_294_matmul_1_0:                                                               {input: bw_in1_matmul_294_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0xbf31440]]}
  e2e_bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0_0:                                              {input: bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x13eee7a0]]}
  e2e_bw_in0_matmul_294_matmul_1_0:                                                               {input: bw_in0_matmul_294_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0xff7c480]]}
  e2e_matmul_273_0:                                                                               {input: matmul_273, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0xb8a1420], [1, 0x10974460], [2, 0x10b1a780], [3, 0xfebbca0]]}
  e2e_matmul_267_0:                                                                               {input: matmul_267, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0xcaf68a0], [5, 0x113ea0c0], [6, 0xf8ec460], [7, 0x1385e780]]}
  e2e__fused_op_76_0:                                                                             {input: _fused_op_76, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x11a7a0e0]]}
  e2e__fused_op_58_0:                                                                             {input: _fused_op_58, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x9ab86c0]]}
  e2e__fused_op_57_0:                                                                             {input: _fused_op_57, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x955b380]]}
  e2e_gelu_256_0:                                                                                 {input: gelu_256, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x9c4bbe0], [4, 0x82c6800], [5, 0xcbba020], [6, 0xb0bc3c0]]}
  e2e__fused_op_55_0:                                                                             {input: _fused_op_55, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x967c3a0], [7, 0x944e6c0], [0, 0x7b1b360], [1, 0x7fa43a0]]}
  e2e__fused_op_81_0:                                                                             {input: _fused_op_81, type: queue, entries: 48, grid_size: [2, 8], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x111aa7a0], [3, 0x1054bcc0], [4, 0x106068e0], [5, 0x1210a100], [6, 0x10c9c4a0], [7, 0x1457e7c0], [0, 0xc5c1460], [1, 0x116944a0], [2, 0x1134e7c0], [3, 0x106efce0], [4, 0x107aa900], [5, 0x122ae120], [6, 0x10e404c0], [7, 0x147227e0], [0, 0xc765480], [1, 0x118384c0]]}
  e2e__fused_op_54_0:                                                                             {input: _fused_op_54, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0xbab4400]]}
  e2e__fused_op_80_0:                                                                             {input: _fused_op_80, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x11004480]]}
  e2e__fused_op_53_0:                                                                             {input: _fused_op_53, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x99e43c0]]}
  e2e__fused_op_52_0:                                                                             {input: _fused_op_52, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x962d3a0]]}
  e2e_matmul_241_0:                                                                               {input: matmul_241, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0xb4f86e0]]}
  e2e_matmul_234_0:                                                                               {input: matmul_234, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0xb68bc00], [4, 0x9d06820], [5, 0xe5fa040], [6, 0xcafc3e0]]}
  e2e__fused_op_50_0:                                                                             {input: _fused_op_50, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0xc8ce700]]}
  e2e_matmul_220_0:                                                                               {input: matmul_220, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0xb4243e0], [2, 0xcf38700], [3, 0xbd1bc20], [4, 0xa396840]]}
  e2e_matmul_214_0:                                                                               {input: matmul_214, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0xec8a060], [6, 0xd18c400], [7, 0xfd4e720], [0, 0x96ff3c0]]}
  e2e__fused_op_46_0:                                                                             {input: _fused_op_46, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5918640]]}
  e2e__fused_op_45_0:                                                                             {input: _fused_op_45, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5a94780]]}
  e2e__fused_op_87_0:                                                                             {input: _fused_op_87, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x114f27e0]]}
  e2e_gelu_203_0:                                                                                 {input: gelu_203, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x5844340], [2, 0x7358660], [3, 0x74ebb80], [4, 0x5b667a0]]}
  e2e__fused_op_43_0:                                                                             {input: _fused_op_43, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5c29fa0], [6, 0x5b6c340], [7, 0x593e660], [0, 0x5979300]]}
  e2e__fused_op_42_0:                                                                             {input: _fused_op_42, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x7669fc0]]}
  e2e__fused_op_41_0:                                                                             {input: _fused_op_41, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x737e680]]}
  e2e__fused_op_40_0:                                                                             {input: _fused_op_40, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x73b9320]]}
  e2e_matmul_188_0:                                                                               {input: matmul_188, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x75ac360]]}
  e2e_matmul_181_0:                                                                               {input: matmul_181, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x7284360], [2, 0x8d98680], [3, 0x8f2bba0], [4, 0x75a67c0]]}
  e2e__fused_op_38_0:                                                                             {input: _fused_op_38, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x973a000]]}
  e2e_bw_in1_matmul_181_transpose_0_0:                                                            {input: bw_in1_matmul_181_transpose_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x10893d00]]}
  e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0:                                              {input: bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x1094e920]]}
  e2e_bw_in0_matmul_188_matmul_1_0:                                                               {input: bw_in0_matmul_188_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x12452140]]}
  e2e_matmul_167_0:                                                                               {input: matmul_167, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x7914380], [2, 0x94286a0], [3, 0x95bbbc0], [4, 0x7c367e0]]}
  e2e_matmul_161_0:                                                                               {input: matmul_161, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x90a9fe0], [6, 0x8fec380], [7, 0x8dbe6a0], [0, 0x748b340]]}
  e2e_bw_in0_matmul_181_matmul_1_0:                                                               {input: bw_in0_matmul_181_matmul_1, type: queue, entries: 96, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x32832060], [0, 0x32b7a080], [0, 0x32ec20a0], [0, 0x3320a0c0]]}
  e2e_bw_in0_matmul_167_matmul_1_0:                                                               {input: bw_in0_matmul_167_matmul_1, type: queue, entries: 96, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x335520e0], [0, 0x3389a100], [0, 0x33be2120], [0, 0x33f2a140]]}
  e2e_bw_in0_matmul_161_matmul_1_0:                                                               {input: bw_in0_matmul_161_matmul_1, type: queue, entries: 96, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x34272160], [0, 0x345ba180], [0, 0x349021a0], [0, 0x34c4a1c0]]}
  e2e__fused_op_90_chip_to_chip_nop_0_0:                                                          {input: _fused_op_90_chip_to_chip_nop_0, type: queue, entries: 96, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31b12040]]}
  e2e__fused_op_34_0:                                                                             {input: _fused_op_34, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb7e8aa0]]}
  e2e__fused_op_33_0:                                                                             {input: _fused_op_33, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xddfd8c0]]}
  e2e_gelu_150_0:                                                                                 {input: gelu_150, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xbceb5e0], [2, 0xa97d880], [3, 0xf178c40], [4, 0xbbed580]]}
  e2e__fused_op_31_0:                                                                             {input: _fused_op_31, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x10bb83a0], [1, 0xd72b600], [2, 0xc3bd8a0], [3, 0x10bb8c60]]}
  e2e__fused_op_30_0:                                                                             {input: _fused_op_30, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xd748a00]]}
  e2e__fused_op_29_0:                                                                             {input: _fused_op_29, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xd67a740]]}
  e2e__fused_op_28_0:                                                                             {input: _fused_op_28, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xd62d5a0]]}
  e2e_matmul_135_0:                                                                               {input: matmul_135, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xf188a20]]}
  e2e_matmul_128_0:                                                                               {input: matmul_128, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xd228ac0], [7, 0xf0ba760], [0, 0x125f83c0], [1, 0xf16b620]]}
  e2e__fused_op_26_0:                                                                             {input: _fused_op_26, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xf74a780]]}
  e2e_bw_in1_matmul_135_matmul_1_0:                                                               {input: bw_in1_matmul_135_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xe55f900]]}
  e2e_bw_in0_matmul_135_matmul_1_0:                                                               {input: bw_in0_matmul_135_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x13318cc0]]}
  e2e_matmul_114_0:                                                                               {input: matmul_114, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x12c883e0], [1, 0xf7fb640], [2, 0xdecf8e0], [3, 0x12c88ca0]]}
  e2e_matmul_108_0:                                                                               {input: matmul_108, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x125f8c80], [4, 0xd6ff5c0], [5, 0x10bc8a40], [6, 0xd8b8ae0]]}
  e2e__fused_op_97_0:                                                                             {input: _fused_op_97, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xfe8b660]]}
  e2e__fused_op_22_0:                                                                             {input: _fused_op_22, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x81db580]]}
  e2e__fused_op_21_0:                                                                             {input: _fused_op_21, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9b8b820]]}
  e2e_gelu_97_0:                                                                                  {input: gelu_97, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x948d520], [5, 0xafe89a0], [6, 0x9646a40], [7, 0x9b6a6e0]]}
  e2e__fused_op_19_0:                                                                             {input: _fused_op_19, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x95a8980], [6, 0x7c06a20], [7, 0x812a6c0], [0, 0x9c28320]]}
  e2e__fused_op_102_0:                                                                            {input: _fused_op_102, type: queue, entries: 48, grid_size: [2, 8], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x11258a60], [6, 0xdf48b00], [7, 0x12bca7a0], [0, 0x13318400], [1, 0x1051b680], [2, 0xebef920], [3, 0x14038ce0], [4, 0xe41f600], [5, 0x113fca80], [6, 0xe0ecb20], [7, 0x12d6e7c0], [0, 0x134bc420], [1, 0x106bf6a0], [2, 0xed93940], [3, 0x141dcd00], [4, 0xe5c3620]]}
  e2e__fused_op_18_0:                                                                             {input: _fused_op_18, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xf178380]]}
  e2e__fused_op_101_0:                                                                            {input: _fused_op_101, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xdd8f5e0]]}
  e2e__fused_op_17_0:                                                                             {input: _fused_op_17, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0xb5aa700]]}
  e2e__fused_op_16_0:                                                                             {input: _fused_op_16, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xb086a60]]}
  e2e_matmul_82_0:                                                                                {input: matmul_82, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9c1b5a0]]}
  e2e_matmul_75_0:                                                                                {input: matmul_75, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9c5d840], [3, 0xe458c00], [4, 0xaecd540], [5, 0xca289c0]]}
  e2e__fused_op_14_0:                                                                             {input: _fused_op_14, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb668340]]}
  e2e_matmul_61_0:                                                                                {input: matmul_61, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb55d560], [5, 0xd0b89e0], [6, 0xb158a80], [7, 0xcfea720]]}
  e2e_matmul_55_0:                                                                                {input: matmul_55, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xeae8360], [1, 0xb65b5c0], [2, 0xa2ed860], [3, 0xeae8c20]]}
  e2e__fused_op_10_0:                                                                             {input: _fused_op_10, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  e2e__fused_op_9_0:                                                                              {input: _fused_op_9, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  e2e__fused_op_108_0:                                                                            {input: _fused_op_108, type: queue, entries: 48, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x115a0aa0]]}
  e2e_gelu_44_0:                                                                                  {input: gelu_44, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900], [5, 0x4fb0900], [6, 0x4fb0900], [7, 0x4fb0900]]}
  e2e__fused_op_7_0:                                                                              {input: _fused_op_7, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x69f0920], [1, 0x5082920], [2, 0x69f0920], [3, 0x69f0920]]}
  e2e__fused_op_6_0:                                                                              {input: _fused_op_6, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x69f0920]]}
  e2e__fused_op_5_0:                                                                              {input: _fused_op_5, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x69f0920]]}
  e2e__fused_op_4_0:                                                                              {input: _fused_op_4, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x69f0920]]}
  e2e_matmul_29_0:                                                                                {input: matmul_29, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  e2e_matmul_22_0:                                                                                {input: matmul_22, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x69f0920], [0, 0x8430940], [1, 0x6ac2940], [2, 0x8430940]]}
  e2e__fused_op_2_0:                                                                              {input: _fused_op_2, type: queue, entries: 192, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8430940]]}
  e2e_bw_in1_matmul_29_matmul_1_0:                                                                {input: bw_in1_matmul_29_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0xe290b40]]}
  e2e_matmul_8_0:                                                                                 {input: matmul_8, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8430940], [5, 0x8430940], [6, 0x6ac2940], [7, 0x7080940]]}
  e2e_matmul_2_0:                                                                                 {input: matmul_2, type: queue, entries: 192, grid_size: [1, 4], t: 1, mblock: [2, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8ac0960], [1, 0x7152960], [2, 0x8ac0960], [3, 0xb8b0960]]}
  e2e_bw_in1_matmul_14_matmul_1_0:                                                                {input: bw_in1_matmul_14_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x12f127e0]]}
  e2e_bw_in0_matmul_14_matmul_1_0:                                                                {input: bw_in0_matmul_14_matmul_1, type: queue, entries: 48, grid_size: [1, 1], t: 16, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x13660440]]}

  # loss
  loss_bert_encoders.output_layernorm_317:                                                        {input: HOST, type: queue, entries: 192, grid_size: [1, 1], t: 1, mblock: [4, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x30069020]]}

  # grad_accumulator
  grad_acc_layer.5.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5a456c0]]}
  grad_acc_layer.5.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x58c76e0]]}
  grad_acc_layer.5.output.dense.bias:                                                             {input: bw_in1_add_314_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5180600]]}
  grad_acc_layer.5.output.dense.weight:                                                           {input: bw_in1_matmul_312_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x51552c0], [4, 0x5117ec0], [5, 0x513aa60], [6, 0x50f4ec0], [7, 0x50daac0], [0, 0x513aa60], [1, 0x5118340], [2, 0x5189220], [3, 0x519b2e0], [4, 0x515dee0], [5, 0x5180a80], [6, 0x513aee0], [7, 0x5120ae0], [0, 0x5180a80], [1, 0x515e360], [2, 0x51cf240]]}
  grad_acc_layer.5.intermediate.dense.bias:                                                       {input: bw_in0_reshape_310.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x51a3f00]]}
  grad_acc_layer.5.intermediate.dense.weight:                                                     {input: bw_in1_matmul_306_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x51c6aa0], [6, 0x5180f00], [7, 0x5166b00], [0, 0x51c6aa0], [1, 0x51a4380], [2, 0x5215260], [3, 0x51e1780], [4, 0x51c6f20], [5, 0x520cac0], [6, 0x51c6f20], [7, 0x51acb20], [0, 0x520cac0], [1, 0x51ea3a0], [2, 0x525b280], [3, 0x52277a0], [4, 0x520cf40]]}
  grad_acc_layer.5.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x520cf40]]}
  grad_acc_layer.5.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x50ec2a0]]}
  grad_acc_layer.5.attention.output.dense.bias:                                                   {input: bw_in1_add_300_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5422cc0]]}
  grad_acc_layer.5.attention.output.dense.weight:                                                 {input: bw_in1_matmul_298_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5252f60], [6, 0x5215b60], [7, 0x51f2fc0], [0, 0x5252f60]]}
  grad_acc_layer.5.attention.self.value.bias:                                                     {input: bw_in1_add_289_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x52a1720]]}
  grad_acc_layer.5.attention.self.value.weight:                                                   {input: bw_in1_matmul_287_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x52907e0], [4, 0x52533e0], [5, 0x5298f80], [6, 0x525bb80]]}
  grad_acc_layer.5.attention.self.key.bias:                                                       {input: bw_in1_add_275_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x4ff6da0]]}
  grad_acc_layer.5.attention.self.key.weight:                                                     {input: bw_in1_matmul_273_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x4fb0900], [3, 0x4fb0900], [4, 0x4fb0900], [5, 0x4fb0900]]}
  grad_acc_layer.5.attention.self.query.bias:                                                     {input: bw_in1_add_269_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x4fb0900]]}
  grad_acc_layer.5.attention.self.query.weight:                                                   {input: bw_in1_matmul_267_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x4fb0900], [1, 0x4fb9520], [2, 0x4ff6920], [3, 0x4ff6920]]}
  grad_acc_layer.4.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x4ff6920]]}
  grad_acc_layer.4.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x4fb9520]]}
  grad_acc_layer.4.output.dense.bias:                                                             {input: bw_in1_add_261_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x4fb0900]]}
  grad_acc_layer.4.output.dense.weight:                                                           {input: bw_in1_matmul_259_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x4fff540], [6, 0x4fb1200], [7, 0x4fc2140], [0, 0x4ff6da0], [1, 0x4fff9c0], [2, 0x505f960], [3, 0x503cdc0], [4, 0x4fff9c0], [5, 0x5045560], [6, 0x4ff7220], [7, 0x5008160], [0, 0x503cdc0], [1, 0x50459e0], [2, 0x50a5980], [3, 0x5082de0], [4, 0x50459e0]]}
  grad_acc_layer.4.intermediate.dense.bias:                                                       {input: bw_in0_reshape_257.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x503d240]]}
  grad_acc_layer.4.intermediate.dense.weight:                                                     {input: bw_in1_matmul_253_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x504e180], [0, 0x5082de0], [1, 0x508ba00], [2, 0x50eb9a0], [3, 0x50c8e00], [4, 0x508ba00], [5, 0x508ba00], [6, 0x5060260], [7, 0x50941a0], [0, 0x50c8e00], [1, 0x50d1a20], [2, 0x51319c0], [3, 0x510ee20], [4, 0x50d1a20], [5, 0x50d1a20], [6, 0x50a6280]]}
  grad_acc_layer.4.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x510ee20]]}
  grad_acc_layer.4.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x51779e0]]}
  grad_acc_layer.4.attention.output.dense.bias:                                                   {input: bw_in1_add_247_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x53dd0c0]]}
  grad_acc_layer.4.attention.output.dense.weight:                                                 {input: bw_in1_matmul_245_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x52cee40], [0, 0x5337580], [1, 0x5303f20], [2, 0x53dc820]]}
  grad_acc_layer.4.attention.self.value.bias:                                                     {input: bw_in1_add_236_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x543d940]]}
  grad_acc_layer.4.attention.self.value.weight:                                                   {input: bw_in1_matmul_234_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5468c80], [6, 0x53e5ce0], [7, 0x5314e60], [0, 0x537d5a0]]}
  grad_acc_layer.4.attention.self.key.bias:                                                       {input: bw_in1_add_222_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x5446560]]}
  grad_acc_layer.4.attention.self.key.weight:                                                     {input: bw_in1_matmul_220_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x54aeca0], [6, 0x542bd00], [7, 0x535ae80], [0, 0x53c35c0]]}
  grad_acc_layer.4.attention.self.query.bias:                                                     {input: bw_in1_add_216_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x5460060]]}
  grad_acc_layer.4.attention.self.query.weight:                                                   {input: bw_in1_matmul_214_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x547b640], [4, 0x544f180], [5, 0x54f4cc0], [6, 0x5471d20]]}
  grad_acc_layer.3.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x54095e0]]}
  grad_acc_layer.3.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x542b8e0]]}
  grad_acc_layer.3.output.dense.bias:                                                             {input: bw_in1_add_208_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x53a1320]]}
  grad_acc_layer.3.output.dense.weight:                                                           {input: bw_in1_matmul_206_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5412200], [1, 0x534acc0], [2, 0x5434500], [3, 0x54c1ae0], [4, 0x5495620], [5, 0x555dd00], [6, 0x54b81c0], [7, 0x53a9f40], [0, 0x5458220], [1, 0x5390ce0], [2, 0x547a520], [3, 0x5507b00], [4, 0x54db640], [5, 0x55a3d20], [6, 0x54fe1e0], [7, 0x53eff60]]}
  grad_acc_layer.3.intermediate.dense.bias:                                                       {input: bw_in0_reshape_204.dc.squeeze.0_operand_commute_clone133_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[6, 0x52a1ba0]]}
  grad_acc_layer.3.intermediate.dense.weight:                                                     {input: bw_in1_matmul_200_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x5239460], [0, 0x5299400], [1, 0x5231140], [2, 0x52f0360], [3, 0x531c820], [4, 0x52df420], [5, 0x5324fc0], [6, 0x52c4bc0], [7, 0x527f480], [0, 0x52df420], [1, 0x5277160], [2, 0x5336380], [3, 0x5362840], [4, 0x5325440], [5, 0x536afe0], [6, 0x530abe0]]}
  grad_acc_layer.3.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x5325440]]}
  grad_acc_layer.3.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x537c3a0]]}
  grad_acc_layer.3.attention.output.dense.bias:                                                   {input: bw_in1_add_194_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[7, 0x52c5920]]}
  grad_acc_layer.3.attention.output.dense.weight:                                                 {input: bw_in1_matmul_192_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x52aa340], [3, 0x52d6800], [4, 0x5299400], [5, 0x52defa0]]}
  grad_acc_layer.3.attention.self.value.bias:                                                     {input: bw_in1_add_183_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x5384fc0]]}
  grad_acc_layer.3.attention.self.value.weight:                                                   {input: bw_in1_matmul_181_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x53a8ce0], [4, 0x536b8e0], [5, 0x53d4020], [6, 0x5351080]]}
  grad_acc_layer.3.attention.self.key.bias:                                                       {input: bw_in1_add_169_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[2, 0x538dbe0]]}
  grad_acc_layer.3.attention.self.key.weight:                                                     {input: bw_in1_matmul_167_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[3, 0x53eed00], [4, 0x53b1900], [5, 0x541a040], [6, 0x53970a0]]}
  grad_acc_layer.3.attention.self.query.bias:                                                     {input: bw_in1_add_163_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[0, 0x532e960]]}
  grad_acc_layer.3.attention.self.query.weight:                                                   {input: bw_in1_matmul_161_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x52bdf00], [2, 0x5396800], [3, 0x5434d20], [4, 0x53f7920]]}
  grad_acc_layer.2.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x80fc2e0]]}
  grad_acc_layer.2.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x81cdea0]]}
  grad_acc_layer.2.output.dense.bias:                                                             {input: bw_in1_add_155_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7b71dc0]]}
  grad_acc_layer.2.output.dense.weight:                                                           {input: bw_in1_matmul_153_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x78fb300], [2, 0x927a240], [3, 0xc061aa0], [4, 0x8be1f00], [5, 0x8bd9760], [6, 0x7296aa0], [7, 0x784c300], [0, 0x9294ac0], [1, 0x7941320], [2, 0x92c0260], [3, 0xc0a7ac0], [4, 0x8c27f20], [5, 0x8c1f780], [6, 0x72dcac0], [7, 0x7892320], [0, 0x92daae0]]}
  grad_acc_layer.2.intermediate.dense.bias:                                                       {input: bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone198_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9306280]]}
  grad_acc_layer.2.intermediate.dense.weight:                                                     {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc0edae0], [4, 0x8c6df40], [5, 0x8c657a0], [6, 0x7322ae0], [7, 0x78d8340], [0, 0x9320b00], [1, 0x79877c0], [2, 0x93292a0], [3, 0xc133b00], [4, 0x8cb3f60], [5, 0x8cab7c0], [6, 0x7368b00], [7, 0x791e360], [0, 0x9366b20], [1, 0x79cd7e0], [2, 0x936f2c0]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8cf9f80]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x73aeb20]]}
  grad_acc_layer.2.attention.output.dense.bias:                                                   {input: bw_in1_add_141_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x928bea0]]}
  grad_acc_layer.2.attention.output.dense.weight:                                                 {input: bw_in1_matmul_139_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7964800], [0, 0x93acfc0], [1, 0x7a36820], [2, 0x93b52e0]]}
  grad_acc_layer.2.attention.self.value.bias:                                                     {input: bw_in1_add_130_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8d48bc0]]}
  grad_acc_layer.2.attention.self.value.weight:                                                   {input: bw_in1_matmul_128_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8d37c80], [6, 0x73fd760], [7, 0x79aa820], [0, 0x93f2fe0]]}
  grad_acc_layer.2.attention.self.key.bias:                                                       {input: bw_in1_add_116_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8d517e0]]}
  grad_acc_layer.2.attention.self.key.weight:                                                     {input: bw_in1_matmul_114_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8d7dca0], [6, 0x7443780], [7, 0x79f0840], [0, 0x9439000]]}
  grad_acc_layer.2.attention.self.query.bias:                                                     {input: bw_in1_add_110_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x93fb780]]}
  grad_acc_layer.2.attention.self.query.weight:                                                   {input: bw_in1_matmul_108_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc179fa0], [4, 0x8d02ba0], [5, 0x8cf1c60], [6, 0x73b7740]]}
  grad_acc_layer.1.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xbf40980]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7175980]]}
  grad_acc_layer.1.output.dense.bias:                                                             {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x91e55e0]]}
  grad_acc_layer.1.output.dense.weight:                                                           {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b4d2a0], [5, 0x8b4d2a0], [6, 0x720a5e0], [7, 0x779d2a0], [0, 0x91ffe60], [1, 0x786f2c0], [2, 0x91ee200], [3, 0xbfd5a60], [4, 0x8b932c0], [5, 0x8b932c0], [6, 0x7250600], [7, 0x77e32c0], [0, 0x9245e80], [1, 0x78b52e0], [2, 0x9234220], [3, 0xc01ba80]]}
  grad_acc_layer.1.intermediate.dense.bias:                                                       {input: bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone273_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x78292e0]]}
  grad_acc_layer.1.intermediate.dense.weight:                                                     {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9150e00], [1, 0x77e2e00], [2, 0x91595a0], [3, 0xbf495a0], [4, 0x8ac1260], [5, 0x8ac1260], [6, 0x717e5a0], [7, 0x7711260], [0, 0x9196e20], [1, 0x7828e20], [2, 0x919f5c0], [3, 0xbf8f5c0], [4, 0x8b07280], [5, 0x8b07280], [6, 0x71c45c0], [7, 0x7757280]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8bd92e0]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9150980]]}
  grad_acc_layer.1.attention.output.dense.bias:                                                   {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7651180]]}
  grad_acc_layer.1.attention.output.dense.weight:                                                 {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x94d7180], [3, 0xc2f2da0], [4, 0x8efe940], [5, 0x8f68200]]}
  grad_acc_layer.1.attention.self.value.bias:                                                     {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7bb7da0]]}
  grad_acc_layer.1.attention.self.value.weight:                                                   {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x955aa20], [1, 0x7b612e0], [2, 0x951d1a0], [3, 0xc338dc0]]}
  grad_acc_layer.1.attention.self.key.bias:                                                       {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7bc09c0]]}
  grad_acc_layer.1.attention.self.key.weight:                                                     {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x95a0a40], [1, 0x7ba7300], [2, 0x95631c0], [3, 0xc37ede0]]}
  grad_acc_layer.1.attention.self.query.bias:                                                     {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8fae6a0]]}
  grad_acc_layer.1.attention.self.query.weight:                                                   {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x760a860], [7, 0x7b71d80], [0, 0x9514a00], [1, 0x7b1b2c0]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                                         {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x95e6a60]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                                       {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x95a91e0]]}
  grad_acc_layer.0.output.dense.bias:                                                             {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7bc9a60]]}
  grad_acc_layer.0.output.dense.weight:                                                           {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x95ef680], [1, 0x7bed7a0], [2, 0x95b1e00], [3, 0xc3c5280], [4, 0x8f456e0], [5, 0x8fda2e0], [6, 0x765a220], [7, 0x7bd2680], [0, 0x96356a0], [1, 0x7c337c0], [2, 0x95f7e20], [3, 0xc40b2a0], [4, 0x8f8b700], [5, 0x9020300], [6, 0x76a0240], [7, 0x7c186a0]]}
  grad_acc_layer.0.intermediate.dense.bias:                                                       {input: bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone335_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x7a36860]]}
  grad_acc_layer.0.intermediate.dense.weight:                                                     {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x947f020], [1, 0x7a7d140], [2, 0x94043a0], [3, 0xc2068e0], [4, 0x8da0420], [5, 0x8e09ce0], [6, 0x74cf7c0], [7, 0x7a59880], [0, 0x94c5040], [1, 0x7ac3160], [2, 0x944a3c0], [3, 0xc24c900], [4, 0x8de6440], [5, 0x8e4fd00], [6, 0x75157e0], [7, 0x7a9f8a0]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                                               {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7b09180]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                                             {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc292920]]}
  grad_acc_layer.0.attention.output.dense.bias:                                                   {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x950b4e0]]}
  grad_acc_layer.0.attention.output.dense.weight:                                                 {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1c08c0], [4, 0x8d5a400], [5, 0x8dc3cc0], [6, 0x74897a0]]}
  grad_acc_layer.0.attention.self.value.bias:                                                     {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc29b540]]}
  grad_acc_layer.0.attention.self.value.weight:                                                   {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8e2c8e0], [5, 0x8e961a0], [6, 0x757e820], [7, 0x7ae5d40]]}
  grad_acc_layer.0.attention.self.key.bias:                                                       {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc2a4160]]}
  grad_acc_layer.0.attention.self.key.weight:                                                     {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8e72900], [5, 0x8edc1c0], [6, 0x75c4840], [7, 0x7b2bd60]]}
  grad_acc_layer.0.attention.self.query.bias:                                                     {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7b126a0]]}
  grad_acc_layer.0.attention.self.query.weight:                                                   {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9491160], [3, 0xc2acd80], [4, 0x8eb8920], [5, 0x8f221e0]]}

graphs:
  fwd_0_0:
    target_device: 0
    input_count: 6
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone2314_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 2], inputs: [_fused_op_0, softmax_18.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_2: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4, _fused_op_1],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_2, matmul_22],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_3, _fused_op_3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_3__fused_op_5: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_3],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_3__fused_op_5: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_1__fused_op_3__fused_op_5],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_4: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_3__fused_op_5, _fused_op_4],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_6: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_5, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_41: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_6, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_7: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_41, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_44: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_7],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_6_add_51: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6_add_51: {type: nop, grid_loc: [4, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_6_add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [4, 9], grid_size: [1, 1], inputs: [matmul_47, buffer_0__fused_op_6_add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_8: {type: fused_op, grid_loc: [4, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [5, 10], grid_size: [1, 1], inputs: [_fused_op_8, _fused_op_8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_8__fused_op_10: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [_fused_op_8],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_8__fused_op_10: {type: nop, grid_loc: [5, 9], grid_size: [1, 1], inputs: [buffer_1__fused_op_8__fused_op_10],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_9: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_10: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_8__fused_op_10, _fused_op_9],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_11: {type: fused_op, grid_loc: [6, 10], grid_size: [1, 1], inputs: [_fused_op_10, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    attention_mask_chip_to_chip_nop_1: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_1:
    target_device: 0
    input_count: 6
    matmul_55: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_11_0, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_61: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_11_0, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_67: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone2339_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_12],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_13: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_12, softmax_71.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_13, lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_75: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [e2e__fused_op_11_0, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_14: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4, _fused_op_13],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_82: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_14, matmul_75],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_90: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_86, e2e__fused_op_11_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_15: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, add_90],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_15, _fused_op_15],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_15__fused_op_17: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_15],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_15__fused_op_17: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_15__fused_op_17],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_16: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_15__fused_op_17, _fused_op_16],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_18: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_17, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_94: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_18, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_19: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_94, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_97: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_19],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_18_add_104: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_18],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_18_add_104: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_18_add_104],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_104: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_100, buffer_0__fused_op_18_add_104],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_20: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, add_104],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_20, _fused_op_20],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_20__fused_op_22: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_20],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_20__fused_op_22: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_20__fused_op_22],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_21: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_22: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_20__fused_op_22, _fused_op_21],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_23: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_22, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_2:
    target_device: 0
    input_count: 6
    matmul_108: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_23_0, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_114: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_23_0, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_120: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_24: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_fork_clone2358_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_124.dc.reduce_max.0: {type: reduce, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_24],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_24, softmax_124.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_124.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_25, lc.input_tensor.softmax_124.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_128: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [e2e__fused_op_23_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_26: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_124.4, _fused_op_25],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_135: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_26, matmul_128],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_143: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_139, e2e__fused_op_23_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_27: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1, add_143],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_27, _fused_op_27],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_27__fused_op_29: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_27],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_27__fused_op_29: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_27__fused_op_29],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_28: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_29: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_27__fused_op_29, _fused_op_28],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_30: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_29, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_147: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_30, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_31: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_147, layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_150: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_31],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_30_add_157: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_30],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_30_add_157: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_30_add_157],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_153, buffer_0__fused_op_30_add_157],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_32: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, add_157],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_32, _fused_op_32],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_32__fused_op_34: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_32],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_32__fused_op_34: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_32__fused_op_34],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_33: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_34: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_32__fused_op_34, _fused_op_33],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_35: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_34, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_3:
    target_device: 1
    input_count: 6
    matmul_161: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_35_0, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_167: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_35_0, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_173: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175_fork_clone2377_tile_bcast_tile_bcast, e2e_attention_mask_chip_to_chip_nop_1_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_177.dc.reduce_max.0: {type: reduce, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_36],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_37: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_36, softmax_177.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_177.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_37, lc.input_tensor.softmax_177.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_181: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [e2e__fused_op_35_0, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_38: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_177.4, _fused_op_37],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_188: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_38, matmul_181],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_196: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_192, e2e__fused_op_35_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_39: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.1, layernorm_197.dc.reduce_sum.0.lc1, add_196],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_39, _fused_op_39],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.4, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_39__fused_op_41: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_39],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_39__fused_op_41: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_39__fused_op_41],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_40: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_197.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_39__fused_op_41, _fused_op_40],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_42: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_41, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_200: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_42, layer.3.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_43: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_200, layer.3.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_203: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_43],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_42_add_210: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_42],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_42_add_210: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_42_add_210],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_206, buffer_0__fused_op_42_add_210],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1, add_210],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_44, _fused_op_44],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_44__fused_op_46: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_44],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_44__fused_op_46: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_44__fused_op_46],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_45: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_211.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_46: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_44__fused_op_46, _fused_op_45],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_47: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_46, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_4:
    target_device: 1
    input_count: 6
    matmul_214: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_47_0, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_220: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_47_0, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_226: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_48: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_226, input_1_multiply_228_fork_clone2396_tile_bcast_tile_bcast, e2e_attention_mask_chip_to_chip_nop_1_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_230.dc.reduce_max.0: {type: reduce, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_48],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_49: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_48, softmax_230.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_230.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_49, lc.input_tensor.softmax_230.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_234: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [e2e__fused_op_47_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_50: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_230.4, _fused_op_49],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_241: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_50, matmul_234],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_249: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_245, e2e__fused_op_47_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_51: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.1, layernorm_250.dc.reduce_sum.0.lc1, add_249],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_250.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_51, _fused_op_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.4, lc.input_tensor.layernorm_250.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_51__fused_op_53: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_51],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_51__fused_op_53: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_51__fused_op_53],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_52: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.6, layernorm_250.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_250.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_51__fused_op_53, _fused_op_52],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_54: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_53, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_253: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_54, layer.4.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_55: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_253, layer.4.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_256: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_55],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_54_add_263: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_54],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_54_add_263: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_54_add_263],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_263: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_259, buffer_0__fused_op_54_add_263],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_264.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_56: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.1, layernorm_264.dc.reduce_sum.0.lc1, add_263],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_264.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_56, _fused_op_56],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_264.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.4, lc.input_tensor.layernorm_264.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_56__fused_op_58: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_56],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_56__fused_op_58: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_56__fused_op_58],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_57: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.6, layernorm_264.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_264.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_58: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_56__fused_op_58, _fused_op_57],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_59: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_58, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}

  fwd_0_5:
    target_device: 1
    input_count: 6
    matmul_267: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e__fused_op_59_0, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_273: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e__fused_op_59_0, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    matmul_279: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_60: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_279, input_1_multiply_281_fork_clone2415_tile_bcast_tile_bcast, e2e_attention_mask_chip_to_chip_nop_1_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_283.dc.reduce_max.0: {type: reduce, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_60],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_61: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_60, softmax_283.dc.reduce_max.0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    softmax_283.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_61, lc.input_tensor.softmax_283.dc.reduce_sum.3.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_287: {type: matmul, grid_loc: [1, 3], grid_size: [1, 4], inputs: [e2e__fused_op_59_0, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 4, min_buffer_input: 0, u_kt: 8}}
    _fused_op_62: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_283.4, _fused_op_61],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 144], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_294: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_62, matmul_287],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [1, 8], grid_size: [1, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_302: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_298, e2e__fused_op_59_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_303.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_63: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.1, layernorm_303.dc.reduce_sum.0.lc1, add_302],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_303.dc.multiply.4: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_63, _fused_op_63],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_303.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.4, lc.input_tensor.layernorm_303.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_63__fused_op_65: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_63],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_63__fused_op_65: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1__fused_op_63__fused_op_65],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_64: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.6, layernorm_303.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_303.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_65: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_63__fused_op_65, _fused_op_64],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5, kernel_broadcast: {input_1: 16}}}
    _fused_op_66: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_65, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6}}
    matmul_306: {type: matmul, grid_loc: [3, 0], grid_size: [4, 4], inputs: [_fused_op_66, layer.5.intermediate.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    _fused_op_67: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_306, layer.5.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 7}}
    gelu_309: {type: gelu, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_67],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [4, 4], grid_size: [4, 4], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    buffer_1__fused_op_66_add_316: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_66],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_66_add_316: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1__fused_op_66_add_316],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_316: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [matmul_312, buffer_0__fused_op_66_add_316],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_317.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_68: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.1, layernorm_317.dc.reduce_sum.0.lc1, add_316],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3, kernel_broadcast: {input_1: 16}}}
    layernorm_317.dc.multiply.4: {type: multiply, grid_loc: [5, 9], grid_size: [1, 1], inputs: [_fused_op_68, _fused_op_68],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_317.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.4, lc.input_tensor.layernorm_317.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    buffer_1__fused_op_68__fused_op_70: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_68],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_68__fused_op_70: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1__fused_op_68__fused_op_70],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_69: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.6, layernorm_317.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_317.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_70: {type: fused_op, grid_loc: [6, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_68__fused_op_70, _fused_op_69],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    _fused_op_71: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [_fused_op_70, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 6, kernel_broadcast: {input_2: 64, input_1: 64}}}
    _fused_op_71_output_nop_0: {type: nop, grid_loc: [6, 10], grid_size: [1, 1], inputs: [_fused_op_71], untilize_output: true,
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}

  bwd_0_6:
    target_device: 1
    input_count: 6
    bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoders.output_layernorm_317], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_317_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e__fused_op_70_0, loss_bert_encoders.output_layernorm_317],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_317_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_72: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [loss_bert_encoders.output_layernorm_317, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_72, lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_72, e2e__fused_op_70_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_73: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_70_0, bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6, _fused_op_72, e2e__fused_op_69_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_314_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0, _fused_op_73], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_312_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [4, 4], inputs: [_fused_op_73, layer.5.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_312_transpose_0: {type: nop, grid_loc: [1, 0], grid_size: [1, 4], inputs: [e2e_gelu_309_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_312_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_312_transpose_0, _fused_op_73], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_74: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 8], inputs: [e2e__fused_op_67_0, bw_in0_matmul_312_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_310.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_310.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0, _fused_op_74], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_306_matmul_1: {type: matmul, grid_loc: [4, 8], grid_size: [4, 4], inputs: [_fused_op_74, layer.5.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_306_transpose_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [e2e__fused_op_66_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_306_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [8, 2], inputs: [bw_in1_matmul_306_transpose_0, _fused_op_74], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    buffer_1__fused_op_73_bw_in0_layernorm_303_combine_add_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_73],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_73_bw_in0_layernorm_303_combine_add_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_1__fused_op_73_bw_in0_layernorm_303_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_303_combine_add_0: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_73_bw_in0_layernorm_303_combine_add_0, bw_in0_matmul_306_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_303_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_303_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [e2e__fused_op_65_0, bw_in0_layernorm_303_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_303_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_75: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_303_combine_add_0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [_fused_op_75, lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_303_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_75, e2e__fused_op_65_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_303_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_76: {type: fused_op, grid_loc: [8, 6], grid_size: [1, 1], inputs: [e2e__fused_op_65_0, bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6, _fused_op_75, e2e__fused_op_64_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_300_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0, _fused_op_76], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_298_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [_fused_op_76, layer.5.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_298_transpose_0: {type: nop, grid_loc: [8, 10], grid_size: [1, 1], inputs: [e2e_matmul_294_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_298_matmul_1: {type: matmul, grid_loc: [9, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_298_transpose_0, _fused_op_76], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_294_matmul_1: {type: matmul, grid_loc: [9, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_298_matmul_1, e2e_matmul_287_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_294_transpose_0: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [e2e__fused_op_62_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_294_matmul_1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_294_transpose_0, bw_in0_matmul_298_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [bw_in1_matmul_294_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}

  bwd_0_7:
    target_device: 1
    input_count: 6
    bw_in1_add_289_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_294_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_287_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0_0, layer.5.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_287_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_59_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_287_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_287_transpose_0, e2e_bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_283_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_294_matmul_1_0, e2e__fused_op_62_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_softmax_283_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_77: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_294_matmul_1_0, bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_62_0, input_1_multiply_281_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_279_matmul_1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_77, e2e_matmul_273_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_279_transpose_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_matmul_267_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_279_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_279_transpose_0, _fused_op_77],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_275_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0, bw_in1_matmul_279_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_274.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_279_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_273_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 4], inputs: [bw_in0_reshape_274.dc.unsqueeze.0_squeeze_0, layer.5.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_273_transpose_0: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [e2e__fused_op_59_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_273_matmul_1: {type: matmul, grid_loc: [1, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_273_transpose_0, bw_in0_reshape_274.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_269_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0, bw_in0_matmul_279_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_267_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_279_matmul_1, layer.5.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_267_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e__fused_op_59_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_267_matmul_1: {type: matmul, grid_loc: [2, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_267_transpose_0, bw_in0_matmul_279_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_78: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_287_matmul_1, bw_in0_matmul_273_matmul_1, bw_in0_matmul_267_matmul_1, e2e__fused_op_76_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 78}}
    bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_78], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_264_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [e2e__fused_op_58_0, _fused_op_78],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_264_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_79: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_78, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_79, lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_264_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_79, e2e__fused_op_58_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_264_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_80: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e__fused_op_58_0, bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6, _fused_op_79, e2e__fused_op_57_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_261_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0, _fused_op_80], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_259_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_80, layer.4.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_259_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 4], inputs: [e2e_gelu_256_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_259_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_259_transpose_0, _fused_op_80], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_81: {type: fused_op, grid_loc: [7, 4], grid_size: [2, 8], inputs: [e2e__fused_op_55_0, bw_in0_matmul_259_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_257.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_257.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0, _fused_op_81], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}

  bwd_0_8:
    target_device: 1
    input_count: 6
    bw_in0_matmul_253_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_81_0, layer.4.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_253_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_54_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_253_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [8, 2], inputs: [bw_in1_matmul_253_transpose_0, e2e__fused_op_81_0], gradient_op: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_250_combine_add_0: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e__fused_op_80_0, bw_in0_matmul_253_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_250_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_250_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [e2e__fused_op_53_0, bw_in0_layernorm_250_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_250_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_82: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_layernorm_250_combine_add_0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_82, lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_250_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_82, e2e__fused_op_53_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [bw_in0_layernorm_250_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_83: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e__fused_op_53_0, bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6, _fused_op_82, e2e__fused_op_52_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_247_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0, _fused_op_83], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_245_matmul_1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 4], inputs: [_fused_op_83, layer.4.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_245_transpose_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e_matmul_241_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_245_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_245_transpose_0, _fused_op_83], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_241_matmul_1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_245_matmul_1, e2e_matmul_234_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_241_transpose_0: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_50_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_241_matmul_1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_241_transpose_0, bw_in0_matmul_245_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_236_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0, bw_in1_matmul_241_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_235.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_241_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_234_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_reshape_235.dc.unsqueeze.0_squeeze_0, layer.4.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_234_transpose_0: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_234_matmul_1: {type: matmul, grid_loc: [4, 7], grid_size: [4, 1], inputs: [bw_in1_matmul_234_transpose_0, bw_in0_reshape_235.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_230_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_241_matmul_1, e2e__fused_op_50_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [bw_in0_softmax_230_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_84: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_241_matmul_1, bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_50_0, input_1_multiply_228_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_226_matmul_1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_84, e2e_matmul_220_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_226_transpose_0: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e_matmul_214_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_226_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_226_transpose_0, _fused_op_84],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_222_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0, bw_in1_matmul_226_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_221.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_226_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_220_matmul_1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [bw_in0_reshape_221.dc.unsqueeze.0_squeeze_0, layer.4.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_220_transpose_0: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_220_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [4, 1], inputs: [bw_in1_matmul_220_transpose_0, bw_in0_reshape_221.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_216_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0, bw_in0_matmul_226_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_214_matmul_1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 4], inputs: [bw_in0_matmul_226_matmul_1, layer.4.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_214_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_214_matmul_1: {type: matmul, grid_loc: [7, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_214_transpose_0, bw_in0_matmul_226_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    buffer_1__fused_op_83__fused_op_85: {type: nop, grid_loc: [7, 2], grid_size: [1, 1], inputs: [_fused_op_83],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_83__fused_op_85: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [buffer_1__fused_op_83__fused_op_85],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_85: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_234_matmul_1, bw_in0_matmul_220_matmul_1, bw_in0_matmul_214_matmul_1, buffer_0__fused_op_83__fused_op_85],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 78}}
    bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_85], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e__fused_op_46_0, _fused_op_85],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_86: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [_fused_op_85, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_86, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [_fused_op_86, e2e__fused_op_46_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_87: {type: fused_op, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e__fused_op_46_0, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6, _fused_op_86, e2e__fused_op_45_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}

  bwd_0_9:
    target_device: 1
    input_count: 6
    bw_in1_add_208_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0, e2e__fused_op_87_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_206_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_87_0, layer.3.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_206_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e_gelu_203_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_206_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_206_transpose_0, e2e__fused_op_87_0], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_88: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 8], inputs: [e2e__fused_op_43_0, bw_in0_matmul_206_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_204.dc.squeeze.0_operand_commute_clone133_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_204.dc.squeeze.0_operand_commute_clone133_brcst_reduce_sum_0.0, _fused_op_88], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_200_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_88, layer.3.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_200_transpose_0: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e__fused_op_42_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_200_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [8, 2], inputs: [bw_in1_matmul_200_transpose_0, _fused_op_88], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_197_combine_add_0: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [e2e__fused_op_87_0, bw_in0_matmul_200_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_197_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 10], grid_size: [1, 1], inputs: [e2e__fused_op_41_0, bw_in0_layernorm_197_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_89: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_combine_add_0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [_fused_op_89, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_89, e2e__fused_op_41_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_90: {type: fused_op, grid_loc: [7, 8], grid_size: [1, 1], inputs: [e2e__fused_op_41_0, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6, _fused_op_89, e2e__fused_op_40_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_194_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0, _fused_op_90], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_192_matmul_1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 4], inputs: [_fused_op_90, layer.3.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_192_transpose_0: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e_matmul_188_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_192_matmul_1: {type: matmul, grid_loc: [8, 7], grid_size: [4, 1], inputs: [bw_in1_matmul_192_transpose_0, _fused_op_90], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_188_matmul_1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_192_matmul_1, e2e_matmul_181_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_188_transpose_0: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [e2e__fused_op_38_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_188_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_188_transpose_0, bw_in0_matmul_192_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_183_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0, bw_in1_matmul_188_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_188_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_181_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0, layer.3.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_181_transpose_0: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [e2e__fused_op_35_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    _fused_op_90_chip_to_chip_nop_0: {type: nop, grid_loc: [7, 9], grid_size: [1, 1], inputs: [_fused_op_90],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  bwd_0_10:
    target_device: 1
    input_count: 6
    bw_in1_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_181_transpose_0_0, e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_177_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_188_matmul_1_0, e2e__fused_op_38_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_177_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_91: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_188_matmul_1_0, bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_38_0, input_1_multiply_175_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_173_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_91, e2e_matmul_167_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_173_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_matmul_161_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_173_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_173_transpose_0, _fused_op_91],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_169_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0, bw_in1_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_173_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_167_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0, layer.3.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_167_transpose_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_35_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_167_matmul_1: {type: matmul, grid_loc: [1, 3], grid_size: [4, 1], inputs: [bw_in1_matmul_167_transpose_0, bw_in0_reshape_168.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_163_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0, bw_in0_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_161_matmul_1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 4], inputs: [bw_in0_matmul_173_matmul_1, layer.3.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_161_transpose_0: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_35_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_161_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [4, 1], inputs: [bw_in1_matmul_161_transpose_0, bw_in0_matmul_173_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}

  bwd_0_11:
    target_device: 0
    input_count: 6
    _fused_op_92: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_181_matmul_1_0, e2e_bw_in0_matmul_167_matmul_1_0, e2e_bw_in0_matmul_161_matmul_1_0, e2e__fused_op_90_chip_to_chip_nop_0_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 78}}
    bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_92], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, _fused_op_92],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_93: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_92, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [_fused_op_93, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [_fused_op_93, e2e__fused_op_34_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_94: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6, _fused_op_93, e2e__fused_op_33_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_155_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0, _fused_op_94], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_153_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [4, 4], inputs: [_fused_op_94, layer.2.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_153_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 4], inputs: [e2e_gelu_150_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_153_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_153_transpose_0, _fused_op_94], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_95: {type: fused_op, grid_loc: [4, 4], grid_size: [2, 8], inputs: [e2e__fused_op_31_0, bw_in0_matmul_153_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone198_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone198_brcst_reduce_sum_0.0, _fused_op_95], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [4, 4], inputs: [_fused_op_95, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [e2e__fused_op_30_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [6, 4], grid_size: [8, 2], inputs: [bw_in1_matmul_147_transpose_0, _fused_op_95], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    buffer_1__fused_op_94_bw_in0_layernorm_144_combine_add_0: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [_fused_op_94],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_94_bw_in0_layernorm_144_combine_add_0: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [buffer_1__fused_op_94_bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_144_combine_add_0: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_94_bw_in0_layernorm_144_combine_add_0, bw_in0_matmul_147_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_144_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 8], grid_size: [1, 1], inputs: [e2e__fused_op_29_0, bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_96: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_combine_add_0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_96, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [_fused_op_96, e2e__fused_op_29_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_97: {type: fused_op, grid_loc: [8, 7], grid_size: [1, 1], inputs: [e2e__fused_op_29_0, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6, _fused_op_96, e2e__fused_op_28_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_141_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0, _fused_op_97], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_139_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [_fused_op_97, layer.2.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_139_transpose_0: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [e2e_matmul_135_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_139_matmul_1: {type: matmul, grid_loc: [9, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_139_transpose_0, _fused_op_97], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_135_matmul_1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [bw_in0_matmul_139_matmul_1, e2e_matmul_128_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_135_transpose_0: {type: nop, grid_loc: [9, 9], grid_size: [1, 1], inputs: [e2e__fused_op_26_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_135_matmul_1: {type: matmul, grid_loc: [9, 10], grid_size: [1, 1], inputs: [bw_in1_matmul_135_transpose_0, bw_in0_matmul_139_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}

  bwd_0_12:
    target_device: 0
    input_count: 6
    bw_in1_add_130_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_135_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_bw_in1_matmul_135_matmul_1_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [0, 7], grid_size: [4, 1], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_reshape_129.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 11], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_135_matmul_1_0, e2e__fused_op_26_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_98: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_135_matmul_1_0, bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_26_0, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_120_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_98, e2e_matmul_114_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_120_transpose_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_matmul_108_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_120_matmul_1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in1_matmul_120_transpose_0, _fused_op_98],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_116_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0, bw_in1_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_120_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_114_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 4], inputs: [bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0, layer.2.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_114_transpose_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_114_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [4, 1], inputs: [bw_in1_matmul_114_transpose_0, bw_in0_reshape_115.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_110_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_108_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [bw_in0_matmul_120_matmul_1, layer.2.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_108_transpose_0: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_108_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_108_transpose_0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_99: {type: fused_op, grid_loc: [2, 11], grid_size: [1, 1], inputs: [bw_in0_matmul_128_matmul_1, bw_in0_matmul_114_matmul_1, bw_in0_matmul_108_matmul_1, e2e__fused_op_97_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 78}}
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_99], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [e2e__fused_op_22_0, _fused_op_99],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_100: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_99, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_100, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_100, e2e__fused_op_22_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_101: {type: fused_op, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e__fused_op_22_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, _fused_op_100, e2e__fused_op_21_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, _fused_op_101], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_101, layer.1.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 4], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_100_transpose_0, _fused_op_101], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_102: {type: fused_op, grid_loc: [8, 0], grid_size: [2, 8], inputs: [e2e__fused_op_19_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone273_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone273_brcst_reduce_sum_0.0, _fused_op_102], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}

  bwd_0_13:
    target_device: 0
    input_count: 6
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_102_0, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_18_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [8, 2], inputs: [bw_in1_matmul_94_transpose_0, e2e__fused_op_102_0], gradient_op: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e__fused_op_101_0, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [e2e__fused_op_17_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_103: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_103, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_103, e2e__fused_op_17_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_104: {type: fused_op, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e__fused_op_17_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, _fused_op_103, e2e__fused_op_16_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, _fused_op_104], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 4], inputs: [_fused_op_104, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [2, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_86_transpose_0, _fused_op_104], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_86_matmul_1, e2e_matmul_75_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_14_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, bw_in0_matmul_86_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [3, 9], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [4, 7], grid_size: [4, 1], inputs: [bw_in1_matmul_75_transpose_0, bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e__fused_op_14_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_105: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_14_0, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [_fused_op_105, e2e_matmul_61_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, _fused_op_105],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 4], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [4, 1], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 4], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [e2e__fused_op_11_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [7, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    buffer_1__fused_op_104__fused_op_106: {type: nop, grid_loc: [7, 2], grid_size: [1, 1], inputs: [_fused_op_104],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_104__fused_op_106: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [buffer_1__fused_op_104__fused_op_106],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_106: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1, bw_in0_matmul_55_matmul_1, buffer_0__fused_op_104__fused_op_106],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 78}}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_106], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, _fused_op_106],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_107: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [_fused_op_106, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [_fused_op_107, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [_fused_op_107, e2e__fused_op_10_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_108: {type: fused_op, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, _fused_op_107, e2e__fused_op_9_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}

  bwd_0_14:
    target_device: 0
    input_count: 6
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, e2e__fused_op_108_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_108_0, layer.0.output.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 4], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [64, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 8], inputs: [bw_in1_matmul_47_transpose_0, e2e__fused_op_108_0], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_109: {type: fused_op, grid_loc: [3, 4], grid_size: [2, 8], inputs: [e2e__fused_op_7_0, bw_in0_matmul_47_matmul_1],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true, fused_op_id: 74}}
    bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone335_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone335_brcst_reduce_sum_0.0, _fused_op_109], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [_fused_op_109, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 16, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e__fused_op_6_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [8, 2], inputs: [bw_in1_matmul_41_transpose_0, _fused_op_109], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [e2e__fused_op_108_0, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 9], grid_size: [1, 1], inputs: [e2e__fused_op_5_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_110: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 72}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [_fused_op_110, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [448, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_110, e2e__fused_op_5_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 32}}
    _fused_op_111: {type: fused_op, grid_loc: [7, 8], grid_size: [1, 1], inputs: [e2e__fused_op_5_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, _fused_op_110, e2e__fused_op_4_0],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 32, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {c: 32}], input_2_tms: [broadcast: {c: 32}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 73}}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, _fused_op_111], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [_fused_op_111, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 8}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [8, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_33_transpose_0, _fused_op_111], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [e2e__fused_op_2_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [9, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_22_transpose_0, bw_in1_matmul_29_matmul_1], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e__fused_op_2_0],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    _fused_op_112: {type: fused_op, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_2_0, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 16, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {z: 16}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 77, kernel_broadcast: {input_3: 1}}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [_fused_op_112, e2e_matmul_8_0],
         t: 16, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose, vslice: 16]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, _fused_op_112],
         t: 16, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 32, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}

  bwd_0_15:
    target_device: 0
    input_count: 6
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_29_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [4, 1], inputs: [bw_in1_matmul_8_transpose_0, e2e_bw_in1_matmul_14_matmul_1_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [0, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_2_transpose_0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true, grid_transpose: true,
         t: 1, mblock: [4, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 6, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q1_shadow: 0, $gptr_q2_shadow: 0, $gptr_q8_shadow: 0, $gptr_q0_shadow: 0, $gptr_q7_shadow: 0, $lptr_q6: 0, $lptr_q8: 0, $gptr_q9: 0, $gptr_q8: 0, $gptr_q1: 0, $lptr_q9: 0, $gptr_q3_shadow: 0, $gptr_q11: 0, $gptr_q7: 0, $lptr_q7: 0, $gptr_q11_shadow: 0, $lptr_q11: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q6_shadow: 0, $lptr_q5: 0, $gptr_q10: 0, $gptr_q5: 0, $gptr_q5_shadow: 0, $lptr_q4: 0, $gptr_q3: 0, $gptr_q9_shadow: 0, $gptr_q2: 0, $lptr_q2: 0, $lptr_q1: 0, $lptr_q10: 0, $lptr_q3: 0, $lptr_q0: 0, $gptr_q0: 0}
    - varinst: [$gptr_q11, set, $gptr_q11_shadow]
    - varinst: [$gptr_q9, set, $gptr_q9_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q5, set, $gptr_q5_shadow]
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -   varinst: [$gptr_q7, set, $gptr_q7_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone2314_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone2339_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_fork_clone2358_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_3, queue_settings: {
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_attention_mask_chip_to_chip_nop_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_fork_clone2377_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 192]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 192]
    -   execute: {graph_name: fwd_0_4, queue_settings: {
               e2e_attention_mask_chip_to_chip_nop_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_47_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_fork_clone2396_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_230.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 192]
    -   varinst: [$gptr_q9_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 192]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: fwd_0_5, queue_settings: {
               e2e_attention_mask_chip_to_chip_nop_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_59_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_fork_clone2415_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_283.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 192]
    -   varinst: [$gptr_q11_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 192]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 384]
    - endloop

  - run_bwd_0:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 6, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q13_shadow: 0, $gptr_q19_shadow: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $lptr_q4: 0, $gptr_q7_shadow: 0, $lptr_q6: 0, $lptr_q1: 0, $lptr_q19: 0, $lptr_q18: 0, $gptr_q19: 0, $gptr_q18: 0, $lptr_q14: 0, $gptr_q20: 0, $lptr_q5: 0, $gptr_q21: 0, $gptr_q7: 0, $lptr_q7: 0, $lptr_q17: 0, $lptr_q8: 0, $lptr_q20: 0, $gptr_q11: 0, $gptr_q1: 0, $lptr_q9: 0, $gptr_q9: 0, $gptr_q22: 0, $gptr_q3: 0, $lptr_q21: 0, $lptr_q22: 0, $gptr_q2_shadow: 0, $gptr_q23: 0, $lptr_q23: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q17: 0, $gptr_q8: 0, $gptr_q14: 0, $lptr_q16: 0, $gptr_q16: 0, $lptr_q13: 0, $gptr_q13: 0, $gptr_q15: 0, $lptr_q12: 0, $gptr_q12: 0, $lptr_q0: 0, $lptr_q3: 0, $lptr_q10: 0, $lptr_q15: 0, $lptr_q11: 0, $gptr_q5: 0, $gptr_q10: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   varinst: [$gptr_q19, set, $gptr_q19_shadow]
    -   varinst: [$gptr_q13, set, $gptr_q13_shadow]
    -   varinst: [$gptr_q7, set, $gptr_q7_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   execute: {graph_name: bwd_0_6, queue_settings: {
               loss_bert_encoders.output_layernorm_317: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_matmul_287_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_294_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_64_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_65_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_67_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_gelu_309_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_69_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_70_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_310.dc.squeeze.0_operand_commute_clone40_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.5.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 384]
    -   execute: {graph_name: bwd_0_7, queue_settings: {
               e2e__fused_op_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_57_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_59_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_267_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_273_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_matmul_294_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in1_matmul_294_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_reshape_288.dc.unsqueeze.0_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_257.dc.squeeze.0_operand_commute_clone81_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_8, queue_settings: {
               e2e__fused_op_45_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_47_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_214_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_220_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_234_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_241_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_52_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_53_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_54_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_80_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_81_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.4.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_9, queue_settings: {
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_181_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_188_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_40_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_42_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_43_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_gelu_203_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_87_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_204.dc.squeeze.0_operand_commute_clone133_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_10, queue_settings: {
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_167_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_bw_in0_matmul_188_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_reshape_182.dc.unsqueeze.0_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_181_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_11, queue_settings: {
               e2e_matmul_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_bw_in0_matmul_181_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_matmul_167_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_matmul_161_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_90_chip_to_chip_nop_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_151.dc.squeeze.0_operand_commute_clone198_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 192]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 192]
    -   execute: {graph_name: bwd_0_12, queue_settings: {
               e2e__fused_op_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_matmul_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_bw_in0_matmul_135_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_bw_in1_matmul_135_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_98.dc.squeeze.0_operand_commute_clone273_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_13, queue_settings: {
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_16_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_101_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_102_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_14, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_reshape_45.dc.squeeze.0_operand_commute_clone335_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q19_shadow, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 96]
    -   execute: {graph_name: bwd_0_15, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_bw_in1_matmul_29_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_bw_in0_matmul_14_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_bw_in1_matmul_14_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 384]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 96]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 384]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 96]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 6, $c_one: 1, $c_zero: 0}


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 1], ublock: [2, 2], output: dest}
        - softmax_18.dc.exp.2: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 2], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.add.5: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.6: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.7: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 4}, tile_broadcast: c], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.2: { type: multiply, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.subtract.3: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  5: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 8], ublock: [2, 4], output: output}
  6: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.13: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  7: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - add_43: { type: add, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  72: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 8], ublock: [2, 4], output: output}
  73: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.4: { type: multiply, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_317_layernorm_bw_0.dc.add.5: { type: add, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.7: { type: multiply, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_317_layernorm_bw_0.dc.subtract.8: { type: subtract, inputs: [input4, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: dest}
        - bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.9: { type: multiply, inputs: [dest, input5], input_1_tms: [tile_broadcast: c], mblock: [2, 8], ublock: [2, 4], output: output}
  74: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_gelu_309_gelu_derivative_0: { type: gelu_derivative, inputs: [input0], mblock: [1, 4], ublock: [2, 4], output: dest}
        - bw_in0_gelu_309_multiply_1: { type: multiply, inputs: [dest, input1], mblock: [1, 4], ublock: [2, 4], output: output}
  77: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - bw_in0_softmax_283_softmax_bw_0.dc.subtract.2: { type: subtract, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_softmax_283_softmax_bw_0.dc.multiply.3: { type: multiply, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_multiply_281_multiply_0: { type: multiply, inputs: [dest, input3], mblock: [2, 1], ublock: [2, 4], output: output}
  78: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - bw_in0_reshape_265.dc.squeeze.0_combine_add_0: { type: add, inputs: [input0, input1], mblock: [2, 8], ublock: [2, 4], output: dest}
        - bw_in0_reshape_265.dc.squeeze.0_combine_add_1: { type: add, inputs: [dest, input2], mblock: [2, 8], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_264_combine_add_0: { type: add, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 8], ublock: [2, 4], output: output}
