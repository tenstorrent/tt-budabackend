# git checkout c80cb4ddc
# pytest pybuda/test/tvm/nlp/pytorch/tests_A/test_t5_small.py::test_t5_past_cache[Wormhole_B0-t5-base]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                        {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                 {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30034d20]]}
  encoder_attention_mask:                                         {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30032c80]]}
  key_value_states:                                               {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30002060]]}

  # output
  t5.output_reshape_1163:                                         {input: t5.output_reshape_1163_tm_nop, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 252], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  t5.output_hstack_1165:                                          {input: t5.output_hstack_1165_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5237f80]]}
  t5.output_hstack_1167:                                          {input: t5.output_hstack_1167_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5180a20]]}
  t5.output_hslice_63:                                            {input: hslice_63_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5106be0]]}
  t5.output_hslice_76:                                            {input: hslice_76_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e11300]]}
  t5.output_hstack_1169:                                          {input: t5.output_hstack_1169_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x831d120]]}
  t5.output_hstack_1171:                                          {input: t5.output_hstack_1171_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e3b4c0]]}
  t5.output_hslice_161:                                           {input: hslice_161_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e8d4c0]]}
  t5.output_hslice_171:                                           {input: hslice_171_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e74ea0]]}
  t5.output_hstack_1173:                                          {input: t5.output_hstack_1173_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x577b8c0]]}
  t5.output_hstack_1175:                                          {input: t5.output_hstack_1175_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dbe180]]}
  t5.output_hslice_256:                                           {input: hslice_256_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa019f20]]}
  t5.output_hslice_266:                                           {input: hslice_266_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5cc8180]]}
  t5.output_hstack_1177:                                          {input: t5.output_hstack_1177_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5777080]]}
  t5.output_hstack_1179:                                          {input: t5.output_hstack_1179_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xadf1c20]]}
  t5.output_hslice_351:                                           {input: hslice_351_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xac1c800]]}
  t5.output_hslice_361:                                           {input: hslice_361_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x89088c0]]}
  t5.output_hstack_1181:                                          {input: t5.output_hstack_1181_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab65ae0]]}
  t5.output_hstack_1183:                                          {input: t5.output_hstack_1183_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x536dda0]]}
  t5.output_hslice_446:                                           {input: hslice_446_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e4e260]]}
  t5.output_hslice_456:                                           {input: hslice_456_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e35c40]]}
  t5.output_hstack_1185:                                          {input: t5.output_hstack_1185_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fca100]]}
  t5.output_hstack_1187:                                          {input: t5.output_hstack_1187_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa94ce00]]}
  t5.output_hslice_541:                                           {input: hslice_541_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa8d2fc0]]}
  t5.output_hslice_551:                                           {input: hslice_551_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3be2040]]}
  t5.output_hstack_1189:                                          {input: t5.output_hstack_1189_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a335c0]]}
  t5.output_hstack_1191:                                          {input: t5.output_hstack_1191_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39d4e60]]}
  t5.output_hslice_636:                                           {input: hslice_636_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x395b020]]}
  t5.output_hslice_646:                                           {input: hslice_646_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x837eec0]]}
  t5.output_hstack_1193:                                          {input: t5.output_hstack_1193_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d3ce00]]}
  t5.output_hstack_1195:                                          {input: t5.output_hstack_1195_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d02fa0]]}
  t5.output_hslice_731:                                           {input: hslice_731_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37c4b40]]}
  t5.output_hslice_741:                                           {input: hslice_741_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37ac520]]}
  t5.output_hstack_1197:                                          {input: t5.output_hstack_1197_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c75880]]}
  t5.output_hstack_1199:                                          {input: t5.output_hstack_1199_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3935ec0]]}
  t5.output_hslice_826:                                           {input: hslice_826_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38bc080]]}
  t5.output_hslice_836:                                           {input: hslice_836_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38a3a60]]}
  t5.output_hstack_1201:                                          {input: t5.output_hstack_1201_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5069160]]}
  t5.output_hstack_1203:                                          {input: t5.output_hstack_1203_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x95985c0]]}
  t5.output_hslice_921:                                           {input: hslice_921_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x951e780]]}
  t5.output_hslice_931:                                           {input: hslice_931_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5445e40]]}
  t5.output_hstack_1205:                                          {input: t5.output_hstack_1205_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46bc040]]}
  t5.output_hstack_1207:                                          {input: t5.output_hstack_1207_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x543e3a0]]}
  t5.output_hslice_1016:                                          {input: hslice_1016_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4119b00]]}
  t5.output_hslice_1026:                                          {input: hslice_1026_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41014e0]]}
  t5.output_hstack_1209:                                          {input: t5.output_hstack_1209_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f5dd40]]}
  t5.output_hstack_1211:                                          {input: t5.output_hstack_1211_tm_nop, type: ram, entries: 15, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8522640]]}
  t5.output_hslice_1111:                                          {input: hslice_1111_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x460ec60]]}
  t5.output_hslice_1121:                                          {input: hslice_1121_output_nop_0, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x45f6640]]}
  t5.output_nop_1212:                                             {input: nop_1212, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x3f0020]}

  # parameter
  t5.shared.weight:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1004, 24], ublock_order: r, df: Float16_b, layout: flat, target_device: 0, loc: dram, dram: [[2, 0x52eeca0]]}
  t5.decoder.block.0.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dcd980]]}
  t5.decoder.block.0.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa2827c0], [4, 0xa18ebc0]]}
  states_1:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5237f80]]}
  t5.decoder.block.0.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d6c160], [0, 0x5ba28a0], [1, 0x4f95980]]}
  states_3:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5180a20]]}
  t5.decoder.block.0.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d0a100], [0, 0x5adf860], [1, 0x4f33920]]}
  t5.decoder.block.0.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x511f200], [3, 0xa220760], [4, 0xa1269c0]]}
  t5.decoder.block.0.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5cfdde0]]}
  t5.decoder.block.0.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa18e340], [4, 0xa0945a0]]}
  states_5:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5106be0]]}
  states_7:                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e11300]]}
  t5.decoder.block.0.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6214e0], [4, 0xa4cc900], [5, 0x6116980]]}
  t5.decoder.block.0.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e04fe0]]}
  t5.decoder.block.0.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa3a80c0], [5, 0x5ff2140], [0, 0x5d72bc0], [1, 0x5153820], [2, 0x84f8680], [3, 0xa58f0c0], [4, 0xa43a4e0], [5, 0x6084560]]}
  t5.decoder.block.0.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x83d3e40], [3, 0xa46a880], [4, 0xa315ca0], [5, 0x5f5fd20], [0, 0x5ce07a0], [1, 0x50c1400], [2, 0x8466260], [3, 0xa4fcca0]]}
  t5.decoder.block.1.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5f53a00]]}
  t5.decoder.block.1.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa3d8460], [4, 0xa283880]]}
  states_25:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x831d120]]}
  t5.decoder.block.1.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ef21e0], [0, 0x5c66960], [1, 0x505f3a0]]}
  states_27:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e3b4c0]]}
  t5.decoder.block.1.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x82bb0c0], [3, 0xa315420], [4, 0xa221820]]}
  t5.decoder.block.1.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dd9ca0], [0, 0x5c04900], [1, 0x4ff79e0]]}
  t5.decoder.block.1.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x576ad60]]}
  t5.decoder.block.1.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9df4700], [5, 0x59efc20]]}
  states_29:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e8d4c0]]}
  states_31:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e74ea0]]}
  t5.decoder.block.1.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5708d00], [1, 0x4bab900], [2, 0x4d8b720]]}
  t5.decoder.block.1.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4b5ac60]]}
  t5.decoder.block.1.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4a870c0], [2, 0x4c66ee0], [3, 0x9d2bd60], [4, 0x9d61260], [5, 0x5956e20], [0, 0x56768e0], [1, 0x4b194e0], [2, 0x4cf9300]]}
  t5.decoder.block.1.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x58325e0], [0, 0x55520a0], [1, 0x49f4ca0], [2, 0x4bd4ac0], [3, 0x9c99940], [4, 0x9ccee40], [5, 0x58c4a00], [0, 0x55e44c0]]}
  t5.decoder.block.2.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4bc87a0]]}
  t5.decoder.block.2.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x54bfc80], [1, 0x4962880]]}
  states_49:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x577b8c0]]}
  t5.decoder.block.2.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4b66f80], [3, 0x9c378e0], [4, 0x9c6cde0]]}
  states_51:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dbe180]]}
  t5.decoder.block.2.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ed1080], [2, 0x50a53c0], [3, 0xa12cb20]]}
  t5.decoder.block.2.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa032540], [5, 0x5c9bd80], [0, 0x5a7d800]]}
  t5.decoder.block.2.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ec4d60]]}
  t5.decoder.block.2.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c09960], [0, 0x59eb3e0]]}
  states_53:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa019f20]]}
  states_55:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5cc8180]]}
  t5.decoder.block.2.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4e62d00], [2, 0x5037040], [3, 0xa0ca280]]}
  t5.decoder.block.2.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa00d3c0]]}
  t5.decoder.block.2.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4f12800], [3, 0x9fa5a40], [4, 0x9f7afa0], [5, 0x5b764c0], [0, 0x59525e0], [1, 0x4dd08e0], [2, 0x4fa4c20], [3, 0xa037e60]]}
  t5.decoder.block.2.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x582dda0], [1, 0x4cac0a0], [2, 0x4e803e0], [3, 0x9f13620], [4, 0x9ee8b80], [5, 0x5ae40a0], [0, 0x58c01c0], [1, 0x4d3e4c0]]}
  t5.decoder.block.3.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f07300]]}
  t5.decoder.block.3.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4c19c80], [2, 0x4dedfc0]]}
  states_73:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5777080]]}
  t5.decoder.block.3.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ea5ae0], [4, 0x9e86b20], [5, 0x5a82040]]}
  states_75:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xadf1c20]]}
  t5.decoder.block.3.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x62a55c0], [1, 0x5800f60], [2, 0x8921720]]}
  t5.decoder.block.3.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab03a80], [4, 0xa978940], [5, 0x65305c0]]}
  t5.decoder.block.3.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9da9a0]]}
  t5.decoder.block.3.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x62131a0], [1, 0x576eb40]]}
  states_77:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xac1c800]]}
  states_79:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x89088c0]]}
  t5.decoder.block.3.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6591de0], [0, 0x61b1980], [1, 0x570d320]]}
  t5.decoder.block.3.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61a5660]]}
  t5.decoder.block.3.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6399a40], [1, 0x58f53e0], [2, 0x8a15ba0], [3, 0xad5f800], [4, 0xab9e160], [5, 0x6736620], [0, 0x642be60], [1, 0x5987800]]}
  t5.decoder.block.3.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaa79920], [5, 0x6611de0], [0, 0x6307620], [1, 0x5862fc0], [2, 0x8983780], [3, 0xaccd3e0], [4, 0xab0bd40], [5, 0x66a4200]]}
  t5.decoder.block.4.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6605ac0]]}
  t5.decoder.block.4.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xac34e20], [4, 0xa9e7500]]}
  states_97:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab65ae0]]}
  t5.decoder.block.4.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5424ac0], [2, 0x871e700], [3, 0xa8717a0]]}
  states_99:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x536dda0]]}
  t5.decoder.block.4.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa6cc7c0], [5, 0x6279180], [0, 0x5ef94e0]]}
  t5.decoder.block.4.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x530c580], [2, 0x86bc6a0], [3, 0xa809de0]]}
  t5.decoder.block.4.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61781a0]]}
  t5.decoder.block.4.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e7d40], [2, 0x8597e60]]}
  states_101:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e4e260]]}
  states_103:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e35c40]]}
  t5.decoder.block.4.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa683540], [4, 0xa53a440], [5, 0x61844c0]]}
  t5.decoder.block.4.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e29920]]}
  t5.decoder.block.4.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6e55a0], [4, 0xa5a7f80], [5, 0x61e6520], [0, 0x5e66880], [1, 0x527a160], [2, 0x862a280], [3, 0xa7779c0], [4, 0xa63a3a0]]}
  t5.decoder.block.4.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6080e20], [1, 0x55e82a0], [2, 0x8875420], [3, 0xaa71660], [4, 0xa8e6520], [5, 0x649e1a0], [0, 0x6113240], [1, 0x567a6c0]]}
  t5.decoder.block.5.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaa65340]]}
  t5.decoder.block.5.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5555e80], [2, 0x87e3000]]}
  states_121:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fca100]]}
  t5.decoder.block.5.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaa03b20], [4, 0xa8844c0], [5, 0x643c140]]}
  states_123:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa94ce00]]}
  t5.decoder.block.5.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f680a0], [1, 0x5492e40], [2, 0x8780fa0]]}
  t5.decoder.block.5.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa8eb5e0], [4, 0xa7c1480], [5, 0x63d4780]]}
  t5.decoder.block.5.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f5bd80]]}
  t5.decoder.block.5.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa72f060], [5, 0x6342360]]}
  states_125:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa8d2fc0]]}
  states_127:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3be2040]]}
  t5.decoder.block.5.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8128a80], [4, 0x82395a0], [5, 0x3c79580]]}
  t5.decoder.block.5.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bd5d20]]}
  t5.decoder.block.5.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8114d60], [5, 0x3b54d40], [0, 0x3b43900], [1, 0x3b1f020], [2, 0x3c0eb20], [3, 0x8096660], [4, 0x81a7180], [5, 0x3be7160]]}
  t5.decoder.block.5.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3aea2e0], [3, 0x7f71e20], [4, 0x8082940], [5, 0x3ac2920], [0, 0x3ab14e0], [1, 0x3a8cc00], [2, 0x3b7c700], [3, 0x8004240]]}
  t5.decoder.block.6.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ab6600]]}
  t5.decoder.block.6.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7edfa00], [4, 0x7ff0520]]}
  states_145:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a335c0]]}
  t5.decoder.block.6.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x96b0b00], [4, 0x9741680], [5, 0x51c4700]]}
  states_147:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39d4e60]]}
  t5.decoder.block.6.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f8e4c0], [5, 0x39f35c0], [0, 0x3a4ec40]]}
  t5.decoder.block.6.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3973640], [2, 0x39d1560], [3, 0x7e78040]]}
  t5.decoder.block.6.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f821a0]]}
  t5.decoder.block.6.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x393f140], [3, 0x7de5c20]]}
  states_149:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x395b020]]}
  states_151:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x837eec0]]}
  t5.decoder.block.6.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3f06720], [1, 0x3e5d5a0], [2, 0x3fde460]]}
  t5.decoder.block.6.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8372ba0]]}
  t5.decoder.block.6.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3d38d60], [2, 0x3eb9c20], [3, 0x82e0780], [4, 0x83fcd80], [5, 0x3f18360], [0, 0x3e74300], [1, 0x3dcb180], [2, 0x3f4c040]]}
  t5.decoder.block.6.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3df3b20], [0, 0x3d4fac0], [1, 0x3ca6940], [2, 0x3e27800], [3, 0x824e360], [4, 0x836a960], [5, 0x3e85f40], [0, 0x3de1ee0]]}
  t5.decoder.block.7.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e1b4e0]]}
  t5.decoder.block.7.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3cbd6a0], [1, 0x3c14520]]}
  states_169:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d3ce00]]}
  t5.decoder.block.7.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3db9cc0], [3, 0x81ec300], [4, 0x8308900]]}
  states_171:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d02fa0]]}
  t5.decoder.block.7.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3cdada0], [0, 0x3bfa660], [1, 0x3bb24c0]]}
  t5.decoder.block.7.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ca1780], [3, 0x818a2a0], [4, 0x829adc0]]}
  t5.decoder.block.7.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3884260]]}
  t5.decoder.block.7.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c7d280], [4, 0x7d2c5a0]]}
  states_173:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37c4b40]]}
  states_175:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37ac520]]}
  t5.decoder.block.7.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3822200], [0, 0x38356e0], [1, 0x37a22a0]]}
  t5.decoder.block.7.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3795f80]]}
  t5.decoder.block.7.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x36fd9c0], [0, 0x3710ea0], [1, 0x3703b60], [2, 0x37198c0], [3, 0x7be9de0], [4, 0x7be2c20], [5, 0x378fde0], [0, 0x37a32c0]]}
  t5.decoder.block.7.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ac55a0], [4, 0x7abe3e0], [5, 0x366b5a0], [0, 0x367ea80], [1, 0x3671740], [2, 0x36874a0], [3, 0x7b579c0], [4, 0x7b50800]]}
  t5.decoder.block.8.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38f25e0]]}
  t5.decoder.block.8.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7eefd80], [5, 0x3960960]]}
  states_193:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c75880]]}
  t5.decoder.block.8.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39ecbe0], [1, 0x38f9800], [2, 0x38dd920]]}
  states_195:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3935ec0]]}
  t5.decoder.block.8.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d7da20], [4, 0x7e2cd40], [5, 0x38fe900]]}
  t5.decoder.block.8.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38d46a0], [1, 0x38977a0], [2, 0x3875f60]]}
  t5.decoder.block.8.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d71700]]}
  t5.decoder.block.8.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3805380], [2, 0x37e3b40]]}
  states_197:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38bc080]]}
  states_199:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38a3a60]]}
  t5.decoder.block.8.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d0f6a0], [4, 0x7dbe9c0], [5, 0x3890580]]}
  t5.decoder.block.8.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5244f00]]}
  t5.decoder.block.8.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x98352c0], [5, 0x52b8340], [0, 0x51b2ae0], [1, 0x4629c20], [2, 0x48a27e0], [3, 0x9836b60], [4, 0x98c76e0], [5, 0x534a760]]}
  t5.decoder.block.8.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x477dfa0], [3, 0x9712320], [4, 0x97a2ea0], [5, 0x5225f20], [0, 0x51206c0], [1, 0x4597800], [2, 0x48103c0], [3, 0x97a4740]]}
  t5.decoder.block.9.layer.0.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x82fc5e0]]}
  t5.decoder.block.9.layer.0.SelfAttention.q.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4504ba0], [2, 0x46ebb80]]}
  states_217:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5069160]]}
  t5.decoder.block.9.layer.0.SelfAttention.k.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x964f2e0], [4, 0x96dfe60], [5, 0x5162ee0]]}
  states_219:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x95985c0]]}
  t5.decoder.block.9.layer.0.SelfAttention.v.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5007100], [1, 0x4441b60], [2, 0x4689b20]]}
  t5.decoder.block.9.layer.0.SelfAttention.o.weight:              {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9536da0], [4, 0x967de00], [5, 0x50fb520]]}
  t5.decoder.block.9.layer.1.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4ffade0]]}
  t5.decoder.block.9.layer.1.EncDecAttention.q.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x95eb9e0], [5, 0x5069100]]}
  states_221:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x951e780]]}
  states_223:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5445e40]]}
  t5.decoder.block.9.layer.1.EncDecAttention.o.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9bd5880], [4, 0x9c05420], [5, 0x5719860]]}
  t5.decoder.block.9.layer.2.layer_norm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5439b20]]}
  t5.decoder.block.9.layer.2.DenseReluDense.wi.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ae0be0], [5, 0x55f5020], [0, 0x53a7700], [1, 0x48ceba0], [2, 0x4ac8000], [3, 0x9b43460], [4, 0x9b73000], [5, 0x5687440]]}
  t5.decoder.block.9.layer.2.DenseReluDense.wo.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x49a37c0], [3, 0x9a1ec20], [4, 0x9a4e7c0], [5, 0x5562c00], [0, 0x53152e0], [1, 0x483c780], [2, 0x4a35be0], [3, 0x9ab1040]]}
  t5.decoder.block.10.layer.0.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55568e0]]}
  t5.decoder.block.10.layer.0.SelfAttention.q.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x998c800], [4, 0x99bc3a0]]}
  states_241:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46bc040]]}
  t5.decoder.block.10.layer.0.SelfAttention.k.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x54f50c0], [0, 0x52b3280], [1, 0x47da720]]}
  states_243:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x543e3a0]]}
  t5.decoder.block.10.layer.0.SelfAttention.v.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4940f20], [3, 0x98c97c0], [4, 0x995a340]]}
  t5.decoder.block.10.layer.0.SelfAttention.o.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x53dcb80], [0, 0x5251220], [1, 0x4772d60]]}
  t5.decoder.block.10.layer.1.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4934c00]]}
  t5.decoder.block.10.layer.1.EncDecAttention.q.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4f5b620], [1, 0x43a1b60]]}
  states_245:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4119b00]]}
  states_247:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41014e0]]}
  t5.decoder.block.10.layer.1.EncDecAttention.o.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x86425c0], [4, 0x876b6e0], [5, 0x41ca660]]}
  t5.decoder.block.10.layer.2.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40f51c0]]}
  t5.decoder.block.10.layer.2.DenseReluDense.wi.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8646ea0], [5, 0x40a5e20], [0, 0x4062da0], [1, 0x4076280], [2, 0x42b9920], [3, 0x85b01a0], [4, 0x86d92c0], [5, 0x4138240]]}
  t5.decoder.block.10.layer.2.DenseReluDense.wo.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x403fc80], [3, 0x83974e0], [4, 0x8490220], [5, 0x3fb1160], [0, 0x3f68780], [1, 0x3ecb0e0], [2, 0x40d20a0], [3, 0x8429900]]}
  t5.decoder.block.11.layer.0.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x863ab80]]}
  t5.decoder.block.11.layer.0.SelfAttention.q.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c5ce0], [3, 0x84bc560]]}
  states_265:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f5dd40]]}
  t5.decoder.block.11.layer.0.SelfAttention.k.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x85d9360], [5, 0x4043dc0], [0, 0x4000d40]]}
  states_267:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 15, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8522640]]}
  t5.decoder.block.11.layer.0.SelfAttention.v.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4014a60], [2, 0x4258100], [3, 0x854e980]]}
  t5.decoder.block.11.layer.0.SelfAttention.o.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4627280], [3, 0x94bcf60], [4, 0x958a1c0]]}
  t5.decoder.block.11.layer.1.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5056c40]]}
  t5.decoder.block.11.layer.1.EncDecAttention.q.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x942ab40], [4, 0x94f7da0]]}
  states_269:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x460ec60]]}
  states_271:                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x45f6640]]}
  t5.decoder.block.11.layer.1.EncDecAttention.o.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4109720], [2, 0x4352720], [3, 0x86a4620]]}
  t5.decoder.block.11.layer.2.layer_norm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x941dfe0]]}
  t5.decoder.block.11.layer.2.DenseReluDense.wi.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [24, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x427d320], [2, 0x44d1e00], [3, 0x938bbc0], [4, 0x945efa0], [5, 0x4fc37a0], [0, 0x4ec9200], [1, 0x430f740], [2, 0x4564220]]}
  t5.decoder.block.11.layer.2.DenseReluDense.wo.weight:           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4e9ef60], [0, 0x4da49c0], [1, 0x41eaf00], [2, 0x443f9e0], [3, 0x92f97a0], [4, 0x93ccb80], [5, 0x4f31380], [0, 0x4e36de0]]}
  t5.decoder.final_layer_norm.weight:                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44336c0]]}
  t5.shared.weight_dup:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [251, 6], ublock: [4, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8705e40], [4, 0x87d9220], [5, 0x42abe40], [0, 0x41b18a0]]}

  # constant
  lc.input_tensor.reduce_avg_2.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ff71a0]]}
  input_1_add_3:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5c040c0]]}
  input_0_subtract_28:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa18e380]]}
  input_1_multiply_29:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5237740]]}
  lc.input_tensor.multiply_29_s_brcst_m2_1_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4f95140]]}
  input_1_embedding_22:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5b41080]]}
  lc.input_tensor.softmax_32.dc.reduce_sum.3.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa093d60]]}
  dc.input_tensor.softmax_32.4:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa1881e0]]}
  lc.input_tensor.softmax_32.dc.reciprocal.6_s_brcst_m1_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa281f80]]}
  lc.input_tensor.reduce_avg_52.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4f330e0]]}
  input_1_add_53:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5adf020]]}
  input_0_subtract_69:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4f328a0]]}
  input_1_multiply_70:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5cfd5a0]]}
  lc.input_tensor.multiply_70_s_brcst_m2_1_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d6b920]]}
  input_0_add_71:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa52e120]]}
  lc.input_tensor.softmax_73.dc.reduce_sum.3.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa682d00]]}
  dc.input_tensor.softmax_73.4:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x858b2e0]]}
  lc.input_tensor.softmax_73.dc.reciprocal.6_s_brcst_m1_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e6480]]}
  lc.input_tensor.reduce_avg_89.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x858aaa0]]}
  input_1_add_90:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e5c40]]}
  lc.input_tensor.reduce_avg_108.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x50c0bc0]]}
  input_1_add_109:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa314be0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_2_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa283040]]}
  input_0_add_128:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa376c40]]}
  lc.input_tensor.softmax_130.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x831c8e0]]}
  dc.input_tensor.softmax_130.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5059200]]}
  lc.input_tensor.softmax_130.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5c66120]]}
  lc.input_tensor.reduce_avg_150.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa220fe0]]}
  input_1_add_151:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa0196e0]]}
  lc.input_tensor.multiply_70_s_brcst_m2_2_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4decf40]]}
  input_0_add_166:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4c0d120]]}
  lc.input_tensor.softmax_168.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x576a520]]}
  dc.input_tensor.softmax_168.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59e9a80]]}
  lc.input_tensor.softmax_168.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9df3ec0]]}
  lc.input_tensor.reduce_avg_184.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59e9240]]}
  input_1_add_185:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9df3680]]}
  lc.input_tensor.reduce_avg_203.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9cce600]]}
  input_1_add_204:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c99100]]}
  lc.input_tensor.multiply_29_s_brcst_m2_3_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4962040]]}
  input_0_add_223:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x545e460]]}
  lc.input_tensor.softmax_225.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x577b080]]}
  dc.input_tensor.softmax_225.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9c66c40]]}
  lc.input_tensor.softmax_225.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c370a0]]}
  lc.input_tensor.reduce_avg_245.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa12c2e0]]}
  input_1_add_246:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50a4b80]]}
  lc.input_tensor.multiply_70_s_brcst_m2_3_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa12baa0]]}
  input_0_add_261:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5098860]]}
  lc.input_tensor.softmax_263.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ec4520]]}
  dc.input_tensor.softmax_263.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59e5240]]}
  lc.input_tensor.softmax_263.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c09120]]}
  lc.input_tensor.reduce_avg_279.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59e4a00]]}
  input_1_add_280:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c088e0]]}
  lc.input_tensor.reduce_avg_298.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ae3860]]}
  input_1_add_299:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ee8340]]}
  lc.input_tensor.multiply_29_s_brcst_m2_4_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4ded780]]}
  input_0_add_318:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa822ca0]]}
  lc.input_tensor.softmax_320.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8982f40]]}
  dc.input_tensor.softmax_320.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xacc7240]]}
  lc.input_tensor.softmax_320.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8aa7fc0]]}
  lc.input_tensor.reduce_avg_340.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x570cae0]]}
  input_1_add_341:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab652a0]]}
  lc.input_tensor.multiply_70_s_brcst_m2_4_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9e6cc0]]}
  input_0_add_356:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x65f97a0]]}
  lc.input_tensor.softmax_358.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8920ee0]]}
  dc.input_tensor.softmax_358.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x65f3600]]}
  lc.input_tensor.softmax_358.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8907840]]}
  lc.input_tensor.reduce_avg_374.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9da160]]}
  input_1_add_375:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8908080]]}
  lc.input_tensor.reduce_avg_393.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5862780]]}
  input_1_add_394:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6306de0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_5_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f5ad00]]}
  input_0_add_413:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x62da9a0]]}
  lc.input_tensor.softmax_415.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa72dfe0]]}
  dc.input_tensor.softmax_415.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa86b600]]}
  lc.input_tensor.softmax_415.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x871dec0]]}
  lc.input_tensor.reduce_avg_435.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5ef8ca0]]}
  input_1_add_436:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6278940]]}
  lc.input_tensor.multiply_70_s_brcst_m2_5_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61e5ce0]]}
  input_0_add_451:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa59bc60]]}
  lc.input_tensor.softmax_453.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6e4d60]]}
  dc.input_tensor.softmax_453.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8591cc0]]}
  lc.input_tensor.softmax_453.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e7500]]}
  lc.input_tensor.reduce_avg_469.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8591480]]}
  input_1_add_470:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e6cc0]]}
  lc.input_tensor.reduce_avg_488.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x649d960]]}
  input_1_add_489:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8e5ce0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_6_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x87e27c0]]}
  input_0_add_508:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x54f4660]]}
  lc.input_tensor.softmax_510.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fc98c0]]}
  dc.input_tensor.softmax_510.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6435fa0]]}
  lc.input_tensor.softmax_510.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa72e820]]}
  lc.input_tensor.reduce_avg_530.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8780760]]}
  input_1_add_531:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5492600]]}
  lc.input_tensor.multiply_70_s_brcst_m2_6_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x877ff20]]}
  input_0_add_546:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x54862e0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f5b540]]}
  dc.input_tensor.softmax_548.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x633c1c0]]}
  lc.input_tensor.softmax_548.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4c19440]]}
  lc.input_tensor.reduce_avg_564.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ca0f40]]}
  input_1_add_565:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3bb1440]]}
  lc.input_tensor.reduce_avg_583.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a8c3c0]]}
  input_1_add_584:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ab0ca0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_7_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ab0460]]}
  input_0_add_603:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a54de0]]}
  lc.input_tensor.softmax_605.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fefce0]]}
  dc.input_tensor.softmax_605.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ed9860]]}
  lc.input_tensor.softmax_605.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a32d80]]}
  lc.input_tensor.reduce_avg_625.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a4e400]]}
  input_1_add_626:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39f2d80]]}
  lc.input_tensor.multiply_70_s_brcst_m2_7_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a8bb80]]}
  input_0_add_641:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ebedc0]]}
  lc.input_tensor.softmax_643.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3f67f40]]}
  dc.input_tensor.softmax_643.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3faafc0]]}
  lc.input_tensor.softmax_643.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x848f9e0]]}
  lc.input_tensor.reduce_avg_659.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3faa780]]}
  input_1_add_660:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x848f1a0]]}
  lc.input_tensor.reduce_avg_678.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x836a120]]}
  input_1_add_679:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3bb1c80]]}
  lc.input_tensor.multiply_29_s_brcst_m2_8_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c13ce0]]}
  input_0_add_698:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c5be80]]}
  lc.input_tensor.softmax_700.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d3c5c0]]}
  dc.input_tensor.softmax_700.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ddfa80]]}
  lc.input_tensor.softmax_700.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81ebac0]]}
  lc.input_tensor.reduce_avg_720.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x824db20]]}
  input_1_add_721:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38a3220]]}
  lc.input_tensor.multiply_70_s_brcst_m2_8_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3803ac0]]}
  input_0_add_736:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3896f00]]}
  lc.input_tensor.softmax_738.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3883a20]]}
  dc.input_tensor.softmax_738.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x366b5a0]]}
  lc.input_tensor.softmax_738.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c7ca40]]}
  lc.input_tensor.reduce_avg_754.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c75040]]}
  input_1_add_755:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c7c200]]}
  lc.input_tensor.reduce_avg_773.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3686c60]]}
  input_1_add_774:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37abce0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_9_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3960120]]}
  input_0_add_793:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e8e560]]}
  lc.input_tensor.softmax_795.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ddf240]]}
  dc.input_tensor.softmax_795.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38d7780]]}
  lc.input_tensor.softmax_795.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38f8fc0]]}
  lc.input_tensor.reduce_avg_815.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3804300]]}
  input_1_add_816:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e2c500]]}
  lc.input_tensor.multiply_70_s_brcst_m2_9_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38f1da0]]}
  input_0_add_831:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e201e0]]}
  lc.input_tensor.softmax_833.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d70ec0]]}
  dc.input_tensor.softmax_833.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37dd9a0]]}
  lc.input_tensor.softmax_833.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3804b40]]}
  lc.input_tensor.reduce_avg_849.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x37dd160]]}
  input_1_add_850:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x49a2f80]]}
  lc.input_tensor.reduce_avg_868.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4596fc0]]}
  input_1_add_869:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x511fe80]]}
  lc.input_tensor.multiply_29_s_brcst_m2_10_1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x46eb340]]}
  input_0_add_888:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x44a3380]]}
  lc.input_tensor.softmax_890.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5068920]]}
  dc.input_tensor.softmax_890.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5062f60]]}
  lc.input_tensor.softmax_890.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x96df620]]}
  lc.input_tensor.reduce_avg_910.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x46892e0]]}
  input_1_add_911:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4441320]]}
  lc.input_tensor.multiply_70_s_brcst_m2_10_1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4688aa0]]}
  input_0_add_926:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4435000]]}
  lc.input_tensor.softmax_928.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4ffa5a0]]}
  dc.input_tensor.softmax_928.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x515cd40]]}
  lc.input_tensor.softmax_928.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4961800]]}
  lc.input_tensor.reduce_avg_944.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4b5a420]]}
  input_1_add_945:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4960fc0]]}
  lc.input_tensor.reduce_avg_963.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x483bf40]]}
  input_1_add_964:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5314aa0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_11_1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x99bbb60]]}
  input_0_add_983:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x992afe0]]}
  lc.input_tensor.softmax_985.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x49a2740]]}
  dc.input_tensor.softmax_985.4:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47d4580]]}
  lc.input_tensor.softmax_985.dc.reciprocal.6_s_brcst_m1_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x52b2a40]]}
  lc.input_tensor.reduce_avg_1005.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9959b00]]}
  input_1_add_1006:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x98c8f80]]}
  lc.input_tensor.multiply_70_s_brcst_m2_11_1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x422be80]]}
  input_0_add_1021:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87ccf00]]}
  lc.input_tensor.softmax_1023.dc.reduce_sum.3.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x86a3de0]]}
  dc.input_tensor.softmax_1023.4:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x434c580]]}
  lc.input_tensor.softmax_1023.dc.reciprocal.6_s_brcst_m1_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4108ee0]]}
  lc.input_tensor.reduce_avg_1039.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x434bd40]]}
  input_1_add_1040:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41086a0]]}
  lc.input_tensor.reduce_avg_1058.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4062560]]}
  input_1_add_1059:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40a55e0]]}
  lc.input_tensor.multiply_29_s_brcst_m2_0_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x84bbd20]]}
  input_0_add_1078:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41644c0]]}
  lc.input_tensor.softmax_1080.dc.reduce_sum.3.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f5d500]]}
  dc.input_tensor.softmax_1080.4:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ffaba0]]}
  lc.input_tensor.softmax_1080.dc.reciprocal.6_s_brcst_m1_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4043580]]}
  lc.input_tensor.reduce_avg_1100.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x44347c0]]}
  input_1_add_1101:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4ff9d60]]}
  lc.input_tensor.multiply_70_s_brcst_m2_0_1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4433f80]]}
  input_0_add_1116:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4feda40]]}
  lc.input_tensor.softmax_1118.dc.reduce_sum.3.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5056400]]}
  dc.input_tensor.softmax_1118.4:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x94f1c00]]}
  lc.input_tensor.softmax_1118.dc.reciprocal.6_s_brcst_m1_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x942a300]]}
  lc.input_tensor.reduce_avg_1134.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5055bc0]]}
  input_1_add_1135:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x94f13c0]]}
  lc.input_tensor.reduce_avg_1153.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x93cc340]]}
  input_1_add_1154:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92f8f60]]}
  input_1_multiply_1159:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41ea6c0]]}

  # epoch_to_epoch
  e2e_softmax_73.dc.multiply.7_0:                                 {input: softmax_73.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3662320]]}
  e2e_hslice_76_0:                                                {input: hslice_76, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x366e640]]}
  e2e_add_50_0:                                                   {input: add_50, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3672760]]}
  e2e_multiply_29_s_brcst_m2_2_1.lc1_0:                           {input: multiply_29_s_brcst_m2_2_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x365a100]]}
  e2e_softmax_130.dc.multiply.7_0:                                {input: softmax_130.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x67c8a40]]}
  e2e_concatenate_137.dc.concatenate.0_0:                         {input: concatenate_137.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x64be280], [1, 0x5a19c20]]}
  e2e_add_106_0:                                                  {input: add_106, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaea8940]]}
  e2e__fused_op_5_attempt_1_input_op_fork_nop0_0:                 {input: _fused_op_5_attempt_1_input_op_fork_nop0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x366a540]]}
  e2e_multiply_70_s_brcst_m2_2_1.lc1_0:                           {input: multiply_70_s_brcst_m2_2_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8aa8800]]}
  e2e_multiply_29_s_brcst_m2_3_1.lc1_0:                           {input: multiply_29_s_brcst_m2_3_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7abd380]]}
  e2e_add_223_0:                                                  {input: add_223, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xac30580]]}
  e2e_softmax_225.dc.multiply.7_0:                                {input: softmax_225.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6644300]]}
  e2e_concatenate_232.dc.concatenate.0_0:                         {input: concatenate_232.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8bce0c0], [3, 0xaec3040]]}
  e2e_add_201_0:                                                  {input: add_201, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a89820]]}
  e2e_multiply_70_s_brcst_m2_3_1.lc1_0:                           {input: multiply_70_s_brcst_m2_3_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xac91da0]]}
  e2e_multiply_29_s_brcst_m2_4_1.lc1_0:                           {input: multiply_29_s_brcst_m2_4_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x366a540]]}
  e2e_add_318_0:                                                  {input: add_318, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x682a260]]}
  e2e_softmax_320.dc.multiply.7_0:                                {input: softmax_320.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xacac4a0]]}
  e2e_concatenate_327.dc.concatenate.0_0:                         {input: concatenate_327.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x66a5b20], [1, 0x5a95b40]]}
  e2e_add_296_0:                                                  {input: add_296, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x695ade0]]}
  e2e_multiply_70_s_brcst_m2_4_1.lc1_0:                           {input: multiply_70_s_brcst_m2_4_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a7b440]]}
  e2e_multiply_29_s_brcst_m2_5_1.lc1_0:                           {input: multiply_29_s_brcst_m2_5_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab61c0]]}
  e2e_add_413_0:                                                  {input: add_413, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x651faa0]]}
  e2e_softmax_415.dc.multiply.7_0:                                {input: softmax_415.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8c2f8e0]]}
  e2e_concatenate_422.dc.concatenate.0_0:                         {input: concatenate_422.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xad0dcc0], [5, 0x6967100]]}
  e2e_add_391_0:                                                  {input: add_391, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaf24860]]}
  e2e_multiply_70_s_brcst_m2_5_1.lc1_0:                           {input: multiply_70_s_brcst_m2_5_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaeb4c60]]}
  e2e_multiply_29_s_brcst_m2_6_1.lc1_0:                           {input: multiply_29_s_brcst_m2_6_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x365a100]]}
  e2e_add_508_0:                                                  {input: add_508, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8aa9860]]}
  e2e_softmax_510.dc.multiply.7_0:                                {input: softmax_510.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6707340]]}
  e2e_concatenate_517.dc.concatenate.0_0:                         {input: concatenate_517.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8c91100], [3, 0xaf30b80]]}
  e2e_add_486_0:                                                  {input: add_486, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5af7360]]}
  e2e_multiply_70_s_brcst_m2_6_1.lc1_0:                           {input: multiply_70_s_brcst_m2_6_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xac92e00]]}
  e2e_multiply_29_s_brcst_m2_7_1.lc1_0:                           {input: multiply_29_s_brcst_m2_7_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3662320]]}
  e2e_add_603_0:                                                  {input: add_603, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x688ba80]]}
  e2e_softmax_605.dc.multiply.7_0:                                {input: softmax_605.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xad6f4e0]]}
  e2e_concatenate_612.dc.concatenate.0_0:                         {input: concatenate_612.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6768b60], [1, 0x5b03680]]}
  e2e_add_581_0:                                                  {input: add_581, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x69c8920]]}
  e2e_multiply_70_s_brcst_m2_7_1.lc1_0:                           {input: multiply_70_s_brcst_m2_7_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a7c4a0]]}
  e2e_multiply_29_s_brcst_m2_8_1.lc1_0:                           {input: multiply_29_s_brcst_m2_8_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3662320]]}
  e2e_add_698_0:                                                  {input: add_698, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65812c0]]}
  e2e_softmax_700.dc.multiply.7_0:                                {input: softmax_700.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaf923a0]]}
  e2e_concatenate_707.dc.concatenate.0_0:                         {input: concatenate_707.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xadd0d00], [5, 0x69d4c40]]}
  e2e_add_676_0:                                                  {input: add_676, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8cf2920]]}
  e2e_multiply_70_s_brcst_m2_8_1.lc1_0:                           {input: multiply_70_s_brcst_m2_8_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaeb5cc0]]}
  e2e_multiply_29_s_brcst_m2_9_1.lc1_0:                           {input: multiply_29_s_brcst_m2_9_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x365a100]]}
  e2e_add_793_0:                                                  {input: add_793, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b0b080]]}
  e2e_softmax_795.dc.multiply.7_0:                                {input: softmax_795.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x67ca380]]}
  e2e_concatenate_802.dc.concatenate.0_0:                         {input: concatenate_802.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8cfec40], [3, 0xaff3bc0]]}
  e2e_add_771_0:                                                  {input: add_771, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5b64ea0]]}
  e2e_multiply_70_s_brcst_m2_9_1.lc1_0:                           {input: multiply_70_s_brcst_m2_9_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x366a540]]}
  e2e_add_831_0:                                                  {input: add_831, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xac93e60]]}
  e2e_multiply_29_s_brcst_m2_10_1.lc1_0:                          {input: multiply_29_s_brcst_m2_10_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x365a100]]}
  e2e_add_888_0:                                                  {input: add_888, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x68ed2a0]]}
  e2e_softmax_890.dc.multiply.7_0:                                {input: softmax_890.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x682bba0]]}
  e2e_concatenate_897.dc.concatenate.0_0:                         {input: concatenate_897.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xae32520], [5, 0x6a36460]]}
  e2e_add_866_0:                                                  {input: add_866, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5b711c0]]}
  e2e_multiply_70_s_brcst_m2_10_1.lc1_0:                          {input: multiply_70_s_brcst_m2_10_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  e2e_add_926_0:                                                  {input: add_926, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a7d500]]}
  e2e_multiply_29_s_brcst_m2_11_1.lc1_0:                          {input: multiply_29_s_brcst_m2_11_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab5160]]}
  e2e_add_983_0:                                                  {input: add_983, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65e2ae0]]}
  e2e_softmax_985.dc.multiply.7_0:                                {input: softmax_985.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb0553e0]]}
  e2e_concatenate_992.dc.concatenate.0_0:                         {input: concatenate_992.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xae93d40], [5, 0x6a97c80]]}
  e2e_add_961_0:                                                  {input: add_961, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x688d3c0]]}
  e2e_multiply_70_s_brcst_m2_11_1.lc1_0:                          {input: multiply_70_s_brcst_m2_11_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab5160]]}
  e2e_add_1021_0:                                                 {input: add_1021, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaeb6d20]]}
  e2e_multiply_29_s_brcst_m2_0_1.lc1_0:                           {input: multiply_29_s_brcst_m2_0_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3662320]]}
  e2e_add_1078_0:                                                 {input: add_1078, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b6c8a0]]}
  e2e_softmax_1080.dc.multiply.7_0:                               {input: softmax_1080.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x68996e0]]}
  e2e_concatenate_1087.dc.concatenate.0_0:                        {input: concatenate_1087.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 2], t: 12, mblock: [16, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb0b6c00], [4, 0xaef5560]]}
  e2e_add_1056_0:                                                 {input: add_1056, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d6c780]]}
  e2e_multiply_70_s_brcst_m2_0_1.lc1_0:                           {input: multiply_70_s_brcst_m2_0_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  e2e_add_1116_0:                                                 {input: add_1116, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x694eac0]]}
  e2e__fused_op_86_0:                                             {input: _fused_op_86, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5b89800]]}
  e2e_index_1170.dc.buffer.1_0:                                   {input: index_1170.dc.buffer.1, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaca0180]]}
  e2e_index_1202.dc.select.0_0:                                   {input: index_1202.dc.select.0, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d60460]]}
  e2e_index_1206.dc.select.0_0:                                   {input: index_1206.dc.select.0, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5b7d4e0]]}
  e2e_index_1210.dc.select.0_0:                                   {input: index_1210.dc.select.0, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6af94a0]]}

  # buffering
  buf_matmul_1162_0:                                              {input: matmul_1162, type: queue, entries: 1, grid_size: [1, 4], t: 251, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x422c6c0], [0, 0x4132120], [1, 0x416af40], [2, 0x43b3f40]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 1
    embedding_0: {type: embedding, grid_loc: [0, 1], grid_size: [1, 1], inputs: [t5.shared.weight, input_1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {num_indices: 32}}
    multiply_1: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [embedding_0, embedding_0],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_2.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [multiply_1, lc.input_tensor.reduce_avg_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 1], inputs: [reduce_avg_2.lc1, input_1_add_3, embedding_0, t5.decoder.block.0.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_10: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [_fused_op_0, t5.decoder.block.0.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_15: {type: matmul, grid_loc: [1, 2], grid_size: [1, 3], inputs: [_fused_op_0, t5.decoder.block.0.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_17.dc.concatenate.0: {type: splice, grid_loc: [1, 5], grid_size: [1, 2], inputs: [states_1, matmul_15],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_20: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [matmul_10, concatenate_17.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_0_subtract_28, attention_mask, input_1_multiply_29],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 16}], input_0_tms: [broadcast: {c: 16}],
         attributes: {fused_op_id: 1, kernel_broadcast: {input_2: 1, input_0: 1}}}
    _fused_op_1_attempt_1_input_op_fork_nop0: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_29_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_1_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_30: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [input_1_embedding_22, multiply_29_s_brcst_m2_1_1.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_31: {type: add, grid_loc: [2, 6], grid_size: [1, 1], inputs: [matmul_20, add_30],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_32.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [1, 1], inputs: [add_31],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_2: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 4], inputs: [add_31, softmax_32.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_32.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_2, lc.input_tensor.softmax_32.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_3: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_32.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_32.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_32.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [_fused_op_3, lc.input_tensor.softmax_32.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_32.dc.multiply.7: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_2, softmax_32.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_37: {type: matmul, grid_loc: [4, 0], grid_size: [1, 3], inputs: [_fused_op_0, t5.decoder.block.0.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [15, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_39.dc.concatenate.0: {type: splice, grid_loc: [4, 3], grid_size: [1, 2], inputs: [states_3, matmul_37],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_43: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [softmax_32.dc.multiply.7, concatenate_39.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_47: {type: matmul, grid_loc: [5, 2], grid_size: [1, 3], inputs: [matmul_43, t5.decoder.block.0.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_50: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [embedding_0, matmul_47],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [292, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_51: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_50, add_50],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_52.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [multiply_51, lc.input_tensor.reduce_avg_52.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_4: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [reduce_avg_52.lc1, input_1_add_53, add_50, t5.decoder.block.0.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_60: {type: matmul, grid_loc: [8, 1], grid_size: [1, 2], inputs: [_fused_op_4, t5.decoder.block.0.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_63: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [states_5],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_66: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [matmul_60, hslice_63],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_5: {type: fused_op, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_0_subtract_69, encoder_attention_mask, input_1_multiply_70],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {fused_op_id: 5, kernel_broadcast: {input_2: 1, input_0: 1}}}
    _fused_op_5_attempt_1_input_op_fork_nop0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [_fused_op_5],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_70_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_1_1.0, _fused_op_5_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_71: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [input_0_add_71, multiply_70_s_brcst_m2_1_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_72: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_66, add_71],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_73.dc.reduce_max.0: {type: reduce, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_72],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_6: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_72, softmax_73.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_73.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [_fused_op_6, lc.input_tensor.softmax_73.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [9, 4], grid_size: [1, 1], inputs: [softmax_73.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_73.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_73.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [_fused_op_7, lc.input_tensor.softmax_73.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_73.dc.multiply.7: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [_fused_op_6, softmax_73.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_76: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [states_7],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_29_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_2_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_3_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_4_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_5_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_6_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_7_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_8_1.0, _fused_op_1_attempt_1_input_op_fork_nop0],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_9_1.0, _fused_op_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_70_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_9_1.0, _fused_op_5],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_10_1.0, _fused_op_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_70_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_10_1.0, _fused_op_5],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_11_1.0, _fused_op_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_70_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_11_1.0, _fused_op_5],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_29_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_29_s_brcst_m2_0_1.0, _fused_op_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    multiply_70_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_0_1.0, _fused_op_5],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    index_1164.dc.select.0: {type: splice, grid_loc: [0, 7], grid_size: [1, 1], inputs: [concatenate_17.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1164.dc.buffer.1: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [index_1164.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1165_tm_nop: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [index_1164.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1166.dc.select.0: {type: splice, grid_loc: [4, 5], grid_size: [1, 1], inputs: [concatenate_39.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1166.dc.buffer.1: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [index_1166.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1167_tm_nop: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [index_1166.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    hslice_63_output_nop_0: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [hslice_63],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    nop_1212: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [key_value_states], untilize_output: true,
         t: 1, mblock: [1, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 1
    matmul_80: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_softmax_73.dc.multiply.7_0, e2e_hslice_76_0],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_84: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_80, t5.decoder.block.0.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_87: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_50_0, matmul_84],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_88: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [add_87, add_87],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_89.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [multiply_88, lc.input_tensor.reduce_avg_89.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_8: {type: fused_op, grid_loc: [3, 6], grid_size: [1, 1], inputs: [reduce_avg_89.lc1, input_1_add_90, add_87, t5.decoder.block.0.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_97: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_8, t5.decoder.block.0.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_103: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [matmul_97, t5.decoder.block.0.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_106: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_87, matmul_103],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_107: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_106, add_106],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_108.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_107, lc.input_tensor.reduce_avg_108.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_9: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [reduce_avg_108.lc1, input_1_add_109, add_106, t5.decoder.block.1.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_116: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [_fused_op_9, t5.decoder.block.1.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_121: {type: matmul, grid_loc: [6, 5], grid_size: [1, 3], inputs: [_fused_op_9, t5.decoder.block.1.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_123.dc.concatenate.0: {type: splice, grid_loc: [7, 0], grid_size: [1, 2], inputs: [states_25, matmul_121],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_126: {type: matmul, grid_loc: [7, 5], grid_size: [1, 2], inputs: [matmul_116, concatenate_123.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_128: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [input_0_add_128, e2e_multiply_29_s_brcst_m2_2_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_129: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_126, add_128],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_130.dc.reduce_max.0: {type: reduce, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_129],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_10: {type: fused_op, grid_loc: [8, 1], grid_size: [1, 4], inputs: [add_129, softmax_130.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_130.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [_fused_op_10, lc.input_tensor.softmax_130.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_11: {type: fused_op, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_130.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_130.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_130.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_11, lc.input_tensor.softmax_130.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_130.dc.multiply.7: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_10, softmax_130.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_135: {type: matmul, grid_loc: [9, 1], grid_size: [1, 3], inputs: [_fused_op_9, t5.decoder.block.1.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_137.dc.concatenate.0: {type: splice, grid_loc: [9, 4], grid_size: [1, 2], inputs: [states_27, matmul_135],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    multiply_70_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_2_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_223: {type: add, grid_loc: [2, 6], grid_size: [1, 1], inputs: [input_0_add_223, e2e_multiply_29_s_brcst_m2_3_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_3_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_318: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [input_0_add_318, e2e_multiply_29_s_brcst_m2_4_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_4_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_413: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [input_0_add_413, e2e_multiply_29_s_brcst_m2_5_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_5_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_508: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [input_0_add_508, e2e_multiply_29_s_brcst_m2_6_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_6_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_603: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [input_0_add_603, e2e_multiply_29_s_brcst_m2_7_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_7_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_698: {type: add, grid_loc: [3, 3], grid_size: [1, 1], inputs: [input_0_add_698, e2e_multiply_29_s_brcst_m2_8_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    multiply_70_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.multiply_70_s_brcst_m2_8_1.0, e2e__fused_op_5_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_793: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [input_0_add_793, e2e_multiply_29_s_brcst_m2_9_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_831: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [input_0_add_831, e2e_multiply_70_s_brcst_m2_9_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_888: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [input_0_add_888, e2e_multiply_29_s_brcst_m2_10_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_926: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [input_0_add_926, e2e_multiply_70_s_brcst_m2_10_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_983: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [input_0_add_983, e2e_multiply_29_s_brcst_m2_11_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_1021: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [input_0_add_1021, e2e_multiply_70_s_brcst_m2_11_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_1078: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [input_0_add_1078, e2e_multiply_29_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    add_1116: {type: add, grid_loc: [2, 4], grid_size: [1, 1], inputs: [input_0_add_1116, e2e_multiply_70_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    hslice_76_output_nop_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_hslice_76_0],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1168.dc.select.0: {type: splice, grid_loc: [7, 2], grid_size: [1, 1], inputs: [concatenate_123.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1168.dc.buffer.1: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [index_1168.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1169_tm_nop: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [index_1168.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1170.dc.select.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 1], inputs: [concatenate_137.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1170.dc.buffer.1: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [index_1170.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 1
    matmul_141: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_130.dc.multiply.7_0, e2e_concatenate_137.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_145: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_141, t5.decoder.block.1.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_148: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_106_0, matmul_145],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_149: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_148, add_148],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_150.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_149, lc.input_tensor.reduce_avg_150.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_150.lc1, input_1_add_151, add_148, t5.decoder.block.1.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_158: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_12, t5.decoder.block.1.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_161: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_29],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_164: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_158, hslice_161],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_166: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_166, e2e_multiply_70_s_brcst_m2_2_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_167: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_164, add_166],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_168.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_167],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_13: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_167, softmax_168.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_168.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_13, lc.input_tensor.softmax_168.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_14: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_168.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_168.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_168.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_14, lc.input_tensor.softmax_168.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_168.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_13, softmax_168.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_171: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_31],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_175: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_168.dc.multiply.7, hslice_171],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_179: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_175, t5.decoder.block.1.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_148_add_182: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_148],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_182: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_148_add_182, matmul_179],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_183: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_182, add_182],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_184.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_183, lc.input_tensor.reduce_avg_184.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_15: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_184.lc1, input_1_add_185, add_182, t5.decoder.block.1.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_192: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_15, t5.decoder.block.1.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_198: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_192, t5.decoder.block.1.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_201: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_182, matmul_198],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_202: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_201, add_201],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_203.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_202, lc.input_tensor.reduce_avg_203.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_203.lc1, input_1_add_204, add_201, t5.decoder.block.2.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_211: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_16, t5.decoder.block.2.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_216: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_16, t5.decoder.block.2.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_218.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_49, matmul_216],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_221: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_211, concatenate_218.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_224: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_221, e2e_add_223_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_225.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_224],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_17: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_224, softmax_225.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_225.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_17, lc.input_tensor.softmax_225.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_18: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_225.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_225.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_225.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_18, lc.input_tensor.softmax_225.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_225.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_17, softmax_225.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_230: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_16, t5.decoder.block.2.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_232.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_51, matmul_230],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_161_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_161],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_171_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_171],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1172.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_218.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1172.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1172.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1173_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1172.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 1
    matmul_236: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_225.dc.multiply.7_0, e2e_concatenate_232.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_240: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_236, t5.decoder.block.2.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_243: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_201_0, matmul_240],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_244: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_243, add_243],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_245.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_244, lc.input_tensor.reduce_avg_245.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_19: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_245.lc1, input_1_add_246, add_243, t5.decoder.block.2.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_253: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_19, t5.decoder.block.2.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_256: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_53],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_259: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_253, hslice_256],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_261: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_261, e2e_multiply_70_s_brcst_m2_3_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_262: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_259, add_261],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_263.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_262],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_262, softmax_263.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_263.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_263.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_263.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_263.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_263.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_21, lc.input_tensor.softmax_263.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_263.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_20, softmax_263.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_266: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_55],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_270: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_263.dc.multiply.7, hslice_266],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_274: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_270, t5.decoder.block.2.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_243_add_277: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_243],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_277: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_243_add_277, matmul_274],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_278: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_277, add_277],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_279.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_278, lc.input_tensor.reduce_avg_279.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_22: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_279.lc1, input_1_add_280, add_277, t5.decoder.block.2.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_287: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_22, t5.decoder.block.2.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_293: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_287, t5.decoder.block.2.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_296: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_277, matmul_293],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_297: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_296, add_296],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_298.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_297, lc.input_tensor.reduce_avg_298.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_23: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_298.lc1, input_1_add_299, add_296, t5.decoder.block.3.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_306: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_23, t5.decoder.block.3.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_311: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_23, t5.decoder.block.3.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_313.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_73, matmul_311],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_316: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_306, concatenate_313.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_319: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_316, e2e_add_318_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_320.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_319],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_24: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_319, softmax_320.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_320.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_320.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_320.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_320.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_320.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_25, lc.input_tensor.softmax_320.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_320.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_24, softmax_320.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_325: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_23, t5.decoder.block.3.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_327.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_75, matmul_325],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_256_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_256],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_266_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_266],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1176.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_313.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1176.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1176.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1177_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1176.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 1
    matmul_331: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_320.dc.multiply.7_0, e2e_concatenate_327.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_335: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_331, t5.decoder.block.3.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_338: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_296_0, matmul_335],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_339: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_338, add_338],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_340.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_339, lc.input_tensor.reduce_avg_340.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_26: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_340.lc1, input_1_add_341, add_338, t5.decoder.block.3.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_348: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_26, t5.decoder.block.3.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_351: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_77],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_354: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_348, hslice_351],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_356: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_356, e2e_multiply_70_s_brcst_m2_4_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_357: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_354, add_356],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_358.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_357],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_27: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_357, softmax_358.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_358.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_27, lc.input_tensor.softmax_358.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_28: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_358.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_358.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_358.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_358.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_358.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_27, softmax_358.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_361: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_79],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_365: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_358.dc.multiply.7, hslice_361],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_369: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_365, t5.decoder.block.3.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_338_add_372: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_338],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_372: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_338_add_372, matmul_369],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_373: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_372, add_372],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_374.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_373, lc.input_tensor.reduce_avg_374.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_29: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_374.lc1, input_1_add_375, add_372, t5.decoder.block.3.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_382: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_29, t5.decoder.block.3.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_388: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_382, t5.decoder.block.3.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_391: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_372, matmul_388],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_392: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_391, add_391],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_393.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_392, lc.input_tensor.reduce_avg_393.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_30: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_393.lc1, input_1_add_394, add_391, t5.decoder.block.4.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_401: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_30, t5.decoder.block.4.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_406: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_30, t5.decoder.block.4.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_408.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_97, matmul_406],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_411: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_401, concatenate_408.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_414: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_411, e2e_add_413_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_415.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_414],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_31: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_414, softmax_415.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_415.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_31, lc.input_tensor.softmax_415.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_32: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_415.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_415.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_415.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_415.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_415.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_31, softmax_415.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_420: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_30, t5.decoder.block.4.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_422.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_99, matmul_420],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_351_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_351],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_361_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_361],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1180.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_408.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1180.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1180.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1181_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1180.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 1
    matmul_426: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_415.dc.multiply.7_0, e2e_concatenate_422.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_430: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_426, t5.decoder.block.4.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_433: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_391_0, matmul_430],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_434: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_433, add_433],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_435.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_434, lc.input_tensor.reduce_avg_435.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_33: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_435.lc1, input_1_add_436, add_433, t5.decoder.block.4.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_443: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_33, t5.decoder.block.4.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_446: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_101],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_449: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_443, hslice_446],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_451: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_451, e2e_multiply_70_s_brcst_m2_5_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_452: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_449, add_451],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_453.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_452],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_34: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_452, softmax_453.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_453.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_34, lc.input_tensor.softmax_453.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_35: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_453.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_453.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_453.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_35, lc.input_tensor.softmax_453.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_453.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_34, softmax_453.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_456: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_103],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_460: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_453.dc.multiply.7, hslice_456],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_464: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_460, t5.decoder.block.4.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_433_add_467: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_433],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_467: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_433_add_467, matmul_464],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_468: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_467, add_467],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_469.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_468, lc.input_tensor.reduce_avg_469.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_36: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_469.lc1, input_1_add_470, add_467, t5.decoder.block.4.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_477: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_36, t5.decoder.block.4.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_483: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_477, t5.decoder.block.4.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_486: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_467, matmul_483],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_487: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_486, add_486],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_488.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_487, lc.input_tensor.reduce_avg_488.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_37: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_488.lc1, input_1_add_489, add_486, t5.decoder.block.5.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_496: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_37, t5.decoder.block.5.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_501: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_37, t5.decoder.block.5.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_503.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_121, matmul_501],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_506: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_496, concatenate_503.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_509: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_506, e2e_add_508_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_510.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_509],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_38: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_509, softmax_510.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_510.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_38, lc.input_tensor.softmax_510.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_39: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_510.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_510.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_510.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_39, lc.input_tensor.softmax_510.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_510.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_38, softmax_510.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_515: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_37, t5.decoder.block.5.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_517.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_123, matmul_515],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_446_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_446],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_456_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_456],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1184.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_503.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1184.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1184.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1185_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1184.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 1
    matmul_521: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_510.dc.multiply.7_0, e2e_concatenate_517.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_525: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_521, t5.decoder.block.5.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_528: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_486_0, matmul_525],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_529: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_528, add_528],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_530.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_529, lc.input_tensor.reduce_avg_530.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_40: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_530.lc1, input_1_add_531, add_528, t5.decoder.block.5.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_538: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_40, t5.decoder.block.5.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_541: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_125],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_544: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_538, hslice_541],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_546: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_546, e2e_multiply_70_s_brcst_m2_6_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_547: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_544, add_546],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_548.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_547],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_41: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_547, softmax_548.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_548.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_41, lc.input_tensor.softmax_548.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_42: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_548.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_548.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_42, lc.input_tensor.softmax_548.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_548.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_41, softmax_548.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_551: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_127],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_555: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_548.dc.multiply.7, hslice_551],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_559: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_555, t5.decoder.block.5.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_528_add_562: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_528],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_562: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_528_add_562, matmul_559],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_563: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_562, add_562],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_564.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_563, lc.input_tensor.reduce_avg_564.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_43: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_564.lc1, input_1_add_565, add_562, t5.decoder.block.5.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_572: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_43, t5.decoder.block.5.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_578: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_572, t5.decoder.block.5.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_581: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_562, matmul_578],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_582: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_581, add_581],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_583.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_582, lc.input_tensor.reduce_avg_583.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_583.lc1, input_1_add_584, add_581, t5.decoder.block.6.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_591: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_44, t5.decoder.block.6.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_596: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_44, t5.decoder.block.6.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_598.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_145, matmul_596],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_601: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_591, concatenate_598.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_604: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_601, e2e_add_603_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_605.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_604],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_45: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_604, softmax_605.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_605.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_45, lc.input_tensor.softmax_605.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_46: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_605.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_605.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_605.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_46, lc.input_tensor.softmax_605.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_605.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_45, softmax_605.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_610: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_44, t5.decoder.block.6.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_612.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_147, matmul_610],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_541_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_541],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_551_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_551],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1188.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_598.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1188.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1188.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1189_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1188.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 1
    matmul_616: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_605.dc.multiply.7_0, e2e_concatenate_612.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_620: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_616, t5.decoder.block.6.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_623: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_581_0, matmul_620],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_624: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_623, add_623],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_625.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_624, lc.input_tensor.reduce_avg_625.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_47: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_625.lc1, input_1_add_626, add_623, t5.decoder.block.6.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_633: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_47, t5.decoder.block.6.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_636: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_149],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_639: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_633, hslice_636],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_641: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_641, e2e_multiply_70_s_brcst_m2_7_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_642: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_639, add_641],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_643.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_642],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_48: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_642, softmax_643.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_643.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_48, lc.input_tensor.softmax_643.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_643.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_643.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_643.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_49, lc.input_tensor.softmax_643.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_643.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_48, softmax_643.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_646: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_151],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_650: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_643.dc.multiply.7, hslice_646],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_654: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_650, t5.decoder.block.6.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_623_add_657: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_623],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_657: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_623_add_657, matmul_654],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_658: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_657, add_657],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_659.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_658, lc.input_tensor.reduce_avg_659.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_50: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_659.lc1, input_1_add_660, add_657, t5.decoder.block.6.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_667: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_50, t5.decoder.block.6.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_673: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_667, t5.decoder.block.6.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_676: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_657, matmul_673],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_677: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_676, add_676],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_678.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_677, lc.input_tensor.reduce_avg_678.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_51: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_678.lc1, input_1_add_679, add_676, t5.decoder.block.7.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_686: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_51, t5.decoder.block.7.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_691: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_51, t5.decoder.block.7.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_693.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_169, matmul_691],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_696: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_686, concatenate_693.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_699: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_696, e2e_add_698_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_700.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_699],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_52: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_699, softmax_700.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_700.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_52, lc.input_tensor.softmax_700.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_700.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_700.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_700.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_53, lc.input_tensor.softmax_700.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_700.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_52, softmax_700.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_705: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_51, t5.decoder.block.7.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_707.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_171, matmul_705],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_636_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_636],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_646_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_646],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1192.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_693.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1192.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1192.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1193_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1192.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 1
    matmul_711: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_700.dc.multiply.7_0, e2e_concatenate_707.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_715: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_711, t5.decoder.block.7.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_718: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_676_0, matmul_715],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_719: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_718, add_718],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_720.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_719, lc.input_tensor.reduce_avg_720.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_54: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_720.lc1, input_1_add_721, add_718, t5.decoder.block.7.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_728: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_54, t5.decoder.block.7.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_731: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_173],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_734: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_728, hslice_731],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_736: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [input_0_add_736, e2e_multiply_70_s_brcst_m2_8_1.lc1_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_737: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_734, add_736],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_738.dc.reduce_max.0: {type: reduce, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_737],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_55: {type: fused_op, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_737, softmax_738.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_738.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_55, lc.input_tensor.softmax_738.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_56: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_738.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_738.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_738.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_56, lc.input_tensor.softmax_738.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_738.dc.multiply.7: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_55, softmax_738.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_741: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [states_175],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_745: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_738.dc.multiply.7, hslice_741],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_749: {type: matmul, grid_loc: [3, 2], grid_size: [1, 3], inputs: [matmul_745, t5.decoder.block.7.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_718_add_752: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_718],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_752: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_718_add_752, matmul_749],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_753: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_752, add_752],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_754.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_753, lc.input_tensor.reduce_avg_754.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_57: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [reduce_avg_754.lc1, input_1_add_755, add_752, t5.decoder.block.7.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_762: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_57, t5.decoder.block.7.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_768: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [matmul_762, t5.decoder.block.7.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_771: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_752, matmul_768],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_772: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_771, add_771],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_773.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [multiply_772, lc.input_tensor.reduce_avg_773.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_58: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [reduce_avg_773.lc1, input_1_add_774, add_771, t5.decoder.block.8.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_781: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [_fused_op_58, t5.decoder.block.8.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_786: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_58, t5.decoder.block.8.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_788.dc.concatenate.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 2], inputs: [states_193, matmul_786],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_791: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_781, concatenate_788.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_794: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_791, e2e_add_793_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_795.dc.reduce_max.0: {type: reduce, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_794],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_59: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 4], inputs: [add_794, softmax_795.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_795.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_59, lc.input_tensor.softmax_795.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_60: {type: fused_op, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_795.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_795.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_795.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_60, lc.input_tensor.softmax_795.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_795.dc.multiply.7: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [_fused_op_59, softmax_795.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_800: {type: matmul, grid_loc: [9, 3], grid_size: [1, 3], inputs: [_fused_op_58, t5.decoder.block.8.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_802.dc.concatenate.0: {type: splice, grid_loc: [9, 6], grid_size: [1, 2], inputs: [states_195, matmul_800],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_731_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_731],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_741_output_nop_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [hslice_741],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1196.dc.select.0: {type: splice, grid_loc: [4, 7], grid_size: [1, 1], inputs: [concatenate_788.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1196.dc.buffer.1: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1196.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1197_tm_nop: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [index_1196.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 1
    matmul_806: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_795.dc.multiply.7_0, e2e_concatenate_802.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_810: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_806, t5.decoder.block.8.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_813: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_771_0, matmul_810],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_814: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_813, add_813],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_815.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_814, lc.input_tensor.reduce_avg_815.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_61: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_815.lc1, input_1_add_816, add_813, t5.decoder.block.8.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_823: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_61, t5.decoder.block.8.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_826: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_197],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_829: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_823, hslice_826],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_832: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_829, e2e_add_831_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_833.dc.reduce_max.0: {type: reduce, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_832],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_62: {type: fused_op, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_832, softmax_833.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_833.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [_fused_op_62, lc.input_tensor.softmax_833.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_63: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [softmax_833.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_833.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_833.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_63, lc.input_tensor.softmax_833.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_833.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_62, softmax_833.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_836: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [states_199],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_840: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_833.dc.multiply.7, hslice_836],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_844: {type: matmul, grid_loc: [3, 1], grid_size: [1, 3], inputs: [matmul_840, t5.decoder.block.8.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_813_add_847: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_813],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_847: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [buffer_0_add_813_add_847, matmul_844],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_848: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_847, add_847],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_849.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [multiply_848, lc.input_tensor.reduce_avg_849.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_64: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [reduce_avg_849.lc1, input_1_add_850, add_847, t5.decoder.block.8.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_857: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_64, t5.decoder.block.8.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_863: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [matmul_857, t5.decoder.block.8.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_866: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_847, matmul_863],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_867: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_866, add_866],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_868.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [multiply_867, lc.input_tensor.reduce_avg_868.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_65: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [reduce_avg_868.lc1, input_1_add_869, add_866, t5.decoder.block.9.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_876: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [_fused_op_65, t5.decoder.block.9.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_881: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_65, t5.decoder.block.9.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_883.dc.concatenate.0: {type: splice, grid_loc: [6, 6], grid_size: [1, 2], inputs: [states_217, matmul_881],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_886: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_876, concatenate_883.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_889: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_886, e2e_add_888_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_890.dc.reduce_max.0: {type: reduce, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_889],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_66: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 4], inputs: [add_889, softmax_890.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_890.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_66, lc.input_tensor.softmax_890.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_67: {type: fused_op, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_890.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_890.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_890.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_67, lc.input_tensor.softmax_890.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_890.dc.multiply.7: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_66, softmax_890.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_895: {type: matmul, grid_loc: [9, 2], grid_size: [1, 3], inputs: [_fused_op_65, t5.decoder.block.9.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_897.dc.concatenate.0: {type: splice, grid_loc: [9, 5], grid_size: [1, 2], inputs: [states_219, matmul_895],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_826_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_826],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_836_output_nop_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [hslice_836],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1200.dc.select.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 1], inputs: [concatenate_883.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1200.dc.buffer.1: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [index_1200.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1201_tm_nop: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1200.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1202.dc.select.0: {type: splice, grid_loc: [9, 7], grid_size: [1, 1], inputs: [concatenate_897.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}

  fwd_0_10_temporal_epoch_10:
    target_device: 0
    input_count: 1
    matmul_901: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_890.dc.multiply.7_0, e2e_concatenate_897.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_905: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_901, t5.decoder.block.9.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_908: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_866_0, matmul_905],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_909: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_908, add_908],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_910.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_909, lc.input_tensor.reduce_avg_910.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_68: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_910.lc1, input_1_add_911, add_908, t5.decoder.block.9.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_918: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_68, t5.decoder.block.9.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_921: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_221],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_924: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_918, hslice_921],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_927: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_924, e2e_add_926_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_928.dc.reduce_max.0: {type: reduce, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_927],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_69: {type: fused_op, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_927, softmax_928.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_928.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [_fused_op_69, lc.input_tensor.softmax_928.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_70: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [softmax_928.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_928.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_928.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_70, lc.input_tensor.softmax_928.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_928.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_69, softmax_928.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_931: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [states_223],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_935: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_928.dc.multiply.7, hslice_931],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_939: {type: matmul, grid_loc: [3, 1], grid_size: [1, 3], inputs: [matmul_935, t5.decoder.block.9.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_908_add_942: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_908],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_942: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [buffer_0_add_908_add_942, matmul_939],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_943: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_942, add_942],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_944.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [multiply_943, lc.input_tensor.reduce_avg_944.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_71: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [reduce_avg_944.lc1, input_1_add_945, add_942, t5.decoder.block.9.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_952: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_71, t5.decoder.block.9.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_958: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [matmul_952, t5.decoder.block.9.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_961: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_942, matmul_958],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_962: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_961, add_961],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_963.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [multiply_962, lc.input_tensor.reduce_avg_963.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_72: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [reduce_avg_963.lc1, input_1_add_964, add_961, t5.decoder.block.10.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_971: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [_fused_op_72, t5.decoder.block.10.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_976: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_72, t5.decoder.block.10.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_978.dc.concatenate.0: {type: splice, grid_loc: [6, 6], grid_size: [1, 2], inputs: [states_241, matmul_976],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_981: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_971, concatenate_978.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_984: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_981, e2e_add_983_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_985.dc.reduce_max.0: {type: reduce, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_984],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_73: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 4], inputs: [add_984, softmax_985.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_985.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_73, lc.input_tensor.softmax_985.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_74: {type: fused_op, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_985.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_985.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_985.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_74, lc.input_tensor.softmax_985.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_985.dc.multiply.7: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_73, softmax_985.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_990: {type: matmul, grid_loc: [9, 2], grid_size: [1, 3], inputs: [_fused_op_72, t5.decoder.block.10.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_992.dc.concatenate.0: {type: splice, grid_loc: [9, 5], grid_size: [1, 2], inputs: [states_243, matmul_990],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_921_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_921],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_931_output_nop_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [hslice_931],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1204.dc.select.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 1], inputs: [concatenate_978.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1204.dc.buffer.1: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [index_1204.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1205_tm_nop: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1204.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1206.dc.select.0: {type: splice, grid_loc: [9, 7], grid_size: [1, 1], inputs: [concatenate_992.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}

  fwd_0_11_temporal_epoch_11:
    target_device: 0
    input_count: 1
    matmul_996: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_985.dc.multiply.7_0, e2e_concatenate_992.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_1000: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_996, t5.decoder.block.10.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_1003: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_961_0, matmul_1000],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1004: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_1003, add_1003],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1005.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_1004, lc.input_tensor.reduce_avg_1005.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_75: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [reduce_avg_1005.lc1, input_1_add_1006, add_1003, t5.decoder.block.10.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_1013: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [_fused_op_75, t5.decoder.block.10.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_1016: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [states_245],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1019: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_1013, hslice_1016],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_1022: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_1019, e2e_add_1021_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_1023.dc.reduce_max.0: {type: reduce, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_1022],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_76: {type: fused_op, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_1022, softmax_1023.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_1023.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [_fused_op_76, lc.input_tensor.softmax_1023.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_77: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [softmax_1023.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1023.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_1023.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_77, lc.input_tensor.softmax_1023.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_1023.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_76, softmax_1023.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_1026: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [states_247],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1030: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_1023.dc.multiply.7, hslice_1026],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_1034: {type: matmul, grid_loc: [3, 1], grid_size: [1, 3], inputs: [matmul_1030, t5.decoder.block.10.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_1003_add_1037: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_1003],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1037: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [buffer_0_add_1003_add_1037, matmul_1034],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1038: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_1037, add_1037],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1039.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [multiply_1038, lc.input_tensor.reduce_avg_1039.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_78: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [reduce_avg_1039.lc1, input_1_add_1040, add_1037, t5.decoder.block.10.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_1047: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_78, t5.decoder.block.10.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_1053: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [matmul_1047, t5.decoder.block.10.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_1056: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_1037, matmul_1053],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1057: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_1056, add_1056],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1058.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [multiply_1057, lc.input_tensor.reduce_avg_1058.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_79: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 1], inputs: [reduce_avg_1058.lc1, input_1_add_1059, add_1056, t5.decoder.block.11.layer.0.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_1066: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [_fused_op_79, t5.decoder.block.11.layer.0.SelfAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    matmul_1071: {type: matmul, grid_loc: [7, 0], grid_size: [1, 3], inputs: [_fused_op_79, t5.decoder.block.11.layer.0.SelfAttention.k.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_1073.dc.concatenate.0: {type: splice, grid_loc: [6, 6], grid_size: [1, 2], inputs: [states_265, matmul_1071],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    matmul_1076: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_1066, concatenate_1073.dc.concatenate.0],
         t: 12, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_1079: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_1076, e2e_add_1078_0],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_1080.dc.reduce_max.0: {type: reduce, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_1079],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 16}}
    _fused_op_80: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 4], inputs: [add_1079, softmax_1080.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {approximate_mode: false, fused_op_id: 2}}
    softmax_1080.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_80, lc.input_tensor.softmax_1080.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 4, min_buffer_input: 0, u_kt: 4}}
    _fused_op_81: {type: fused_op, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_1080.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1080.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_1080.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [_fused_op_81, lc.input_tensor.softmax_1080.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_1080.dc.multiply.7: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [_fused_op_80, softmax_1080.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [88, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    matmul_1085: {type: matmul, grid_loc: [9, 2], grid_size: [1, 3], inputs: [_fused_op_79, t5.decoder.block.11.layer.0.SelfAttention.v.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    concatenate_1087.dc.concatenate.0: {type: splice, grid_loc: [9, 5], grid_size: [1, 2], inputs: [states_267, matmul_1085],
         t: 12, mblock: [16, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12], input_0_tms: [vstack: 15, hslice: 12],
         attributes: {input0: [0, 15, 15], input1: [0, 1, 1]}}
    hslice_1016_output_nop_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [hslice_1016],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_1026_output_nop_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [hslice_1026],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    index_1208.dc.select.0: {type: splice, grid_loc: [7, 3], grid_size: [1, 1], inputs: [concatenate_1073.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1208.dc.buffer.1: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [index_1208.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1209_tm_nop: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [index_1208.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1210.dc.select.0: {type: splice, grid_loc: [9, 7], grid_size: [1, 1], inputs: [concatenate_1087.dc.concatenate.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}

  fwd_0_12_temporal_epoch_12:
    target_device: 0
    input_count: 1
    matmul_1091: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_softmax_1080.dc.multiply.7_0, e2e_concatenate_1087.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 4}}
    matmul_1095: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [matmul_1091, t5.decoder.block.11.layer.0.SelfAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    add_1098: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_1056_0, matmul_1095],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1099: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_1098, add_1098],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1100.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [multiply_1099, lc.input_tensor.reduce_avg_1100.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_82: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [reduce_avg_1100.lc1, input_1_add_1101, add_1098, t5.decoder.block.11.layer.1.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_1108: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [_fused_op_82, t5.decoder.block.11.layer.1.EncDecAttention.q.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    hslice_1111: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [states_269],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1114: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [matmul_1108, hslice_1111],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_1117: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_1114, e2e_add_1116_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_1118.dc.reduce_max.0: {type: reduce, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_1117],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 2}}
    _fused_op_83: {type: fused_op, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_1117, softmax_1118.dc.reduce_max.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}],
         attributes: {approximate_mode: false, fused_op_id: 6}}
    softmax_1118.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_83, lc.input_tensor.softmax_1118.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 2}, broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_84: {type: fused_op, grid_loc: [3, 2], grid_size: [1, 1], inputs: [softmax_1118.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_1118.4],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    softmax_1118.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_84, lc.input_tensor.softmax_1118.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_1118.dc.multiply.7: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_83, softmax_1118.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, input_buf_min_size_tiles: [14, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}]}
    hslice_1121: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [states_271],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1125: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_1118.dc.multiply.7, hslice_1121],
         t: 24, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_1129: {type: matmul, grid_loc: [4, 0], grid_size: [1, 3], inputs: [matmul_1125, t5.decoder.block.11.layer.1.EncDecAttention.o.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 24],
         attributes: {m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_1098_add_1132: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_1098],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [480], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1132: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_0_add_1098_add_1132, matmul_1129],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [76, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1133: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_1132, add_1132],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1134.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [multiply_1133, lc.input_tensor.reduce_avg_1134.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_85: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 1], inputs: [reduce_avg_1134.lc1, input_1_add_1135, add_1132, t5.decoder.block.11.layer.2.layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    matmul_1142: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [_fused_op_85, t5.decoder.block.11.layer.2.DenseReluDense.wi.weight],
         t: 3, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 24}}
    matmul_1148: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [matmul_1142, t5.decoder.block.11.layer.2.DenseReluDense.wo.weight],
         t: 1, mblock: [1, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 16}}
    add_1151: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_1132, matmul_1148],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [336, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    multiply_1152: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [add_1151, add_1151],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    reduce_avg_1153.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [multiply_1152, lc.input_tensor.reduce_avg_1153.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_86: {type: fused_op, grid_loc: [9, 7], grid_size: [1, 1], inputs: [reduce_avg_1153.lc1, input_1_add_1154, add_1151, t5.decoder.final_layer_norm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 120, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 1}], input_1_tms: [broadcast: {r: 1}],
         attributes: {approximate_mode: false, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    t5.output_hstack_1171_tm_nop: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [e2e_index_1170.dc.buffer.1_0],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1174.dc.select.0: {type: splice, grid_loc: [2, 2], grid_size: [1, 1], inputs: [e2e_concatenate_232.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1174.dc.buffer.1: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [index_1174.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1175_tm_nop: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [index_1174.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1178.dc.select.0: {type: splice, grid_loc: [2, 3], grid_size: [1, 1], inputs: [e2e_concatenate_327.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1178.dc.buffer.1: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [index_1178.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1179_tm_nop: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [index_1178.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1182.dc.select.0: {type: splice, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_concatenate_422.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1182.dc.buffer.1: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [index_1182.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1183_tm_nop: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [index_1182.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1186.dc.select.0: {type: splice, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e_concatenate_517.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1186.dc.buffer.1: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [index_1186.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1187_tm_nop: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [index_1186.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1190.dc.select.0: {type: splice, grid_loc: [2, 6], grid_size: [1, 1], inputs: [e2e_concatenate_612.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1190.dc.buffer.1: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [index_1190.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1191_tm_nop: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [index_1190.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1194.dc.select.0: {type: splice, grid_loc: [2, 7], grid_size: [1, 1], inputs: [e2e_concatenate_707.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1194.dc.buffer.1: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [index_1194.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1195_tm_nop: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [index_1194.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1198.dc.select.0: {type: splice, grid_loc: [3, 0], grid_size: [1, 1], inputs: [e2e_concatenate_802.dc.concatenate.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [15, 1, 1]}}
    index_1198.dc.buffer.1: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [index_1198.dc.select.0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1199_tm_nop: {type: nop, grid_loc: [9, 1], grid_size: [1, 1], inputs: [index_1198.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1202.dc.buffer.1: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [e2e_index_1202.dc.select.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1203_tm_nop: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [index_1202.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1206.dc.buffer.1: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [e2e_index_1206.dc.select.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1207_tm_nop: {type: nop, grid_loc: [9, 3], grid_size: [1, 1], inputs: [index_1206.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    index_1210.dc.buffer.1: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [e2e_index_1210.dc.select.0_0],
         t: 12, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    t5.output_hstack_1211_tm_nop: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [index_1210.dc.buffer.1],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    hslice_1111_output_nop_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [hslice_1111],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    hslice_1121_output_nop_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [hslice_1121],
         t: 12, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_13_temporal_epoch_13:
    target_device: 0
    input_count: 1
    multiply_1159: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_86_0, input_1_multiply_1159],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}, broadcast: {r: 1}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    matmul_1162: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [multiply_1159, t5.shared.weight_dup],
         t: 251, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hslice: 251], input_0_tms: [broadcast: {r: 251}, vslice: 251],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    t5.output_reshape_1163_tm_nop: {type: nop, grid_loc: [1, 0], grid_size: [1, 4], inputs: [buf_matmul_1162_0], untilize_output: true,
         t: 1, mblock: [1, 63], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 251], input_0_pad: {rt: 0, ct: 4, pad_value: 0}}


programs:
  - run_fwd_0:
    - param: [$p_outer_increment, $p_inner_increment, $p_outer_loop_count, $p_inner_loop_count, $p_cache_write_index]
    - var: {$v_cache_write_index: 0, $c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q16_shadow: 0, $gptr_q10_shadow: 0, $gptr_q8_shadow: 0, $gptr_q6_shadow: 0, $gptr_q19: 0, $lptr_q18: 0, $lptr_q16: 0, $gptr_q13: 0, $gptr_q16: 0, $lptr_q13: 0, $gptr_q18: 0, $lptr_q14: 0, $gptr_q3: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q0: 0, $lptr_q4: 0, $lptr_q19: 0, $lptr_q1: 0, $gptr_q20: 0, $lptr_q17: 0, $gptr_q7: 0, $lptr_q7: 0, $gptr_q11: 0, $lptr_q9: 0, $gptr_q1: 0, $lptr_q20: 0, $lptr_q8: 0, $gptr_q14_shadow: 0, $v_cache_read: 0, $gptr_q14: 0, $gptr_q8: 0, $lptr_q6: 0, $gptr_q12_shadow: 0, $gptr_q17: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q15: 0, $lptr_q12: 0, $lptr_q5: 0, $lptr_q3: 0, $lptr_q0: 0, $lptr_q10: 0, $gptr_q12: 0, $gptr_q4_shadow: 0, $gptr_q5: 0, $gptr_q10: 0, $gptr_q9: 0, $lptr_q15: 0, $lptr_q11: 0}
    - varinst: [$v_cache_write_index, set, $p_cache_write_index]
    - loop: $p_outer_loop_count
    -   loop: $p_inner_loop_count
    -     varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -     varinst: [$gptr_q6, set, $gptr_q6_shadow]
    -     varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -     varinst: [$gptr_q10, set, $gptr_q10_shadow]
    -     varinst: [$gptr_q12, set, $gptr_q12_shadow]
    -     varinst: [$gptr_q14, set, $gptr_q14_shadow]
    -     varinst: [$gptr_q16, set, $gptr_q16_shadow]
    -     execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
                 input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
                 attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
                 encoder_attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
                 t5.output_hstack_1165: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1167: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hslice_63: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 key_value_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
                 t5.shared.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_2.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_3: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_1: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 input_0_subtract_28: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_multiply_29: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_1_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_embedding_22: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_32.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_32.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_32.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_3: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_52.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_53: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_5: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_subtract_69: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_multiply_70: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_1_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_71: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_73.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_73.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_73.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_7: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_2_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_3_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_4_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_5_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_6_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_7_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_8_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_9_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_9_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_10_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_10_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_11_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_11_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_29_s_brcst_m2_0_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_0_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q0, incwrap, $c_microbatch_size, 4]
    -     varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -     execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
                 t5.output_hslice_76: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1169: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e__fused_op_5_attempt_1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_softmax_73.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_hslice_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_70_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_10_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_70_s_brcst_m2_10_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_11_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_70_s_brcst_m2_11_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_29_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 e2e_multiply_70_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
                 t5.decoder.block.0.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_89.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_90: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.0.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_108.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_109: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_25: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 input_0_add_128: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_130.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_130.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_130.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_27: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_2_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_223: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_3_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_318: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_4_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_413: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_5_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_508: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_6_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_603: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_7_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_698: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.multiply_70_s_brcst_m2_8_1.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_793: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_831: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_888: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_926: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_983: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_1021: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_1078: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_0_add_1116: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
                 t5.output_hslice_161: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_171: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1173: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_106_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
                 e2e_softmax_130.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
                 e2e_concatenate_137.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
                 e2e_multiply_70_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
                 e2e_add_223_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
                 t5.decoder.block.1.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_150.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_151: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_29: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_166: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_168.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_168.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_168.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_31: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_184.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_185: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.1.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_203.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_204: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_49: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_225.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_225.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_225.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_51: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
                 t5.output_hslice_256: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_266: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1177: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_201_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
                 e2e_softmax_225.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
                 e2e_concatenate_232.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
                 e2e_multiply_70_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
                 e2e_add_318_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
                 t5.decoder.block.2.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_245.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_246: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_53: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_261: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_263.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_263.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_263.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_55: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_279.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_280: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.2.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_298.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_299: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_73: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_320.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_320.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_320.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_75: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
                 t5.output_hslice_351: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_361: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1181: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_296_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
                 e2e_softmax_320.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
                 e2e_concatenate_327.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
                 e2e_multiply_70_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
                 e2e_add_413_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
                 t5.decoder.block.3.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_340.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_341: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_77: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_356: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_358.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_358.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_358.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_79: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_374.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_375: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.3.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_393.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_394: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_97: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_415.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_415.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_415.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_99: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
                 t5.output_hslice_446: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_456: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1185: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_391_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
                 e2e_softmax_415.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
                 e2e_concatenate_422.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
                 e2e_multiply_70_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
                 e2e_add_508_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
                 t5.decoder.block.4.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_435.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_436: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_101: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_451: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_453.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_453.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_453.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_103: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_469.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_470: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.4.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_488.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_489: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_121: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_510.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_510.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_510.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_123: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q7, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q8, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
                 t5.output_hslice_541: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_551: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1189: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_486_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
                 e2e_softmax_510.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
                 e2e_concatenate_517.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
                 e2e_multiply_70_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
                 e2e_add_603_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
                 t5.decoder.block.5.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_530.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_531: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_125: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_546: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_548.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_548.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_548.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_127: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_564.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_565: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.5.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_583.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_584: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_145: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_605.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_605.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_605.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_147: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q9, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q10, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
                 t5.output_hslice_636: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_646: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1193: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_581_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
                 e2e_softmax_605.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
                 e2e_concatenate_612.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
                 e2e_multiply_70_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
                 e2e_add_698_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
                 t5.decoder.block.6.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_625.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_626: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_149: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_641: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_643.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_643.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_643.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_151: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_659.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_660: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.6.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_678.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_679: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_169: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_700.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_700.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_700.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_171: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
                 t5.output_hslice_731: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_741: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1197: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_676_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
                 e2e_softmax_700.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
                 e2e_concatenate_707.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
                 e2e_multiply_70_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
                 e2e_add_793_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
                 t5.decoder.block.7.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_720.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_721: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_173: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 input_0_add_736: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_738.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_738.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_738.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_175: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_754.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_755: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.7.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_773.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_774: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_193: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_795.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_795.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_795.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_195: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q13, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q13, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q14, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
                 t5.output_hslice_826: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_836: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1201: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_771_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
                 e2e_softmax_795.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
                 e2e_concatenate_802.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
                 e2e_add_831_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
                 e2e_add_888_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
                 t5.decoder.block.8.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_815.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_816: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_197: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_833.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_833.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_833.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_199: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_849.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_850: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.8.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_868.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_869: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_217: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_890.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_890.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_890.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_219: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q15, incwrap, $c_microbatch_size, 2]
    -     varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q15, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q16, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_10_temporal_epoch_10, queue_settings: {
                 t5.output_hslice_921: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_931: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1205: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_866_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
                 e2e_softmax_890.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
                 e2e_concatenate_897.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
                 e2e_add_926_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
                 e2e_add_983_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
                 t5.decoder.block.9.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_910.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_911: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_221: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_928.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_928.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_928.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_223: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_944.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_945: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.9.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_963.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_964: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_241: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_985.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_985.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_985.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_243: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q17, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q17, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_11_temporal_epoch_11, queue_settings: {
                 t5.output_hslice_1016: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_1026: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hstack_1209: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 e2e_add_961_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
                 e2e_softmax_985.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
                 e2e_concatenate_992.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
                 e2e_add_1021_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
                 e2e_add_1078_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
                 t5.decoder.block.10.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1005.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1006: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_245: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1023.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_1023.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1023.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_247: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1039.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1040: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.10.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1058.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1059: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.0.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.0.SelfAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_265: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.0.SelfAttention.k.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1080.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_1080.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1080.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_267: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.0.SelfAttention.v.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q18, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q18, incwrap, $c_microbatch_size, 2]
    -     execute: {graph_name: fwd_0_12_temporal_epoch_12, queue_settings: {
                 t5.output_hstack_1171: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1175: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1179: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1183: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1187: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1191: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1195: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1199: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1203: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1207: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hstack_1211: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 15},
                 t5.output_hslice_1111: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 t5.output_hslice_1121: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $v_cache_write_index, global_wrptr_autoinc: 1},
                 e2e_concatenate_232.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_327.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_422.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_517.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_612.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_707.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_802.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_add_1056_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_softmax_1080.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_concatenate_1087.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_add_1116_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_index_1170.dc.buffer.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_index_1202.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_index_1206.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 e2e_index_1210.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
                 t5.decoder.block.11.layer.0.SelfAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1100.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1101: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.1.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.1.EncDecAttention.q.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 states_269: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1118.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 dc.input_tensor.softmax_1118.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 lc.input_tensor.softmax_1118.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 states_271: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, read_only: true, rd_ptr_global: $v_cache_read, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.1.EncDecAttention.o.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1134.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1135: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.2.layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.2.DenseReluDense.wi.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 t5.decoder.block.11.layer.2.DenseReluDense.wo.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
                 lc.input_tensor.reduce_avg_1153.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_add_1154: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.decoder.final_layer_norm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     varinst: [$gptr_q19, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q19, incwrap, $c_microbatch_size, 2]
    -     allocate_queue: [buf_matmul_1162_0]
    -     execute: {graph_name: fwd_0_13_temporal_epoch_13, queue_settings: {
                 e2e__fused_op_86_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
                 buf_matmul_1162_0: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 input_1_multiply_1159: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
                 t5.shared.weight_dup: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -     deallocate_queue: [buf_matmul_1162_0]
    -     varinst: [$gptr_q20, incwrap, $c_microbatch_size, 2]
    -     varinst: [$lptr_q20, incwrap, $c_microbatch_size, 2]
    -     varinst: [$v_cache_write_index, inc, $p_inner_increment]
    -   endloop
    -   varinst: [$v_cache_write_index, inc, $p_outer_increment]
    - endloop


fused_ops:
  0: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - add_3: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 1], output: dest}
        - sqrt_4: { type: sqrt, inputs: [dest], mblock: [1, 1], ublock: [1, 1], output: dest}
        - reciprocal_5: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [1, 1], output: intermed0}
      -
        - multiply_6: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 24}, tile_broadcast: c], pop_last: [intermed0], mblock: [1, 6], ublock: [1, 4], output: dest}
        - multiply_7: { type: multiply, inputs: [input3, dest], mblock: [1, 6], ublock: [1, 4], output: output}
  1: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - subtract_28: { type: subtract, inputs: [input0, input1], mblock: [1, 4], ublock: [1, 4], output: dest}
        - multiply_29: { type: multiply, inputs: [dest, input2], mblock: [1, 4], ublock: [1, 4], output: output}
  2: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_32.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [1, 1], ublock: [1, 4], output: dest}
        - softmax_32.dc.exp.2: { type: exp, inputs: [dest], mblock: [1, 1], ublock: [1, 4], output: output}
  3: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_32.dc.add.5: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 1], output: dest}
        - softmax_32.dc.reciprocal.6: { type: reciprocal, inputs: [dest], mblock: [1, 1], ublock: [1, 1], output: output}
  5: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - subtract_69: { type: subtract, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 2], output: dest}
        - multiply_70: { type: multiply, inputs: [dest, input2], mblock: [1, 1], ublock: [1, 2], output: output}
  6: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_73.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [1, 1], ublock: [1, 2], output: dest}
        - softmax_73.dc.exp.2: { type: exp, inputs: [dest], mblock: [1, 1], ublock: [1, 2], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
    overrides:
      input_1:
        type: Uniform
        uniform_lower_bound: 0
        uniform_upper_bound: 32127  # embedding table size - 1
      lc.input_tensor.reduce_avg_.*:
        type: Constant
        constant_value: 0.001953125  # 1/seq_len
      input_1_add_.*:
        type: Constant
        constant_value: 1.0  # must be positive to feed sqrt
  io-config:
    inputs: [input_1, attention_mask, encoder_attention_mask, key_value_states]
    outputs: [t5.output_nop_1212]
  test-args:
    cache_write_index: 0
    inner_increment: 0
    inner_loop_count: 1
    outer_increment: 0
    outer_loop_count: 1
