devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31a40020]]}

  # output
  bert_encoders.output_layernorm_611:                                           {input: layernorm_611.dc.add.10, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ad5a80]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6bed6e0]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5eab320]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5eaea40]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6b4fec0]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f94c00]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6a12dc0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6561ac0]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ea7ca0]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x655b1a0]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x650c580], [4, 0x64ac0e0], [5, 0x5e59080], [6, 0x5f45b60], [7, 0x5e5be00], [0, 0x6aff200], [1, 0x6551740], [2, 0x6a80540]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6537320]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64e8700], [2, 0x6a314a0], [3, 0x64bd960], [4, 0x645d4c0], [5, 0x5e0a460], [6, 0x5ef6f40], [7, 0x5e0d1e0], [0, 0x6ab05e0]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fac080]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6bfada0]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fa5760]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ec8ea0]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x656c180]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b79bc0]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6bf4480]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6531ec0]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6b732a0]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65b4320]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f48b40]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x64125e0]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6441c00]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x63f2fe0], [2, 0x691d560], [3, 0x63c39c0], [4, 0x6363520], [5, 0x5d1b1e0], [6, 0x5d5fc00], [7, 0x5c75ea0], [0, 0x69192a0]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c5ba80]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c0ce60], [0, 0x68ca200], [1, 0x63a43c0], [2, 0x68ce940], [3, 0x6374da0], [4, 0x6314900], [5, 0x5ccc5c0], [6, 0x5d10fe0]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cc5ca0]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5c06540]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cbf380]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62707c0]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68c5f80]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6306720]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bffc20]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x62d07e0]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5e068c0]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5e59720]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6456ba0]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5e52e00]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6450280]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6401660], [5, 0x5dbaf40], [6, 0x5e041e0], [7, 0x5db7820], [0, 0x69c3d20], [1, 0x6497a40], [2, 0x69dbf60], [3, 0x6468420]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x69c1b40]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6972f20], [3, 0x6419380], [4, 0x63b2a40], [5, 0x5d6c320], [6, 0x5db55c0], [7, 0x5d68c00], [0, 0x6975100], [1, 0x6448e20]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x696e7e0]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x696c600]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6967ec0]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5cc4ac0]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6cbb9a0]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x61b66c0]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6904f80]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6e0b1a0]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x61afda0]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x620cc80]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6764200]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6206360]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x675d8e0]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x670ecc0], [4, 0x68af5c0], [5, 0x61b7740], [6, 0x6160d00], [7, 0x61f46c0], [0, 0x6dba4e0], [1, 0x689d8e0], [2, 0x6e7a360]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x68834c0]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x68348a0], [2, 0x6e2b2c0], [3, 0x66c00a0], [4, 0x68609a0], [5, 0x6168b20], [6, 0x61120e0], [7, 0x61a5aa0], [0, 0x6d6b8c0]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x619f180]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ea92c0]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ed9ae0]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x679fc40]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62b46e0]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x624ae00]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x699e160]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x690d940]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ed31c0]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69004c0]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6244060]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x68f9ba0]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x68fe1e0]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d10ee0], [3, 0x65cc5a0], [4, 0x66c9660], [5, 0x606cf60], [6, 0x6011ca0], [7, 0x600e760], [0, 0x6c77dc0], [1, 0x6745aa0]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c5d9a0]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c0ed80], [1, 0x66f6a00], [2, 0x6cc22c0], [3, 0x657d980], [4, 0x667aa40], [5, 0x601e340], [6, 0x5fc3080], [7, 0x5fbfb40]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fbc760]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c08460]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f84100]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5f7a200]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6574fc0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c1e180]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c01b40]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x65d6480]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c17860]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x66588e0]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fb5e40]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6104ea0]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6853760]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6804b40], [5, 0x6113160], [6, 0x60b6280], [7, 0x61500e0], [0, 0x6d1c3a0], [1, 0x67e3be0], [2, 0x6dd5d80], [3, 0x666ab60]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6dbb960]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d6cd40], [3, 0x661bac0], [4, 0x67b5f20], [5, 0x60c4540], [6, 0x6067660], [7, 0x61014c0], [0, 0x6ccd780], [1, 0x6794fc0]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d5fb00]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6d66420]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6cc69e0]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x605d380]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60bbb80]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6718280]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5d622e0]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x58dae60]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ee5e40]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x65641e0]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65eb780]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x655d8c0]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65e4e60]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6596240], [1, 0x5fb8440], [2, 0x650eca0], [3, 0x5e96da0], [4, 0x5e913a0], [5, 0x588a1a0], [6, 0x5896ae0], [7, 0x5817380]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x587c6c0]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x582daa0], [7, 0x57c82e0], [0, 0x6547620], [1, 0x5f69820], [2, 0x64c0080], [3, 0x5e48180], [4, 0x5e42780], [5, 0x583b580]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5e3be60]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6608c20]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6602780]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5876680]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5978f80]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ee7660]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6602300]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58fb2e0]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ee0d40]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ef3080]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6014b80]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5eec760]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x586fd60]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5640920], [7, 0x5640920], [0, 0x6360940], [1, 0x5cd0940], [2, 0x6360940], [3, 0x5cd0940], [4, 0x5cd0940], [5, 0x56de140]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d7b3c0]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d2c7a0], [5, 0x572d660], [6, 0x5733b00], [7, 0x5698380], [0, 0x6453b20], [1, 0x5dd08c0], [2, 0x63bfc40], [3, 0x5d54680]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x63b9320]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d25e80]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x63b2a00]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5d2c780]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x568f9c0]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5695e60]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5d1f560]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x63af560]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x568f540]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5640920]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57c19c0]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5f60e60]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x57bb0a0]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x576c480], [0, 0x64f8100], [1, 0x5f12240], [2, 0x646a6c0], [3, 0x5df27c0], [4, 0x5deb1a0], [5, 0x57e5bc0], [6, 0x57d8560]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57cb7a0]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x56e6fa0], [0, 0x64a2740], [1, 0x5e1f4e0], [2, 0x640e860], [3, 0x5da32a0], [4, 0x5d957e0], [5, 0x577c700], [6, 0x5789040]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6463da0]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5de4400]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x645d480]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5e6e100]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5782720]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c1e240]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c11000]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x625fc40]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b269c0]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c30b60]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6217d80]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5c2a240]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6211460]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x61c2840], [5, 0x5bc1ae0], [6, 0x5bdb620], [7, 0x5ad7920], [0, 0x67d59a0], [1, 0x620ef80], [2, 0x67c3660], [3, 0x61c6aa0]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x67a9240]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x675a620], [3, 0x6177a00], [4, 0x6173c20], [5, 0x5b72ec0], [6, 0x5b8ca00], [7, 0x5a88d00], [0, 0x6786d80], [1, 0x61c0360]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6780460]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cd55a0]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x625cc80]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6232fc0]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62fdd60]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x682bc60]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5ccec80]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6827e60]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6825340]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b61b00]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c17920]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x61b2ca0]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x60cc680]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x607da60], [4, 0x5fdc8e0], [5, 0x5a72a80], [6, 0x5a98f00], [7, 0x59731a0], [0, 0x668cde0], [1, 0x60c63c0], [2, 0x665eee0]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x60abfa0]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x605d380], [2, 0x660fe40], [3, 0x602ee40], [4, 0x5f8dcc0], [5, 0x5a23e60], [6, 0x5a4a2e0], [7, 0x5924580], [0, 0x663e1c0]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x591dc60]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6056a60]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5917340]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x59a61a0]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f85300]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f911a0]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6050140]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x597f8a0]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b6c5a0]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x60d6400]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6753d00]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x60cfae0]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x674d3e0]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ac16a0], [6, 0x5ae7b20], [7, 0x59c1dc0], [0, 0x66dba00], [1, 0x6114fe0], [2, 0x66adb00], [3, 0x60d2fa0], [4, 0x602b980]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a65f20]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [96, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a17300], [0, 0x672aaa0], [1, 0x6164080], [2, 0x66fe7c0], [3, 0x61284e0], [4, 0x6080ec0], [5, 0x5b1d500], [6, 0x5b37040]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b16be0]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a109e0]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b102c0]]}

  # constant
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65b3ea0]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f9b520]]}
  constant_1_multiply_17:                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64fb600]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x65683e0]]}
  lc.input_tensor.softmax_19.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65a6c80]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5eaaea0]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64fb180]]}
  lc.input_tensor.layernorm_37.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6acf160]]}
  lc.input_tensor.layernorm_37.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x65a0360]]}
  dc.input_tensor.layernorm_37.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b4de20]]}
  lc.input_tensor.layernorm_37.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5eaaa20]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f94780]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x64fad00]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6a800c0]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5eae5c0]]}
  lc.input_tensor.layernorm_50.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5f666c0]]}
  lc.input_tensor.layernorm_50.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x65cfb60]]}
  dc.input_tensor.layernorm_50.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6572aa0]]}
  lc.input_tensor.layernorm_50.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6c173e0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6658460]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f83c80]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x65cf6e0]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x656b880]]}
  constant_1_multiply_68:                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f4f460]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fa22c0]]}
  lc.input_tensor.softmax_70.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ebbc80]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x656bd00]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6bf4000]]}
  lc.input_tensor.layernorm_88.dc.reduce_avg.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5f9b9a0]]}
  lc.input_tensor.layernorm_88.dc.reduce_avg.3.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5eb5360]]}
  dc.input_tensor.layernorm_88.4:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x652fe20]]}
  lc.input_tensor.layernorm_88.dc.reciprocal.7_s_brcst_m1_0_0.0:                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6467fa0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x63b2140]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x696c180]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6918e20]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d10b60]]}
  lc.input_tensor.layernorm_101.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x630dfe0]]}
  lc.input_tensor.layernorm_101.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x636e480]]}
  dc.input_tensor.layernorm_101.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x68cc8a0]]}
  lc.input_tensor.layernorm_101.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68c5b00]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x68c9d80]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5d106e0]]}
  lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x636e000]]}
  lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x68c9900]]}
  constant_1_multiply_119:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cdbec0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cbbee0]]}
  lc.input_tensor.softmax_121.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62635a0]]}
  lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x63a3f40]]}
  lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5e09fe0]]}
  lc.input_tensor.layernorm_139.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x64b7040]]}
  lc.input_tensor.layernorm_139.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6a2ab80]]}
  dc.input_tensor.layernorm_139.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x64e6660]]}
  lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6a12940]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5e06440]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5e09b60]]}
  lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5d69e00]]}
  lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64489a0]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6651b40]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5daeca0]]}
  dc.input_tensor.layernorm_152.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d6a280]]}
  lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x63b25c0]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6418f00]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6448520]]}
  lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5dae820]]}
  lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62aa4a0]]}
  constant_1_multiply_170:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x676ab20]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ecf8a0]]}
  lc.input_tensor.softmax_172.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x68ec980]]}
  lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6243760]]}
  lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6904b00]]}
  lc.input_tensor.layernorm_190.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ec8f80]]}
  lc.input_tensor.layernorm_190.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x685a080]]}
  dc.input_tensor.layernorm_190.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e09100]]}
  lc.input_tensor.layernorm_190.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x62432e0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x61af920]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x68ec500]]}
  lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e79ee0]]}
  lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6d6b440]]}
  lc.input_tensor.layernorm_203.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x610b7c0]]}
  lc.input_tensor.layernorm_203.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6162200]]}
  dc.input_tensor.layernorm_203.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x625a800]]}
  lc.input_tensor.layernorm_203.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6243be0]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x624a980]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x699dce0]]}
  lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x69ab160]]}
  lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ee0400]]}
  constant_1_multiply_221:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6eafbe0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62b1240]]}
  lc.input_tensor.softmax_223.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x625c8a0]]}
  lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x679f7c0]]}
  lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ea8e40]]}
  lc.input_tensor.layernorm_241.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6253ee0]]}
  lc.input_tensor.layernorm_241.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62aa920]]}
  dc.input_tensor.layernorm_241.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x690b8a0]]}
  lc.input_tensor.layernorm_241.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x679f340]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6ed2d40]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ea89c0]]}
  lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6745620]]}
  lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5fbf6c0]]}
  lc.input_tensor.layernorm_254.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6017a20]]}
  lc.input_tensor.layernorm_254.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6674120]]}
  dc.input_tensor.layernorm_254.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x657b8e0]]}
  lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ccd300]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x66f6580]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5fbf240]]}
  lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6673ca0]]}
  lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x66f6100]]}
  constant_1_multiply_272:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5f8aa20]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5fb29a0]]}
  lc.input_tensor.softmax_274.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5f6cfe0]]}
  lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6574b40]]}
  lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6c016c0]]}
  lc.input_tensor.layernorm_292.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x66b9780]]}
  lc.input_tensor.layernorm_292.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6e249a0]]}
  dc.input_tensor.layernorm_292.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6832800]]}
  lc.input_tensor.layernorm_292.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6d6afc0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x619ed00]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x6161d80]]}
  lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x666a6e0]]}
  lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6794b40]]}
  lc.input_tensor.layernorm_305.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x60faba0]]}
  lc.input_tensor.layernorm_305.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6060d40]]}
  dc.input_tensor.layernorm_305.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x60c24a0]]}
  lc.input_tensor.layernorm_305.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x67b5aa0]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x661b640]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x67946c0]]}
  lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x60608c0]]}
  lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x661b1c0]]}
  constant_1_multiply_323:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5735bc0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5865fa0]]}
  lc.input_tensor.softmax_325.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58ec020]]}
  lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ee0440]]}
  lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x60074e0]]}
  lc.input_tensor.layernorm_343.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65f20a0]]}
  lc.input_tensor.layernorm_343.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x58e5700]]}
  dc.input_tensor.layernorm_343.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x58d8dc0]]}
  lc.input_tensor.layernorm_343.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65471a0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5ee59c0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6007060]]}
  lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5816f00]]}
  lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x583b100]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5e41860]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x64b9760]]}
  dc.input_tensor.layernorm_356.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f67780]]}
  lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5edffc0]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f90d20]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x604fcc0]]}
  lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5998b00]]}
  lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5f908a0]]}
  constant_1_multiply_374:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x601b4a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65ff2e0]]}
  lc.input_tensor.softmax_376.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6007960]]}
  lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5978b00]]}
  lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6601e80]]}
  lc.input_tensor.layernorm_394.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x65f89c0]]}
  lc.input_tensor.layernorm_394.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5869440]]}
  dc.input_tensor.layernorm_394.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x58f9240]]}
  lc.input_tensor.layernorm_394.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5978680]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5ee08c0]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6601a00]]}
  lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x577c280]]}
  lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d54200]]}
  lc.input_tensor.layernorm_407.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5dc9fa0]]}
  lc.input_tensor.layernorm_407.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x644d200]]}
  dc.input_tensor.layernorm_407.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x56962e0]]}
  lc.input_tensor.layernorm_407.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5733680]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x572d1e0]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d53d80]]}
  lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x644cd80]]}
  lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x572cd60]]}
  constant_1_multiply_425:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5d1f560]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x63af560]]}
  lc.input_tensor.softmax_427.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5d1f560]]}
  lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x568f540]]}
  lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5dead20]]}
  lc.input_tensor.layernorm_445.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5827180]]}
  lc.input_tensor.layernorm_445.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x58347e0]]}
  dc.input_tensor.layernorm_445.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5e39dc0]]}
  lc.input_tensor.layernorm_445.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5e413e0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x64b92e0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6546d20]]}
  lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x57d80e0]]}
  lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5df2340]]}
  lc.input_tensor.layernorm_458.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5f0b920]]}
  lc.input_tensor.layernorm_458.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64f17e0]]}
  dc.input_tensor.layernorm_458.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x576a3e0]]}
  lc.input_tensor.layernorm_458.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x57d7c60]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57cb320]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5df1ec0]]}
  lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x64f1360]]}
  lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cce380]]}
  constant_1_multiply_476:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x621e6a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x621bfe0]]}
  lc.input_tensor.softmax_478.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6818ba0]]}
  lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6824a40]]}
  lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c10b80]]}
  lc.input_tensor.layernorm_496.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x62156c0]]}
  lc.input_tensor.layernorm_496.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b860e0]]}
  dc.input_tensor.layernorm_496.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x625dba0]]}
  lc.input_tensor.layernorm_496.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x68245c0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b26540]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c10700]]}
  lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x61c6620]]}
  lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x61bfee0]]}
  lc.input_tensor.layernorm_509.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5a823e0]]}
  lc.input_tensor.layernorm_509.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6812280]]}
  dc.input_tensor.layernorm_509.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6304680]]}
  lc.input_tensor.layernorm_509.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x68c9480]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bff7a0]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5cbba60]]}
  lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x68c5680]]}
  lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5bff320]]}
  constant_1_multiply_527:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5b2d2e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x62597e0]]}
  lc.input_tensor.softmax_529.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6225da0]]}
  lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62fd8e0]]}
  lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5cce800]]}
  lc.input_tensor.layernorm_547.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x6252ec0]]}
  lc.input_tensor.layernorm_547.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x621f480]]}
  dc.input_tensor.layernorm_547.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6825dc0]]}
  lc.input_tensor.layernorm_547.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62fd460]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6824ec0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x602b500]]}
  lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x665ea60]]}
  lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x663dd40]]}
  lc.input_tensor.layernorm_560.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5a439c0]]}
  lc.input_tensor.layernorm_560.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a1d540]]}
  dc.input_tensor.layernorm_560.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5f8bc20]]}
  lc.input_tensor.layernorm_560.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x602e9c0]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5f84e80]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x663d8c0]]}
  lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a1d0c0]]}
  lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x660f540]]}
  constant_1_multiply_578:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x66090a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5913ea0]]}
  lc.input_tensor.softmax_580.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5998f80]]}
  lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x660f9c0]]}
  lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6177580]]}
  lc.input_tensor.layernorm_598.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x61b95c0]]}
  lc.input_tensor.layernorm_598.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6779b40]]}
  dc.input_tensor.layernorm_598.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5a80340]]}
  lc.input_tensor.layernorm_598.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b85c60]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b6c120]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6177100]]}
  lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x67796c0]]}
  lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b36bc0]]}
  lc.input_tensor.layernorm_611.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x607a5a0]]}
  lc.input_tensor.layernorm_611.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x6121bc0]]}
  dc.input_tensor.layernorm_611.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x66fc720]]}
  lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6163c00]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x672a620]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5b36740]]}

  # epoch_to_epoch
  e2e_add_48_0:                                                                 {input: add_48, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  e2e_buffer_0_layernorm_37.dc.add.10_add_49_0:                                 {input: buffer_0_layernorm_37.dc.add.10_add_49, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  e2e_attention_mask_s_brcst_m2_10_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_10_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  e2e_matmul_116_0:                                                             {input: matmul_116, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x62bb000]]}
  e2e_attention_mask_s_brcst_m2_9_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_9_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900]]}
  e2e_layernorm_101.dc.add.10_0:                                                {input: layernorm_101.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x683d460]]}
  e2e_layernorm_101.dc.add.10_1:                                                {input: layernorm_101.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x69ab5e0]]}
  e2e_attention_mask_s_brcst_m2_8_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_8_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  e2e_buffer_0_add_189_layernorm_190.dc.subtract.1_0:                           {input: buffer_0_add_189_layernorm_190.dc.subtract.1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x6269ac0]]}
  e2e_layernorm_190.dc.reduce_avg.0.lc1_0:                                      {input: layernorm_190.dc.reduce_avg.0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x62e8620]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900]]}
  e2e_matmul_244_0:                                                             {input: matmul_244, type: queue, entries: 128, grid_size: [1, 8], t: 1, mblock: [4, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x69a4a80], [2, 0x6ee0880], [3, 0x755d480], [4, 0x76cb600], [5, 0x7cfb020], [6, 0x6f89ae0], [7, 0x6374640], [0, 0x7c04420]]}
  e2e_layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0:                     {input: layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x6ee4400]]}
  e2e_layernorm_241.dc.add.10_0:                                                {input: layernorm_241.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7034aa0]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  e2e_add_303_0:                                                                {input: add_303, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x7bed4a0]]}
  e2e_buffer_0_layernorm_292.dc.add.10_add_304_0:                               {input: buffer_0_layernorm_292.dc.add.10_add_304, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x75708a0]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  e2e_matmul_371_0:                                                             {input: matmul_371, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x7619b00]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5640920]]}
  e2e_layernorm_356.dc.add.10_0:                                                {input: layernorm_356.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x7d5b620]]}
  e2e_layernorm_356.dc.add.10_1:                                                {input: layernorm_356.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x838b040]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5cd0920]]}
  e2e_buffer_0_add_444_layernorm_445.dc.subtract.1_0:                           {input: buffer_0_add_444_layernorm_445.dc.subtract.1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8294440]]}
  e2e_layernorm_445.dc.reduce_avg.0.lc1_0:                                      {input: layernorm_445.dc.reduce_avg.0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x6a04660]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5cd0920]]}
  e2e_matmul_499_0:                                                             {input: matmul_499, type: queue, entries: 128, grid_size: [1, 8], t: 1, mblock: [4, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x82908c0], [3, 0x890d4c0], [4, 0x8a7b640], [5, 0x90ab060], [6, 0x9059b20], [7, 0x6a90680], [0, 0x8fb4460], [1, 0x8a74ae0]]}
  e2e_layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0:                     {input: layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x89208e0]]}
  e2e_layernorm_496.dc.add.10_0:                                                {input: layernorm_496.dc.add.10, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x7d54ac0]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5640920]]}
  e2e_add_558_0:                                                                {input: add_558, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8f9d4e0]]}
  e2e_buffer_0_layernorm_547.dc.add.10_add_559_0:                               {input: buffer_0_layernorm_547.dc.add.10_add_559, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x910b660]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x5640920]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [hidden_states, layer.0.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_4: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [matmul_2, layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_8: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [hidden_states, layer.0.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_10: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_8, layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_14: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [add_4, add_10],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_17: {type: multiply, grid_loc: [2, 10], grid_size: [1, 1], inputs: [matmul_14, constant_1_multiply_17],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    add_18: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [multiply_17, attention_mask_s_brcst_m2_11_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_19.dc.exp.0: {type: exp, grid_loc: [3, 1], grid_size: [1, 4], inputs: [add_18],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_19.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0, lc.input_tensor.softmax_19.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_19.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_19.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_19.dc.exp.0_softmax_19.dc.multiply.3: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_19.dc.multiply.3: {type: multiply, grid_loc: [3, 8], grid_size: [1, 1], inputs: [buffer_0_softmax_19.dc.exp.0_softmax_19.dc.multiply.3, softmax_19.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_hidden_states_matmul_22: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [hidden_states],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_hidden_states_matmul_22: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [buffer_1_hidden_states_matmul_22],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_22: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_hidden_states_matmul_22, layer.0.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_24: {type: add, grid_loc: [3, 10], grid_size: [1, 1], inputs: [matmul_22, layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_29: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [softmax_19.dc.multiply.3, add_24],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_29, layer.0.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_35: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [matmul_33, layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_hidden_states_add_36: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [hidden_states],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_hidden_states_add_36: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [buffer_1_hidden_states_add_36],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_36: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [add_35, buffer_0_hidden_states_add_36],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_36, lc.input_tensor.layernorm_37.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_36_layernorm_37.dc.subtract.1: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_36],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_37.dc.subtract.1: {type: subtract, grid_loc: [5, 5], grid_size: [1, 1], inputs: [buffer_0_add_36_layernorm_37.dc.subtract.1, layernorm_37.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_37.dc.multiply.2: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_37.dc.subtract.1, layernorm_37.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_37.dc.multiply.2, lc.input_tensor.layernorm_37.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_37.dc.add.5: {type: add, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_37.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_37.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37.dc.sqrt.6: {type: sqrt, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_37.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_37.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_37.dc.reciprocal.7, lc.input_tensor.layernorm_37.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_37.dc.subtract.1_layernorm_37.dc.multiply.8: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_37.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_37.dc.subtract.1_layernorm_37.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_1_layernorm_37.dc.subtract.1_layernorm_37.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_37.dc.multiply.8: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_37.dc.subtract.1_layernorm_37.dc.multiply.8, layernorm_37.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_37.dc.multiply.9: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_37.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_37.dc.add.10: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_37.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_40: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [layernorm_37.dc.add.10, layer.0.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_42: {type: add, grid_loc: [7, 9], grid_size: [1, 2], inputs: [matmul_40, layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_43: {type: gelu, grid_loc: [8, 0], grid_size: [1, 6], inputs: [add_42],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_46: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_43, layer.0.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.0.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0, layer.0.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_48: {type: add, grid_loc: [9, 9], grid_size: [1, 1], inputs: [matmul_46, layer.0.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_37.dc.add.10_add_49: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_37.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_37.dc.add.10_add_49: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_37.dc.add.10_add_49],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_10_1.lc1: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_10_1.lc1: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_10_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_10_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_9_1.lc1: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_9_1.lc1: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_9_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_9_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_8_1.lc1: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_8_1.lc1: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_8_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_8_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_7_1.lc1: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_7_1.lc1: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_7_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_7_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_6_1.lc1: {type: nop, grid_loc: [0, 8], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_6_1.lc1: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_6_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_6_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_5_1.lc1: {type: nop, grid_loc: [0, 9], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_5_1.lc1: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_5_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_5_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_4_1.lc1: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_4_1.lc1: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_4_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_4_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_3_1.lc1: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_3_1.lc1: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_3_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_3_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_2_1.lc1: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_2_1.lc1: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_2_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_2_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_1_1.lc1: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_1_1.lc1: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_1_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_1_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_0_1.lc1: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_0_1.lc1: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_0_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_0_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 128
    add_49: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_add_48_0, e2e_buffer_0_layernorm_37.dc.add.10_add_49_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_50.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_49, lc.input_tensor.layernorm_50.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_49_layernorm_50.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_49],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_50.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [buffer_0_add_49_layernorm_50.dc.subtract.1, layernorm_50.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_50.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_50.dc.subtract.1, layernorm_50.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_50.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_50.dc.multiply.2, lc.input_tensor.layernorm_50.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_50.dc.add.5: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_50.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_50.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_50.dc.sqrt.6: {type: sqrt, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_50.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_50.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_50.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_50.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_50.dc.reciprocal.7, lc.input_tensor.layernorm_50.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_50.dc.subtract.1_layernorm_50.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_50.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_50.dc.subtract.1_layernorm_50.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_50.dc.subtract.1_layernorm_50.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_50.dc.multiply.8: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_50.dc.subtract.1_layernorm_50.dc.multiply.8, layernorm_50.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_50.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_50.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_50.dc.add.10: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_50.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_53: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_50.dc.add.10, layer.1.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_55: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_53, layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_59: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_50.dc.add.10, layer.1.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_61: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_59, layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_65: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_55, add_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_68: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_65, constant_1_multiply_68],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_69: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [multiply_68, e2e_attention_mask_s_brcst_m2_10_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_70.dc.exp.0: {type: exp, grid_loc: [2, 2], grid_size: [1, 4], inputs: [add_69],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_70.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_70.dc.exp.0, lc.input_tensor.softmax_70.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_70.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_70.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_70.dc.exp.0_softmax_70.dc.multiply.3: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_70.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_70.dc.multiply.3: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_70.dc.exp.0_softmax_70.dc.multiply.3, softmax_70.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_50.dc.add.10_matmul_73: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_50.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_50.dc.add.10_matmul_73: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_50.dc.add.10_matmul_73],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_73: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_50.dc.add.10_matmul_73, layer.1.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_75: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_73, layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_80: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_70.dc.multiply.3, add_75],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_84: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_80, layer.1.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_86: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_84, layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_50.dc.add.10_add_87: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_50.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_50.dc.add.10_add_87: {type: nop, grid_loc: [3, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_50.dc.add.10_add_87],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_87: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [add_86, buffer_0_layernorm_50.dc.add.10_add_87],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_88.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_87, lc.input_tensor.layernorm_88.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_87_layernorm_88.dc.subtract.1: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_87],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_88.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_add_87_layernorm_88.dc.subtract.1, layernorm_88.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_88.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_88.dc.subtract.1, layernorm_88.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_88.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_88.dc.multiply.2, lc.input_tensor.layernorm_88.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_88.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_88.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_88.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_88.dc.sqrt.6: {type: sqrt, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_88.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_88.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_88.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_88.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_88.dc.reciprocal.7, lc.input_tensor.layernorm_88.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_88.dc.subtract.1_layernorm_88.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_88.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_88.dc.subtract.1_layernorm_88.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_88.dc.subtract.1_layernorm_88.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_88.dc.multiply.8: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_88.dc.subtract.1_layernorm_88.dc.multiply.8, layernorm_88.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_88.dc.multiply.9: {type: multiply, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_88.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_88.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_88.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_91: {type: matmul, grid_loc: [5, 2], grid_size: [1, 8], inputs: [layernorm_88.dc.add.10, layer.1.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_93: {type: add, grid_loc: [6, 0], grid_size: [1, 2], inputs: [matmul_91, layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_94: {type: gelu, grid_loc: [6, 2], grid_size: [1, 6], inputs: [add_93],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_97: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_94, layer.1.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.1.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0, layer.1.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_99: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [matmul_97, layer.1.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_88.dc.add.10_add_100: {type: nop, grid_loc: [7, 10], grid_size: [1, 1], inputs: [layernorm_88.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_88.dc.add.10_add_100: {type: nop, grid_loc: [7, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_88.dc.add.10_add_100],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_100: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_99, buffer_0_layernorm_88.dc.add.10_add_100],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_101.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_100, lc.input_tensor.layernorm_101.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_100_layernorm_101.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_100],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_101.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [buffer_0_add_100_layernorm_101.dc.subtract.1, layernorm_101.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_101.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_101.dc.subtract.1, layernorm_101.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_101.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_101.dc.multiply.2, lc.input_tensor.layernorm_101.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_101.dc.add.5: {type: add, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_101.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_101.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_101.dc.sqrt.6: {type: sqrt, grid_loc: [8, 9], grid_size: [1, 1], inputs: [layernorm_101.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_101.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_101.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_101.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_101.dc.reciprocal.7, lc.input_tensor.layernorm_101.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_101.dc.subtract.1_layernorm_101.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_101.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_101.dc.subtract.1_layernorm_101.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_101.dc.subtract.1_layernorm_101.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_101.dc.multiply.8: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_101.dc.subtract.1_layernorm_101.dc.multiply.8, layernorm_101.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_101.dc.multiply.9: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_101.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_101.dc.add.10: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_101.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_104: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_101.dc.add.10, layer.2.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0, layer.2.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_106: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [matmul_104, layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_110: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_101.dc.add.10, layer.2.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0, layer.2.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_112: {type: add, grid_loc: [9, 10], grid_size: [1, 1], inputs: [matmul_110, layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_116: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [add_106, add_112],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_2:
    target_device: 0
    input_count: 128
    multiply_119: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_116_0, constant_1_multiply_119],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_120: {type: add, grid_loc: [0, 1], grid_size: [1, 1], inputs: [multiply_119, e2e_attention_mask_s_brcst_m2_9_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_121.dc.exp.0: {type: exp, grid_loc: [0, 2], grid_size: [1, 4], inputs: [add_120],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_121.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_121.dc.exp.0, lc.input_tensor.softmax_121.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_121.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 8], grid_size: [1, 1], inputs: [softmax_121.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_121.dc.exp.0_softmax_121.dc.multiply.3: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [softmax_121.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_121.dc.multiply.3: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_121.dc.exp.0_softmax_121.dc.multiply.3, softmax_121.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_101.dc.add.10_matmul_124: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e_layernorm_101.dc.add.10_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_101.dc.add.10_matmul_124: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_101.dc.add.10_matmul_124],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_124: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_101.dc.add.10_matmul_124, layer.2.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0, layer.2.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_126: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_124, layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_131: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_121.dc.multiply.3, add_126],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_135: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_131, layer.2.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.2.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_137: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_135, layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_101.dc.add.10_add_138: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [e2e_layernorm_101.dc.add.10_1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_101.dc.add.10_add_138: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_101.dc.add.10_add_138],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_138: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_137, buffer_0_layernorm_101.dc.add.10_add_138],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_139.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_138, lc.input_tensor.layernorm_139.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_138_layernorm_139.dc.subtract.1: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_138],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_139.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [1, 1], inputs: [buffer_0_add_138_layernorm_139.dc.subtract.1, layernorm_139.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_139.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_139.dc.subtract.1, layernorm_139.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_139.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.2, lc.input_tensor.layernorm_139.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_139.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_139.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_139.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_139.dc.sqrt.6: {type: sqrt, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_139.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_139.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_139.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_139.dc.reciprocal.7, lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_139.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_139.dc.multiply.8: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8, layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_139.dc.multiply.9: {type: multiply, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_139.dc.add.10: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_142: {type: matmul, grid_loc: [3, 2], grid_size: [1, 8], inputs: [layernorm_139.dc.add.10, layer.2.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_144: {type: add, grid_loc: [4, 0], grid_size: [1, 2], inputs: [matmul_142, layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_145: {type: gelu, grid_loc: [4, 2], grid_size: [1, 6], inputs: [add_144],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_148: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_145, layer.2.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.2.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0, layer.2.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_150: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [matmul_148, layer.2.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_139.dc.add.10_add_151: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_139.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_139.dc.add.10_add_151: {type: nop, grid_loc: [5, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_139.dc.add.10_add_151],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_151: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_150, buffer_0_layernorm_139.dc.add.10_add_151],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_152.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_151, lc.input_tensor.layernorm_152.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_151_layernorm_152.dc.subtract.1: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_151],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_152.dc.subtract.1: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [buffer_0_add_151_layernorm_152.dc.subtract.1, layernorm_152.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_152.dc.multiply.2: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_152.dc.subtract.1, layernorm_152.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_152.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_152.dc.multiply.2, lc.input_tensor.layernorm_152.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_152.dc.add.5: {type: add, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_152.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_152.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_152.dc.sqrt.6: {type: sqrt, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_152.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_152.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_152.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_152.dc.reciprocal.7, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_152.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_152.dc.multiply.8: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8, layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_152.dc.multiply.9: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_152.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_152.dc.add.10: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_152.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_155: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_152.dc.add.10, layer.3.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0, layer.3.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_157: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_155, layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_161: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [layernorm_152.dc.add.10, layer.3.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0, layer.3.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_163: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [matmul_161, layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_167: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_157, add_163],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_170: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_167, constant_1_multiply_170],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_171: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [multiply_170, e2e_attention_mask_s_brcst_m2_8_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_172.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [1, 4], inputs: [add_171],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_172.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_172.dc.exp.0, lc.input_tensor.softmax_172.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_172.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 8], grid_size: [1, 1], inputs: [softmax_172.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_172.dc.exp.0_softmax_172.dc.multiply.3: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_172.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_172.dc.multiply.3: {type: multiply, grid_loc: [8, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_172.dc.exp.0_softmax_172.dc.multiply.3, softmax_172.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_152.dc.add.10_matmul_175: {type: nop, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_152.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_152.dc.add.10_matmul_175: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_152.dc.add.10_matmul_175],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_175: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_152.dc.add.10_matmul_175, layer.3.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0, layer.3.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_177: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_175, layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_182: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_172.dc.multiply.3, add_177],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_186: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [matmul_182, layer.3.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.3.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_188: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [matmul_186, layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_152.dc.add.10_add_189: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_152.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_152.dc.add.10_add_189: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_152.dc.add.10_add_189],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_189: {type: add, grid_loc: [9, 9], grid_size: [1, 1], inputs: [add_188, buffer_0_layernorm_152.dc.add.10_add_189],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_190.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [add_189, lc.input_tensor.layernorm_190.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_189_layernorm_190.dc.subtract.1: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [add_189],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}

  fwd_3:
    target_device: 0
    input_count: 128
    layernorm_190.dc.subtract.1: {type: subtract, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_buffer_0_add_189_layernorm_190.dc.subtract.1_0, e2e_layernorm_190.dc.reduce_avg.0.lc1_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_190.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_190.dc.subtract.1, layernorm_190.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_190.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_190.dc.multiply.2, lc.input_tensor.layernorm_190.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_190.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_190.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_190.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_190.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_190.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_190.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_190.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_190.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_190.dc.reciprocal.7, lc.input_tensor.layernorm_190.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_190.dc.subtract.1_layernorm_190.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_190.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_190.dc.subtract.1_layernorm_190.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_190.dc.subtract.1_layernorm_190.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_190.dc.multiply.8: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_190.dc.subtract.1_layernorm_190.dc.multiply.8, layernorm_190.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_190.dc.multiply.9: {type: multiply, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_190.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_190.dc.add.10: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_190.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_193: {type: matmul, grid_loc: [1, 2], grid_size: [1, 8], inputs: [layernorm_190.dc.add.10, layer.3.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_195: {type: add, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_193, layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_196: {type: gelu, grid_loc: [2, 2], grid_size: [1, 6], inputs: [add_195],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_199: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_196, layer.3.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.3.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0, layer.3.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_201: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [matmul_199, layer.3.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_190.dc.add.10_add_202: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [layernorm_190.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_190.dc.add.10_add_202: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_190.dc.add.10_add_202],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_202: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_201, buffer_0_layernorm_190.dc.add.10_add_202],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_203.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_202, lc.input_tensor.layernorm_203.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_202_layernorm_203.dc.subtract.1: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_202],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_203.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_0_add_202_layernorm_203.dc.subtract.1, layernorm_203.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_203.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_203.dc.subtract.1, layernorm_203.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_203.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_203.dc.multiply.2, lc.input_tensor.layernorm_203.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_203.dc.add.5: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_203.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_203.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_203.dc.sqrt.6: {type: sqrt, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_203.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_203.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_203.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_203.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_203.dc.reciprocal.7, lc.input_tensor.layernorm_203.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_203.dc.subtract.1_layernorm_203.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_203.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_203.dc.subtract.1_layernorm_203.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_203.dc.subtract.1_layernorm_203.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_203.dc.multiply.8: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_203.dc.subtract.1_layernorm_203.dc.multiply.8, layernorm_203.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_203.dc.multiply.9: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_203.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_203.dc.add.10: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_203.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_206: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_203.dc.add.10, layer.4.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.4.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0, layer.4.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_208: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_206, layer.4.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_212: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_203.dc.add.10, layer.4.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.4.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0, layer.4.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_214: {type: add, grid_loc: [5, 10], grid_size: [1, 1], inputs: [matmul_212, layer.4.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_218: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_208, add_214],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_221: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_218, constant_1_multiply_221],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_222: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_221, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_223.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [1, 4], inputs: [add_222],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_223.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_223.dc.exp.0, lc.input_tensor.softmax_223.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_223.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 8], grid_size: [1, 1], inputs: [softmax_223.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_223.dc.exp.0_softmax_223.dc.multiply.3: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_223.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_223.dc.multiply.3: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_223.dc.exp.0_softmax_223.dc.multiply.3, softmax_223.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_203.dc.add.10_matmul_226: {type: nop, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_203.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_203.dc.add.10_matmul_226: {type: nop, grid_loc: [6, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_203.dc.add.10_matmul_226],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_226: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_203.dc.add.10_matmul_226, layer.4.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.4.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0, layer.4.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_228: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [matmul_226, layer.4.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_233: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_223.dc.multiply.3, add_228],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_237: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_233, layer.4.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.4.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.4.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_239: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_237, layer.4.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_203.dc.add.10_add_240: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_203.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_203.dc.add.10_add_240: {type: nop, grid_loc: [7, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_203.dc.add.10_add_240],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_240: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [add_239, buffer_0_layernorm_203.dc.add.10_add_240],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_241.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_240, lc.input_tensor.layernorm_241.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_240_layernorm_241.dc.subtract.1: {type: nop, grid_loc: [7, 10], grid_size: [1, 1], inputs: [add_240],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_241.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_0_add_240_layernorm_241.dc.subtract.1, layernorm_241.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_241.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_241.dc.subtract.1, layernorm_241.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_241.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_241.dc.multiply.2, lc.input_tensor.layernorm_241.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_241.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_241.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_241.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_241.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_241.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_241.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_241.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_241.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_241.dc.reciprocal.7, lc.input_tensor.layernorm_241.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_241.dc.subtract.1_layernorm_241.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_241.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_241.dc.subtract.1_layernorm_241.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_241.dc.subtract.1_layernorm_241.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_241.dc.multiply.8: {type: multiply, grid_loc: [8, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_241.dc.subtract.1_layernorm_241.dc.multiply.8, layernorm_241.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_241.dc.multiply.9: {type: multiply, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_241.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_241.dc.add.10: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_241.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_244: {type: matmul, grid_loc: [9, 2], grid_size: [1, 8], inputs: [layernorm_241.dc.add.10, layer.4.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_4:
    target_device: 0
    input_count: 128
    add_246: {type: add, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_244_0, e2e_layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_247: {type: gelu, grid_loc: [0, 2], grid_size: [1, 6], inputs: [add_246],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_250: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [gelu_247, layer.4.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.4.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0, layer.4.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_252: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_250, layer.4.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_241.dc.add.10_add_253: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e_layernorm_241.dc.add.10_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_241.dc.add.10_add_253: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_241.dc.add.10_add_253],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_253: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_252, buffer_0_layernorm_241.dc.add.10_add_253],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_254.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_253, lc.input_tensor.layernorm_254.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_253_layernorm_254.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_253],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_254.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0_add_253_layernorm_254.dc.subtract.1, layernorm_254.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_254.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_254.dc.subtract.1, layernorm_254.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_254.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_254.dc.multiply.2, lc.input_tensor.layernorm_254.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_254.dc.add.5: {type: add, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_254.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_254.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_254.dc.sqrt.6: {type: sqrt, grid_loc: [2, 9], grid_size: [1, 1], inputs: [layernorm_254.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_254.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_254.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_254.dc.reciprocal.7, lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_254.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_254.dc.multiply.8: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8, layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_254.dc.multiply.9: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_254.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_254.dc.add.10: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_254.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_257: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_254.dc.add.10, layer.5.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.5.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0, layer.5.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_259: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_257, layer.5.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_263: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [layernorm_254.dc.add.10, layer.5.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.5.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0, layer.5.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_265: {type: add, grid_loc: [3, 10], grid_size: [1, 1], inputs: [matmul_263, layer.5.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_269: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_259, add_265],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_272: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_269, constant_1_multiply_272],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_273: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [multiply_272, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_274.dc.exp.0: {type: exp, grid_loc: [4, 2], grid_size: [1, 4], inputs: [add_273],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_274.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [softmax_274.dc.exp.0, lc.input_tensor.softmax_274.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_274.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 8], grid_size: [1, 1], inputs: [softmax_274.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_274.dc.exp.0_softmax_274.dc.multiply.3: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [softmax_274.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_274.dc.multiply.3: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_274.dc.exp.0_softmax_274.dc.multiply.3, softmax_274.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_254.dc.add.10_matmul_277: {type: nop, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_254.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_254.dc.add.10_matmul_277: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_254.dc.add.10_matmul_277],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_277: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_254.dc.add.10_matmul_277, layer.5.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.5.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0, layer.5.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_279: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_277, layer.5.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_284: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [softmax_274.dc.multiply.3, add_279],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_288: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_284, layer.5.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.5.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.5.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_290: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_288, layer.5.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_254.dc.add.10_add_291: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_254.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_254.dc.add.10_add_291: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_254.dc.add.10_add_291],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_291: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [add_290, buffer_0_layernorm_254.dc.add.10_add_291],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_292.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_291, lc.input_tensor.layernorm_292.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_291_layernorm_292.dc.subtract.1: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [add_291],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_292.dc.subtract.1: {type: subtract, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_0_add_291_layernorm_292.dc.subtract.1, layernorm_292.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_292.dc.multiply.2: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_292.dc.subtract.1, layernorm_292.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_292.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_292.dc.multiply.2, lc.input_tensor.layernorm_292.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_292.dc.add.5: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_292.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_292.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_292.dc.sqrt.6: {type: sqrt, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_292.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_292.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_292.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_292.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_292.dc.reciprocal.7, lc.input_tensor.layernorm_292.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_292.dc.subtract.1_layernorm_292.dc.multiply.8: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_292.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_292.dc.subtract.1_layernorm_292.dc.multiply.8: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_292.dc.subtract.1_layernorm_292.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_292.dc.multiply.8: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_292.dc.subtract.1_layernorm_292.dc.multiply.8, layernorm_292.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_292.dc.multiply.9: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_292.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_292.dc.add.10: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_292.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_295: {type: matmul, grid_loc: [7, 2], grid_size: [1, 8], inputs: [layernorm_292.dc.add.10, layer.5.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.5.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_297: {type: add, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_295, layer.5.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_298: {type: gelu, grid_loc: [8, 2], grid_size: [1, 6], inputs: [add_297],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_301: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_298, layer.5.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.5.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0, layer.5.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_303: {type: add, grid_loc: [9, 9], grid_size: [1, 1], inputs: [matmul_301, layer.5.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_292.dc.add.10_add_304: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_292.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_292.dc.add.10_add_304: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_292.dc.add.10_add_304],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}

  fwd_5:
    target_device: 0
    input_count: 128
    add_304: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_add_303_0, e2e_buffer_0_layernorm_292.dc.add.10_add_304_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_305.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_304, lc.input_tensor.layernorm_305.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_304_layernorm_305.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_304],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_305.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [buffer_0_add_304_layernorm_305.dc.subtract.1, layernorm_305.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_305.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_305.dc.subtract.1, layernorm_305.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_305.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_305.dc.multiply.2, lc.input_tensor.layernorm_305.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_305.dc.add.5: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_305.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_305.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_305.dc.sqrt.6: {type: sqrt, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_305.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_305.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_305.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_305.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_305.dc.reciprocal.7, lc.input_tensor.layernorm_305.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_305.dc.subtract.1_layernorm_305.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_305.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_305.dc.subtract.1_layernorm_305.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_305.dc.subtract.1_layernorm_305.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_305.dc.multiply.8: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_305.dc.subtract.1_layernorm_305.dc.multiply.8, layernorm_305.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_305.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_305.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_305.dc.add.10: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_305.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_308: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_305.dc.add.10, layer.6.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.6.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0, layer.6.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_310: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_308, layer.6.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_314: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_305.dc.add.10, layer.6.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.6.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0, layer.6.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_316: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_314, layer.6.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_320: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_310, add_316],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_323: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_320, constant_1_multiply_323],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_324: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [multiply_323, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_325.dc.exp.0: {type: exp, grid_loc: [2, 2], grid_size: [1, 4], inputs: [add_324],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_325.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_325.dc.exp.0, lc.input_tensor.softmax_325.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_325.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_325.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_325.dc.exp.0_softmax_325.dc.multiply.3: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_325.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_325.dc.multiply.3: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_325.dc.exp.0_softmax_325.dc.multiply.3, softmax_325.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_305.dc.add.10_matmul_328: {type: nop, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_305.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_305.dc.add.10_matmul_328: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_305.dc.add.10_matmul_328],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_328: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_305.dc.add.10_matmul_328, layer.6.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.6.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0, layer.6.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_330: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_328, layer.6.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_335: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_325.dc.multiply.3, add_330],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_339: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_335, layer.6.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.6.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.6.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_341: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_339, layer.6.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_305.dc.add.10_add_342: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_305.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_305.dc.add.10_add_342: {type: nop, grid_loc: [3, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_305.dc.add.10_add_342],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_342: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [add_341, buffer_0_layernorm_305.dc.add.10_add_342],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_343.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_342, lc.input_tensor.layernorm_343.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_342_layernorm_343.dc.subtract.1: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_342],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_343.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_add_342_layernorm_343.dc.subtract.1, layernorm_343.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_343.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_343.dc.subtract.1, layernorm_343.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_343.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_343.dc.multiply.2, lc.input_tensor.layernorm_343.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_343.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_343.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_343.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_343.dc.sqrt.6: {type: sqrt, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_343.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_343.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_343.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_343.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_343.dc.reciprocal.7, lc.input_tensor.layernorm_343.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_343.dc.subtract.1_layernorm_343.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_343.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_343.dc.subtract.1_layernorm_343.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_343.dc.subtract.1_layernorm_343.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_343.dc.multiply.8: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_343.dc.subtract.1_layernorm_343.dc.multiply.8, layernorm_343.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_343.dc.multiply.9: {type: multiply, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_343.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_343.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_343.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_346: {type: matmul, grid_loc: [5, 2], grid_size: [1, 8], inputs: [layernorm_343.dc.add.10, layer.6.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.6.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_348: {type: add, grid_loc: [6, 0], grid_size: [1, 2], inputs: [matmul_346, layer.6.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_349: {type: gelu, grid_loc: [6, 2], grid_size: [1, 6], inputs: [add_348],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_352: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_349, layer.6.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.6.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0, layer.6.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_354: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [matmul_352, layer.6.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_343.dc.add.10_add_355: {type: nop, grid_loc: [7, 10], grid_size: [1, 1], inputs: [layernorm_343.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_343.dc.add.10_add_355: {type: nop, grid_loc: [7, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_343.dc.add.10_add_355],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_355: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_354, buffer_0_layernorm_343.dc.add.10_add_355],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_355_layernorm_356.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_355],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [buffer_0_add_355_layernorm_356.dc.subtract.1, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_356.dc.add.5: {type: add, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.sqrt.6: {type: sqrt, grid_loc: [8, 9], grid_size: [1, 1], inputs: [layernorm_356.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_356.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_356.dc.reciprocal.7, lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_356.dc.subtract.1_layernorm_356.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_356.dc.subtract.1_layernorm_356.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_356.dc.subtract.1_layernorm_356.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_356.dc.multiply.8: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_356.dc.subtract.1_layernorm_356.dc.multiply.8, layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_356.dc.multiply.9: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_356.dc.add.10: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_359: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_356.dc.add.10, layer.7.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.7.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0, layer.7.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_361: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [matmul_359, layer.7.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_365: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_356.dc.add.10, layer.7.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.7.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0, layer.7.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_367: {type: add, grid_loc: [9, 10], grid_size: [1, 1], inputs: [matmul_365, layer.7.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_371: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [add_361, add_367],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_6:
    target_device: 0
    input_count: 128
    multiply_374: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_371_0, constant_1_multiply_374],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_375: {type: add, grid_loc: [0, 1], grid_size: [1, 1], inputs: [multiply_374, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_376.dc.exp.0: {type: exp, grid_loc: [0, 2], grid_size: [1, 4], inputs: [add_375],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_376.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_376.dc.exp.0, lc.input_tensor.softmax_376.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_376.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 8], grid_size: [1, 1], inputs: [softmax_376.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_376.dc.exp.0_softmax_376.dc.multiply.3: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [softmax_376.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_376.dc.multiply.3: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_376.dc.exp.0_softmax_376.dc.multiply.3, softmax_376.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_356.dc.add.10_matmul_379: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [e2e_layernorm_356.dc.add.10_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_356.dc.add.10_matmul_379: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_356.dc.add.10_matmul_379],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_379: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_356.dc.add.10_matmul_379, layer.7.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.7.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0, layer.7.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_381: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_379, layer.7.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_386: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_376.dc.multiply.3, add_381],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_390: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_386, layer.7.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.7.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.7.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_392: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_390, layer.7.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_356.dc.add.10_add_393: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [e2e_layernorm_356.dc.add.10_1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_356.dc.add.10_add_393: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_356.dc.add.10_add_393],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_393: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_392, buffer_0_layernorm_356.dc.add.10_add_393],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_394.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_393, lc.input_tensor.layernorm_394.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_393_layernorm_394.dc.subtract.1: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_393],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_394.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [1, 1], inputs: [buffer_0_add_393_layernorm_394.dc.subtract.1, layernorm_394.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_394.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_394.dc.subtract.1, layernorm_394.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_394.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_394.dc.multiply.2, lc.input_tensor.layernorm_394.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_394.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_394.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_394.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_394.dc.sqrt.6: {type: sqrt, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_394.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_394.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_394.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_394.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_394.dc.reciprocal.7, lc.input_tensor.layernorm_394.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_394.dc.subtract.1_layernorm_394.dc.multiply.8: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_394.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_394.dc.subtract.1_layernorm_394.dc.multiply.8: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_394.dc.subtract.1_layernorm_394.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_394.dc.multiply.8: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_394.dc.subtract.1_layernorm_394.dc.multiply.8, layernorm_394.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_394.dc.multiply.9: {type: multiply, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_394.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_394.dc.add.10: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_394.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_397: {type: matmul, grid_loc: [3, 2], grid_size: [1, 8], inputs: [layernorm_394.dc.add.10, layer.7.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.7.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_399: {type: add, grid_loc: [4, 0], grid_size: [1, 2], inputs: [matmul_397, layer.7.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_400: {type: gelu, grid_loc: [4, 2], grid_size: [1, 6], inputs: [add_399],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_403: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_400, layer.7.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.7.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0, layer.7.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_405: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [matmul_403, layer.7.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_394.dc.add.10_add_406: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_394.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_394.dc.add.10_add_406: {type: nop, grid_loc: [5, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_394.dc.add.10_add_406],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_406: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [add_405, buffer_0_layernorm_394.dc.add.10_add_406],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_407.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_406, lc.input_tensor.layernorm_407.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_406_layernorm_407.dc.subtract.1: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_406],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_407.dc.subtract.1: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [buffer_0_add_406_layernorm_407.dc.subtract.1, layernorm_407.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_407.dc.multiply.2: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_407.dc.subtract.1, layernorm_407.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_407.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_407.dc.multiply.2, lc.input_tensor.layernorm_407.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_407.dc.add.5: {type: add, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_407.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_407.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_407.dc.sqrt.6: {type: sqrt, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_407.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_407.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_407.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_407.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_407.dc.reciprocal.7, lc.input_tensor.layernorm_407.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_407.dc.subtract.1_layernorm_407.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_407.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_407.dc.subtract.1_layernorm_407.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_407.dc.subtract.1_layernorm_407.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_407.dc.multiply.8: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_407.dc.subtract.1_layernorm_407.dc.multiply.8, layernorm_407.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_407.dc.multiply.9: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_407.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_407.dc.add.10: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_407.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_410: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_407.dc.add.10, layer.8.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.8.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0, layer.8.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_412: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_410, layer.8.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_416: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [layernorm_407.dc.add.10, layer.8.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.8.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0, layer.8.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_418: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [matmul_416, layer.8.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_422: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_412, add_418],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_425: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_422, constant_1_multiply_425],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_426: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [multiply_425, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_427.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [1, 4], inputs: [add_426],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_427.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_427.dc.exp.0, lc.input_tensor.softmax_427.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_427.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 8], grid_size: [1, 1], inputs: [softmax_427.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_427.dc.exp.0_softmax_427.dc.multiply.3: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_427.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_427.dc.multiply.3: {type: multiply, grid_loc: [8, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_427.dc.exp.0_softmax_427.dc.multiply.3, softmax_427.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_407.dc.add.10_matmul_430: {type: nop, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_407.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_407.dc.add.10_matmul_430: {type: nop, grid_loc: [8, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_407.dc.add.10_matmul_430],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_430: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_407.dc.add.10_matmul_430, layer.8.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.8.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0, layer.8.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_432: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_430, layer.8.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_437: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_427.dc.multiply.3, add_432],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_441: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [matmul_437, layer.8.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.8.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.8.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_443: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [matmul_441, layer.8.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_407.dc.add.10_add_444: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_407.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_407.dc.add.10_add_444: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_407.dc.add.10_add_444],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_444: {type: add, grid_loc: [9, 9], grid_size: [1, 1], inputs: [add_443, buffer_0_layernorm_407.dc.add.10_add_444],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_445.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [add_444, lc.input_tensor.layernorm_445.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_444_layernorm_445.dc.subtract.1: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [add_444],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}

  fwd_7:
    target_device: 0
    input_count: 128
    layernorm_445.dc.subtract.1: {type: subtract, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_buffer_0_add_444_layernorm_445.dc.subtract.1_0, e2e_layernorm_445.dc.reduce_avg.0.lc1_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_445.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_445.dc.subtract.1, layernorm_445.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_445.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_445.dc.multiply.2, lc.input_tensor.layernorm_445.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_445.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_445.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_445.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_445.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_445.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_445.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_445.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_445.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_445.dc.reciprocal.7, lc.input_tensor.layernorm_445.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_445.dc.subtract.1_layernorm_445.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_445.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_445.dc.subtract.1_layernorm_445.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_445.dc.subtract.1_layernorm_445.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_445.dc.multiply.8: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_445.dc.subtract.1_layernorm_445.dc.multiply.8, layernorm_445.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_445.dc.multiply.9: {type: multiply, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_445.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_445.dc.add.10: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_445.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_448: {type: matmul, grid_loc: [1, 2], grid_size: [1, 8], inputs: [layernorm_445.dc.add.10, layer.8.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.8.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_450: {type: add, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_448, layer.8.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_451: {type: gelu, grid_loc: [2, 2], grid_size: [1, 6], inputs: [add_450],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_454: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_451, layer.8.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.8.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0, layer.8.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_456: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [matmul_454, layer.8.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_445.dc.add.10_add_457: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [layernorm_445.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_445.dc.add.10_add_457: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_445.dc.add.10_add_457],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_457: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_456, buffer_0_layernorm_445.dc.add.10_add_457],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_458.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_457, lc.input_tensor.layernorm_458.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_457_layernorm_458.dc.subtract.1: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_457],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_458.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_0_add_457_layernorm_458.dc.subtract.1, layernorm_458.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_458.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_458.dc.subtract.1, layernorm_458.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_458.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_458.dc.multiply.2, lc.input_tensor.layernorm_458.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_458.dc.add.5: {type: add, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_458.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_458.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_458.dc.sqrt.6: {type: sqrt, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_458.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_458.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_458.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_458.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_458.dc.reciprocal.7, lc.input_tensor.layernorm_458.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_458.dc.subtract.1_layernorm_458.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_458.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_458.dc.subtract.1_layernorm_458.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_458.dc.subtract.1_layernorm_458.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_458.dc.multiply.8: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_458.dc.subtract.1_layernorm_458.dc.multiply.8, layernorm_458.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_458.dc.multiply.9: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_458.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_458.dc.add.10: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_458.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_461: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_458.dc.add.10, layer.9.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.9.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0, layer.9.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_463: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_461, layer.9.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_467: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_458.dc.add.10, layer.9.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.9.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0, layer.9.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_469: {type: add, grid_loc: [5, 10], grid_size: [1, 1], inputs: [matmul_467, layer.9.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_473: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_463, add_469],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_476: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_473, constant_1_multiply_476],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_477: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_476, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_478.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [1, 4], inputs: [add_477],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_478.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_478.dc.exp.0, lc.input_tensor.softmax_478.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_478.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 8], grid_size: [1, 1], inputs: [softmax_478.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_478.dc.exp.0_softmax_478.dc.multiply.3: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_478.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_478.dc.multiply.3: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_478.dc.exp.0_softmax_478.dc.multiply.3, softmax_478.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_458.dc.add.10_matmul_481: {type: nop, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_458.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_458.dc.add.10_matmul_481: {type: nop, grid_loc: [6, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_458.dc.add.10_matmul_481],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_481: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_458.dc.add.10_matmul_481, layer.9.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.9.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0, layer.9.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_483: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [matmul_481, layer.9.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_488: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_478.dc.multiply.3, add_483],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_492: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_488, layer.9.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.9.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.9.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_494: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_492, layer.9.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_458.dc.add.10_add_495: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_458.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_458.dc.add.10_add_495: {type: nop, grid_loc: [7, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_458.dc.add.10_add_495],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_495: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [add_494, buffer_0_layernorm_458.dc.add.10_add_495],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_496.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_495, lc.input_tensor.layernorm_496.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_495_layernorm_496.dc.subtract.1: {type: nop, grid_loc: [7, 10], grid_size: [1, 1], inputs: [add_495],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_496.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_0_add_495_layernorm_496.dc.subtract.1, layernorm_496.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_496.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_496.dc.subtract.1, layernorm_496.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_496.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_496.dc.multiply.2, lc.input_tensor.layernorm_496.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_496.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_496.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_496.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_496.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_496.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_496.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_496.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_496.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_496.dc.reciprocal.7, lc.input_tensor.layernorm_496.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_496.dc.subtract.1_layernorm_496.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_496.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_496.dc.subtract.1_layernorm_496.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_496.dc.subtract.1_layernorm_496.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_496.dc.multiply.8: {type: multiply, grid_loc: [8, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_496.dc.subtract.1_layernorm_496.dc.multiply.8, layernorm_496.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_496.dc.multiply.9: {type: multiply, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_496.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_496.dc.add.10: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_496.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_499: {type: matmul, grid_loc: [9, 2], grid_size: [1, 8], inputs: [layernorm_496.dc.add.10, layer.9.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_8:
    target_device: 0
    input_count: 128
    add_501: {type: add, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_499_0, e2e_layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_502: {type: gelu, grid_loc: [0, 2], grid_size: [1, 6], inputs: [add_501],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_505: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [gelu_502, layer.9.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.9.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0, layer.9.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_507: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_505, layer.9.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_496.dc.add.10_add_508: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [e2e_layernorm_496.dc.add.10_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_496.dc.add.10_add_508: {type: nop, grid_loc: [1, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_496.dc.add.10_add_508],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_508: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_507, buffer_0_layernorm_496.dc.add.10_add_508],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_509.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_508, lc.input_tensor.layernorm_509.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_508_layernorm_509.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_508],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_509.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0_add_508_layernorm_509.dc.subtract.1, layernorm_509.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_509.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_509.dc.subtract.1, layernorm_509.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_509.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_509.dc.multiply.2, lc.input_tensor.layernorm_509.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_509.dc.add.5: {type: add, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_509.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_509.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_509.dc.sqrt.6: {type: sqrt, grid_loc: [2, 9], grid_size: [1, 1], inputs: [layernorm_509.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_509.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_509.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_509.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_509.dc.reciprocal.7, lc.input_tensor.layernorm_509.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_509.dc.subtract.1_layernorm_509.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_509.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_509.dc.subtract.1_layernorm_509.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_509.dc.subtract.1_layernorm_509.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_509.dc.multiply.8: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_509.dc.subtract.1_layernorm_509.dc.multiply.8, layernorm_509.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_509.dc.multiply.9: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_509.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_509.dc.add.10: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_509.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_512: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_509.dc.add.10, layer.10.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.10.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0, layer.10.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_514: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_512, layer.10.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_518: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [layernorm_509.dc.add.10, layer.10.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.10.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0, layer.10.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_520: {type: add, grid_loc: [3, 10], grid_size: [1, 1], inputs: [matmul_518, layer.10.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_524: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_514, add_520],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_527: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_524, constant_1_multiply_527],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_528: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [multiply_527, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_529.dc.exp.0: {type: exp, grid_loc: [4, 2], grid_size: [1, 4], inputs: [add_528],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_529.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [softmax_529.dc.exp.0, lc.input_tensor.softmax_529.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_529.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 8], grid_size: [1, 1], inputs: [softmax_529.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_529.dc.exp.0_softmax_529.dc.multiply.3: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [softmax_529.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_529.dc.multiply.3: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0_softmax_529.dc.exp.0_softmax_529.dc.multiply.3, softmax_529.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_509.dc.add.10_matmul_532: {type: nop, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_509.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_509.dc.add.10_matmul_532: {type: nop, grid_loc: [4, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_509.dc.add.10_matmul_532],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_532: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_509.dc.add.10_matmul_532, layer.10.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.10.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0, layer.10.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_534: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_532, layer.10.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_539: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [softmax_529.dc.multiply.3, add_534],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_543: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_539, layer.10.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.10.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.10.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_545: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_543, layer.10.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_509.dc.add.10_add_546: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_509.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_509.dc.add.10_add_546: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_509.dc.add.10_add_546],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_546: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [add_545, buffer_0_layernorm_509.dc.add.10_add_546],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_547.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_546, lc.input_tensor.layernorm_547.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_546_layernorm_547.dc.subtract.1: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [add_546],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_547.dc.subtract.1: {type: subtract, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_0_add_546_layernorm_547.dc.subtract.1, layernorm_547.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_547.dc.multiply.2: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_547.dc.subtract.1, layernorm_547.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_547.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_547.dc.multiply.2, lc.input_tensor.layernorm_547.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_547.dc.add.5: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_547.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_547.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_547.dc.sqrt.6: {type: sqrt, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_547.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_547.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_547.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_547.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_547.dc.reciprocal.7, lc.input_tensor.layernorm_547.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_547.dc.subtract.1_layernorm_547.dc.multiply.8: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_547.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_547.dc.subtract.1_layernorm_547.dc.multiply.8: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_547.dc.subtract.1_layernorm_547.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_547.dc.multiply.8: {type: multiply, grid_loc: [6, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_547.dc.subtract.1_layernorm_547.dc.multiply.8, layernorm_547.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_547.dc.multiply.9: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_547.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_547.dc.add.10: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_547.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_550: {type: matmul, grid_loc: [7, 2], grid_size: [1, 8], inputs: [layernorm_547.dc.add.10, layer.10.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.10.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_552: {type: add, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_550, layer.10.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_553: {type: gelu, grid_loc: [8, 2], grid_size: [1, 6], inputs: [add_552],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_556: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_553, layer.10.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.10.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0, layer.10.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_558: {type: add, grid_loc: [9, 9], grid_size: [1, 1], inputs: [matmul_556, layer.10.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_547.dc.add.10_add_559: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_547.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_547.dc.add.10_add_559: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [buffer_1_layernorm_547.dc.add.10_add_559],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}

  fwd_9:
    target_device: 0
    input_count: 128
    add_559: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_add_558_0, e2e_buffer_0_layernorm_547.dc.add.10_add_559_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_560.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_559, lc.input_tensor.layernorm_560.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_559_layernorm_560.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_559],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_560.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [buffer_0_add_559_layernorm_560.dc.subtract.1, layernorm_560.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_560.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_560.dc.subtract.1, layernorm_560.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_560.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_560.dc.multiply.2, lc.input_tensor.layernorm_560.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_560.dc.add.5: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_560.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_560.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_560.dc.sqrt.6: {type: sqrt, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_560.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_560.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_560.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_560.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_560.dc.reciprocal.7, lc.input_tensor.layernorm_560.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_560.dc.subtract.1_layernorm_560.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_560.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_560.dc.subtract.1_layernorm_560.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_560.dc.subtract.1_layernorm_560.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_560.dc.multiply.8: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_560.dc.subtract.1_layernorm_560.dc.multiply.8, layernorm_560.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_560.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_560.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_560.dc.add.10: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_560.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_563: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_560.dc.add.10, layer.11.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.11.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0, layer.11.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_565: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_563, layer.11.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_569: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_560.dc.add.10, layer.11.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.11.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0, layer.11.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_571: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_569, layer.11.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_575: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_565, add_571],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_578: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_575, constant_1_multiply_578],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_579: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [multiply_578, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_580.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [1, 4], inputs: [add_579],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_580.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_580.dc.exp.0, lc.input_tensor.softmax_580.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_580.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_580.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_580.dc.exp.0_softmax_580.dc.multiply.3: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [softmax_580.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    softmax_580.dc.multiply.3: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [buffer_0_softmax_580.dc.exp.0_softmax_580.dc.multiply.3, softmax_580.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    buffer_1_layernorm_560.dc.add.10_matmul_583: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_560.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_560.dc.add.10_matmul_583: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [buffer_1_layernorm_560.dc.add.10_matmul_583],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    matmul_583: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_560.dc.add.10_matmul_583, layer.11.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 4, u_kt: 6}}
    layer.11.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0, layer.11.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_585: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_583, layer.11.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_590: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_580.dc.multiply.3, add_585],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_594: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_590, layer.11.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 4, u_kt: 6}}
    layer.11.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.11.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_596: {type: add, grid_loc: [3, 8], grid_size: [1, 1], inputs: [matmul_594, layer.11.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_560.dc.add.10_add_597: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_560.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_560.dc.add.10_add_597: {type: nop, grid_loc: [2, 9], grid_size: [1, 1], inputs: [buffer_1_layernorm_560.dc.add.10_add_597],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_597: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [add_596, buffer_0_layernorm_560.dc.add.10_add_597],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_598.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_597, lc.input_tensor.layernorm_598.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_597_layernorm_598.dc.subtract.1: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_597],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_598.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_add_597_layernorm_598.dc.subtract.1, layernorm_598.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_598.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_598.dc.subtract.1, layernorm_598.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_598.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_598.dc.multiply.2, lc.input_tensor.layernorm_598.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_598.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_598.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_598.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_598.dc.sqrt.6: {type: sqrt, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_598.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_598.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_598.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_598.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_598.dc.reciprocal.7, lc.input_tensor.layernorm_598.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_598.dc.subtract.1_layernorm_598.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_598.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_598.dc.subtract.1_layernorm_598.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [buffer_1_layernorm_598.dc.subtract.1_layernorm_598.dc.multiply.8],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_598.dc.multiply.8: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [buffer_0_layernorm_598.dc.subtract.1_layernorm_598.dc.multiply.8, layernorm_598.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_598.dc.multiply.9: {type: multiply, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_598.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_598.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_598.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_601: {type: matmul, grid_loc: [5, 2], grid_size: [1, 8], inputs: [layernorm_598.dc.add.10, layer.11.intermediate.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    layer.11.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_603: {type: add, grid_loc: [6, 0], grid_size: [1, 2], inputs: [matmul_601, layer.11.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_604: {type: gelu, grid_loc: [6, 3], grid_size: [1, 6], inputs: [add_603],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_607: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_604, layer.11.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    layer.11.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0, layer.11.output.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_609: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [matmul_607, layer.11.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_598.dc.add.10_add_610: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_598.dc.add.10],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_598.dc.add.10_add_610: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_598.dc.add.10_add_610],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    add_610: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [add_609, buffer_0_layernorm_598.dc.add.10_add_610],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_611.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_610, lc.input_tensor.layernorm_611.dc.reduce_avg.0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_610_layernorm_611.dc.subtract.1: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_610],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_611.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [buffer_0_add_610_layernorm_611.dc.subtract.1, layernorm_611.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_611.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_611.dc.subtract.1, layernorm_611.dc.subtract.1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_611.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_611.dc.multiply.2, lc.input_tensor.layernorm_611.dc.reduce_avg.3.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_611.dc.add.5: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_611.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_611.4],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_611.dc.sqrt.6: {type: sqrt, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_611.dc.add.5],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_611.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_611.dc.sqrt.6],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [layernorm_611.dc.reciprocal.7, lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_611.dc.subtract.1],
         t: 1, mblock: [1, 12], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8],
         t: 1, mblock: [1, 24], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}
    layernorm_611.dc.multiply.8: {type: multiply, grid_loc: [8, 10], grid_size: [1, 1], inputs: [buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8, layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_611.dc.multiply.9: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_611.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_611.dc.add.10: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_611.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q9: 0, $lptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q7: 0, $gptr_q4: 0}
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_17: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_19.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_37.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_1, queue_settings: {
               e2e_add_48_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_buffer_0_layernorm_37.dc.add.10_add_49_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_s_brcst_m2_10_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.layernorm_50.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_50.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_50.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_50.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_68: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_70.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_88.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_88.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_88.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_88.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_101.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_101.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_101.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_101.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_2, queue_settings: {
               e2e_layernorm_101.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_101.dc.add.10_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_116_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               constant_1_multiply_119: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_121.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_139.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_139.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_139.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_152.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_170: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_172.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_190.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_3, queue_settings: {
               e2e_layernorm_190.dc.reduce_avg.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_add_189_layernorm_190.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.layernorm_190.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_190.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_190.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_203.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_203.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_203.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_203.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_221: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_223.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_241.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_241.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_241.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_241.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_4, queue_settings: {
               e2e_layernorm_241.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_244_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_254.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_254.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_254.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_272: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_274.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_292.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_292.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_292.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_292.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_5, queue_settings: {
               e2e_add_303_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_buffer_0_layernorm_292.dc.add.10_add_304_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_305.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_305.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_305.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_305.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_323: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_325.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_343.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_343.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_343.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_343.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_6, queue_settings: {
               e2e_layernorm_356.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_356.dc.add.10_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_371_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               constant_1_multiply_374: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_376.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_394.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_394.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_394.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_394.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_407.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_407.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_407.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_407.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_425: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_427.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_445.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_7, queue_settings: {
               e2e_layernorm_445.dc.reduce_avg.0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_buffer_0_add_444_layernorm_445.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.layernorm_445.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_445.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_445.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_458.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_458.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_458.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_458.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_476: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_478.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_496.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_496.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_496.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_496.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_8, queue_settings: {
               e2e_layernorm_496.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_499_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_509.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_509.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_509.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_509.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_527: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_529.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_547.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_547.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_547.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_547.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_9, queue_settings: {
               e2e_add_558_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_buffer_0_layernorm_547.dc.add.10_add_559_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               lc.input_tensor.layernorm_560.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_560.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_560.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_560.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_578: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_580.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_598.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_598.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_598.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_598.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_611.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_611.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_611.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    - endloop


test-config:
  test-args:
    microbatch_count: 1
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.75
    check_pcc: 0.97
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.25
  io-config:
    inputs: [hidden_states, attention_mask]
    outputs: [bert_encoders.output_layernorm_611]
