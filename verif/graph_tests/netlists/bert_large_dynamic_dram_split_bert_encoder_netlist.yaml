# git checkout 8a16cde13
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[training-Grayskull-chip1-enc2-base]

devices:
  arch: grayskull

test-config:
  stimulus-config:
    type: Normal
    normal_mean: 0.5
    normal_stddev: 0.1
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoder.output_layernorm_105]
    outputs: [bert_encoder.output_layernorm_105]
  test-args:
    sequence_lenth: 384
    head_size: 16

queues:

  # input
  input_1:                                                                                {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                                         {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300c3020]]}

  # output
  bert_encoder.output_layernorm_105:                                                      {input: _fused_op_19_output_nop_0, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                                    {input: opt_in1_layer.0.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a5660], [6, 0x70762c0], [7, 0x700c0a0], [0, 0x7120ce0], [1, 0x7052a20], [2, 0x706a800], [3, 0x70762e0], [4, 0x705ed20]]}
  layer.0.attention.self.query.bias:                                                      {input: _fused_op_33, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x72dabe0]]}
  layer.0.attention.self.key.weight:                                                      {input: opt_in1_layer.0.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72709c0], [0, 0x738f880], [1, 0x72da3e0], [2, 0x72e56a0], [3, 0x72d9bc0], [4, 0x72c2600], [5, 0x72f4220], [6, 0x72e6f00]]}
  layer.0.attention.self.key.bias:                                                        {input: _fused_op_34, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72952e0]]}
  layer.0.attention.self.value.weight:                                                    {input: opt_in1_layer.0.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7318b40], [6, 0x730b820], [7, 0x72a1600], [0, 0x73b49e0], [1, 0x72fed00], [2, 0x730a800], [3, 0x7316b00], [4, 0x72e7760]]}
  layer.0.attention.self.value.bias:                                                      {input: _fused_op_35, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x733d460]]}
  layer.0.attention.output.dense.weight:                                                  {input: opt_in0_layer.0.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7330140], [7, 0x72c5f20], [0, 0x73d9300], [1, 0x7323620], [2, 0x732f120], [3, 0x733b420], [4, 0x730c080], [5, 0x7349780]]}
  layer.0.attention.output.dense.bias:                                                    {input: _fused_op_36, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7354a60]]}
  layer.0.attention.output.LayerNorm.weight:                                              {input: _fused_op_37, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x736e0a0]]}
  layer.0.attention.output.LayerNorm.bias:                                                {input: _fused_op_38, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7360d80]]}
  layer.0.intermediate.dense.weight:                                                      {input: opt_in0_layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 6], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72f6b60], [0, 0x7416240], [1, 0x7360560], [2, 0x7354280], [3, 0x7361de0], [4, 0x7332a40], [5, 0x737a3c0], [6, 0x736d0a0], [7, 0x7327780], [0, 0x7446e60], [1, 0x7391180], [2, 0x7384ea0], [3, 0x7392a00], [4, 0x7363660], [5, 0x73aafe0], [6, 0x739dcc0], [7, 0x73583a0], [0, 0x7477a80], [1, 0x73c1da0], [2, 0x73b5ac0], [3, 0x73c3620], [4, 0x7394280], [5, 0x73dbc00], [6, 0x73ce8e0]]}
  layer.0.intermediate.dense.bias:                                                        {input: _fused_op_39, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7388fc0]]}
  layer.0.output.dense.weight:                                                            {input: opt_in0_layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [2, 12], t: 1, mblock: [24, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x74a86a0], [1, 0x73f29c0], [2, 0x73e66e0], [3, 0x73f4240], [4, 0x73c4ea0], [5, 0x740c820], [6, 0x73ff500], [7, 0x73b9be0], [0, 0x74d92c0], [1, 0x74235e0], [2, 0x7417300], [3, 0x7424e60], [4, 0x73f5ac0], [5, 0x743d440], [6, 0x7430120], [7, 0x73ea800], [0, 0x7509ee0], [1, 0x7454200], [2, 0x7447f20], [3, 0x7455a80], [4, 0x74266e0], [5, 0x746e060], [6, 0x7460d40], [7, 0x741b420]]}
  layer.0.output.dense.bias:                                                              {input: _fused_op_40, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x753ab00]]}
  layer.0.output.LayerNorm.weight:                                                        {input: _fused_op_41, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x712e0e0]]}
  layer.0.output.LayerNorm.bias:                                                          {input: _fused_op_42, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x70c3ec0]]}
  layer.1.attention.self.query.weight:                                                    {input: opt_in0_layer.1.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x71e45e0], [1, 0x710a840], [2, 0x712e100], [3, 0x712e100], [4, 0x7116b40], [5, 0x715f520], [6, 0x713a400], [7, 0x70d01e0]]}
  layer.1.attention.self.query.bias:                                                      {input: _fused_op_43, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7208f00]]}
  layer.1.attention.self.key.weight:                                                      {input: opt_in0_layer.1.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x712f160], [2, 0x7152a20], [3, 0x7152a20], [4, 0x713b460], [5, 0x7183e40], [6, 0x715ed20], [7, 0x70f4b00], [0, 0x7215220]]}
  layer.1.attention.self.key.bias:                                                        {input: _fused_op_44, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7153a80]]}
  layer.1.attention.self.value.weight:                                                    {input: opt_in0_layer.1.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7183640], [7, 0x7119420], [0, 0x7239b40], [1, 0x715fda0], [2, 0x7177b80], [3, 0x7177b80], [4, 0x71783a0], [5, 0x71a8fa0]]}
  layer.1.attention.self.value.bias:                                                      {input: _fused_op_45, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x71a7f60]]}
  layer.1.attention.output.dense.weight:                                                  {input: opt_in0_layer.1.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7138b60], [6, 0x71097c0], [7, 0x709f5a0], [0, 0x71bfcc0], [1, 0x70e5f20], [2, 0x71097e0], [3, 0x71097e0], [4, 0x70f2220]]}
  layer.1.attention.output.dense.bias:                                                    {input: _fused_op_46, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x725e460]]}
  layer.1.attention.output.LayerNorm.weight:                                              {input: _fused_op_47, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x713fde0]]}
  layer.1.attention.output.LayerNorm.bias:                                                {input: _fused_op_48, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72ea840]]}
  layer.1.intermediate.dense.weight:                                                      {input: opt_in0_layer.1.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 6], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7184f00], [2, 0x71b4ac0], [3, 0x71b4ac0], [4, 0x719d500], [5, 0x71cf960], [6, 0x71b6320], [7, 0x714c100], [0, 0x726afc0], [1, 0x71b5b20], [2, 0x71e56e0], [3, 0x71e56e0], [4, 0x71ce120], [5, 0x7200580], [6, 0x71e6f40], [7, 0x717cd20], [0, 0x729bbe0], [1, 0x71e6740], [2, 0x7216300], [3, 0x7216300], [4, 0x71fed40], [5, 0x72311a0], [6, 0x7217b60], [7, 0x71ad940], [0, 0x72cc800]]}
  layer.1.intermediate.dense.bias:                                                        {input: _fused_op_49, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7217360]]}
  layer.1.output.dense.weight:                                                            {input: opt_in0_layer.1.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [2, 12], t: 1, mblock: [24, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7246f20], [3, 0x7246f20], [4, 0x722f960], [5, 0x7261dc0], [6, 0x7248780], [7, 0x71de560], [0, 0x72fd420], [1, 0x7247f80], [2, 0x7277b40], [3, 0x7277b40], [4, 0x7260580], [5, 0x72929e0], [6, 0x72793a0], [7, 0x720f180], [0, 0x732e040], [1, 0x7278ba0], [2, 0x72a8760], [3, 0x72a8760], [4, 0x72911a0], [5, 0x72c3600], [6, 0x72a9fc0], [7, 0x723fda0], [0, 0x735ec60], [1, 0x72a97c0]]}
  layer.1.output.dense.bias:                                                              {input: _fused_op_50, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x72d9380]]}
  layer.1.output.LayerNorm.weight:                                                        {input: _fused_op_51, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7690020]]}
  layer.1.output.LayerNorm.bias:                                                          {input: _fused_op_52, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75aafa0]]}

  # constant
  input_1_multiply_16_fork_clone713:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x73b41a0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7309fc0]]}
  dc.input_tensor.softmax_18.4:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x72fe4e0]]}
  lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x72e6f20]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x72c1dc0]]}
  dc.input_tensor.layernorm_38.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x73fdc20], [1, 0x7347f40]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7353a40]]}
  dc.input_tensor.layernorm_38.6:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x735fd40]]}
  dc.input_tensor.layernorm_38.8:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x73309a0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7484e20]]}
  dc.input_tensor.layernorm_52.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7478b40], [3, 0x74866a0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7457300]]}
  dc.input_tensor.layernorm_52.6:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x713dd40]]}
  dc.input_tensor.layernorm_52.8:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x715d480]]}
  input_1_multiply_69_fork_clone731:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7177340]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7177340]]}
  dc.input_tensor.softmax_71.4:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x715fd80]]}
  lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x71a8760]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x71846c0]]}
  dc.input_tensor.layernorm_91.1:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x719c4a0], [3, 0x719c4a0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x719ccc0]]}
  dc.input_tensor.layernorm_91.6:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x71cd8c0]]}
  dc.input_tensor.layernorm_91.8:                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x71b4280]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x72d9380]]}
  dc.input_tensor.layernorm_105.1:                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75d00a0], [6, 0x75c2d80]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x749ec80]]}
  dc.input_tensor.layernorm_105.6:                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7589760]]}
  dc.input_tensor.layernorm_105.8:                                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75e7ec0]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75ce880]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x757cc40]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75ce040]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75aa760]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7571140], [0, 0x7677a00]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x75c2540]]}
  lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x74e9fa0]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7485660]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x744c040]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x726a780]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6fb1940]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6fbd420], [3, 0x6fbd420]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6fb1940]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7005e20]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70d5160]]}
  input_1_multiply_69:                                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ffb3c0]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x701ec80]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x701fce0]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x70741c0]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x711ebe0]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7068700]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x70741e0]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7044e40], [5, 0x708bfc0]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6e676e0]]}
  lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ef9b40]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e676e0]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6fb1100]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fbcbe0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6fb1100]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6fc8ee0], [6, 0x6fc8ee0]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f5f4e0]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7108760]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70ee8a0]]}
  input_1_multiply_16:                                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x70bf500]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x70552e0]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70c0580]]}

  # epoch_to_epoch
  e2e_gelu_44_0:                                                                          {input: gelu_44, type: queue, entries: 2, grid_size: [1, 4], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6db0900], [2, 0x6db0900], [3, 0x6db0900], [4, 0x6db0900]]}
  e2e__fused_op_5_0:                                                                      {input: _fused_op_5, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6db0900]]}
  e2e__fused_op_15_0:                                                                     {input: _fused_op_15, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x758b800]]}
  e2e__fused_op_18_0:                                                                     {input: _fused_op_18, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76d18e0]]}
  e2e__fused_op_17_0:                                                                     {input: _fused_op_17, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7661520]]}
  e2e_gelu_97_0:                                                                          {input: gelu_97, type: queue, entries: 2, grid_size: [1, 4], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7611980], [0, 0x77224c0], [1, 0x763d440], [2, 0x75f7b40]]}
  e2e_bw_in1_matmul_100_transpose_0_0:                                                    {input: bw_in1_matmul_100_transpose_0, type: queue, entries: 2, grid_size: [1, 4], t: 1, mblock: [48, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7640580], [5, 0x7641da0], [6, 0x76962a0], [7, 0x76a3e00]]}
  e2e__fused_op_21_0:                                                                     {input: _fused_op_21, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x77b4940], [1, 0x76cf8c0]]}
  e2e_matmul_94_0:                                                                        {input: matmul_94, type: queue, entries: 2, grid_size: [2, 12], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x760f920], [5, 0x7611140], [6, 0x7665640], [7, 0x76731a0], [0, 0x7783ce0], [1, 0x769ec60], [2, 0x7659360], [3, 0x7733100], [4, 0x761fd40], [5, 0x7621560], [6, 0x7675a60], [7, 0x76835c0], [0, 0x7794100], [1, 0x76af080], [2, 0x7669780], [3, 0x7743520], [4, 0x7630160], [5, 0x7631980], [6, 0x7685e80], [7, 0x76939e0], [0, 0x77a4520], [1, 0x76bf4a0], [2, 0x7679ba0], [3, 0x7753940]]}
  e2e_bw_in0_matmul_100_matmul_1_0:                                                       {input: bw_in0_matmul_100_matmul_1, type: queue, entries: 2, grid_size: [2, 12], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x76f7ac0], [7, 0x7705620], [0, 0x77e5560], [1, 0x77004e0], [2, 0x76a25e0], [3, 0x777c380], [4, 0x76ba3c0], [5, 0x76bbbe0], [6, 0x7707ee0], [7, 0x7715a40], [0, 0x77f5980], [1, 0x7710900], [2, 0x76b2a00], [3, 0x778c7a0], [4, 0x76ca7e0], [5, 0x76cc000], [6, 0x7718300], [7, 0x7725e60], [0, 0x7805da0], [1, 0x7720d20], [2, 0x76c2e20], [3, 0x779cbc0], [4, 0x76dac00], [5, 0x76dc420]]}
  e2e_bw_in1_matmul_94_transpose_0_0:                                                     {input: bw_in1_matmul_94_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7689fc0], [3, 0x7763d60], [4, 0x76a1da0], [5, 0x76a35c0]]}
  e2e__fused_op_14_0:                                                                     {input: _fused_op_14, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75c35e0]]}
  e2e__fused_op_13_0:                                                                     {input: _fused_op_13, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75db3e0]]}
  e2e_matmul_82_0:                                                                        {input: matmul_82, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x76a8660]]}
  e2e_bw_in1_matmul_86_transpose_0_0:                                                     {input: bw_in1_matmul_86_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x76eb020], [5, 0x76ec840], [6, 0x7740d40], [7, 0x774e8a0]]}
  e2e__fused_op_24_0:                                                                     {input: _fused_op_24, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77c6f00], [7, 0x77d4a60]]}
  e2e_bw_in0_matmul_86_matmul_1_0:                                                        {input: bw_in0_matmul_86_matmul_1, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77babe0], [7, 0x77c8740], [0, 0x789c380], [1, 0x77dbc00], [2, 0x784d000], [3, 0x797c2a0], [4, 0x77dece0], [5, 0x7811100]]}
  e2e_matmul_75_0:                                                                        {input: matmul_75, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75f6280], [4, 0x7595ac0], [5, 0x75f49e0], [6, 0x7648ee0], [7, 0x75f9340], [0, 0x7709e80], [1, 0x7624e00], [2, 0x75df500]]}
  e2e_softmax_71.dc.multiply.7_0:                                                         {input: softmax_71.dc.multiply.7, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x76025a0]]}
  e2e_bw_in1_matmul_82_transpose_0_0:                                                     {input: bw_in1_matmul_82_transpose_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x76d3240]]}
  e2e__fused_op_9_0:                                                                      {input: _fused_op_9, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x75db3a0]]}
  e2e_bw_in1_matmul_75_transpose_0_0:                                                     {input: bw_in1_matmul_75_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7728720], [7, 0x7736280], [0, 0x78161c0], [1, 0x7731140]]}
  e2e_lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0:  {input: lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x782e7e0]]}
  e2e_input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0_0:                                {input: input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7761d80]]}
  e2e_matmul_61_0:                                                                        {input: matmul_61, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7655200], [7, 0x7605660], [0, 0x77161a0], [1, 0x7631120], [2, 0x75eb820], [3, 0x76c55c0], [4, 0x7603600], [5, 0x7604e20]]}
  e2e_matmul_55_0:                                                                        {input: matmul_55, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x769c340], [1, 0x75b72c0], [2, 0x75cf0c0], [3, 0x75e9f60], [4, 0x75897a0], [5, 0x75e86c0], [6, 0x763cbc0], [7, 0x75ed020]]}
  e2e_bw_in1_matmul_67_transpose_0_0:                                                     {input: bw_in1_matmul_67_transpose_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x77acfe0]]}
  e2e_bw_in1_matmul_61_transpose_0_0:                                                     {input: bw_in1_matmul_61_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7749760], [2, 0x7796260], [3, 0x780e800], [4, 0x7703640]]}
  e2e_bw_in1_matmul_55_transpose_0_0:                                                     {input: bw_in1_matmul_55_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7704e60], [6, 0x7759360], [7, 0x7766ec0], [0, 0x783ab00]]}
  e2e__fused_op_8_0:                                                                      {input: _fused_op_8, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x75a1de0]]}
  e2e__fused_op_7_0:                                                                      {input: _fused_op_7, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7600d00]]}
  e2e_bw_in1_matmul_47_transpose_0_0:                                                     {input: bw_in1_matmul_47_transpose_0, type: queue, entries: 2, grid_size: [1, 4], t: 1, mblock: [48, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x77ae880], [3, 0x7826e20], [4, 0x771bc60], [5, 0x771d480]]}
  e2e__fused_op_28_0:                                                                     {input: _fused_op_28, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x77b4940], [1, 0x76cf8c0]]}
  e2e_matmul_41_0:                                                                        {input: matmul_41, type: queue, entries: 2, grid_size: [2, 12], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6dbcc20], [7, 0x6dbcc20], [0, 0x6e1e440], [1, 0x6e1e440], [2, 0x6e1e440], [3, 0x6e1e440], [4, 0x6e1e440], [5, 0x6e1e440], [6, 0x6dcd040], [7, 0x6dcd040], [0, 0x6e2e860], [1, 0x6e2e860], [2, 0x6e2e860], [3, 0x6e2e860], [4, 0x6e2e860], [5, 0x6e2e860], [6, 0x6ddd460], [7, 0x6ddd460], [0, 0x6e3ec80], [1, 0x6e3ec80], [2, 0x6e3ec80], [3, 0x6e3ec80], [4, 0x6e3ec80], [5, 0x6e3ec80]]}
  e2e_bw_in0_matmul_47_matmul_1_0:                                                        {input: bw_in0_matmul_47_matmul_1, type: queue, entries: 2, grid_size: [2, 12], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x78a86a0], [1, 0x77e7f20], [2, 0x7859320], [3, 0x79885c0], [4, 0x77eb000], [5, 0x781d420], [6, 0x77f7b20], [7, 0x7805680], [0, 0x78b8ac0], [1, 0x77f8340], [2, 0x7869740], [3, 0x79989e0], [4, 0x77fb420], [5, 0x782d840], [6, 0x7807f40], [7, 0x7815aa0], [0, 0x78c8ee0], [1, 0x7808760], [2, 0x7879b60], [3, 0x79a8e00], [4, 0x780b840], [5, 0x783dc60], [6, 0x7818360], [7, 0x7825ec0]]}
  e2e_bw_in1_matmul_41_transpose_0_0:                                                     {input: bw_in1_matmul_41_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7771980], [7, 0x777f4e0], [0, 0x7853120], [1, 0x77929a0]]}
  e2e__fused_op_4_0:                                                                      {input: _fused_op_4, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6ded880]]}
  e2e__fused_op_3_0:                                                                      {input: _fused_op_3, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ded880]]}
  e2e_matmul_29_0:                                                                        {input: matmul_29, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6db0900]]}
  e2e_bw_in1_matmul_33_transpose_0_0:                                                     {input: bw_in1_matmul_33_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x78349e0], [3, 0x7963c80], [4, 0x77c66c0], [5, 0x77f8ae0]]}
  e2e__fused_op_31_0:                                                                     {input: _fused_op_31, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7689fc0], [3, 0x7763d60]]}
  e2e_bw_in0_matmul_33_matmul_1_0:                                                        {input: bw_in0_matmul_33_matmul_1, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77dece0], [5, 0x7811100], [6, 0x77babe0], [7, 0x77c8740], [0, 0x789c380], [1, 0x77dbc00], [2, 0x784d000], [3, 0x797c2a0]]}
  e2e_matmul_22_0:                                                                        {input: matmul_22, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e4f0a0], [1, 0x6e4f0a0], [2, 0x6e4f0a0], [3, 0x6e4f0a0], [4, 0x6e4f0a0], [5, 0x6e4f0a0], [6, 0x6e4f0a0], [7, 0x6df19a0]]}
  e2e_softmax_18.dc.multiply.7_0:                                                         {input: softmax_18.dc.multiply.7, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e5b3c0]]}
  e2e_bw_in1_matmul_29_transpose_0_0:                                                     {input: bw_in1_matmul_29_transpose_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x78a0c60]]}
  e2e_bw_in1_matmul_22_transpose_0_0:                                                     {input: bw_in1_matmul_22_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7789fa0], [7, 0x7797b00], [0, 0x786b740], [1, 0x77aafc0]]}
  e2e_lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0:  {input: lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x78286c0]]}
  e2e_input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0_0:                                {input: input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7795aa0]]}
  e2e_matmul_8_0:                                                                         {input: matmul_8, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e5b3c0], [2, 0x6e5b3c0], [3, 0x6e5b3c0], [4, 0x6e5b3c0], [5, 0x6e5b3c0], [6, 0x6e5b3c0], [7, 0x6dfdcc0], [0, 0x6f1e3e0]]}
  e2e_matmul_2_0:                                                                         {input: matmul_2, type: queue, entries: 2, grid_size: [1, 8], t: 1, mblock: [2, 3], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6db0900], [7, 0x6db0900], [0, 0x6e12120], [1, 0x6e12120], [2, 0x6e12120], [3, 0x6e12120], [4, 0x6e12120], [5, 0x6e12120]]}
  e2e_bw_in1_matmul_14_transpose_0_0:                                                     {input: bw_in1_matmul_14_transpose_0, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x77972c0]]}
  e2e_bw_in1_matmul_8_transpose_0_0:                                                      {input: bw_in1_matmul_8_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x78100a0], [3, 0x7888640], [4, 0x777d480], [5, 0x777eca0]]}
  e2e_bw_in1_matmul_2_transpose_0_0:                                                      {input: bw_in1_matmul_2_transpose_0, type: queue, entries: 2, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77a25c0], [7, 0x77b0120], [0, 0x7883d60], [1, 0x77c35e0]]}
  e2e_input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0_0:                  {input: input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7640580], [5, 0x7641da0]]}
  e2e_input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0_0:                {input: input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x76962a0]]}
  e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0:                  {input: input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x76a3e00], [0, 0x77b4940]]}

  # optimizer_parameter
  input_opt_layer.0.attention.self.query.weight_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x705e4e0]]}
  input_opt_layer.0.attention.self.query.bias_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70f11a0]]}
  input_opt_layer.0.attention.self.key.weight_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7138320]]}
  input_opt_layer.0.attention.self.key.bias_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7108f80]]}
  input_opt_layer.0.attention.self.value.weight_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x709ed60]]}
  input_opt_layer.0.attention.self.value.bias_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x71bf480]]}
  input_opt_layer.0.attention.output.dense.weight_0.lr:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70e56e0]]}
  input_opt_layer.0.attention.output.dense.bias_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7108fa0]]}
  input_opt_layer.0.attention.output.LayerNorm.weight_0.lr:                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7108fa0]]}
  input_opt_layer.0.attention.output.LayerNorm.bias_0.lr:                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70f19e0]]}
  input_opt_layer.0.intermediate.dense.weight_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70519a0]]}
  input_opt_layer.0.intermediate.dense.bias_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x700a7e0]]}
  input_opt_layer.0.output.dense.weight_0.lr:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x711f420]]}
  input_opt_layer.0.output.dense.bias_0.lr:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7051160]]}
  input_opt_layer.0.output.LayerNorm.weight_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7068f40]]}
  input_opt_layer.0.output.LayerNorm.bias_0.lr:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7074a20]]}
  input_opt_layer.1.attention.self.query.weight_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x705d460]]}
  input_opt_layer.1.attention.self.query.bias_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a45e0]]}
  input_opt_layer.1.attention.self.key.weight_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7075240]]}
  input_opt_layer.1.attention.self.key.bias_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x700b020]]}
  input_opt_layer.1.attention.self.value.weight_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x711fc60]]}
  input_opt_layer.1.attention.self.value.bias_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7074a00]]}
  input_opt_layer.1.attention.output.dense.weight_0.lr:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7069780]]}
  input_opt_layer.1.attention.output.dense.bias_0.lr:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7075260]]}
  input_opt_layer.1.attention.output.LayerNorm.weight_0.lr:                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x705dca0]]}
  input_opt_layer.1.attention.output.LayerNorm.bias_0.lr:                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a4e20]]}
  input_opt_layer.1.intermediate.dense.weight_0.lr:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7075a80]]}
  input_opt_layer.1.intermediate.dense.bias_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x700b860]]}
  input_opt_layer.1.output.dense.weight_0.lr:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x71204a0]]}
  input_opt_layer.1.output.dense.bias_0.lr:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70521e0]]}
  input_opt_layer.1.output.LayerNorm.weight_0.lr:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7069fc0]]}
  input_opt_layer.1.output.LayerNorm.bias_0.lr:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7075aa0]]}

  # loss
  loss_bert_encoder.output_layernorm_105:                                                 {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300e3840]]}

  # grad_accumulator
  grad_acc_layer.1.output.LayerNorm.bias:                                                 {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x757d480]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                               {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75dbba0]]}
  grad_acc_layer.1.output.dense.bias:                                                     {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x75c3d80]]}
  grad_acc_layer.1.output.dense.weight:                                                   {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [2, 12], t: 1, mblock: [24, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7531920], [6, 0x75300e0], [7, 0x74dece0], [0, 0x75e55a0], [1, 0x7518300], [2, 0x753bbe0], [3, 0x7549740], [4, 0x74ea7e0], [5, 0x7562540], [6, 0x7560d00], [7, 0x750f900], [0, 0x76161c0], [1, 0x7548f20], [2, 0x756c800], [3, 0x757a360], [4, 0x751b400], [5, 0x7593160], [6, 0x7591920], [7, 0x7540520], [0, 0x7646de0], [1, 0x7579b40], [2, 0x759d420], [3, 0x75aaf80], [4, 0x754c020]]}
  grad_acc_layer.1.intermediate.dense.bias:                                               {input: bw_in1_add_96_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75235c0], [3, 0x7531120]]}
  grad_acc_layer.1.intermediate.dense.weight:                                             {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [4, 6], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7491160], [3, 0x749ecc0], [4, 0x7457b40], [5, 0x749f4c0], [6, 0x749dc80], [7, 0x744c880], [0, 0x7553140], [1, 0x7485ea0], [2, 0x74c1d80], [3, 0x74cf8e0], [4, 0x7488760], [5, 0x74d00e0], [6, 0x74ce8a0], [7, 0x747d4a0], [0, 0x7583d60], [1, 0x74b6ac0], [2, 0x74f29a0], [3, 0x7500500], [4, 0x74b9380], [5, 0x7500d00], [6, 0x74ff4c0], [7, 0x74ae0c0], [0, 0x75b4980], [1, 0x74e76e0]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                                       {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7546e20]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                                     {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7491960]]}
  grad_acc_layer.1.attention.output.dense.bias:                                           {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6fe1500]]}
  grad_acc_layer.1.attention.output.dense.weight:                                         {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6fe1500], [7, 0x6f5fd20], [0, 0x708bf20], [1, 0x6fb2180], [2, 0x6fd5a40], [3, 0x6fd5a40], [4, 0x6fb2180], [5, 0x6fed820]]}
  grad_acc_layer.1.attention.self.value.bias:                                             {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f84640]]}
  grad_acc_layer.1.attention.self.value.weight:                                           {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70b0840], [1, 0x6fd6aa0], [2, 0x6ffa360], [3, 0x6ffa360], [4, 0x6fd6aa0], [5, 0x7012140], [6, 0x7006660], [7, 0x6f90960]]}
  grad_acc_layer.1.attention.self.key.bias:                                               {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x707fc00]]}
  grad_acc_layer.1.attention.self.key.weight:                                             {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ffb3c0], [5, 0x7036a60], [6, 0x702af80], [7, 0x6fb5280], [0, 0x70d59a0], [1, 0x6ffbc00], [2, 0x701f4c0], [3, 0x702afa0]]}
  grad_acc_layer.1.attention.self.query.bias:                                             {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x705b380]]}
  grad_acc_layer.1.attention.self.query.weight:                                           {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x704f8a0], [7, 0x6fd9ba0], [0, 0x70fa2c0], [1, 0x7020520], [2, 0x7043de0], [3, 0x704f8c0], [4, 0x7020520], [5, 0x70676a0]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                                 {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ffe4c0]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                               {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7044e40]]}
  grad_acc_layer.0.output.dense.bias:                                                     {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6e676e0]]}
  grad_acc_layer.0.output.dense.weight:                                                   {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [2, 12], t: 1, mblock: [24, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6e676e0], [5, 0x6e676e0], [6, 0x6e676e0], [7, 0x6e09fe0], [0, 0x6f2a700], [1, 0x6e67f20], [2, 0x6e67f20], [3, 0x6e73a00], [4, 0x6e98300], [5, 0x6e98300], [6, 0x6e98300], [7, 0x6e3ac00], [0, 0x6f5b320], [1, 0x6e98b40], [2, 0x6e98b40], [3, 0x6ea4620], [4, 0x6ec8f20], [5, 0x6ec8f20], [6, 0x6ec8f20], [7, 0x6e6b820], [0, 0x6f8bf40], [1, 0x6ec9760], [2, 0x6ec9760], [3, 0x6ed5240]]}
  grad_acc_layer.0.intermediate.dense.bias:                                               {input: bw_in1_add_43_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f1e460], [6, 0x6f1e460]]}
  grad_acc_layer.0.intermediate.dense.weight:                                             {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [4, 6], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ec0d60], [0, 0x6fe1480], [1, 0x6f1eca0], [2, 0x6f1eca0], [3, 0x6f2a780], [4, 0x6f1eca0], [5, 0x6f36a80], [6, 0x6f36a80], [7, 0x6ef1980], [0, 0x70120a0], [1, 0x6f4f8c0], [2, 0x6f4f8c0], [3, 0x6f5b3a0], [4, 0x6f4f8c0], [5, 0x6f676a0], [6, 0x6f676a0], [7, 0x6f225a0], [0, 0x7042cc0], [1, 0x6f804e0], [2, 0x6f804e0], [3, 0x6f8bfc0], [4, 0x6f804e0], [5, 0x6f982c0], [6, 0x6f982c0]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                                       {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f531c0]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                                     {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6fb1100]]}
  grad_acc_layer.0.attention.output.dense.bias:                                           {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x70738e0]]}
  grad_acc_layer.0.attention.output.dense.weight:                                         {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6ef9b40], [6, 0x6ef9b40], [7, 0x6e9c440], [0, 0x6fbcb60], [1, 0x6efa380], [2, 0x6efa380], [3, 0x6f05e60], [4, 0x6efa380]]}
  grad_acc_layer.0.attention.self.value.bias:                                             {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x701ec80]]}
  grad_acc_layer.0.attention.self.value.weight:                                           {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70c9f80], [6, 0x709abe0], [7, 0x70309c0], [0, 0x7145600], [1, 0x7077340], [2, 0x708f120], [3, 0x709ac00], [4, 0x7083640]]}
  grad_acc_layer.0.attention.self.key.bias:                                               {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7169f20]]}
  grad_acc_layer.0.attention.self.key.weight:                                             {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x709bc60], [2, 0x70b3a40], [3, 0x70bf520], [4, 0x70a7f60], [5, 0x70ef0e0], [6, 0x70bfd40], [7, 0x7055b20], [0, 0x7176240]]}
  grad_acc_layer.0.attention.self.query.bias:                                             {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x70d8360]]}
  grad_acc_layer.0.attention.self.query.weight:                                           {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [4, 2], t: 1, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x70e3e40], [4, 0x70cc880], [5, 0x7113a00], [6, 0x70e4660], [7, 0x707a440], [0, 0x719ab60], [1, 0x70c0dc0], [2, 0x70e4680]]}

graphs:
  fwd_0_0:
    target_device: 0
    input_count: 2
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_16_fork_clone713_splt_brcst_1_0: {type: nop, grid_loc: [0, 9], grid_size: [1, 1], inputs: [input_1_multiply_16_fork_clone713],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_16_fork_clone713_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [input_1_multiply_16_fork_clone713_splt_brcst_1_0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    multiply_16: {type: multiply, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone713_splt_brcst_1_0_splt_brcst_3_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_17: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [multiply_16, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_17],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_0: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 4], inputs: [add_17, softmax_18.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [6, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 0}}
    lc.input_tensor.softmax_18.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_18.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_1: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_18.dc.multiply.7: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_0, softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_22: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_29: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_18.dc.multiply.7, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [3, 4], grid_size: [1, 8], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 56], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 2}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [_fused_op_2, _fused_op_2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_2_buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_0__fused_op_2_buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_2__fused_op_4: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [buffer_0__fused_op_2_buffer_0__fused_op_2__fused_op_4],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_3: {type: fused_op, grid_loc: [4, 9], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    _fused_op_4: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_2__fused_op_4, _fused_op_3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_4, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 5}}
    matmul_41: {type: matmul, grid_loc: [5, 0], grid_size: [2, 12], inputs: [_fused_op_5, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    gelu_44: {type: gelu, grid_loc: [7, 0], grid_size: [1, 4], inputs: [matmul_41],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_0_1:
    target_device: 0
    input_count: 2
    matmul_47: {type: matmul, grid_loc: [0, 0], grid_size: [4, 6], inputs: [e2e_gelu_44_0, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, u_kt: 12}}
    add_51: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_47, e2e__fused_op_5_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 56], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 2}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_6, _fused_op_6],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_6_buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_6],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [buffer_0__fused_op_6_buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_6__fused_op_8: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_0__fused_op_6_buffer_0__fused_op_6__fused_op_8],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_7: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    _fused_op_8: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_6__fused_op_8, _fused_op_7],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 4}}
    _fused_op_9: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_8, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 5}}
    matmul_55: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [_fused_op_9, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_61: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [_fused_op_9, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_67: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_69_fork_clone731_splt_brcst_1_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [input_1_multiply_69_fork_clone731],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_69_fork_clone731_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 8], grid_size: [1, 1], inputs: [input_1_multiply_69_fork_clone731_splt_brcst_1_0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    multiply_69: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone731_splt_brcst_1_0_splt_brcst_3_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_70: {type: add, grid_loc: [2, 10], grid_size: [1, 1], inputs: [multiply_69, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 16}}}
    softmax_71.dc.reduce_max.0: {type: reduce, grid_loc: [2, 11], grid_size: [1, 1], inputs: [add_70],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 4}}
    _fused_op_10: {type: fused_op, grid_loc: [3, 6], grid_size: [1, 4], inputs: [add_70, softmax_71.dc.reduce_max.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [6, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: false, fused_op_id: 0}}
    lc.input_tensor.softmax_71.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_71.dc.reduce_sum.3.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_71.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [_fused_op_10, lc.input_tensor.softmax_71.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_11: {type: fused_op, grid_loc: [4, 8], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_71.4],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 1}}
    softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_11, lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_71.dc.multiply.7: {type: multiply, grid_loc: [4, 10], grid_size: [1, 1], inputs: [_fused_op_10, softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_75: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [_fused_op_9, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    matmul_82: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [softmax_71.dc.multiply.7, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, kernel_broadcast: {input_2: 6}, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0__fused_op_9_buffer_0__fused_op_9_buffer_0__fused_op_9_add_90: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [_fused_op_9],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_9_buffer_0__fused_op_9_add_90: {type: nop, grid_loc: [5, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_9_buffer_0__fused_op_9_buffer_0__fused_op_9_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_9_add_90: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_9_buffer_0__fused_op_9_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [5, 11], grid_size: [1, 1], inputs: [matmul_86, buffer_0__fused_op_9_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, add_90],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 56], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 2}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [7, 10], grid_size: [1, 1], inputs: [_fused_op_12, _fused_op_12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_12_buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14: {type: nop, grid_loc: [6, 11], grid_size: [1, 1], inputs: [_fused_op_12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14: {type: nop, grid_loc: [7, 8], grid_size: [1, 1], inputs: [buffer_0__fused_op_12_buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_12__fused_op_14: {type: nop, grid_loc: [7, 9], grid_size: [1, 1], inputs: [buffer_0__fused_op_12_buffer_0__fused_op_12__fused_op_14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_13: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    _fused_op_14: {type: fused_op, grid_loc: [8, 1], grid_size: [1, 1], inputs: [buffer_0__fused_op_12__fused_op_14, _fused_op_13],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 4}}
    _fused_op_15: {type: fused_op, grid_loc: [8, 2], grid_size: [1, 1], inputs: [_fused_op_14, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 5}}

  fwd_0_2:
    target_device: 0
    input_count: 2
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [2, 12], inputs: [e2e__fused_op_15_0, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    gelu_97: {type: gelu, grid_loc: [2, 0], grid_size: [1, 4], inputs: [matmul_94],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [2, 4], grid_size: [4, 6], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 8, min_buffer_input: 0, u_kt: 12}}
    add_104: {type: add, grid_loc: [2, 10], grid_size: [1, 1], inputs: [matmul_100, e2e__fused_op_15_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, add_104],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 56], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 2}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [_fused_op_16, _fused_op_16],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    buffer_0__fused_op_16_buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_16],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_0__fused_op_16_buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_16__fused_op_18: {type: nop, grid_loc: [3, 10], grid_size: [1, 1], inputs: [buffer_0__fused_op_16_buffer_0__fused_op_16__fused_op_18],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_17: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 3}}
    _fused_op_18: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_0__fused_op_16__fused_op_18, _fused_op_17],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 4}}
    _fused_op_19: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_18, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 5}}
    _fused_op_19_output_nop_0: {type: nop, grid_loc: [4, 10], grid_size: [1, 1], inputs: [_fused_op_19], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  bwd_0_3:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoder.output_layernorm_105], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_18_0, loss_bert_encoder.output_layernorm_105],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_20: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [loss_bert_encoder.output_layernorm_105, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 20}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [_fused_op_20, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [236, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_20, e2e__fused_op_18_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e__fused_op_18_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, _fused_op_20, e2e__fused_op_17_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 56, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 21}}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, _fused_op_21], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 12], inputs: [_fused_op_21, layer.1.output.dense.weight],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [0, 3], grid_size: [1, 4], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [48, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [0, 7], grid_size: [4, 1], inputs: [e2e__fused_op_15_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [0, 8], grid_size: [4, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [0, 10], grid_size: [4, 1], inputs: [e2e__fused_op_9_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_69_splt_brcst_1_0: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [input_1_multiply_69],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [input_1_multiply_69_splt_brcst_1_0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [1, 2], grid_size: [4, 1], inputs: [e2e__fused_op_9_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [1, 3], grid_size: [4, 1], inputs: [e2e__fused_op_9_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 4], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [48, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [1, 9], grid_size: [4, 1], inputs: [e2e__fused_op_5_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [1, 11], grid_size: [4, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [2, 1], grid_size: [4, 1], inputs: [input_1],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_16_splt_brcst_1_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [input_1_multiply_16],
         t: 12, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 12}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [input_1_multiply_16_splt_brcst_1_0],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 12]}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [3, 0], grid_size: [4, 1], inputs: [input_1],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [input_1],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_0_4:
    target_device: 0
    input_count: 2
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 12], inputs: [e2e_bw_in1_matmul_100_transpose_0_0, e2e__fused_op_21_0], gradient_op: true,
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_22: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 12], inputs: [e2e_matmul_94_0, e2e_bw_in0_matmul_100_matmul_1_0],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 22}}
    bw_in1_add_96_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0, _fused_op_22], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 6], inputs: [_fused_op_22, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 12}}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [4, 6], inputs: [e2e_bw_in1_matmul_94_transpose_0_0, _fused_op_22], gradient_op: true,
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [e2e__fused_op_21_0, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e__fused_op_14_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_23: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 20}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_23, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [236, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_23, e2e__fused_op_14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_24: {type: fused_op, grid_loc: [8, 10], grid_size: [1, 2], inputs: [e2e__fused_op_14_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, _fused_op_23, e2e__fused_op_13_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 56, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 21}}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, _fused_op_24], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [_fused_op_24, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 2}}

  bwd_0_5:
    target_device: 0
    input_count: 2
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_86_transpose_0_0, e2e__fused_op_24_0], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_86_matmul_1_0, e2e_matmul_75_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in1_matmul_82_transpose_0_0, e2e_bw_in0_matmul_86_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 8], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [0, 10], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_75_transpose_0_0, bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, e2e_lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_25: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_71.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {fused_op_id: 25}}
    bw_in0_multiply_69_multiply_0: {type: multiply, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_25, e2e_input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [bw_in0_multiply_69_multiply_0, e2e_matmul_61_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [32, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [e2e_bw_in1_matmul_67_transpose_0_0, bw_in0_multiply_69_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 8], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [4, 10], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_61_transpose_0_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 8], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [72, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_55_transpose_0_0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_26: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 2], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1, bw_in0_matmul_55_matmul_1, e2e__fused_op_24_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 26}}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_26], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [e2e__fused_op_8_0, _fused_op_26],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_27: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_26, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 20}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [_fused_op_27, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [236, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_27, e2e__fused_op_8_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_28: {type: fused_op, grid_loc: [5, 8], grid_size: [1, 2], inputs: [e2e__fused_op_8_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, _fused_op_27, e2e__fused_op_7_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 56, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 21}}
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, _fused_op_28], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 12], inputs: [_fused_op_28, layer.0.output.dense.weight],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 4}}

  bwd_0_6:
    target_device: 0
    input_count: 2
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 12], inputs: [e2e_bw_in1_matmul_47_transpose_0_0, e2e__fused_op_28_0], gradient_op: true,
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_29: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 12], inputs: [e2e_matmul_41_0, e2e_bw_in0_matmul_47_matmul_1_0],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false, fused_op_id: 22}}
    bw_in1_add_43_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0, _fused_op_29], gradient_op: true,
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 6], inputs: [_fused_op_29, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 12}}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [4, 6], inputs: [e2e_bw_in1_matmul_41_transpose_0_0, _fused_op_29], gradient_op: true,
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [e2e__fused_op_28_0, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [e2e__fused_op_4_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    _fused_op_30: {type: fused_op, grid_loc: [8, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {fused_op_id: 20}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_30, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [236, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_30, e2e__fused_op_4_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 24, min_buffer_input: 0, u_kt: 1}}
    _fused_op_31: {type: fused_op, grid_loc: [8, 10], grid_size: [1, 2], inputs: [e2e__fused_op_4_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, _fused_op_30, e2e__fused_op_3_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 56, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 21}}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, _fused_op_31], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [_fused_op_31, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 2}}

  bwd_0_7:
    target_device: 0
    input_count: 2
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_33_transpose_0_0, e2e__fused_op_31_0], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_33_matmul_1_0, e2e_matmul_22_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in1_matmul_29_transpose_0_0, e2e_bw_in0_matmul_33_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_22_transpose_0_0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, e2e_lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    _fused_op_32: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1, e2e_softmax_18.dc.multiply.7_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {fused_op_id: 25}}
    bw_in0_multiply_16_multiply_0: {type: multiply, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_32, e2e_input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_multiply_16_multiply_0, e2e_matmul_8_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_bw_in1_matmul_14_transpose_0_0, bw_in0_multiply_16_multiply_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [1, 9], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_8_transpose_0_0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 4, min_buffer_input: 0, u_kt: 1}}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [4, 2], inputs: [e2e_bw_in1_matmul_2_transpose_0_0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}

  opt_0_8:
    target_device: 0
    input_count: 1
    input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.query.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in1_layer.0.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [0, 1], grid_size: [4, 2], inputs: [grad_acc_layer.0.attention.self.query.weight, input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in1_layer.0.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [4, 2], inputs: [layer.0.attention.self.query.weight, opt_in1_layer.0.attention.self.query.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.key.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in1_layer.0.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [0, 7], grid_size: [4, 2], inputs: [grad_acc_layer.0.attention.self.key.weight, input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in1_layer.0.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [0, 9], grid_size: [4, 2], inputs: [layer.0.attention.self.key.weight, opt_in1_layer.0.attention.self.key.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.value.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in1_layer.0.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [1, 5], grid_size: [4, 2], inputs: [grad_acc_layer.0.attention.self.value.weight, input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in1_layer.0.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 2], inputs: [layer.0.attention.self.value.weight, opt_in1_layer.0.attention.self.value.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.output.dense.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.0.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [4, 2], grid_size: [4, 2], inputs: [grad_acc_layer.0.attention.output.dense.weight, input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in0_layer.0.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 7], grid_size: [4, 2], inputs: [layer.0.attention.output.dense.weight, opt_in0_layer.0.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [4, 9], grid_size: [1, 2], inputs: [input_opt_layer.0.intermediate.dense.weight_0.lr],
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 96}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.bias, input_opt_layer.0.attention.self.query.bias_0.lr, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_34: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.bias, input_opt_layer.0.attention.self.key.bias_0.lr, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_35: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.bias, input_opt_layer.0.attention.self.value.bias_0.lr, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_36: {type: fused_op, grid_loc: [2, 11], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.bias, input_opt_layer.0.attention.output.dense.bias_0.lr, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_37: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.weight, input_opt_layer.0.attention.output.LayerNorm.weight_0.lr, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_38: {type: fused_op, grid_loc: [3, 11], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.bias, input_opt_layer.0.attention.output.LayerNorm.bias_0.lr, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}

  opt_0_9:
    target_device: 0
    input_count: 1
    opt_in0_layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 6], inputs: [grad_acc_layer.0.intermediate.dense.weight, e2e_input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}]}
    opt_in0_layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [0, 6], grid_size: [4, 6], inputs: [layer.0.intermediate.dense.weight, opt_in0_layer.0.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [4, 1], grid_size: [1, 2], inputs: [input_opt_layer.0.output.dense.weight_0.lr],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [2, 12], inputs: [grad_acc_layer.0.output.dense.weight, input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 96}],
         attributes: {kernel_broadcast: {input_1: 4}}}
    opt_in0_layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [7, 0], grid_size: [2, 12], inputs: [layer.0.output.dense.weight, opt_in0_layer.0.output.dense.weight_multiply_1],
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.query.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    _fused_op_39: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.bias, input_opt_layer.0.intermediate.dense.bias_0.lr, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 96}],
         attributes: {fused_op_id: 39, kernel_broadcast: {input_1: 1}}}
    _fused_op_40: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.dense.bias, input_opt_layer.0.output.dense.bias_0.lr, layer.0.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_41: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.weight, input_opt_layer.0.output.LayerNorm.weight_0.lr, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_42: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.bias, input_opt_layer.0.output.LayerNorm.bias_0.lr, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}

  opt_0_10:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 2], inputs: [grad_acc_layer.1.attention.self.query.weight, e2e_input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in0_layer.1.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 2], grid_size: [4, 2], inputs: [layer.1.attention.self.query.weight, opt_in0_layer.1.attention.self.query.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.key.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.1.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [0, 6], grid_size: [4, 2], inputs: [grad_acc_layer.1.attention.self.key.weight, input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in0_layer.1.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [0, 8], grid_size: [4, 2], inputs: [layer.1.attention.self.key.weight, opt_in0_layer.1.attention.self.key.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.value.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.1.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [1, 4], grid_size: [4, 2], inputs: [grad_acc_layer.1.attention.self.value.weight, input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in0_layer.1.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [1, 10], grid_size: [4, 2], inputs: [layer.1.attention.self.value.weight, opt_in0_layer.1.attention.self.value.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.output.dense.weight_0.lr],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.1.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [4, 2], grid_size: [4, 2], inputs: [grad_acc_layer.1.attention.output.dense.weight, input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {kernel_broadcast: {input_1: 24}}}
    opt_in0_layer.1.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 6], grid_size: [4, 2], inputs: [layer.1.attention.output.dense.weight, opt_in0_layer.1.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 2], inputs: [input_opt_layer.1.intermediate.dense.weight_0.lr],
         t: 1, mblock: [1, 12], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 96}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    _fused_op_43: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.bias, input_opt_layer.1.attention.self.query.bias_0.lr, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_44: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.bias, input_opt_layer.1.attention.self.key.bias_0.lr, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_45: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.bias, input_opt_layer.1.attention.self.value.bias_0.lr, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_46: {type: fused_op, grid_loc: [4, 8], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.bias, input_opt_layer.1.attention.output.dense.bias_0.lr, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_47: {type: fused_op, grid_loc: [4, 9], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.weight, input_opt_layer.1.attention.output.LayerNorm.weight_0.lr, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_48: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.bias, input_opt_layer.1.attention.output.LayerNorm.bias_0.lr, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}

  opt_0_11:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 6], inputs: [grad_acc_layer.1.intermediate.dense.weight, e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}]}
    opt_in0_layer.1.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [0, 6], grid_size: [4, 6], inputs: [layer.1.intermediate.dense.weight, opt_in0_layer.1.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [4, 1], grid_size: [1, 2], inputs: [input_opt_layer.1.output.dense.weight_0.lr],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 24}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    opt_in0_layer.1.output.dense.weight_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [2, 12], inputs: [grad_acc_layer.1.output.dense.weight, input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 96}],
         attributes: {kernel_broadcast: {input_1: 4}}}
    opt_in0_layer.1.output.dense.weight_subtract_2: {type: subtract, grid_loc: [7, 0], grid_size: [2, 12], inputs: [layer.1.output.dense.weight, opt_in0_layer.1.output.dense.weight_multiply_1],
         t: 1, mblock: [24, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_49: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.bias, input_opt_layer.1.intermediate.dense.bias_0.lr, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 96}],
         attributes: {fused_op_id: 39, kernel_broadcast: {input_1: 1}}}
    _fused_op_50: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.dense.bias, input_opt_layer.1.output.dense.bias_0.lr, layer.1.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_51: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.weight, input_opt_layer.1.output.LayerNorm.weight_0.lr, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}
    _fused_op_52: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.bias, input_opt_layer.1.output.LayerNorm.bias_0.lr, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {fused_op_id: 33, kernel_broadcast: {input_1: 1}}}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - staticvar: {$lptr_q4: 0, $lptr_q3: 0, $gptr_q1_shadow: 0, $gptr_q4: 0, $gptr_q4_shadow: 0, $lptr_q2: 0, $gptr_q3_shadow: 0, $gptr_q3: 0, $gptr_q2: 0, $gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q1: 0, $gptr_q0_shadow: 0}
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone713: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone731: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_0_2, queue_settings: {
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    - endloop

  - run_bwd_0:
    - param: [$p_loop_count, $p_zero_grad]
    - var: {$gptr_q5: 0, $gptr_q9: 0, $lptr_q5: 0, $lptr_q9: 0, $gptr_q11: 0, $gptr_q7: 0, $v_zero_grad: 0, $c_microbatch_size: 2, $lptr_q7: 0, $lptr_q11: 0, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q10: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q2: 0, $gptr_q10: 0, $gptr_q3_shadow: 0, $gptr_q8: 0, $lptr_q6: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q8: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   varinst: [$gptr_q3, set, $gptr_q3_shadow]
    # This command has 22 queues and more than 60 buffers, needs to be split into 2 commands.
    -   allocate_queue: [e2e__fused_op_21_0, e2e_bw_in0_matmul_100_matmul_1_0, e2e_bw_in1_matmul_100_transpose_0_0, e2e_bw_in1_matmul_94_transpose_0_0, e2e_bw_in1_matmul_86_transpose_0_0, e2e_bw_in1_matmul_82_transpose_0_0, e2e_bw_in1_matmul_75_transpose_0_0, e2e_lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0, e2e_input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0_0, e2e_bw_in1_matmul_67_transpose_0_0, e2e_bw_in1_matmul_61_transpose_0_0, e2e_bw_in1_matmul_55_transpose_0_0, e2e_bw_in1_matmul_47_transpose_0_0, e2e_bw_in1_matmul_41_transpose_0_0, e2e_bw_in1_matmul_33_transpose_0_0, e2e_bw_in1_matmul_29_transpose_0_0, e2e_bw_in1_matmul_22_transpose_0_0, e2e_lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0, e2e_input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0_0, e2e_bw_in1_matmul_14_transpose_0_0, e2e_bw_in1_matmul_8_transpose_0_0, e2e_bw_in1_matmul_2_transpose_0_0]
    -   execute: {graph_name: bwd_0_3, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               loss_bert_encoder.output_layernorm_105: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_softmax_18.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_softmax_71.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e__fused_op_24_0, e2e_bw_in0_matmul_86_matmul_1_0]
    -   execute: {graph_name: bwd_0_4, queue_settings: {
               e2e__fused_op_13_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in0_matmul_100_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in1_matmul_100_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in1_matmul_94_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_21_0, e2e_bw_in0_matmul_100_matmul_1_0, e2e_bw_in1_matmul_100_transpose_0_0, e2e_bw_in1_matmul_94_transpose_0_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e__fused_op_28_0, e2e_bw_in0_matmul_47_matmul_1_0]
    -   execute: {graph_name: bwd_0_5, queue_settings: {
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_softmax_71.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_24_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_matmul_86_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_86_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_82_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_75_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_67_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_61_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_55_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_24_0, e2e_bw_in0_matmul_86_matmul_1_0, e2e_bw_in1_matmul_86_transpose_0_0, e2e_bw_in1_matmul_82_transpose_0_0, e2e_bw_in1_matmul_75_transpose_0_0, e2e_lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0, e2e_input_1_multiply_69_splt_brcst_1_0_splt_brcst_3_0_0, e2e_bw_in1_matmul_67_transpose_0_0, e2e_bw_in1_matmul_61_transpose_0_0, e2e_bw_in1_matmul_55_transpose_0_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e__fused_op_31_0, e2e_bw_in0_matmul_33_matmul_1_0]
    -   execute: {graph_name: bwd_0_6, queue_settings: {
               e2e__fused_op_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_matmul_47_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in1_matmul_47_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in1_matmul_41_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_28_0, e2e_bw_in0_matmul_47_matmul_1_0, e2e_bw_in1_matmul_47_transpose_0_0, e2e_bw_in1_matmul_41_transpose_0_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: bwd_0_7, queue_settings: {
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_softmax_18.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_matmul_33_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_33_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_29_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_22_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_14_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_8_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in1_matmul_2_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_31_0, e2e_bw_in0_matmul_33_matmul_1_0, e2e_bw_in1_matmul_33_transpose_0_0, e2e_bw_in1_matmul_29_transpose_0_0, e2e_bw_in1_matmul_22_transpose_0_0, e2e_lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0_0, e2e_input_1_multiply_16_splt_brcst_1_0_splt_brcst_3_0_0, e2e_bw_in1_matmul_14_transpose_0_0, e2e_bw_in1_matmul_8_transpose_0_0, e2e_bw_in1_matmul_2_transpose_0_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $lptr_q0: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0}
    - allocate_queue: [e2e_input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_0_8, queue_settings: {
             layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - allocate_queue: [e2e_input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_0_9, queue_settings: {
             e2e_input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
             layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q0, incwrap, $c_one, 2]
    - varinst: [$lptr_q0, incwrap, $c_one, 2]
    - allocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_0_10, queue_settings: {
             e2e_input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
             layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q1, incwrap, $c_one, 2]
    - varinst: [$lptr_q1, incwrap, $c_one, 2]
    - execute: {graph_name: opt_0_11, queue_settings: {
             e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
             layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q2, incwrap, $c_one, 2]
    - varinst: [$lptr_q2, incwrap, $c_one, 2]


fused_ops:
  0: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.subtract.1: { type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.exp.2: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.add.5: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.6: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  2: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.multiply.2: { type: multiply, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - layernorm_38.dc.subtract.3: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  4: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 6], ublock: [2, 4], output: output}
  5: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.13: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  20: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  21: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.4: { type: multiply, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_105_layernorm_bw_0.dc.add.5: { type: add, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.7: { type: multiply, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_105_layernorm_bw_0.dc.subtract.8: { type: subtract, inputs: [input4, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: dest}
        - bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9: { type: multiply, inputs: [dest, input5], input_1_tms: [tile_broadcast: c], mblock: [2, 3], ublock: [2, 4], output: output}
  22: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_gelu_97_gelu_derivative_0: { type: gelu_derivative, inputs: [input0], mblock: [1, 2], ublock: [2, 4], output: dest}
        - bw_in0_gelu_97_multiply_1: { type: multiply, inputs: [dest, input1], mblock: [1, 2], ublock: [2, 4], output: output}
  25: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - bw_in0_softmax_71_softmax_bw_0.dc.subtract.2: { type: subtract, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_softmax_71_softmax_bw_0.dc.multiply.3: { type: multiply, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: output}
  26: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - bw_in0_reshape_53.dc.squeeze.0_combine_add_0: { type: add, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 4], output: dest}
        - bw_in0_reshape_53.dc.squeeze.0_combine_add_1: { type: add, inputs: [dest, input2], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_52_combine_add_0: { type: add, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: output}
  33: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - opt_in1_layer.0.attention.self.query.bias_multiply_1: { type: multiply, inputs: [input0, input1], mblock: [1, 6], ublock: [1, 4], output: intermed0}
        - opt_in1_layer.0.attention.self.query.bias_subtract_2: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [1, 6], ublock: [1, 4], output: output}
  39: 
    inputs: 3
    intermediates: 1
    schedules: 
      -
        - opt_in1_layer.0.intermediate.dense.bias_multiply_1: { type: multiply, inputs: [input0, input1], mblock: [1, 24], ublock: [1, 4], output: intermed0}
        - opt_in1_layer.0.intermediate.dense.bias_subtract_2: { type: subtract, inputs: [input2, intermed0], pop: [intermed0], mblock: [1, 24], ublock: [1, 4], output: output}
