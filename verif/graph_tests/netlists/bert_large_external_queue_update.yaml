# git checkout ef840c36
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 3 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_DISABLE_DRAM0=1 PYBUDA_FUSE_OPS=1 PYBUDA_NLP_MANUAL_TARGET=250000 --auto_transpose

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                     {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                    {input: HOST, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8898bc0], [4, 0x8928720], [5, 0x88b93c0]]}

  # output
  bert_encoders.output_layernorm_1271:               {input: _fused_op_95_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5b412a0], [4, 0x5b5a8e0], [5, 0x5b190c0], [6, 0x5c042a0], [7, 0x5b7e9c0], [1, 0x5c21a00], [2, 0x5af9920], [3, 0x5b822c0]]}
  layer.0.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x87963c0], [1, 0x88cf0e0], [2, 0x877dde0], [3, 0x8853a80]]}
  layer.0.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88a66e0], [5, 0x8874280], [6, 0x8895280], [7, 0x879a4e0], [1, 0x88d3200], [2, 0x8781f00], [3, 0x8857ba0], [4, 0x88e7700]]}
  layer.0.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x88b52a0], [6, 0x88d62a0], [7, 0x87db500], [1, 0x8914220]]}
  layer.0.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x87df620], [1, 0x8918340], [2, 0x87c3760], [3, 0x8aa0be0], [4, 0x8b30740], [5, 0x8ac13e0], [6, 0x88dac00], [7, 0x8820640]]}
  layer.0.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8779cc0], [3, 0x884f960], [4, 0x88a25c0], [5, 0x8870160]]}
  layer.0.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8804780], [3, 0x8ae1c00], [4, 0x8b71760], [5, 0x8b02400], [6, 0x891bc20], [7, 0x8861660], [1, 0x8959ba0], [2, 0x88457a0]]}
  layer.0.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8b22c20], [4, 0x8bb2780], [5, 0x8b43420], [6, 0x895cc40]]}
  layer.0.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8960d60]]}
  layer.0.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x88a2ec0]]}
  layer.0.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x899b400], [2, 0x8888040], [3, 0x8b285c0], [4, 0x8bb8120], [5, 0x8b48dc0], [6, 0x8971180], [7, 0x88b32e0], [1, 0x89dc420], [2, 0x88c9060], [3, 0x8b695e0], [4, 0x8bf9140], [5, 0x8b89de0], [6, 0x89b21a0], [7, 0x88f4300], [1, 0x8a1d440], [2, 0x890a080], [3, 0x8baa600], [4, 0x8c3a160], [5, 0x8bcae00], [6, 0x89f31c0], [7, 0x8935320], [1, 0x8a5e460], [2, 0x894b0a0], [3, 0x8beb620], [4, 0x8c7b180], [5, 0x8c0be20], [6, 0x8a341e0], [7, 0x8976340], [1, 0x8a9f480], [2, 0x898c0c0], [3, 0x8c2c640], [4, 0x8cbc1a0]]}
  layer.0.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8798360], [6, 0x8743ea0], [7, 0x86bede0], [1, 0x87f7b00], [2, 0x86a6800], [3, 0x877c4a0], [4, 0x8795ae0], [5, 0x87a0580]]}
  layer.0.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x860e980], [6, 0x85ba4c0], [7, 0x8535400], [1, 0x866e120], [2, 0x854d200], [3, 0x8624700], [4, 0x860d140], [5, 0x864f9a0], [6, 0x85fb4e0], [7, 0x8576420], [1, 0x86af140], [2, 0x858e220], [3, 0x8665720], [4, 0x864e160], [5, 0x86909c0], [6, 0x863c500], [7, 0x85b7440], [1, 0x86f0160], [2, 0x85cf240], [3, 0x86a6740], [4, 0x868f180], [5, 0x86d19e0], [6, 0x867d520], [7, 0x85f8460], [1, 0x8731180], [2, 0x8610260], [3, 0x86e7760], [4, 0x86d01a0], [5, 0x8712a00], [6, 0x86be540], [7, 0x8639480], [1, 0x87721a0]]}
  layer.0.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8651280], [3, 0x8728780], [4, 0x87111c0], [5, 0x8753a20], [6, 0x86ff560], [7, 0x867a4a0], [1, 0x87b31c0], [2, 0x8653320]]}
  layer.0.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x86553c0]]}
  layer.0.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x872b060]]}
  layer.1.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8713aa0], [5, 0x8757340], [6, 0x8702e80], [7, 0x867ddc0], [1, 0x87b6ae0], [2, 0x86657e0], [3, 0x873b480], [4, 0x8754ac0]]}
  layer.1.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x866a000], [2, 0x85490e0], [3, 0x86205e0], [4, 0x8609020]]}
  layer.1.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x874c0c0], [7, 0x86c7000], [1, 0x87ffd20], [2, 0x86aea20], [3, 0x87846c0], [4, 0x879dd00], [5, 0x87a87a0], [6, 0x878d0e0]]}
  layer.1.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8708020], [1, 0x8840d40], [2, 0x86efa40], [3, 0x87c56e0]]}
  layer.1.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x87ce100], [7, 0x870c140], [1, 0x8844e60], [2, 0x86f3b60], [3, 0x87c9800], [4, 0x87df560], [5, 0x87ea000], [6, 0x880f120]]}
  layer.1.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x874d160], [1, 0x8885e80], [2, 0x8734b80], [3, 0x880a820]]}
  layer.1.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8820580], [5, 0x882b020], [6, 0x8850140], [7, 0x8751280], [1, 0x8889fa0], [2, 0x8738ca0], [3, 0x880e940], [4, 0x88615a0]]}
  layer.1.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x886c040], [6, 0x8891160], [7, 0x87922a0], [1, 0x88cafc0]]}
  layer.1.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8fb77c0]]}
  layer.1.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8d530e0]]}
  layer.1.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8cd41c0], [1, 0x8d88640], [2, 0x8cae080], [3, 0x8f5a900], [4, 0x901b060], [5, 0x8fc7be0], [6, 0x8d63500], [7, 0x8d151e0], [1, 0x8dc9660], [2, 0x8cef0a0], [3, 0x8f9b920], [4, 0x905c080], [5, 0x9008c00], [6, 0x8da4520], [7, 0x8d56200], [1, 0x8e0a680], [2, 0x8d300c0], [3, 0x8fdc940], [4, 0x909d0a0], [5, 0x9049c20], [6, 0x8de5540], [7, 0x8d97220], [1, 0x8e4b6a0], [2, 0x8d710e0], [3, 0x901d960], [4, 0x90de0c0], [5, 0x908ac40], [6, 0x8e26560], [7, 0x8dd8240], [1, 0x8e8c6c0], [2, 0x8db2100], [3, 0x905e980]]}
  layer.1.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x911f0e0], [5, 0x90cbc60], [6, 0x8e67580], [7, 0x8e19260], [1, 0x8ecd6e0], [2, 0x8df3120], [3, 0x909f9a0], [4, 0x9127300]]}
  layer.1.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x90d3e80], [6, 0x8e6f7a0], [7, 0x8e21480], [1, 0x8ed5900], [2, 0x8dfb340], [3, 0x90a7bc0], [4, 0x912f520], [5, 0x9114ea0], [6, 0x8eb07c0], [7, 0x8e624a0], [1, 0x8f16920], [2, 0x8e3c360], [3, 0x90e8be0], [4, 0x9170540], [5, 0x9155ec0], [6, 0x8ef17e0], [7, 0x8ea34c0], [1, 0x8f57940], [2, 0x8e7d380], [3, 0x9129c00], [4, 0x91b1560], [5, 0x9196ee0], [6, 0x8f32800], [7, 0x8ee44e0], [1, 0x8f98960], [2, 0x8ebe3a0], [3, 0x916ac20], [4, 0x91f2580], [5, 0x91d7f00], [6, 0x8f73820], [7, 0x8f25500], [1, 0x8fd9980]]}
  layer.1.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8eff3c0], [3, 0x91abc40], [4, 0x92335a0], [5, 0x9218f20], [6, 0x8fb4840], [7, 0x8f66520], [1, 0x901a9a0], [2, 0x8f01460]]}
  layer.1.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8f03500]]}
  layer.1.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x91be100]]}
  layer.2.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9235e80], [5, 0x921c840], [6, 0x8fb8160], [7, 0x8f69e40], [1, 0x901e2c0], [2, 0x8f13920], [3, 0x91ce520], [4, 0x9276ea0]]}
  layer.2.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x925d860], [6, 0x8ff9180], [7, 0x8faae60], [1, 0x905f2e0]]}
  layer.2.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8f54940], [3, 0x920f540], [4, 0x92b7ec0], [5, 0x9261980], [6, 0x8ffd2a0], [7, 0x8faef80], [1, 0x9063400], [2, 0x8f95960]]}
  layer.2.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9250560], [4, 0x92f8ee0], [5, 0x92a29a0], [6, 0x903e2c0]]}
  layer.2.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x89b7360], [1, 0x8ae04a0], [2, 0x89cd0e0], [3, 0x8c6d660], [4, 0x8cfd1c0], [5, 0x8c5d260], [6, 0x8a75a40], [7, 0x89f8380]]}
  layer.2.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8b214c0], [2, 0x8a0e100], [3, 0x8cae680], [4, 0x8d3e1e0]]}
  layer.2.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8c9e280], [6, 0x8ab6a60], [7, 0x8a393a0], [1, 0x8b255e0], [2, 0x8a12220], [3, 0x8cb27a0], [4, 0x8d42300], [5, 0x8cdf2a0]]}
  layer.2.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8af7a80], [7, 0x8a7a3c0], [1, 0x8b66600], [2, 0x8a53240]]}
  layer.2.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8c4ce40]]}
  layer.2.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8cf4000]]}
  layer.2.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8d83b60], [5, 0x8d21b40], [6, 0x8afd420], [7, 0x8a7fd60], [1, 0x8b6bfa0], [2, 0x8a57ba0], [3, 0x8d04420], [4, 0x8dc4b80], [5, 0x8d62b60], [6, 0x8b3e440], [7, 0x8ac0d80], [1, 0x8bacfc0], [2, 0x8a98bc0], [3, 0x8d45440], [4, 0x8e05ba0], [5, 0x8da3b80], [6, 0x8b7f460], [7, 0x8b01da0], [1, 0x8bedfe0], [2, 0x8ad9be0], [3, 0x8d86460], [4, 0x8e46bc0], [5, 0x8de4ba0], [6, 0x8bc0480], [7, 0x8b42dc0], [1, 0x8c2f000], [2, 0x8b1ac00], [3, 0x8dc7480], [4, 0x8e87be0], [5, 0x8e25bc0], [6, 0x8c014a0], [7, 0x8b83de0]]}
  layer.2.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8c70020], [2, 0x8b5bc20], [3, 0x8e084a0], [4, 0x8ec8c00], [5, 0x8e66be0], [6, 0x8c424c0], [7, 0x8bc4e00], [1, 0x8c78240]]}
  layer.2.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b63e40], [3, 0x8e106c0], [4, 0x8ed0e20], [5, 0x8e6ee00], [6, 0x8c4a6e0], [7, 0x8bcd020], [1, 0x8c80460], [2, 0x8ba4e60], [3, 0x8e516e0], [4, 0x8f11e40], [5, 0x8eafe20], [6, 0x8c8b700], [7, 0x8c0e040], [1, 0x8cc1480], [2, 0x8be5e80], [3, 0x8e92700], [4, 0x8f52e60], [5, 0x8ef0e40], [6, 0x8ccc720], [7, 0x8c4f060], [1, 0x8d024a0], [2, 0x8c26ea0], [3, 0x8ed3720], [4, 0x8f93e80], [5, 0x8f31e60], [6, 0x8d0d740], [7, 0x8c90080], [1, 0x8d434c0], [2, 0x8c67ec0], [3, 0x8f14740], [4, 0x8fd4ea0], [5, 0x8f72e80]]}
  layer.2.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8d4e760], [7, 0x8cd10a0], [1, 0x8d844e0], [2, 0x8ca8ee0], [3, 0x8f55760], [4, 0x9015ec0], [5, 0x8fb3ea0], [6, 0x8d50800]]}
  layer.2.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8531a80]]}
  layer.2.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7cd1580]]}
  layer.3.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d88280], [4, 0x7da2900], [5, 0x7d6e420], [6, 0x7dbb720], [7, 0x7cdafe0], [1, 0x7e0b2e0], [2, 0x7ce19a0], [3, 0x7dc92a0]]}
  layer.3.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7de3920], [5, 0x7daf440], [6, 0x7dfc740], [7, 0x7d1c000]]}
  layer.3.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7e4c300], [2, 0x7d229c0], [3, 0x7e0a2c0], [4, 0x7de7a40], [5, 0x7db3560], [6, 0x7e00860], [7, 0x7d20120], [1, 0x7e8d320]]}
  layer.3.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7d639e0], [3, 0x7e4b2e0], [4, 0x7e28a60], [5, 0x7df4580]]}
  layer.3.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7ece340], [2, 0x7d67b00], [3, 0x7e4f400], [4, 0x7e2cb80], [5, 0x7df86a0], [6, 0x7e420c0], [7, 0x7d61980], [1, 0x7f0f360]]}
  layer.3.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d6a300], [6, 0x7db7600], [7, 0x7cd6ec0], [1, 0x7e071c0]]}
  layer.3.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e90420], [4, 0x7e6dba0], [5, 0x7e396c0], [6, 0x7e830e0], [7, 0x7da29a0], [1, 0x7f50380], [2, 0x7da9360], [3, 0x7ed1440]]}
  layer.3.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7eaebc0], [5, 0x7e7a6e0], [6, 0x7ec4100], [7, 0x7de39c0]]}
  layer.3.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7de7ae0]]}
  layer.3.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f91be0]]}
  layer.3.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7deabc0], [3, 0x7f13ce0], [4, 0x7eb4560], [5, 0x7e80080], [6, 0x7ec9aa0], [7, 0x7df7f00], [1, 0x7fa2000], [2, 0x7e2bbe0], [3, 0x7f54d00], [4, 0x7ef5580], [5, 0x7ec10a0], [6, 0x7f0aac0], [7, 0x7e38f20], [1, 0x7fe3020], [2, 0x7e6cc00], [3, 0x7f95d20], [4, 0x7f365a0], [5, 0x7f020c0], [6, 0x7f4bae0], [7, 0x7e79f40], [1, 0x8024040], [2, 0x7eadc20], [3, 0x7fd6d40], [4, 0x7f775c0], [5, 0x7f430e0], [6, 0x7f8cb00], [7, 0x7ebaf60], [1, 0x8065060], [2, 0x7eeec40], [3, 0x8017d60], [4, 0x7fb85e0], [5, 0x7f84100]]}
  layer.3.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7d2fc00], [2, 0x7c3afc0], [3, 0x7cf1cc0], [4, 0x7d0fc20], [5, 0x7cd7e60], [6, 0x7cebb40], [7, 0x7c48300], [1, 0x7d37e20]]}
  layer.3.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b82120], [5, 0x7b4a360], [6, 0x7b5e040], [7, 0x7aba800], [1, 0x7bd6600], [2, 0x7ae3220], [3, 0x7b69320], [4, 0x7bc3140], [5, 0x7b8b380], [6, 0x7b9f060], [7, 0x7afb820], [1, 0x7c17620], [2, 0x7b24240], [3, 0x7baa340], [4, 0x7c04160], [5, 0x7bcc3a0], [6, 0x7be0080], [7, 0x7b3c840], [1, 0x7c58640], [2, 0x7b65260], [3, 0x7beb360], [4, 0x7c45180], [5, 0x7c0d3c0], [6, 0x7c210a0], [7, 0x7b7d860], [1, 0x7c99660], [2, 0x7ba6280], [3, 0x7c2c380], [4, 0x7c861a0], [5, 0x7c4e3e0], [6, 0x7c620c0], [7, 0x7bbe880]]}
  layer.3.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7cda680], [2, 0x7be72a0], [3, 0x7c6d3a0], [4, 0x7cc71c0], [5, 0x7c8f400], [6, 0x7ca30e0], [7, 0x7bff8a0], [1, 0x7cdc720]]}
  layer.3.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7cde7c0]]}
  layer.3.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7be9b80]]}
  layer.4.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c6fc80], [4, 0x7ccaae0], [5, 0x7c92d20], [6, 0x7ca6a00], [7, 0x7c031c0], [1, 0x7ceebe0], [2, 0x7bf9fa0], [3, 0x7cb0ca0]]}
  layer.4.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d0bb00], [5, 0x7cd3d40], [6, 0x7ce7a20], [7, 0x7c441e0]]}
  layer.4.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ae72e0], [4, 0x7b41100], [5, 0x7b09340], [6, 0x7b1d020], [7, 0x7a797e0], [1, 0x7b955e0], [2, 0x7aa2200], [3, 0x7b28300]]}
  layer.4.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7c431e0], [3, 0x7cf9ee0], [4, 0x7d17e40], [5, 0x7ce0080]]}
  layer.4.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7d40040], [2, 0x7c47300], [3, 0x7cfe000], [4, 0x7d1bf60], [5, 0x7ce41a0], [6, 0x7cf45a0], [7, 0x7c50d60], [1, 0x7d81060]]}
  layer.4.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7c88320], [3, 0x7d3f020], [4, 0x7d5cf80], [5, 0x7d251c0]]}
  layer.4.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7d355c0], [7, 0x7c91d80], [1, 0x7dc2080], [2, 0x7c8c440], [3, 0x7d43140], [4, 0x7d610a0], [5, 0x7d292e0], [6, 0x7d765e0]]}
  layer.4.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7cd2da0], [1, 0x7e030a0], [2, 0x7ccd460], [3, 0x7d84160]]}
  layer.4.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x831f680]]}
  layer.4.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x82cb1c0]]}
  layer.4.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8218de0], [1, 0x834e220], [2, 0x82207e0], [3, 0x8336440], [4, 0x83174a0], [5, 0x832faa0], [6, 0x82db5e0], [7, 0x8259e00], [1, 0x838f240], [2, 0x8261800], [3, 0x8377460], [4, 0x83584c0], [5, 0x8370ac0], [6, 0x831c600], [7, 0x829ae20], [1, 0x83d0260], [2, 0x82a2820], [3, 0x83b8480], [4, 0x83994e0], [5, 0x83b1ae0], [6, 0x835d620], [7, 0x82dbe40], [1, 0x8411280], [2, 0x82e3840], [3, 0x83f94a0], [4, 0x83da500], [5, 0x83f2b00], [6, 0x839e640], [7, 0x831ce60], [1, 0x84522a0], [2, 0x8324860], [3, 0x843a4c0]]}
  layer.4.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x841b520], [5, 0x8433b20], [6, 0x83df660], [7, 0x835de80], [1, 0x84932c0], [2, 0x8365880], [3, 0x847b4e0], [4, 0x8423740]]}
  layer.4.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x843bd40], [6, 0x83e7880], [7, 0x83660a0], [1, 0x849b4e0], [2, 0x836daa0], [3, 0x8483700], [4, 0x842b960], [5, 0x847cd60], [6, 0x84288a0], [7, 0x83a70c0], [1, 0x84dc500], [2, 0x83aeac0], [3, 0x84c4720], [4, 0x846c980], [5, 0x84bdd80], [6, 0x84698c0], [7, 0x83e80e0], [1, 0x851d520], [2, 0x83efae0], [3, 0x8505740], [4, 0x84ad9a0], [5, 0x84feda0], [6, 0x84aa8e0], [7, 0x8429100], [1, 0x855e540], [2, 0x8430b00], [3, 0x8546760], [4, 0x84ee9c0], [5, 0x853fdc0], [6, 0x84eb900], [7, 0x846a120], [1, 0x859f560]]}
  layer.4.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8471b20], [3, 0x8587780], [4, 0x852f9e0], [5, 0x8580de0], [6, 0x852c920], [7, 0x84ab140], [1, 0x85e0580], [2, 0x8473bc0]]}
  layer.4.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8475c60]]}
  layer.4.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x858a060]]}
  layer.5.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8541ea0], [5, 0x8584700], [6, 0x8530240], [7, 0x84aea60], [1, 0x85e3ea0], [2, 0x8486080], [3, 0x859a480], [4, 0x8582ec0]]}
  layer.5.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x85c5720], [6, 0x8571260], [7, 0x84efa80], [1, 0x8624ec0]]}
  layer.5.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x84c70a0], [3, 0x85db4a0], [4, 0x85c3ee0], [5, 0x85c9840], [6, 0x8575380], [7, 0x84f3ba0], [1, 0x8628fe0], [2, 0x85080c0]]}
  layer.5.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x861c4c0], [4, 0x8604f00], [5, 0x860a860], [6, 0x85b63a0]]}
  layer.5.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7efbf80], [1, 0x80a6080], [2, 0x7f2fc60], [3, 0x8058d80], [4, 0x7ff9600], [5, 0x7fc5120], [6, 0x7fddf40], [7, 0x7f3cfa0]]}
  layer.5.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x80e70a0], [2, 0x7f70c80], [3, 0x8099da0], [4, 0x803a620]]}
  layer.5.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8006140], [6, 0x801ef60], [7, 0x7f7dfc0], [1, 0x80eb1c0], [2, 0x7f74da0], [3, 0x809dec0], [4, 0x803e740], [5, 0x8047160]]}
  layer.5.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x805ff80], [7, 0x7fbefe0], [1, 0x812c1e0], [2, 0x7fb5dc0]]}
  layer.5.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fb9ee0]]}
  layer.5.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7fcdb20]]}
  layer.5.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x807ffa0], [5, 0x8089a00], [6, 0x8065920], [7, 0x7fc4980], [1, 0x8131b80], [2, 0x7fca300], [3, 0x80dff60], [4, 0x80c0fc0], [5, 0x80caa20], [6, 0x80a6940], [7, 0x80059a0], [1, 0x8172ba0], [2, 0x800b320], [3, 0x8120f80], [4, 0x8101fe0], [5, 0x810ba40], [6, 0x80e7960], [7, 0x80469c0], [1, 0x81b3bc0], [2, 0x804c340], [3, 0x8161fa0], [4, 0x8143000], [5, 0x814ca60], [6, 0x8128980], [7, 0x80879e0], [1, 0x81f4be0], [2, 0x808d360], [3, 0x81a2fc0], [4, 0x8184020], [5, 0x818da80], [6, 0x81699a0], [7, 0x80c8a00]]}
  layer.5.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8235c00], [2, 0x80ce380], [3, 0x81e3fe0], [4, 0x81c5040], [5, 0x81ceaa0], [6, 0x81aa9c0], [7, 0x8109a20], [1, 0x823de20]]}
  layer.5.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x80d65a0], [3, 0x81ec200], [4, 0x81cd260], [5, 0x81d6cc0], [6, 0x81b2be0], [7, 0x8111c40], [1, 0x8246040], [2, 0x81175c0], [3, 0x822d220], [4, 0x820e280], [5, 0x8217ce0], [6, 0x81f3c00], [7, 0x8152c60], [1, 0x8287060], [2, 0x81585e0], [3, 0x826e240], [4, 0x824f2a0], [5, 0x8258d00], [6, 0x8234c20], [7, 0x8193c80], [1, 0x82c8080], [2, 0x8199600], [3, 0x82af260], [4, 0x82902c0], [5, 0x8299d20], [6, 0x8275c40], [7, 0x81d4ca0], [1, 0x83090a0], [2, 0x81da620], [3, 0x82f0280], [4, 0x82d12e0], [5, 0x82dad40]]}
  layer.5.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x82b6c60], [7, 0x8215cc0], [1, 0x834a0c0], [2, 0x821b640], [3, 0x83312a0], [4, 0x8312300], [5, 0x831bd60], [6, 0x82b8d00]]}
  layer.5.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x82bada0]]}
  layer.5.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9e4e2e0]]}
  layer.6.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9d96320], [5, 0x9c94bc0], [6, 0x9b334a0], [7, 0x9a67aa0], [1, 0x9b881c0], [2, 0x9a7d000], [3, 0x9c4a9a0], [4, 0x9dd7340]]}
  layer.6.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9f25800], [2, 0x9e1ef60], [3, 0x9f6f220], [4, 0xa0fbbc0]]}
  layer.6.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa043660], [6, 0x9ef2340], [7, 0x9e5e700], [1, 0x9f29920], [2, 0x9e23080], [3, 0x9f73340], [4, 0xa0ffce0], [5, 0xa084680]]}
  layer.6.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9f33360], [7, 0x9e9f720], [1, 0x9f6a940], [2, 0x9e640a0]]}
  layer.6.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa0c56a0], [6, 0x9f37480], [7, 0x9ea3840], [1, 0x9f6ea60], [2, 0x9e681c0], [3, 0x9fb4ba0], [4, 0xa141540], [5, 0xa1066c0]]}
  layer.6.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9f784a0], [7, 0x9ee4860], [1, 0x9fafa80], [2, 0x9ea91e0]]}
  layer.6.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9e70300], [7, 0x9e0d2c0], [1, 0x9ee47e0], [2, 0x9dddf40], [3, 0x9f2e200], [4, 0xa0baba0], [5, 0xa002640], [6, 0x9eb1320]]}
  layer.6.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa182560], [5, 0xa1476e0], [6, 0x9f7c5c0], [7, 0x9ee8980]]}
  layer.6.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9eecaa0]]}
  layer.6.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9fb43e0]]}
  layer.6.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9eadb40], [3, 0xa007860], [4, 0xa187f00], [5, 0xa14d080], [6, 0x9f81f60], [7, 0x9efcec0], [1, 0x9fc4800], [2, 0x9eeeb60], [3, 0xa048880], [4, 0xa1c8f20], [5, 0xa18e0a0], [6, 0x9fc2f80], [7, 0x9f3dee0], [1, 0xa005820], [2, 0x9f2fb80], [3, 0xa0898a0], [4, 0xa209f40], [5, 0xa1cf0c0], [6, 0xa003fa0], [7, 0x9f7ef00], [1, 0xa046840], [2, 0x9f70ba0], [3, 0xa0ca8c0], [4, 0xa24af60], [5, 0xa2100e0], [6, 0xa044fc0], [7, 0x9fbff20], [1, 0xa087860], [2, 0x9fb1bc0], [3, 0xa10b8e0], [4, 0xa28bf80], [5, 0xa251100]]}
  layer.6.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa085fe0], [7, 0xa000f40], [1, 0xa0c8880], [2, 0x9ff2be0], [3, 0xa14c900], [4, 0xa2ccfa0], [5, 0xa292120], [6, 0xa08e200]]}
  layer.6.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9ca0fe0], [7, 0x9c04980], [1, 0x9d18da0], [2, 0x9bd1d20], [3, 0x9d9bde0], [4, 0x9f28780], [5, 0x9e73b00], [6, 0x9ce2000], [7, 0x9c459a0], [1, 0x9d59dc0], [2, 0x9c12d40], [3, 0x9ddce00], [4, 0x9f697a0], [5, 0x9eb4b20], [6, 0x9d23020], [7, 0x9c869c0], [1, 0x9d9ade0], [2, 0x9c53d60], [3, 0x9e1de20], [4, 0x9faa7c0], [5, 0x9ef5b40], [6, 0x9d64040], [7, 0x9cc79e0], [1, 0x9ddbe00], [2, 0x9c94d80], [3, 0x9e5ee40], [4, 0x9feb7e0], [5, 0x9f36b60], [6, 0x9da5060], [7, 0x9d08a00], [1, 0x9e1ce20], [2, 0x9cd5da0]]}
  layer.6.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9c4ba60], [7, 0x9bb0c60], [1, 0x9c94480], [2, 0x9b892c0], [3, 0x9d53380], [4, 0x9edfd20], [5, 0x9e2b0a0], [6, 0x9c4db00]]}
  layer.6.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9c4fba0]]}
  layer.6.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9bb3540]]}
  layer.7.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9c96d60], [2, 0x9b8cbe0], [3, 0x9d56ca0], [4, 0x9ee3640], [5, 0x9e2e9c0], [6, 0x9c5ffc0], [7, 0x9bc3960], [1, 0x9cd7d80]]}
  layer.7.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9bcdc00], [3, 0x9d97cc0], [4, 0x9f24660], [5, 0x9e6f9e0]]}
  layer.7.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9da9060], [6, 0x9c0aa40], [7, 0x9b6fc40], [1, 0x9c53460], [2, 0x9b482a0], [3, 0x9d12360], [4, 0x9e9ed00], [5, 0x9dea080]]}
  layer.7.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9e9fe60], [4, 0xa02c800], [5, 0x9f77b80], [6, 0x9de6080]]}
  layer.7.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9d16dc0], [3, 0x9ea3f80], [4, 0xa030920], [5, 0x9f7bca0], [6, 0x9dea1a0], [7, 0x9d4a260], [1, 0x9e5e680], [2, 0x9d57de0]]}
  layer.7.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ee4fa0], [4, 0xa071940], [5, 0x9fbccc0], [6, 0x9e2b1c0]]}
  layer.7.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9d8b280], [1, 0x9e9f6a0], [2, 0x9d98e00], [3, 0x9ee90c0], [4, 0xa075a60], [5, 0x9fc0de0], [6, 0x9e2f2e0], [7, 0x9dcc2a0]]}
  layer.7.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9ee06c0], [2, 0x9dd9e20], [3, 0x9f2a0e0], [4, 0xa0b6a80]]}
  layer.7.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa3b3280]]}
  layer.7.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa2f4380]]}
  layer.7.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa3c36a0], [2, 0xa356be0], [3, 0xa473a00], [4, 0xa5f40a0], [5, 0xa57c320], [6, 0xa355b80], [7, 0xa3047a0], [1, 0xa4046c0], [2, 0xa397c00], [3, 0xa4b4a20], [4, 0xa6350c0], [5, 0xa5bd340], [6, 0xa396ba0], [7, 0xa3457c0], [1, 0xa4456e0], [2, 0xa3d8c20], [3, 0xa4f5a40], [4, 0xa6760e0], [5, 0xa5fe360], [6, 0xa3d7bc0], [7, 0xa3867e0], [1, 0xa486700], [2, 0xa419c40], [3, 0xa536a60], [4, 0xa6b7100], [5, 0xa63f380], [6, 0xa418be0], [7, 0xa3c7800], [1, 0xa4c7720], [2, 0xa45ac60], [3, 0xa577a80], [4, 0xa6f8120]]}
  layer.7.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa6803a0], [6, 0xa459c00], [7, 0xa408820], [1, 0xa508740], [2, 0xa49bc80], [3, 0xa5b8aa0], [4, 0xa739140], [5, 0xa6885c0]]}
  layer.7.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa461e20], [7, 0xa410a40], [1, 0xa510960], [2, 0xa4a3ea0], [3, 0xa5c0cc0], [4, 0xa741360], [5, 0xa6907e0], [6, 0xa4a2e40], [7, 0xa451a60], [1, 0xa551980], [2, 0xa4e4ec0], [3, 0xa601ce0], [4, 0xa782380], [5, 0xa6d1800], [6, 0xa4e3e60], [7, 0xa492a80], [1, 0xa5929a0], [2, 0xa525ee0], [3, 0xa642d00], [4, 0xa7c33a0], [5, 0xa712820], [6, 0xa524e80], [7, 0xa4d3aa0], [1, 0xa5d39c0], [2, 0xa566f00], [3, 0xa683d20], [4, 0xa8043c0], [5, 0xa753840], [6, 0xa565ea0], [7, 0xa514ac0], [1, 0xa6149e0], [2, 0xa5a7f20]]}
  layer.7.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6c4d40], [4, 0xa8453e0], [5, 0xa794860], [6, 0xa5a6ec0], [7, 0xa555ae0], [1, 0xa655a00], [2, 0xa5e8f40], [3, 0xa6c6de0]]}
  layer.7.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa557b80]]}
  layer.7.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa657aa0]]}
  layer.8.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa5eafe0], [3, 0xa6c8e80], [4, 0xa847cc0], [5, 0xa797140], [6, 0xa5a97a0], [7, 0xa567fa0], [1, 0xa667ec0], [2, 0xa62c000]]}
  layer.8.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa709ea0], [4, 0xa888ce0], [5, 0xa7d8160], [6, 0xa5ea7c0]]}
  layer.8.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa5a8fc0], [1, 0xa6a8ee0], [2, 0xa66d020], [3, 0xa70dfc0], [4, 0xa88ce00], [5, 0xa7dc280], [6, 0xa5ee8e0], [7, 0xa5e9fe0]]}
  layer.8.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa6e9f00], [2, 0xa6ae040], [3, 0xa74efe0], [4, 0xa8cde20]]}
  layer.8.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa3d9240], [5, 0xa39e3c0], [6, 0xa19a4a0], [7, 0xa14e200], [1, 0xa215b40], [2, 0xa13fea0], [3, 0xa299bc0], [4, 0xa41a260]]}
  layer.8.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa3df3e0], [6, 0xa1db4c0], [7, 0xa18f220], [1, 0xa256b60]]}
  layer.8.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa180ec0], [3, 0xa2dabe0], [4, 0xa45b280], [5, 0xa3e3500], [6, 0xa1df5e0], [7, 0xa193340], [1, 0xa25ac80], [2, 0xa1c1ee0]]}
  layer.8.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa31bc00], [4, 0xa49c2a0], [5, 0xa424520], [6, 0xa220600]]}
  layer.8.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa1d4ba0]]}
  layer.8.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa224720]]}
  layer.8.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa009160], [1, 0xa0d0aa0], [2, 0x9ffae00], [3, 0xa154b20], [4, 0xa2d51c0], [5, 0xa29a340], [6, 0xa096420], [7, 0xa04a180], [1, 0xa111ac0], [2, 0xa03be20], [3, 0xa195b40], [4, 0xa3161e0], [5, 0xa2db360], [6, 0xa0d7440], [7, 0xa08b1a0], [1, 0xa152ae0], [2, 0xa07ce40], [3, 0xa1d6b60], [4, 0xa357200], [5, 0xa31c380], [6, 0xa118460], [7, 0xa0cc1c0], [1, 0xa193b00], [2, 0xa0bde60], [3, 0xa217b80], [4, 0xa398220], [5, 0xa35d3a0], [6, 0xa159480], [7, 0xa10d1e0], [1, 0xa1d4b20], [2, 0xa0fee80], [3, 0xa258ba0]]}
  layer.8.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa29c4e0], [2, 0xa204780], [3, 0xa3215a0], [4, 0xa4a1c40], [5, 0xa429ec0], [6, 0xa234b40], [7, 0xa1e4fc0], [1, 0xa2a4700]]}
  layer.8.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa20c9a0], [3, 0xa3297c0], [4, 0xa4a9e60], [5, 0xa4320e0], [6, 0xa23cd60], [7, 0xa1ed1e0], [1, 0xa2ac920], [2, 0xa24d9c0], [3, 0xa36a7e0], [4, 0xa4eae80], [5, 0xa473100], [6, 0xa27dd80], [7, 0xa22e200], [1, 0xa2ed940], [2, 0xa28e9e0], [3, 0xa3ab800], [4, 0xa52bea0], [5, 0xa4b4120], [6, 0xa2beda0], [7, 0xa26f220], [1, 0xa32e960], [2, 0xa2cfa00], [3, 0xa3ec820], [4, 0xa56cec0], [5, 0xa4f5140], [6, 0xa2ffdc0], [7, 0xa2b0240], [1, 0xa36f980], [2, 0xa310a20], [3, 0xa42d840], [4, 0xa5adee0], [5, 0xa536160]]}
  layer.8.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa340de0], [7, 0xa2f1260], [1, 0xa3b09a0], [2, 0xa351a40], [3, 0xa46e860], [4, 0xa5eef00], [5, 0xa577180], [6, 0xa342e80]]}
  layer.8.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ff5bc0]]}
  layer.8.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa344f20]]}
  layer.9.attention.self.query.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x94adc60], [4, 0x95e07e0], [5, 0x954bb40], [6, 0x9318060], [7, 0x928a5a0], [1, 0x933ea20], [2, 0x92ade80], [3, 0x94eec80]]}
  layer.9.attention.self.query.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9621800], [5, 0x958cb60], [6, 0x9359080], [7, 0x92cb5c0]]}
  layer.9.attention.self.key.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x937fa40], [2, 0x92eeea0], [3, 0x952fca0], [4, 0x9625920], [5, 0x9590c80], [6, 0x935d1a0], [7, 0x92cf6e0], [1, 0x93c0a60]]}
  layer.9.attention.self.key.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x932fec0], [3, 0x9570cc0], [4, 0x9666940], [5, 0x95d1ca0]]}
  layer.9.attention.self.value.weight:               {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9401a80], [2, 0x9333fe0], [3, 0x9574de0], [4, 0x966aa60], [5, 0x95d5dc0], [6, 0x939ea00], [7, 0x9310f40], [1, 0x9442aa0]]}
  layer.9.attention.self.value.bias:                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9375000], [3, 0x95b5e00], [4, 0x96aba80], [5, 0x9616de0]]}
  layer.9.attention.output.dense.weight:             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x922be40], [3, 0x946cc40], [4, 0x959f7c0], [5, 0x950ab20], [6, 0x92d7040], [7, 0x9249580], [1, 0x92fda00], [2, 0x926ce60]]}
  layer.9.attention.output.dense.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x95b9f20], [4, 0x96afba0], [5, 0x961af00], [6, 0x93e12a0]]}
  layer.9.attention.output.LayerNorm.weight:         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x93e53c0]]}
  layer.9.attention.output.LayerNorm.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9354020]]}
  layer.9.intermediate.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9485b80], [2, 0x937c220], [3, 0x95bf8c0], [4, 0x96b5540], [5, 0x96208a0], [6, 0x93f57e0], [7, 0x9364440], [1, 0x94c6ba0], [2, 0x93bd240], [3, 0x96008e0], [4, 0x96f6560], [5, 0x96618c0], [6, 0x9436800], [7, 0x93a5460], [1, 0x9507bc0], [2, 0x93fe260], [3, 0x9641900], [4, 0x9737580], [5, 0x96a28e0], [6, 0x9477820], [7, 0x93e6480], [1, 0x9548be0], [2, 0x943f280], [3, 0x9682920], [4, 0x97785a0], [5, 0x96e3900], [6, 0x94b8840], [7, 0x94274a0], [1, 0x9589c00], [2, 0x94802a0], [3, 0x96c3940], [4, 0x97b95c0]]}
  layer.9.intermediate.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9724920], [6, 0x94f9860], [7, 0x94684c0], [1, 0x95cac20], [2, 0x94c12c0], [3, 0x9704960], [4, 0x97fa5e0], [5, 0x972cb40]]}
  layer.9.output.dense.weight:                       {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x933b800], [6, 0x9107d20], [7, 0x907db40], [1, 0x9131fc0], [2, 0x9064520], [3, 0x92e2220], [4, 0x93d45c0], [5, 0x937c820], [6, 0x9148d40], [7, 0x90beb60], [1, 0x9172fe0], [2, 0x90a5540], [3, 0x9323240], [4, 0x94155e0], [5, 0x93bd840], [6, 0x9189d60], [7, 0x90ffb80], [1, 0x91b4000], [2, 0x90e6560], [3, 0x9364260], [4, 0x9456600], [5, 0x93fe860], [6, 0x91cad80], [7, 0x9140ba0], [1, 0x91f5020], [2, 0x9127580], [3, 0x93a5280], [4, 0x9497620], [5, 0x943f880], [6, 0x920bda0], [7, 0x9181bc0], [1, 0x9236040]]}
  layer.9.output.dense.bias:                         {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x92fd000], [5, 0x92a6ac0], [6, 0x90423e0], [7, 0x8ff40c0], [1, 0x90a8540], [2, 0x8fdaaa0], [3, 0x92587a0], [4, 0x92ff0a0]]}
  layer.9.output.LayerNorm.weight:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9301140]]}
  layer.9.output.LayerNorm.bias:                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x92a93a0]]}
  layer.10.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9044cc0], [7, 0x8ff79e0], [1, 0x90abe60], [2, 0x8fde3c0], [3, 0x925c0c0], [4, 0x9311560], [5, 0x92b97c0], [6, 0x9085ce0]]}
  layer.10.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9038a00], [1, 0x90ece80], [2, 0x901f3e0], [3, 0x929d0e0]]}
  layer.10.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9352580], [5, 0x92fa7e0], [6, 0x90c6d00], [7, 0x903cb20], [1, 0x90f0fa0], [2, 0x9023500], [3, 0x92a1200], [4, 0x93935a0]]}
  layer.10.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8feffa0], [1, 0x90a4420], [2, 0x8fd6980], [3, 0x9254680]]}
  layer.10.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x94d8640], [5, 0x94808a0], [6, 0x924cdc0], [7, 0x91c2be0], [1, 0x9277060], [2, 0x9168de0], [3, 0x93e6ae0], [4, 0x9519660]]}
  layer.10.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x94c18c0], [6, 0x928dde0], [7, 0x9203c00], [1, 0x92b8080]]}
  layer.10.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x91a9e00], [3, 0x9427b00], [4, 0x955a680], [5, 0x94c59e0], [6, 0x9291f00], [7, 0x9207d20], [1, 0x92bc1a0], [2, 0x91eae20]]}
  layer.10.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9468b20], [4, 0x959b6a0], [5, 0x9506a00], [6, 0x92d2f20]]}
  layer.10.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9b32b20]]}
  layer.10.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9a313c0]]}
  layer.10.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x989f0a0], [7, 0x97d36a0], [1, 0x9932d40], [2, 0x98293e0], [3, 0x9a2fb80], [4, 0x9b42f40], [5, 0x9a417e0], [6, 0x98e00c0], [7, 0x98146c0], [1, 0x9973d60], [2, 0x986a400], [3, 0x9a70ba0], [4, 0x9b83f60], [5, 0x9a82800], [6, 0x99210e0], [7, 0x98556e0], [1, 0x99b4d80], [2, 0x98ab420], [3, 0x9ab1bc0], [4, 0x9bc4f80], [5, 0x9ac3820], [6, 0x9962100], [7, 0x9896700], [1, 0x99f5da0], [2, 0x98ec440], [3, 0x9af2be0], [4, 0x9c05fa0], [5, 0x9b04840], [6, 0x99a3120], [7, 0x98d7720], [1, 0x9a36dc0], [2, 0x992d460]]}
  layer.10.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9b33c00], [4, 0x9c46fc0], [5, 0x9b45860], [6, 0x99e4140], [7, 0x9918740], [1, 0x9a77de0], [2, 0x996e480], [3, 0x9b3be20]]}
  layer.10.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9c4f1e0], [5, 0x9b4da80], [6, 0x99ec360], [7, 0x9920960], [1, 0x9a80000], [2, 0x99766a0], [3, 0x9b44040], [4, 0x9c90200], [5, 0x9b8eaa0], [6, 0x9a2d380], [7, 0x9961980], [1, 0x9ac1020], [2, 0x99b76c0], [3, 0x9b85060], [4, 0x9cd1220], [5, 0x9bcfac0], [6, 0x9a6e3a0], [7, 0x99a29a0], [1, 0x9b02040], [2, 0x99f86e0], [3, 0x9bc6080], [4, 0x9d12240], [5, 0x9c10ae0], [6, 0x9aaf3c0], [7, 0x99e39c0], [1, 0x9b43060], [2, 0x9a39700], [3, 0x9c070a0], [4, 0x9d53260], [5, 0x9c51b00], [6, 0x9af03e0], [7, 0x9a249e0]]}
  layer.10.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9b84080], [2, 0x9a7a720], [3, 0x9c480c0], [4, 0x9d94280], [5, 0x9c92b20], [6, 0x9b31400], [7, 0x9a65a00], [1, 0x9b86120]]}
  layer.10.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9cd5be0]]}
  layer.10.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9b744c0]]}
  layer.11.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9aa8ac0], [1, 0x9bc91e0], [2, 0x9abe020], [3, 0x9c8b9c0], [4, 0x9e18360], [5, 0x9ce6000], [6, 0x9b848e0], [7, 0x9ae9ae0]]}
  layer.11.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9c0a200], [2, 0x9aff040], [3, 0x9ccc9e0], [4, 0x9e59380]]}
  layer.11.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9d27020], [6, 0x9bc5900], [7, 0x9b2ab00], [1, 0x9c0e320], [2, 0x9b03160], [3, 0x9cd0b00], [4, 0x9e5d4a0], [5, 0x9d68040]]}
  layer.11.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9c06920], [7, 0x9b6bb20], [1, 0x9c4f340], [2, 0x9b44180]]}
  layer.11.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x968c4a0], [7, 0x9639040], [1, 0x975e8a0], [2, 0x9654f40], [3, 0x985b6e0], [4, 0x995ff00], [5, 0x988eb80], [6, 0x96cd4c0]]}
  layer.11.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9810c00], [4, 0x9906880], [5, 0x9838de0], [6, 0x9646b20]]}
  layer.11.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x95b5780], [1, 0x9717ee0], [2, 0x960e580], [3, 0x9814d20], [4, 0x990a9a0], [5, 0x983cf00], [6, 0x964ac40], [7, 0x95f67a0]]}
  layer.11.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9758f00], [2, 0x964f5a0], [3, 0x9855d40], [4, 0x994b9c0]]}
  layer.11.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x994fae0]]}
  layer.11.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x987e760]]}
  layer.11.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9501a80], [7, 0x94706e0], [1, 0x95d2e40], [2, 0x94c94e0], [3, 0x970cb80], [4, 0x9802800], [5, 0x9734d60], [6, 0x9542aa0], [7, 0x94b1700], [1, 0x9613e60], [2, 0x950a500], [3, 0x974dba0], [4, 0x9843820], [5, 0x9775d80], [6, 0x9583ac0], [7, 0x94f2720], [1, 0x9654e80], [2, 0x954b520], [3, 0x978ebc0], [4, 0x9884840], [5, 0x97b6da0], [6, 0x95c4ae0], [7, 0x9533740], [1, 0x9695ea0], [2, 0x958c540], [3, 0x97cfbe0], [4, 0x98c5860], [5, 0x97f7dc0], [6, 0x9605b00], [7, 0x9574760], [1, 0x96d6ec0], [2, 0x95cd560]]}
  layer.11.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x967a060], [1, 0x979f8c0], [2, 0x9695f60], [3, 0x989c700], [4, 0x99a0f20], [5, 0x98cfba0], [6, 0x970e4e0], [7, 0x9682280]]}
  layer.11.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x97a7ae0], [2, 0x969e180], [3, 0x98a4920], [4, 0x99a9140], [5, 0x98d7dc0], [6, 0x9716700], [7, 0x968a4a0], [1, 0x97e8b00], [2, 0x96df1a0], [3, 0x98e5940], [4, 0x99ea160], [5, 0x9918de0], [6, 0x9757720], [7, 0x96cb4c0], [1, 0x9829b20], [2, 0x97201c0], [3, 0x9926960], [4, 0x9a2b180], [5, 0x9959e00], [6, 0x9798740], [7, 0x970c4e0], [1, 0x986ab40], [2, 0x97611e0], [3, 0x9967980], [4, 0x9a6c1a0], [5, 0x999ae20], [6, 0x97d9760], [7, 0x974d500], [1, 0x98abb60], [2, 0x97a2200], [3, 0x99a89a0], [4, 0x9aad1c0]]}
  layer.11.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x99dbe40], [6, 0x981a780], [7, 0x978e520], [1, 0x98ecb80], [2, 0x97e3220], [3, 0x99e99c0], [4, 0x9aee1e0], [5, 0x99ddee0]]}
  layer.11.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x99dff80]]}
  layer.11.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x91adce0]]}
  layer.12.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x981d060], [7, 0x9790e00], [1, 0x98f04a0], [2, 0x97e6b40], [3, 0x99ed2e0], [4, 0x9af1b00], [5, 0x99f03a0], [6, 0x985e080]]}
  layer.12.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5741b40], [4, 0x57074e0], [5, 0x5708520], [6, 0x5756860]]}
  layer.12.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b9b900], [5, 0x5b5a0e0], [6, 0x5c452c0], [7, 0x5bbf9e0], [1, 0x5c62a20], [2, 0x5b3a940], [3, 0x5bc32e0], [4, 0x5bdc920]]}
  layer.12.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b9b100], [6, 0x5c862e0], [7, 0x5c00a00], [1, 0x5ca3a40]]}
  layer.12.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5c1d940], [5, 0x5b9f220], [6, 0x5c8a400], [7, 0x5c04b20], [1, 0x5ca7b60], [2, 0x5b7c1a0], [3, 0x5c04b40], [4, 0x5c5e960]]}
  layer.12.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5be0240], [6, 0x5ccb420], [7, 0x5c45b40], [1, 0x5ce8b80]]}
  layer.12.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5bbd1c0], [3, 0x5c45b60], [4, 0x5c9f980], [5, 0x5be4360], [6, 0x5ccf540], [7, 0x5c49c60], [1, 0x5cecca0], [2, 0x5bfe1e0]]}
  layer.12.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5c00180], [7, 0x5b7a8a0], [1, 0x5c1d8e0], [2, 0x5af5800]]}
  layer.12.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5d11de0]]}
  layer.12.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5c8b4c0]]}
  layer.12.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d2e500], [2, 0x5c40a80], [3, 0x5c89c80], [4, 0x5ce3aa0], [5, 0x5c28480], [6, 0x5d22200], [7, 0x5c9b8e0], [1, 0x5d6f520], [2, 0x5c81aa0], [3, 0x5ccaca0], [4, 0x5d24ac0], [5, 0x5c694a0], [6, 0x5d63220], [7, 0x5cdc900], [1, 0x5db0540], [2, 0x5cc2ac0], [3, 0x5d0bcc0], [4, 0x5d65ae0], [5, 0x5caa4c0], [6, 0x5da4240], [7, 0x5d1d920], [1, 0x5df1560], [2, 0x5d03ae0], [3, 0x5d4cce0], [4, 0x5da6b00], [5, 0x5ceb4e0], [6, 0x5de5260], [7, 0x5d5e940], [1, 0x5e32580], [2, 0x5d44b00], [3, 0x5d8dd00], [4, 0x5de7b20]]}
  layer.12.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d2c500], [6, 0x5e26280], [7, 0x5d9f960], [1, 0x5e735a0], [2, 0x5d85b20], [3, 0x5dced20], [4, 0x5e28b40], [5, 0x5d34720]]}
  layer.12.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5e2e4a0], [7, 0x5da7b80], [1, 0x5e7b7c0], [2, 0x5d8dd40], [3, 0x5dd6f40], [4, 0x5e30d60], [5, 0x5d3c940], [6, 0x5e6f4c0], [7, 0x5de8ba0], [1, 0x5ebc7e0], [2, 0x5dced60], [3, 0x5e17f60], [4, 0x5e71d80], [5, 0x5d7d960], [6, 0x5eb04e0], [7, 0x5e29bc0], [1, 0x5efd800], [2, 0x5e0fd80], [3, 0x5e58f80], [4, 0x5eb2da0], [5, 0x5dbe980], [6, 0x5ef1500], [7, 0x5e6abe0], [1, 0x5f3e820], [2, 0x5e50da0], [3, 0x5e99fa0], [4, 0x5ef3dc0], [5, 0x5dff9a0], [6, 0x5f32520], [7, 0x5eabc00], [1, 0x5f7f840], [2, 0x5e91dc0]]}
  layer.12.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5a8e5e0], [6, 0x5b3a840], [7, 0x5af1e60], [1, 0x5b546c0], [2, 0x5a694e0], [3, 0x5ab4f80], [4, 0x5ad1ea0], [5, 0x5a90680]]}
  layer.12.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x59fea20]]}
  layer.12.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59fc180]]}
  layer.13.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a777e0], [7, 0x5a6bd00], [1, 0x5ace560], [2, 0x59e3380], [3, 0x5a2ee20], [4, 0x5a0ee40], [5, 0x5a0c5a0], [6, 0x5ab8800]]}
  layer.13.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5aacd20], [1, 0x5b0f580], [2, 0x5a243a0], [3, 0x5a6fe40]]}
  layer.13.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5a4fe60], [5, 0x5a4d5c0], [6, 0x5af9820], [7, 0x5ab0e40], [1, 0x5b136a0], [2, 0x5a284c0], [3, 0x5a73f60], [4, 0x5a90e80]]}
  layer.13.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ac8bc0], [2, 0x59dd9e0], [3, 0x5a29480], [4, 0x59fa900]]}
  layer.13.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5b56760], [2, 0x5a6b580], [3, 0x5ab7020], [4, 0x5ad3f40], [5, 0x5a92720], [6, 0x5b3d120], [7, 0x5af4740], [1, 0x5b97780]]}
  layer.13.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5aac5a0], [3, 0x5af8040], [4, 0x5b14f60], [5, 0x5ad3740]]}
  layer.13.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b7e140], [7, 0x5b35760], [1, 0x5bd87a0], [2, 0x5ab06c0], [3, 0x5afc160], [4, 0x5b19080], [5, 0x5ad7860], [6, 0x5bbf160]]}
  layer.13.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5b76780], [1, 0x5c197c0], [2, 0x5af16e0], [3, 0x5b3d180]]}
  layer.13.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x62afb60]]}
  layer.13.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x618e420]]}
  layer.13.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x62069c0], [4, 0x6256d80], [5, 0x615f8a0], [6, 0x6292420], [7, 0x61cec00], [1, 0x62bff80], [2, 0x619e840], [3, 0x62479e0], [4, 0x6297da0], [5, 0x61a08c0], [6, 0x62d3440], [7, 0x620fc20], [1, 0x6300fa0], [2, 0x61df860], [3, 0x6288a00], [4, 0x62d8dc0], [5, 0x61e18e0], [6, 0x6314460], [7, 0x6250c40], [1, 0x6341fc0], [2, 0x6220880], [3, 0x62c9a20], [4, 0x6319de0], [5, 0x6222900], [6, 0x6355480], [7, 0x6291c60], [1, 0x6382fe0], [2, 0x62618a0], [3, 0x630aa40], [4, 0x635ae00], [5, 0x6263920], [6, 0x63964a0]]}
  layer.13.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x62d2c80], [1, 0x63c4000], [2, 0x62a28c0], [3, 0x634ba60], [4, 0x639be20], [5, 0x62a4940], [6, 0x63d74c0], [7, 0x62daea0]]}
  layer.13.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x63cc220], [2, 0x62aaae0], [3, 0x6353c80], [4, 0x63a4040], [5, 0x62acb60], [6, 0x63df6e0], [7, 0x62e30c0], [1, 0x640d240], [2, 0x62ebb00], [3, 0x6394ca0], [4, 0x63e5060], [5, 0x62edb80], [6, 0x6420700], [7, 0x63240e0], [1, 0x644e260], [2, 0x632cb20], [3, 0x63d5cc0], [4, 0x6426080], [5, 0x632eba0], [6, 0x6461720], [7, 0x6365100], [1, 0x648f280], [2, 0x636db40], [3, 0x6416ce0], [4, 0x64670a0], [5, 0x636fbc0], [6, 0x64a2740], [7, 0x63a6120], [1, 0x64d02a0], [2, 0x63aeb60], [3, 0x6457d00], [4, 0x64a80c0]]}
  layer.13.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x63b0be0], [6, 0x64e3760], [7, 0x63e7140], [1, 0x65112c0], [2, 0x63efb80], [3, 0x6498d20], [4, 0x64e90e0], [5, 0x63b2c80]]}
  layer.13.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6432c40]]}
  layer.13.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x64dbde0]]}
  layer.14.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x652c1a0], [5, 0x63f5d40], [6, 0x6527060], [7, 0x642aa40], [1, 0x65953a0], [2, 0x6443060], [3, 0x64ec200], [4, 0x656d1c0]]}
  layer.14.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6436d60], [6, 0x6568080], [7, 0x646ba60], [1, 0x65d63c0]]}
  layer.14.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6484080], [3, 0x652d220], [4, 0x65ae1e0], [5, 0x643ae80], [6, 0x656c1a0], [7, 0x646fb80], [1, 0x65da4e0], [2, 0x64c50a0]]}
  layer.14.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x656e240], [4, 0x65ef200], [5, 0x647bea0], [6, 0x65ad1c0]]}
  layer.14.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x60659e0], [4, 0x60fd740], [5, 0x5fcc420], [6, 0x60fefa0], [7, 0x603b780], [1, 0x611df60], [2, 0x602cc00], [3, 0x60a6a00]]}
  layer.14.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5ff0ca0], [1, 0x60c48e0], [2, 0x5fd6e60], [3, 0x6020060]]}
  layer.14.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6079e80], [5, 0x5f85a60], [6, 0x60b85e0], [7, 0x5ff4dc0], [1, 0x60c8a00], [2, 0x5fdaf80], [3, 0x6024180], [4, 0x60baea0]]}
  layer.14.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fc6a80], [6, 0x60f9600], [7, 0x6035de0], [1, 0x6109a20]]}
  layer.14.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x610db40]]}
  layer.14.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x601c7e0]]}
  layer.14.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5edafc0], [4, 0x5f34de0], [5, 0x5e409c0], [6, 0x5f73540], [7, 0x5eecc20], [1, 0x5fc0860], [2, 0x5ed2de0], [3, 0x5f1bfe0], [4, 0x5f75e00], [5, 0x5e819e0], [6, 0x5fb4560], [7, 0x5f2dc40], [1, 0x6001880], [2, 0x5f13e00], [3, 0x5f5d000], [4, 0x5fb6e20], [5, 0x5ec2a00], [6, 0x5ff5580], [7, 0x5f6ec60], [1, 0x60428a0], [2, 0x5f54e20], [3, 0x5f9e020], [4, 0x5ff7e40], [5, 0x5f03a20], [6, 0x60365a0], [7, 0x5fafc80], [1, 0x60838c0], [2, 0x5f95e40], [3, 0x5fdf040], [4, 0x6038e60], [5, 0x5f44a40], [6, 0x60775c0]]}
  layer.14.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x613e760], [5, 0x600d440], [6, 0x613ffc0], [7, 0x607c7a0], [1, 0x615ef80], [2, 0x606dc20], [3, 0x60e7a20], [4, 0x6146980]]}
  layer.14.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6015660], [6, 0x61481e0], [7, 0x60849c0], [1, 0x61671a0], [2, 0x6075e40], [3, 0x60efc40], [4, 0x614eba0], [5, 0x6056680], [6, 0x6189200], [7, 0x60c59e0], [1, 0x61a81c0], [2, 0x60b6e60], [3, 0x6130c60], [4, 0x618fbc0], [5, 0x60976a0], [6, 0x61ca220], [7, 0x6106a00], [1, 0x61e91e0], [2, 0x60f7e80], [3, 0x6171c80], [4, 0x61d0be0], [5, 0x60d86c0], [6, 0x620b240], [7, 0x6147a20], [1, 0x622a200], [2, 0x6138ea0], [3, 0x61b2ca0], [4, 0x6211c00], [5, 0x61196e0], [6, 0x624c260], [7, 0x6188a40], [1, 0x626b220]]}
  layer.14.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6179ec0], [3, 0x61f3cc0], [4, 0x6252c20], [5, 0x615a700], [6, 0x628d280], [7, 0x61c9a60], [1, 0x62ac240], [2, 0x617bf60]]}
  layer.14.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x617e000]]}
  layer.14.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x61f65a0]]}
  layer.15.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6513360], [2, 0x63f1c20], [3, 0x649adc0], [4, 0x64eb180], [5, 0x63b4d20], [6, 0x64e6040], [7, 0x63e9a20], [1, 0x6554380]]}
  layer.15.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50c2560], [6, 0x50fbb80], [7, 0x50bec80], [1, 0x514b740]]}
  layer.15.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50c2da0], [1, 0x514f860], [2, 0x50d5a60], [3, 0x5106660], [4, 0x50c6ec0], [5, 0x50ca7a0], [6, 0x51100c0], [7, 0x5103dc0]]}
  layer.15.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5190880], [2, 0x5116a80], [3, 0x5147680], [4, 0x5107ee0]]}
  layer.15.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5144de0], [1, 0x51949a0], [2, 0x511aba0], [3, 0x514b7a0], [4, 0x510c000], [5, 0x510c000], [6, 0x5151920], [7, 0x5185e00]]}
  layer.15.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51d59c0], [2, 0x515bbc0], [3, 0x518c7c0], [4, 0x514d020]]}
  layer.15.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x514d020], [6, 0x5192940], [7, 0x51c6e20], [1, 0x51d9ae0], [2, 0x515fce0], [3, 0x51908e0], [4, 0x5151140], [5, 0x518e040]]}
  layer.15.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50d1940], [3, 0x5102540], [4, 0x50c2da0], [5, 0x50c6680]]}
  layer.15.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x51d7aa0]]}
  layer.15.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x520a720]]}
  layer.15.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x521d3e0], [2, 0x51a4620], [3, 0x51d5220], [4, 0x5195a80], [5, 0x51d2980], [6, 0x51e7ec0], [7, 0x521ab40], [1, 0x525e400], [2, 0x51e5640], [3, 0x5216240], [4, 0x51d6aa0], [5, 0x52139a0], [6, 0x5228ee0], [7, 0x525bb60], [1, 0x529f420], [2, 0x5226660], [3, 0x5257260], [4, 0x5217ac0], [5, 0x52549c0], [6, 0x5269f00], [7, 0x529cb80], [1, 0x52e0440], [2, 0x5267680], [3, 0x5298280], [4, 0x5258ae0], [5, 0x52959e0], [6, 0x52aaf20], [7, 0x52ddba0], [1, 0x5321460], [2, 0x52a86a0], [3, 0x52d92a0], [4, 0x5299b00]]}
  layer.15.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52d6a00], [6, 0x52ebf40], [7, 0x531ebc0], [1, 0x5362480], [2, 0x52e96c0], [3, 0x531a2c0], [4, 0x52dab20], [5, 0x52dec20]]}
  layer.15.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x52f4160], [7, 0x5326de0], [1, 0x536a6a0], [2, 0x52f18e0], [3, 0x53224e0], [4, 0x52e2d40], [5, 0x52e6e40], [6, 0x5335180], [7, 0x5367e00], [1, 0x53ab6c0], [2, 0x5332900], [3, 0x5363500], [4, 0x5323d60], [5, 0x5327e60], [6, 0x53761a0], [7, 0x53a8e20], [1, 0x53ec6e0], [2, 0x5373920], [3, 0x53a4520], [4, 0x5364d80], [5, 0x5368e80], [6, 0x53b71c0], [7, 0x53e9e40], [1, 0x542d700], [2, 0x53b4940], [3, 0x53e5540], [4, 0x53a5da0], [5, 0x53a9ea0], [6, 0x53f81e0], [7, 0x542ae60], [1, 0x546e720], [2, 0x53f5960]]}
  layer.15.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x51d3960], [7, 0x5207e40], [1, 0x521ab00], [2, 0x51a0d00], [3, 0x51d1900], [4, 0x5192160], [5, 0x51cf060], [6, 0x51d5a00]]}
  layer.15.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fb1140]]}
  layer.15.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4fb1140]]}
  layer.16.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb1140], [4, 0x4fb2180], [5, 0x4fb2180], [6, 0x4fb2180], [7, 0x4fb2180], [1, 0x4fc1560], [2, 0x4fc1560], [3, 0x4ff2160]]}
  layer.16.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4ff31a0], [5, 0x4ff31a0], [6, 0x4ff31a0], [7, 0x4ff31a0]]}
  layer.16.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5002580], [2, 0x5002580], [3, 0x5033180], [4, 0x4ff72c0], [5, 0x4ff72c0], [6, 0x4ff72c0], [7, 0x4ff72c0], [1, 0x50435a0]]}
  layer.16.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50435a0], [3, 0x50741a0], [4, 0x50382e0], [5, 0x50382e0]]}
  layer.16.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x50845c0], [2, 0x50476c0], [3, 0x50782c0], [4, 0x503c400], [5, 0x503c400], [6, 0x5038b20], [7, 0x5038b20], [1, 0x50c55e0]]}
  layer.16.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50886e0], [3, 0x50b92e0], [4, 0x507d420], [5, 0x507d420]]}
  layer.16.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5079b40], [7, 0x5079b40], [1, 0x5106600], [2, 0x508c800], [3, 0x50bd400], [4, 0x5081540], [5, 0x5081540], [6, 0x50bab60]]}
  layer.16.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50bab60], [1, 0x5147620], [2, 0x50cd820], [3, 0x50fe420]]}
  layer.16.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x570b600]]}
  layer.16.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x570c640]]}
  layer.16.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x575a980], [7, 0x574de60], [1, 0x57a02c0], [2, 0x56f3840], [3, 0x57474e0], [4, 0x571ba20], [5, 0x571ca60], [6, 0x579b9a0], [7, 0x578ee80], [1, 0x57e12e0], [2, 0x5734860], [3, 0x5788500], [4, 0x575ca40], [5, 0x575da80], [6, 0x57dc9c0], [7, 0x57cfea0], [1, 0x5822300], [2, 0x5775880], [3, 0x57c9520], [4, 0x579da60], [5, 0x579eaa0], [6, 0x581d9e0], [7, 0x5810ec0], [1, 0x5863320], [2, 0x57b68a0], [3, 0x580a540], [4, 0x57dea80], [5, 0x57dfac0], [6, 0x585ea00], [7, 0x5851ee0], [1, 0x58a4340], [2, 0x57f78c0]]}
  layer.16.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x584b560], [4, 0x581faa0], [5, 0x5820ae0], [6, 0x589fa20], [7, 0x5892f00], [1, 0x58e5360], [2, 0x58388e0], [3, 0x5853780]]}
  layer.16.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5827cc0], [5, 0x5828d00], [6, 0x58a7c40], [7, 0x589b120], [1, 0x58ed580], [2, 0x5840b00], [3, 0x585b9a0], [4, 0x5868ce0], [5, 0x5869d20], [6, 0x58e8c60], [7, 0x58dc140], [1, 0x592e5a0], [2, 0x5881b20], [3, 0x589c9c0], [4, 0x58a9d00], [5, 0x58aad40], [6, 0x5929c80], [7, 0x591d160], [1, 0x596f5c0], [2, 0x58c2b40], [3, 0x58dd9e0], [4, 0x58ead20], [5, 0x58ebd60], [6, 0x596aca0], [7, 0x595e180], [1, 0x59b05e0], [2, 0x5903b60], [3, 0x591ea00], [4, 0x592bd40], [5, 0x592cd80], [6, 0x59abcc0], [7, 0x599f1a0]]}
  layer.16.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x59f1600], [2, 0x5944b80], [3, 0x595fa20], [4, 0x596cd60], [5, 0x596dda0], [6, 0x59ecce0], [7, 0x59e01c0], [1, 0x59f36a0]]}
  layer.16.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x59f5740]]}
  layer.16.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5947460]]}
  layer.17.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5962300], [4, 0x5970680], [5, 0x59716c0], [6, 0x59f0600], [7, 0x59e3ae0], [1, 0x5a05b60], [2, 0x5957880], [3, 0x59a3320]]}
  layer.17.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x59b16a0], [5, 0x59b26e0], [6, 0x5a31620], [7, 0x5a24b00]]}
  layer.17.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5a46b80], [2, 0x59988a0], [3, 0x59e4340], [4, 0x59b57c0], [5, 0x59b6800], [6, 0x5a35740], [7, 0x5a28c20], [1, 0x5a87ba0]]}
  layer.17.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59d98c0], [3, 0x5a25360], [4, 0x59f67e0], [5, 0x59f7820]]}
  layer.17.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55b0f80], [4, 0x55af720], [5, 0x5576920], [6, 0x55c4c60], [7, 0x55ba9e0], [1, 0x560ce40], [2, 0x55907a0], [3, 0x55f1fa0]]}
  layer.17.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x556ff00], [1, 0x55b37c0], [2, 0x553aa00], [3, 0x556b600]]}
  layer.17.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x552be60], [5, 0x552ff60], [6, 0x557e2a0], [7, 0x5574020], [1, 0x55b78e0], [2, 0x553eb20], [3, 0x556f720], [4, 0x556ce80]]}
  layer.17.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5570f80], [6, 0x55bf2c0], [7, 0x55b5040], [1, 0x55f8900]]}
  layer.17.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55fca20]]}
  layer.17.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5580380]]}
  layer.17.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5426560], [4, 0x53e6dc0], [5, 0x53eaec0], [6, 0x5439200], [7, 0x546be80], [1, 0x54af740], [2, 0x5436980], [3, 0x5467580], [4, 0x5427de0], [5, 0x542bee0], [6, 0x547a220], [7, 0x54acea0], [1, 0x54f0760], [2, 0x54779a0], [3, 0x54a85a0], [4, 0x5468e00], [5, 0x546cf00], [6, 0x54bb240], [7, 0x54edec0], [1, 0x5531780], [2, 0x54b89c0], [3, 0x54e95c0], [4, 0x54a9e20], [5, 0x54adf20], [6, 0x54fc260], [7, 0x552eee0], [1, 0x55727a0], [2, 0x54f99e0], [3, 0x552a5e0], [4, 0x54eae40], [5, 0x54eef40], [6, 0x553d280]]}
  layer.17.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x55f0740], [5, 0x55b7940], [6, 0x5605c80], [7, 0x55fba00], [1, 0x564de60], [2, 0x55d17c0], [3, 0x5632fc0], [4, 0x55f8960]]}
  layer.17.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55bfb60], [6, 0x560dea0], [7, 0x5603c20], [1, 0x5656080], [2, 0x55d99e0], [3, 0x563b1e0], [4, 0x5600b80], [5, 0x5600b80], [6, 0x564eec0], [7, 0x5644c40], [1, 0x56970a0], [2, 0x561aa00], [3, 0x567c200], [4, 0x5641ba0], [5, 0x5641ba0], [6, 0x568fee0], [7, 0x5685c60], [1, 0x56d80c0], [2, 0x565ba20], [3, 0x56bd220], [4, 0x5682bc0], [5, 0x5682bc0], [6, 0x56d0f00], [7, 0x56c6c80], [1, 0x57190e0], [2, 0x569ca40], [3, 0x56fe240], [4, 0x56c3be0], [5, 0x56c3be0], [6, 0x5711f20], [7, 0x5707ca0], [1, 0x575a100]]}
  layer.17.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56dda60], [3, 0x573f260], [4, 0x5704c00], [5, 0x5704c00], [6, 0x5752f40], [7, 0x5748cc0], [1, 0x579b120], [2, 0x56dfb00]]}
  layer.17.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56e1ba0]]}
  layer.17.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x50ffca0]]}
  layer.18.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7999820], [3, 0x79d2e40], [4, 0x7a69b60], [5, 0x7a011a0], [6, 0x7a51d80], [7, 0x79ae540], [1, 0x7acdc20], [2, 0x79da840]]}
  layer.18.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x729fac0], [2, 0x70e4dc0], [3, 0x7117a40], [4, 0x718df60]]}
  layer.18.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70b1920], [6, 0x71f7140], [7, 0x710d7c0], [1, 0x72a3be0], [2, 0x70e8ee0], [3, 0x711bb60], [4, 0x7192080], [5, 0x70f2940]]}
  layer.18.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7238160], [7, 0x714e7e0], [1, 0x72e4c00], [2, 0x7129f00]]}
  layer.18.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7133960], [6, 0x723c280], [7, 0x7152900], [1, 0x72e8d20], [2, 0x712e020], [3, 0x715d3c0], [4, 0x71d38e0], [5, 0x7174980]]}
  layer.18.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x727d2a0], [7, 0x7193920], [1, 0x7329d40], [2, 0x716f040]]}
  layer.18.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x719e3e0], [4, 0x7214900], [5, 0x71b59a0], [6, 0x72813c0], [7, 0x7197a40], [1, 0x732de60], [2, 0x7173160], [3, 0x71df400]]}
  layer.18.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7189e40], [5, 0x70ad800], [6, 0x71f3020], [7, 0x71096a0]]}
  layer.18.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71da2e0]]}
  layer.18.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x736f6c0]]}
  layer.18.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71b49c0], [3, 0x7221ca0], [4, 0x7258a20], [5, 0x71f9ac0], [6, 0x72c54e0], [7, 0x71ea700], [1, 0x737fae0], [2, 0x71f59e0], [3, 0x7262cc0], [4, 0x7299a40], [5, 0x723aae0], [6, 0x7306500], [7, 0x722b720], [1, 0x73c0b00], [2, 0x7236a00], [3, 0x72a3ce0], [4, 0x72daa60], [5, 0x727bb00], [6, 0x7347520], [7, 0x726c740], [1, 0x7401b20], [2, 0x7277a20], [3, 0x72e4d00], [4, 0x731ba80], [5, 0x72bcb20], [6, 0x7388540], [7, 0x72ad760], [1, 0x7442b40], [2, 0x72b8a40], [3, 0x7325d20], [4, 0x735caa0], [5, 0x72fdb40]]}
  layer.18.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x73c9560], [7, 0x72ee780], [1, 0x7483b60], [2, 0x72f9a60], [3, 0x7366d40], [4, 0x739dac0], [5, 0x733eb60], [6, 0x73d1780]]}
  layer.18.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72f69a0], [1, 0x748bd80], [2, 0x7301c80], [3, 0x736ef60], [4, 0x73a5ce0], [5, 0x7346d80], [6, 0x73d99a0], [7, 0x73379c0], [1, 0x74ccda0], [2, 0x7342ca0], [3, 0x73aff80], [4, 0x73e6d00], [5, 0x7387da0], [6, 0x741a9c0], [7, 0x73789e0], [1, 0x750ddc0], [2, 0x7383cc0], [3, 0x73f0fa0], [4, 0x7427d20], [5, 0x73c8dc0], [6, 0x745b9e0], [7, 0x73b9a00], [1, 0x754ede0], [2, 0x73c4ce0], [3, 0x7431fc0], [4, 0x7468d40], [5, 0x7409de0], [6, 0x749ca00], [7, 0x73faa20], [1, 0x758fe00], [2, 0x7405d00], [3, 0x7472fe0]]}
  layer.18.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x708cf60], [4, 0x70c4500], [5, 0x7024dc0], [6, 0x7129e00], [7, 0x707d380], [1, 0x72137a0], [2, 0x705c380], [3, 0x708f000]]}
  layer.18.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6f88f00]]}
  layer.18.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6ffab00]]}
  layer.19.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70014a0], [5, 0x6f9ec60], [6, 0x70a3ca0], [7, 0x6ff7220], [1, 0x718d640], [2, 0x6f99320], [3, 0x700af20], [4, 0x70424c0]]}
  layer.19.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6fdfc80], [6, 0x70e4cc0], [7, 0x7038240], [1, 0x71ce660]]}
  layer.19.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6fda340], [3, 0x704bf40], [4, 0x70834e0], [5, 0x6fe3da0], [6, 0x70e8de0], [7, 0x703c360], [1, 0x71d2780], [2, 0x701b360]]}
  layer.19.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x709e300], [7, 0x6ff1880], [1, 0x7187ca0], [2, 0x6f84de0]]}
  layer.19.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x712bea0], [7, 0x707f420], [1, 0x7215840], [2, 0x705e420], [3, 0x70910a0], [4, 0x70c6de0], [5, 0x70276a0], [6, 0x716cec0]]}
  layer.19.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x70c0440], [1, 0x7256860], [2, 0x709f440], [3, 0x70d20c0]]}
  layer.19.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7107e00], [5, 0x70686c0], [6, 0x71adee0], [7, 0x70c4560], [1, 0x725a980], [2, 0x70a3560], [3, 0x70d61e0], [4, 0x7148e20]]}
  layer.19.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x70a96e0], [6, 0x71eef00], [7, 0x7105580], [1, 0x729b9a0]]}
  layer.19.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7736020]]}
  layer.19.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x776f640]]}
  layer.19.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77d5760], [5, 0x776cda0], [6, 0x77fc900], [7, 0x775a920], [1, 0x78b2e00], [2, 0x7746440], [3, 0x777fa60], [4, 0x7816780], [5, 0x77addc0], [6, 0x783d920], [7, 0x779b940], [1, 0x78f3e20], [2, 0x7787460], [3, 0x77c0a80], [4, 0x78577a0], [5, 0x77eede0], [6, 0x787e940], [7, 0x77dc960], [1, 0x7934e40], [2, 0x77c8480], [3, 0x7801aa0], [4, 0x78987c0], [5, 0x782fe00], [6, 0x78bf960], [7, 0x781d980], [1, 0x7975e60], [2, 0x78094a0], [3, 0x7842ac0], [4, 0x78d97e0], [5, 0x7870e20], [6, 0x7900980], [7, 0x785e9a0]]}
  layer.19.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x79b6e80], [2, 0x784a4c0], [3, 0x7883ae0], [4, 0x791a800], [5, 0x78b1e40], [6, 0x79419a0], [7, 0x789f9c0], [1, 0x79bf0a0]]}
  layer.19.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x78526e0], [3, 0x788bd00], [4, 0x7922a20], [5, 0x78ba060], [6, 0x7949bc0], [7, 0x78a7be0], [1, 0x79c72c0], [2, 0x7893700], [3, 0x78ccd20], [4, 0x7963a40], [5, 0x78fb080], [6, 0x798abe0], [7, 0x78e8c00], [1, 0x7a082e0], [2, 0x78d4720], [3, 0x790dd40], [4, 0x79a4a60], [5, 0x793c0a0], [6, 0x79cbc00], [7, 0x7929c20], [1, 0x7a49300], [2, 0x7915740], [3, 0x794ed60], [4, 0x79e5a80], [5, 0x797d0c0], [6, 0x7a0cc20], [7, 0x796ac40], [1, 0x7a8a320], [2, 0x7956760], [3, 0x798fd80], [4, 0x7a26aa0], [5, 0x79be0e0]]}
  layer.19.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7a4dc40], [7, 0x79abc60], [1, 0x7acb340], [2, 0x7997780], [3, 0x79d0da0], [4, 0x7a67ac0], [5, 0x79ff100], [6, 0x7a4fce0]]}
  layer.19.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7a13e60]]}
  layer.19.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7aaab80]]}
  layer.20.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7a421c0], [6, 0x7a92da0], [7, 0x79ef560], [1, 0x7b0ec40], [2, 0x7a1b860], [3, 0x7a24280], [4, 0x7abafa0], [5, 0x7a831e0]]}
  layer.20.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7ad3dc0], [7, 0x7a30580], [1, 0x7b4fc60], [2, 0x7a5c880]]}
  layer.20.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7a652a0], [4, 0x7afbfc0], [5, 0x7ac4200], [6, 0x7ad7ee0], [7, 0x7a346a0], [1, 0x7b53d80], [2, 0x7a609a0], [3, 0x7aa62c0]]}
  layer.20.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b3cfe0], [5, 0x7b05220], [6, 0x7b18f00], [7, 0x7a756c0]]}
  layer.20.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7634780], [5, 0x7613760], [6, 0x7669480], [7, 0x75c74a0], [1, 0x771f980], [2, 0x75a4420], [3, 0x760de20], [4, 0x76757a0]]}
  layer.20.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x76d4ea0], [2, 0x754ada0], [3, 0x75b8080], [4, 0x75eee00]]}
  layer.20.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x758fea0], [6, 0x7622ac0], [7, 0x7580ae0], [1, 0x76d8fc0], [2, 0x754eec0], [3, 0x75bc1a0], [4, 0x75f2f20], [5, 0x75d0ec0]]}
  layer.20.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7663ae0], [7, 0x75c1b00], [1, 0x7719fe0], [2, 0x758fee0]]}
  layer.20.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7594000]]}
  layer.20.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75fda00]]}
  layer.20.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x74a9d60], [5, 0x744ae00], [6, 0x74dda20], [7, 0x743ba40], [1, 0x75d0e20], [2, 0x7446d20], [3, 0x74b4000], [4, 0x74ead80], [5, 0x748be20], [6, 0x751ea40], [7, 0x747ca60], [1, 0x7611e40], [2, 0x7487d40], [3, 0x74f5020], [4, 0x752bda0], [5, 0x74cce40], [6, 0x755fa60], [7, 0x74bda80], [1, 0x7652e60], [2, 0x74c8d60], [3, 0x7536040], [4, 0x756cdc0], [5, 0x750de60], [6, 0x75a0a80], [7, 0x74feaa0], [1, 0x7693e80], [2, 0x7509d80], [3, 0x7577060], [4, 0x75adde0], [5, 0x754ee80], [6, 0x75e1aa0], [7, 0x753fac0]]}
  layer.20.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7654780], [6, 0x76aa4a0], [7, 0x76084c0], [1, 0x77609a0], [2, 0x75e5440], [3, 0x764ee40], [4, 0x76b67c0], [5, 0x765c9a0]]}
  layer.20.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x76b26c0], [7, 0x76106e0], [1, 0x7768bc0], [2, 0x75ed660], [3, 0x7657060], [4, 0x76be9e0], [5, 0x7664bc0], [6, 0x76f36e0], [7, 0x7651700], [1, 0x77a9be0], [2, 0x762e680], [3, 0x7698080], [4, 0x76ffa00], [5, 0x76a5be0], [6, 0x7734700], [7, 0x7692720], [1, 0x77eac00], [2, 0x766f6a0], [3, 0x76d90a0], [4, 0x7740a20], [5, 0x76e6c00], [6, 0x7775720], [7, 0x76d3740], [1, 0x782bc20], [2, 0x76b06c0], [3, 0x771a0c0], [4, 0x7781a40], [5, 0x7727c20], [6, 0x77b6740], [7, 0x7714760], [1, 0x786cc40], [2, 0x76f16e0]]}
  layer.20.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x775b0e0], [4, 0x77c2a60], [5, 0x7768c40], [6, 0x77f7760], [7, 0x7755780], [1, 0x78adc60], [2, 0x7732700], [3, 0x775d180]]}
  layer.20.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x775f220]]}
  layer.20.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77c5340]]}
  layer.21.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6e94a00], [6, 0x6f89e60], [7, 0x6f1a2e0], [1, 0x707fb00], [2, 0x6eb9b40], [3, 0x6f2f020], [4, 0x6f392a0], [5, 0x6ed5a20]]}
  layer.21.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x670b140], [5, 0x65dffa0], [6, 0x66cea60], [7, 0x663e6e0]]}
  layer.21.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6730280], [2, 0x665ef00], [3, 0x668e2a0], [4, 0x670f260], [5, 0x65e40c0], [6, 0x66d2b80], [7, 0x6642800], [1, 0x67712a0]]}
  layer.21.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x669ff20], [3, 0x66cf2c0], [4, 0x6750280], [5, 0x66250e0]]}
  layer.21.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x67b22c0], [2, 0x66a4040], [3, 0x66d33e0], [4, 0x67543a0], [5, 0x6629200], [6, 0x67143e0], [7, 0x6684060], [1, 0x67f32e0]]}
  layer.21.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x66e5060], [3, 0x6714400], [4, 0x67953c0], [5, 0x666a220]]}
  layer.21.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6755400], [7, 0x66c5080], [1, 0x6834300], [2, 0x66e9180], [3, 0x6718520], [4, 0x67994e0], [5, 0x666e340], [6, 0x6796420]]}
  layer.21.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x663a5c0], [1, 0x672c160], [2, 0x665ade0], [3, 0x668a180]]}
  layer.21.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x675adc0]]}
  layer.21.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x67dad40]]}
  layer.21.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x66afba0], [6, 0x67d8cc0], [7, 0x67091a0], [1, 0x6878420], [2, 0x672d2a0], [3, 0x676b1e0], [4, 0x67eb160], [5, 0x66f0bc0], [6, 0x6819ce0], [7, 0x674a1c0], [1, 0x68b9440], [2, 0x676e2c0], [3, 0x67ac200], [4, 0x682c180], [5, 0x6731be0], [6, 0x685ad00], [7, 0x678b1e0], [1, 0x68fa460], [2, 0x67af2e0], [3, 0x67ed220], [4, 0x686d1a0], [5, 0x6772c00], [6, 0x689bd20], [7, 0x67cc200], [1, 0x693b480], [2, 0x67f0300], [3, 0x682e240], [4, 0x68ae1c0], [5, 0x67b3c20], [6, 0x68dcd40], [7, 0x680d220], [1, 0x697c4a0]]}
  layer.21.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6831320], [3, 0x686f260], [4, 0x68ef1e0], [5, 0x67f4c40], [6, 0x691dd60], [7, 0x684e240], [1, 0x69bd4c0], [2, 0x6839540]]}
  layer.21.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6877480], [4, 0x68f7400], [5, 0x67fce60], [6, 0x6925f80], [7, 0x6856460], [1, 0x69c56e0], [2, 0x6841760], [3, 0x68b84a0], [4, 0x6938420], [5, 0x683de80], [6, 0x6966fa0], [7, 0x6897480], [1, 0x6a06700], [2, 0x6882780], [3, 0x68f94c0], [4, 0x6979440], [5, 0x687eea0], [6, 0x69a7fc0], [7, 0x68d84a0], [1, 0x6a47720], [2, 0x68c37a0], [3, 0x693a4e0], [4, 0x69ba460], [5, 0x68bfec0], [6, 0x69e8fe0], [7, 0x69194c0], [1, 0x6a88740], [2, 0x69047c0], [3, 0x697b500], [4, 0x69fb480], [5, 0x6900ee0], [6, 0x6a2a000]]}
  layer.21.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6643f80], [7, 0x6574c80], [1, 0x66a3720], [2, 0x6591bc0], [3, 0x65fde60], [4, 0x667ee20], [5, 0x6557560], [6, 0x6646020]]}
  layer.21.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x64840e0]]}
  layer.21.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65b1b20]]}
  layer.22.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x64b1c20], [1, 0x661d5c0], [2, 0x650ba60], [3, 0x6577d00], [4, 0x65f8cc0], [5, 0x6494500], [6, 0x65c1f40], [7, 0x64f2c40]]}
  layer.22.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x665e5e0], [2, 0x654ca80], [3, 0x65b8d20], [4, 0x6639ce0]]}
  layer.22.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x64d5520], [6, 0x6602f60], [7, 0x6533c60], [1, 0x6662700], [2, 0x6550ba0], [3, 0x65bce40], [4, 0x663de00], [5, 0x6516540]]}
  layer.22.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x65060c0], [3, 0x6572360], [4, 0x65f3320], [5, 0x647ffc0]]}
  layer.22.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6593c60], [3, 0x65fff00], [4, 0x6680ec0], [5, 0x6559600], [6, 0x66480c0], [7, 0x6577560], [1, 0x66a6000], [2, 0x65d4c80]]}
  layer.22.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6640f20], [4, 0x66c1ee0], [5, 0x659a620], [6, 0x66890e0]]}
  layer.22.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x65b8580], [1, 0x66e7020], [2, 0x6615ca0], [3, 0x6645040], [4, 0x66c6000], [5, 0x659e740], [6, 0x668d200], [7, 0x65f95a0]]}
  layer.22.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6728040], [2, 0x6656cc0], [3, 0x6686060], [4, 0x6707020]]}
  layer.22.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6c31200]]}
  layer.22.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d26660]]}
  layer.22.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6c85ee0], [1, 0x6deb700], [2, 0x6c646c0], [3, 0x6cdb400], [4, 0x6d1e480], [5, 0x6c41620], [6, 0x6d36a80], [7, 0x6cc6f00], [1, 0x6e2c720], [2, 0x6ca56e0], [3, 0x6d1c420], [4, 0x6d5f4a0], [5, 0x6c82640], [6, 0x6d77aa0], [7, 0x6d07f20], [1, 0x6e6d740], [2, 0x6ce6700], [3, 0x6d5d440], [4, 0x6da04c0], [5, 0x6cc3660], [6, 0x6db8ac0], [7, 0x6d48f40], [1, 0x6eae760], [2, 0x6d27720], [3, 0x6d9e460], [4, 0x6de14e0], [5, 0x6d04680], [6, 0x6df9ae0], [7, 0x6d89f60], [1, 0x6eef780], [2, 0x6d68740], [3, 0x6ddf480]]}
  layer.22.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6e22500], [5, 0x6d456a0], [6, 0x6e3ab00], [7, 0x6dcaf80], [1, 0x6f307a0], [2, 0x6da9760], [3, 0x6e204a0], [4, 0x6e2a720]]}
  layer.22.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6d4d8c0], [6, 0x6e42d20], [7, 0x6dd31a0], [1, 0x6f389c0], [2, 0x6db1980], [3, 0x6e286c0], [4, 0x6e32940], [5, 0x6d8e8e0], [6, 0x6e83d40], [7, 0x6e141c0], [1, 0x6f799e0], [2, 0x6df29a0], [3, 0x6e696e0], [4, 0x6e73960], [5, 0x6dcf900], [6, 0x6ec4d60], [7, 0x6e551e0], [1, 0x6fbaa00], [2, 0x6e339c0], [3, 0x6eaa700], [4, 0x6eb4980], [5, 0x6e10920], [6, 0x6f05d80], [7, 0x6e96200], [1, 0x6ffba20], [2, 0x6e749e0], [3, 0x6eeb720], [4, 0x6ef59a0], [5, 0x6e51940], [6, 0x6f46da0], [7, 0x6ed7220], [1, 0x703ca40]]}
  layer.22.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6eb5a00], [3, 0x6f2c740], [4, 0x6f369c0], [5, 0x6e92960], [6, 0x6f87dc0], [7, 0x6f18240], [1, 0x707da60], [2, 0x6eb7aa0]]}
  layer.22.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6fcae80]]}
  layer.22.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6f5b300]]}
  layer.23.attention.self.query.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x70c0b20], [2, 0x6efab60], [3, 0x6f70040], [4, 0x6f7a2c0], [5, 0x6f16a40], [6, 0x6fdb2a0], [7, 0x6f6b720], [1, 0x7101b40]]}
  layer.23.attention.self.query.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6f3bb80], [3, 0x6fb1060], [4, 0x6fbb2e0], [5, 0x6f57a60]]}
  layer.23.attention.self.key.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x701c2c0], [7, 0x6fac740], [1, 0x7142b60], [2, 0x6f3fca0], [3, 0x6fb5180], [4, 0x6fbf400], [5, 0x6f5bb80], [6, 0x705d2e0]]}
  layer.23.attention.self.key.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6fed760], [1, 0x7183b80], [2, 0x6f80cc0], [3, 0x6ff61a0]]}
  layer.23.attention.self.value.weight:              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ae4f00], [1, 0x6c920c0], [2, 0x6ad1240], [3, 0x6b47f80], [4, 0x6b8b000], [5, 0x6a9f600], [6, 0x6bc4e40], [7, 0x6b25f20]]}
  layer.23.attention.self.value.bias:                {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6b40520], [5, 0x6a45f80], [6, 0x6b6f0a0], [7, 0x6a9f580]]}
  layer.23.attention.output.dense.weight:            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6c0e800], [2, 0x6a8a880], [3, 0x6b015c0], [4, 0x6b44640], [5, 0x6a4a0a0], [6, 0x6b731c0], [7, 0x6aa36a0], [1, 0x6c4f820]]}
  layer.23.attention.output.dense.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6acb8a0], [3, 0x6b425e0], [4, 0x6b85660], [5, 0x6a8b0c0]]}
  layer.23.attention.output.LayerNorm.weight:        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6a8f1e0]]}
  layer.23.attention.output.LayerNorm.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6bb4a20]]}
  layer.23.intermediate.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x695a4e0], [1, 0x6ac9760], [2, 0x69457e0], [3, 0x69bc520], [4, 0x6a3c4a0], [5, 0x6941f00], [6, 0x6a6b020], [7, 0x699b500], [1, 0x6b0a780], [2, 0x6986800], [3, 0x69fd540], [4, 0x6a7d4c0], [5, 0x6982f20], [6, 0x6aac040], [7, 0x69dc520], [1, 0x6b4b7a0], [2, 0x69c7820], [3, 0x6a3e560], [4, 0x6abe4e0], [5, 0x69c3f40], [6, 0x6aed060], [7, 0x6a1d540], [1, 0x6b8c7c0], [2, 0x6a08840], [3, 0x6a7f580], [4, 0x6aff500], [5, 0x6a04f60], [6, 0x6b2e080], [7, 0x6a5e560], [1, 0x6bcd7e0], [2, 0x6a49860], [3, 0x6ac05a0]]}
  layer.23.intermediate.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6cd30e0], [2, 0x6b12260], [3, 0x6b88fa0], [4, 0x6bcc020], [5, 0x6ae0620], [6, 0x6c05e60], [7, 0x6b66f40], [1, 0x6cdb300]]}
  layer.23.output.dense.weight:                      {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6b1a480], [3, 0x6b911c0], [4, 0x6bd4240], [5, 0x6ae8840], [6, 0x6c0e080], [7, 0x6b6f160], [1, 0x6ce3520], [2, 0x6b5b4a0], [3, 0x6bd21e0], [4, 0x6c15260], [5, 0x6b29860], [6, 0x6c4f0a0], [7, 0x6bb0180], [1, 0x6d24540], [2, 0x6b9c4c0], [3, 0x6c13200], [4, 0x6c56280], [5, 0x6b6a880], [6, 0x6c900c0], [7, 0x6bf11a0], [1, 0x6d65560], [2, 0x6bdd4e0], [3, 0x6c54220], [4, 0x6c972a0], [5, 0x6bab8a0], [6, 0x6cd10e0], [7, 0x6c321c0], [1, 0x6da6580], [2, 0x6c1e500], [3, 0x6c95240], [4, 0x6cd82c0], [5, 0x6bec8c0]]}
  layer.23.output.dense.bias:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d12100], [7, 0x6c731e0], [1, 0x6de75a0], [2, 0x6c5f520], [3, 0x6cd6260], [4, 0x6d192e0], [5, 0x6c2d8e0], [6, 0x6d141a0]]}
  layer.23.output.LayerNorm.weight:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d16240]]}
  layer.23.output.LayerNorm.bias:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6c75ac0]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x87c2f20]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x88da3c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x88a2680]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x899abc0]]}
  dc.input_tensor.layernorm_38.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x88867c0], [3, 0x8b26d40], [4, 0x8bb68a0], [5, 0x8b47540]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x872a820]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8713260]]}
  dc.input_tensor.layernorm_52.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8755ac0], [6, 0x8701600], [7, 0x867c540], [1, 0x87b5260]]}
  input_1_multiply_69_tile_bcast_tile_bcast:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87ded20]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x87e97c0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8959360]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8cd3980]]}
  dc.input_tensor.layernorm_91.4:                    {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d86dc0], [2, 0x8cac800], [3, 0x8f59080], [4, 0x90197e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8d528a0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9235640]]}
  dc.input_tensor.layernorm_105.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x921afc0], [6, 0x8fb68e0], [7, 0x8f685c0], [1, 0x901ca40]]}
  input_1_multiply_122_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8a57360]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x8a75200]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8cf37c0]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8d83320]]}
  dc.input_tensor.layernorm_144.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8d202c0], [6, 0x8afbba0], [7, 0x8a7e4e0], [1, 0x8b6a720]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8cd3140]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d86580]]}
  dc.input_tensor.layernorm_158.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8caaf80], [3, 0x8f57800], [4, 0x9017f60], [5, 0x8fb5f40]]}
  input_1_multiply_175_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7e41880]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7d61140]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f913a0]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7dea380]]}
  dc.input_tensor.layernorm_197.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f12460], [4, 0x7eb2ce0], [5, 0x7e7e800], [6, 0x7ec8220]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7be9340]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c6f440]]}
  dc.input_tensor.layernorm_211.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7cc9260], [5, 0x7c914a0], [6, 0x7ca5180], [7, 0x7c01940]]}
  input_1_multiply_228_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7cf3d60]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7c50520]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7da20c0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7da8b20]]}
  dc.input_tensor.layernorm_250.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x834c9a0], [2, 0x821ef60], [3, 0x8334bc0], [4, 0x8315c20]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8589820]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x82185a0]]}
  dc.input_tensor.layernorm_264.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8582e80], [6, 0x852e9c0], [7, 0x84ad1e0], [1, 0x85e2620]]}
  input_1_multiply_281_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8534bc0]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80df720]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80deee0]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x807f760]]}
  dc.input_tensor.layernorm_303.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8088180], [6, 0x80640a0], [7, 0x7fc3100], [1, 0x8130300]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8217d60]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x834c160]]}
  dc.input_tensor.layernorm_317.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x821d6e0], [3, 0x8333340], [4, 0x83143a0], [5, 0x831de00]]}
  input_1_multiply_334_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fb4360]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa140d00]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9fb3ba0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9ead300]]}
  dc.input_tensor.layernorm_356.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa005fe0], [4, 0xa186680], [5, 0xa14b800], [6, 0x9f806e0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9bb2d00]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9c96520]]}
  dc.input_tensor.layernorm_370.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9b8b360], [3, 0x9d55420], [4, 0x9ee1dc0], [5, 0x9e2d140]]}
  input_1_multiply_387_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9d49a20]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e5de40]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa001e00]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa2f3b40]]}
  dc.input_tensor.layernorm_409.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa355360], [3, 0xa472180], [4, 0xa5f2820], [5, 0xa57aaa0]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa847480]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa796900]]}
  dc.input_tensor.layernorm_423.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81d2a0], [6, 0xa62f900], [7, 0xa62b000], [1, 0xa6ee020]]}
  input_1_multiply_440_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa5a8f60]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa355340]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa1d4360]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa29bca0]]}
  dc.input_tensor.layernorm_462.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa202f00], [3, 0xa31fd20], [4, 0xa4a03c0], [5, 0xa428640]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa2f3300]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa3b2a40]]}
  dc.input_tensor.layernorm_476.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa353ae0], [3, 0xa470900], [4, 0xa5f0fa0], [5, 0xa579220]]}
  input_1_multiply_493_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x939e1c0]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9310700]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x93537e0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9485340]]}
  dc.input_tensor.layernorm_515.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x937a9a0], [3, 0x95be040], [4, 0x96b3cc0], [5, 0x961f020]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x92a8b60]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9044480]]}
  dc.input_tensor.layernorm_529.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8ff6160], [1, 0x90aa5e0], [2, 0x8fdcb40], [3, 0x925a840]]}
  input_1_multiply_546_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x91685a0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x93e62a0]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x9248d40]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x92fd1c0]]}
  dc.input_tensor.layernorm_568.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x93dfa20], [7, 0x9351f60], [1, 0x9483ac0], [2, 0x9379120]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9a7c7c0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c4a160]]}
  dc.input_tensor.layernorm_582.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x97d1e20], [1, 0x99314c0], [2, 0x9827b60], [3, 0x9a2e300]]}
  input_1_multiply_599_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d11b20]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9e9e4c0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x987df20]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x968bc60]]}
  dc.input_tensor.layernorm_621.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x96377c0], [1, 0x975d020], [2, 0x96536c0], [3, 0x9859e60]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x981c820]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x97905c0]]}
  dc.input_tensor.layernorm_635.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x98eec20], [2, 0x97e52c0], [3, 0x99eba60], [4, 0x9af0280]]}
  input_1_multiply_652_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5b7b960]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5c04300]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5c8ac80]]}
  lc.input_tensor.layernorm_674.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d2dcc0]]}
  dc.input_tensor.layernorm_674.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c3f200], [3, 0x5c88400], [4, 0x5ce2220], [5, 0x5c26c00]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59fb940]]}
  lc.input_tensor.layernorm_688.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a76fa0]]}
  dc.input_tensor.layernorm_688.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5a6a480], [1, 0x5accce0], [2, 0x59e1b00], [3, 0x5a2d5a0]]}
  input_1_multiply_705_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5b3c8e0]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5af3f00]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5b5a0a0]]}
  lc.input_tensor.layernorm_727.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b18880]]}
  dc.input_tensor.layernorm_727.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5c86b80], [4, 0x5ce09a0], [5, 0x5c25380], [6, 0x5d10560]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x64e5800]]}
  lc.input_tensor.layernorm_741.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x63e91e0]]}
  dc.input_tensor.layernorm_741.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6255500], [5, 0x615e020], [6, 0x6290ba0], [7, 0x61cd380]]}
  input_1_multiply_758_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x64b0ba0]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x661b500]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x601bfa0]]}
  lc.input_tensor.layernorm_780.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x60651a0]]}
  dc.input_tensor.layernorm_780.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x60fbec0], [5, 0x5fcaba0], [6, 0x60fd720], [7, 0x6039f00]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x61f5d60]]}
  lc.input_tensor.layernorm_794.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6254cc0]]}
  dc.input_tensor.layernorm_794.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x615c7a0], [6, 0x628f320], [7, 0x61cbb00], [1, 0x62ae2e0]]}
  input_1_multiply_811_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x510b7c0]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x51510e0]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5209ee0]]}
  lc.input_tensor.layernorm_833.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x521cba0]]}
  dc.input_tensor.layernorm_833.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x51a2da0], [3, 0x51d39a0], [4, 0x5194200], [5, 0x51d1100]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}
  lc.input_tensor.layernorm_847.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  dc.input_tensor.layernorm_847.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900], [5, 0x4fb0900], [6, 0x4fb0900], [7, 0x4fb0900]]}
  input_1_multiply_864_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x50382e0]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x50382e0]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x50c2560]]}
  lc.input_tensor.layernorm_886.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}
  dc.input_tensor.layernorm_886.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x596ee00], [5, 0x596fe40], [6, 0x59eed80], [7, 0x59e2260]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5946c20]]}
  lc.input_tensor.layernorm_900.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5961ac0]]}
  dc.input_tensor.layernorm_900.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x574c5e0], [1, 0x579ea40], [2, 0x56f1fc0], [3, 0x5745c60]]}
  input_1_multiply_917_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x5a76760]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x5a69c40]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x557fb40]]}
  lc.input_tensor.layernorm_939.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x55b0740]]}
  dc.input_tensor.layernorm_939.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x55adea0], [5, 0x55750a0], [6, 0x55c33e0], [7, 0x55b9160]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x5741300]]}
  lc.input_tensor.layernorm_953.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x5706ca0]]}
  dc.input_tensor.layernorm_953.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5706ca0], [6, 0x5754fe0], [7, 0x574ad60], [1, 0x579d1c0]]}
  input_1_multiply_970_tile_bcast_tile_bcast:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x715cb80]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.1.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x71d30a0]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x736ee80]]}
  lc.input_tensor.layernorm_992.dc.reduce_avg.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71b4180]]}
  dc.input_tensor.layernorm_992.4:                   {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7220420], [4, 0x72571a0], [5, 0x71f8240], [6, 0x72c3c60]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6ffa2c0]]}
  lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7000c60]]}
  dc.input_tensor.layernorm_1006.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f9d3e0], [6, 0x70a2420], [7, 0x6ff59a0], [1, 0x718bdc0]]}
  input_1_multiply_1023_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x70c65a0]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7026e60]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x70e4580]]}
  lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7117200]]}
  dc.input_tensor.layernorm_1045.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7255920], [5, 0x71f69c0], [6, 0x72c23e0], [7, 0x71d8a60]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x79add00]]}
  lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7acd3e0]]}
  dc.input_tensor.layernorm_1059.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x776b520], [6, 0x77fb080], [7, 0x77590a0], [1, 0x78b1580]]}
  input_1_multiply_1076_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7b94da0]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7aa19c0]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75fd1c0]]}
  lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7633f40]]}
  dc.input_tensor.layernorm_1098.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7611ee0], [6, 0x7667c00], [7, 0x75c5c20], [1, 0x771e100]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77c4b00]]}
  lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x776ace0]]}
  dc.input_tensor.layernorm_1112.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77f9800], [7, 0x7757820], [1, 0x78afd00], [2, 0x77347a0]]}
  input_1_multiply_1129_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6713ba0]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6683820]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x67da500]]}
  lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x66af360]]}
  dc.input_tensor.layernorm_1151.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x67d7440], [7, 0x6707920], [1, 0x6876ba0], [2, 0x672ba20]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65b12e0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x64b13e0]]}
  dc.input_tensor.layernorm_1165.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x661bd40], [2, 0x650a1e0], [3, 0x6576480], [4, 0x65f7440]]}
  input_1_multiply_1182_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6576d20]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x66a57c0]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x65df760]]}
  lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x66ce220]]}
  dc.input_tensor.layernorm_1204.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x67060a0], [1, 0x6875320], [2, 0x672a1a0], [3, 0x6759540]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6f2e7e0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6f38a60]]}
  dc.input_tensor.layernorm_1218.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6de9e80], [2, 0x6c62e40], [3, 0x6cd9b80], [4, 0x6d1cc00]]}
  input_1_multiply_1235_tile_bcast_tile_bcast:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7000420]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.1.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f9cba0]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6bb41e0]]}
  lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6ae46c0]]}
  dc.input_tensor.layernorm_1257.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6c90840], [2, 0x6acf9c0], [3, 0x6b46700], [4, 0x6b89780]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6c75280]]}
  lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6de9640]]}
  dc.input_tensor.layernorm_1271.4:                  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c615c0], [3, 0x6cd8300], [4, 0x6d1b380], [5, 0x6c2f980]]}

  # epoch_to_epoch
  e2e_gelu_44_0:                                     {input: gelu_44, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xa753100], [4, 0xa8d1f40], [5, 0xa81eb20], [6, 0xa631180], [7, 0xa62c880], [1, 0xa6ef8a0], [2, 0xc732180], [3, 0xc7d3120], [4, 0xc951f60], [5, 0xc89eb40], [6, 0xc6b11a0]]}
  e2e__fused_op_2_0:                                 {input: _fused_op_2, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xc6ac8a0], [1, 0xc76f8c0], [2, 0xe7b21a0], [3, 0xe853140]]}
  e2e_layernorm_91.dc.subtract.1_0:                  {input: layernorm_91.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe9d1f80], [5, 0xe91eb60], [6, 0xe7311c0], [7, 0xdf0c8c0]]}
  e2e__fused_op_6_0:                                 {input: _fused_op_6, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xdfcf8e0], [2, 0x100121c0], [3, 0x100b3160], [4, 0x10231fa0]]}
  e2e_matmul_120_0:                                  {input: matmul_120, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x118721e0], [3, 0x11913180], [4, 0x11a91fc0], [5, 0xc07eb40]]}
  e2e__fused_op_7_0:                                 {input: _fused_op_7, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xa631180], [7, 0xa62c880], [1, 0xa6ef8a0]]}
  e2e_gelu_150_0:                                    {input: gelu_150, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xa8d1f40], [5, 0x152beb60], [6, 0xd6f11c0], [7, 0xd6ec8c0], [1, 0xd7af8e0], [2, 0xbf12180], [3, 0xc7d3120], [4, 0xc951f60], [5, 0x1733eb80], [6, 0xf7711e0], [7, 0xf76c8e0]]}
  e2e__fused_op_10_0:                                {input: _fused_op_10, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xbe911a0], [7, 0xbe8c8a0], [1, 0xbf4f8c0], [2, 0xa6b2160]]}
  e2e_layernorm_197.dc.subtract.1_0:                 {input: layernorm_197.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xf82f900], [2, 0xdf921a0], [3, 0xe853140], [4, 0xe9d1f80]]}
  e2e__fused_op_14_0:                                {input: _fused_op_14, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xa631180], [7, 0xa62c880], [1, 0xa6ef8a0]]}
  e2e_matmul_226_0:                                  {input: matmul_226, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xbe911a0], [7, 0xbe8c8a0], [1, 0xbf4f8c0], [2, 0xbf12180]]}
  e2e__fused_op_15_0:                                {input: _fused_op_15, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xa753100], [4, 0xa8d1f40], [5, 0xc07eb40]]}
  e2e_gelu_256_0:                                    {input: gelu_256, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x150cc8c0], [1, 0x1518f8e0], [2, 0x151521a0], [3, 0xd813140], [4, 0xd991f80], [5, 0xf13eb80], [6, 0x169311e0], [7, 0x1714c8e0], [1, 0x1720f900], [2, 0x171d21c0], [3, 0xf893160], [4, 0xfa11fa0]]}
  e2e__fused_op_18_0:                                {input: _fused_op_18, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbfb3120], [4, 0xc131f60], [5, 0xd8deb60], [6, 0x150d11c0]]}
  e2e_layernorm_303.dc.subtract.1_0:                 {input: layernorm_303.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xa631180], [7, 0xa62c880], [1, 0xa6ef8a0]]}
  e2e__fused_op_22_0:                                {input: _fused_op_22, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xa753100], [4, 0xa8d1f40], [5, 0xc07eb40]]}
  e2e_matmul_332_0:                                  {input: matmul_332, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbfb3120], [4, 0xc131f60], [5, 0xd8deb60], [6, 0xd6f11c0]]}
  e2e__fused_op_23_0:                                {input: _fused_op_23, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xbe911a0], [7, 0xbe8c8a0], [1, 0xbf4f8c0], [2, 0xbf12180]]}
  e2e_gelu_362_0:                                    {input: gelu_362, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x15371f80], [5, 0xa81eb20], [6, 0x169311e0], [7, 0xd6ec8c0], [1, 0xd7af8e0], [2, 0xefd21c0], [3, 0x16a53160], [4, 0x173f1fa0], [5, 0x16b1eb80], [6, 0x189b1200], [7, 0xf76c8e0], [1, 0xf82f900]]}
  e2e__fused_op_26_0:                                {input: _fused_op_26, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa62c880], [1, 0xa6ef8a0], [2, 0xd7721a0], [3, 0x151f3140]]}
  e2e_layernorm_409.dc.subtract.1_0:                 {input: layernorm_409.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xa753100], [4, 0xa8d1f40], [5, 0x18b9eba0]]}
  e2e__fused_op_30_0:                                {input: _fused_op_30, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0xbf4f8c0], [2, 0xbf12180]]}
  e2e_matmul_438_0:                                  {input: matmul_438, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xd6ec8c0], [1, 0xd7af8e0], [2, 0xd7721a0], [3, 0xd813140]]}
  e2e__fused_op_31_0:                                {input: _fused_op_31, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbfb3120], [4, 0xc131f60], [5, 0xa81eb20], [6, 0xbe911a0]]}
  e2e_gelu_468_0:                                    {input: gelu_468, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa6ef8a0], [2, 0xa6b2160], [3, 0x16a53160], [4, 0xd991f80], [5, 0xd8deb60], [6, 0xef511e0], [7, 0x1692c8e0], [1, 0x169ef900], [2, 0x169b21c0], [3, 0x18ad3180], [4, 0xfa11fa0], [5, 0xf95eb80]]}
  e2e__fused_op_34_0:                                {input: _fused_op_34, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8d1f40], [5, 0xc07eb40], [6, 0xd6f11c0], [7, 0xa62c880]]}
  e2e_layernorm_515.dc.subtract.1_0:                 {input: layernorm_515.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0x18a6f920], [2, 0x18a321e0]]}
  e2e__fused_op_38_0:                                {input: _fused_op_38, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xc131f60], [5, 0xa81eb20], [6, 0xbe911a0]]}
  e2e_matmul_544_0:                                  {input: matmul_544, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd991f80], [5, 0xc07eb40], [6, 0xd6f11c0], [7, 0xbe8c8a0]]}
  e2e__fused_op_39_0:                                {input: _fused_op_39, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa62c880], [1, 0xa6ef8a0], [2, 0xa6b2160], [3, 0xbfb3120]]}
  e2e_gelu_574_0:                                    {input: gelu_574, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x152beb60], [6, 0xa631180], [7, 0x150cc8c0], [1, 0xd7af8e0], [2, 0xd7721a0], [3, 0xf073160], [4, 0x16bd1fa0], [5, 0x1733eb80], [6, 0x169311e0], [7, 0x1714c8e0], [1, 0xf82f900], [2, 0xf7f21c0]]}
  e2e__fused_op_42_0:                                {input: _fused_op_42, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xbf4f8c0], [2, 0xbf12180], [3, 0xd813140], [4, 0xa8d1f40]]}
  e2e_layernorm_621.dc.subtract.1_0:                 {input: layernorm_621.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xc131f60], [5, 0xa81eb20], [6, 0x189b1200]]}
  e2e__fused_op_46_0:                                {input: _fused_op_46, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa62c880], [1, 0xa6ef8a0], [2, 0xa6b2160], [3, 0xbfb3120]]}
  e2e_matmul_650_0:                                  {input: matmul_650, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xbf4f8c0], [2, 0xbf12180], [3, 0xd813140], [4, 0xc131f60]]}
  e2e__fused_op_47_0:                                {input: _fused_op_47, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8d1f40], [5, 0xc07eb40], [6, 0xa631180], [7, 0xbe8c8a0]]}
  e2e_gelu_680_0:                                    {input: gelu_680, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x151521a0], [3, 0xa753100], [4, 0x15371f80], [5, 0xd8deb60], [6, 0xd6f11c0], [7, 0xef4c8e0], [1, 0x169ef900], [2, 0x171d21c0], [3, 0x16a53160], [4, 0x173f1fa0], [5, 0xf95eb80], [6, 0xf7711e0]]}
  e2e__fused_op_50_0:                                {input: _fused_op_50, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xbe911a0], [7, 0xd6ec8c0], [1, 0x1518f8e0]]}
  e2e_layernorm_727.dc.subtract.1_0:                 {input: layernorm_727.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa62c880], [1, 0xa6ef8a0], [2, 0xa6b2160], [3, 0x18ad3180]]}
  e2e__fused_op_54_0:                                {input: _fused_op_54, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8d1f40], [5, 0xc07eb40], [6, 0xa631180], [7, 0xbe8c8a0]]}
  e2e_matmul_756_0:                                  {input: matmul_756, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xd8deb60], [6, 0xbe911a0], [7, 0xd6ec8c0], [1, 0xd7af8e0]]}
  e2e__fused_op_55_0:                                {input: _fused_op_55, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xbf4f8c0], [2, 0xbf12180], [3, 0xa753100], [4, 0xc131f60]]}
  e2e_gelu_786_0:                                    {input: gelu_786, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x150d11c0], [7, 0xa62c880], [1, 0x169ef900], [2, 0xd7721a0], [3, 0xd813140], [4, 0xf1f1fa0], [5, 0x16b1eb80], [6, 0x171511e0], [7, 0x1692c8e0], [1, 0x18a6f920], [2, 0xf7f21c0], [3, 0xf893160]]}
  e2e__fused_op_58_0:                                {input: _fused_op_58, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xbfb3120], [4, 0xd991f80], [5, 0xa81eb20]]}
  e2e_layernorm_833.dc.subtract.1_0:                 {input: layernorm_833.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8d1f40], [5, 0xc07eb40], [6, 0xa631180], [7, 0x189ac900]]}
  e2e__fused_op_62_0:                                {input: _fused_op_62, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa6ef8a0], [2, 0xbf12180], [3, 0xa753100], [4, 0xc131f60]]}
  e2e_matmul_862_0:                                  {input: matmul_862, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xd7721a0], [3, 0xbfb3120], [4, 0xd991f80], [5, 0xc07eb40]]}
  e2e__fused_op_63_0:                                {input: _fused_op_63, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xbe911a0], [7, 0xa62c880], [1, 0xbf4f8c0]]}
  e2e_gelu_892_0:                                    {input: gelu_892, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x151f3140], [4, 0xa8d1f40], [5, 0x152beb60], [6, 0xd6f11c0], [7, 0xd6ec8c0], [1, 0xf00f900], [2, 0x169b21c0], [3, 0x17273160], [4, 0x16bd1fa0], [5, 0x1733eb80], [6, 0xf7711e0], [7, 0xf76c8e0]]}
  e2e__fused_op_66_0:                                {input: _fused_op_66, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0xd7af8e0], [2, 0xa6b2160]]}
  e2e_layernorm_939.dc.subtract.1_0:                 {input: layernorm_939.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa6ef8a0], [2, 0xbf12180], [3, 0xa753100], [4, 0x18c51fc0]]}
  e2e__fused_op_70_0:                                {input: _fused_op_70, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xbe911a0], [7, 0xa62c880], [1, 0xbf4f8c0]]}
  e2e_matmul_968_0:                                  {input: matmul_968, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xd6f11c0], [7, 0xbe8c8a0], [1, 0xd7af8e0], [2, 0xbf12180]]}
  e2e__fused_op_71_0:                                {input: _fused_op_71, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xbfb3120], [4, 0xa8d1f40], [5, 0xc07eb40]]}
  e2e_gelu_998_0:                                    {input: gelu_998, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x150cc8c0], [1, 0xa6ef8a0], [2, 0x151521a0], [3, 0xd813140], [4, 0xd991f80], [5, 0xf13eb80], [6, 0x169311e0], [7, 0x1714c8e0], [1, 0x169ef900], [2, 0x171d21c0], [3, 0xf893160], [4, 0xfa11fa0]]}
  e2e__fused_op_74_0:                                {input: _fused_op_74, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xc131f60], [5, 0xd8deb60], [6, 0xa631180]]}
  e2e_layernorm_1045.dc.subtract.1_0:                {input: layernorm_1045.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xa81eb20], [6, 0xbe911a0], [7, 0xa62c880], [1, 0x18a6f920]]}
  e2e__fused_op_78_0:                                {input: _fused_op_78, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xbfb3120], [4, 0xa8d1f40], [5, 0xc07eb40]]}
  e2e_matmul_1074_0:                                 {input: matmul_1074, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd813140], [4, 0xc131f60], [5, 0xd8deb60], [6, 0xbe911a0]]}
  e2e__fused_op_79_0:                                {input: _fused_op_79, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0xa6ef8a0], [2, 0xbf12180]]}
  e2e_gelu_1104_0:                                   {input: gelu_1104, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x15371f80], [5, 0xa81eb20], [6, 0x150d11c0], [7, 0xd6ec8c0], [1, 0xd7af8e0], [2, 0xefd21c0], [3, 0x16a53160], [4, 0x173f1fa0], [5, 0x16b1eb80], [6, 0x171511e0], [7, 0xf76c8e0], [1, 0xf82f900]]}
  e2e__fused_op_82_0:                                {input: _fused_op_82, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa62c880], [1, 0xbf4f8c0], [2, 0xd7721a0], [3, 0xa753100]]}
  e2e_layernorm_1151.dc.subtract.1_0:                {input: layernorm_1151.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa6b2160], [3, 0xbfb3120], [4, 0xa8d1f40], [5, 0x18b9eba0]]}
  e2e__fused_op_86_0:                                {input: _fused_op_86, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0xa6ef8a0], [2, 0xbf12180]]}
  e2e_matmul_1180_0:                                 {input: matmul_1180, type: queue, entries: 128, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xd6ec8c0], [1, 0xbf4f8c0], [2, 0xd7721a0], [3, 0xbfb3120]]}
  e2e__fused_op_87_0:                                {input: _fused_op_87, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xc131f60], [5, 0xa81eb20], [6, 0xbe911a0]]}
  e2e_gelu_1210_0:                                   {input: gelu_1210, type: queue, entries: 128, grid_size: [6, 2], t: 1, mblock: [1, 16], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1518f8e0], [2, 0xa6b2160], [3, 0x151f3140], [4, 0xd991f80], [5, 0xd8deb60], [6, 0xef511e0], [7, 0x1692c8e0], [1, 0x1720f900], [2, 0x169b21c0], [3, 0x17273160], [4, 0xfa11fa0], [5, 0xf95eb80]]}
  e2e__fused_op_90_0:                                {input: _fused_op_90, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa8d1f40], [5, 0xc07eb40], [6, 0xd6f11c0], [7, 0xa62c880]]}
  e2e_layernorm_1257.dc.subtract.1_0:                {input: layernorm_1257.dc.subtract.1, type: queue, entries: 128, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa631180], [7, 0xbe8c8a0], [1, 0xa6ef8a0], [2, 0x18a321e0]]}
  e2e__fused_op_94_0:                                {input: _fused_op_94, type: queue, entries: 128, grid_size: [4, 1], t: 1, mblock: [3, 8], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa753100], [4, 0xc131f60], [5, 0xa81eb20], [6, 0xbe911a0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_14: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [2, 2], grid_size: [2, 3], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [2, 7], grid_size: [2, 2], inputs: [_fused_op_1, matmul_22],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_33: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_37: {type: add, grid_loc: [2, 9], grid_size: [2, 2], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 2], inputs: [add_37, layernorm_38.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 2], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_2: {type: fused_op, grid_loc: [4, 9], grid_size: [4, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4, layernorm_38.dc.subtract.1, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_41: {type: matmul, grid_loc: [6, 0], grid_size: [4, 8], inputs: [_fused_op_2, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_44: {type: gelu, grid_loc: [4, 10], grid_size: [6, 2], inputs: [matmul_41],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_1:
    target_device: 0
    input_count: 128
    matmul_47: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_44_0, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_51: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_47, e2e__fused_op_2_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_3: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4, layernorm_52.dc.subtract.1, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_55: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_61: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_67: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_4: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_4, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_75: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_3, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_5: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_5, matmul_75],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_86: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_3_add_90: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_3],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_86, buffer_0__fused_op_3_add_90],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_2:
    target_device: 0
    input_count: 128
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_91.dc.subtract.1_0, e2e_layernorm_91.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_6: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4, e2e_layernorm_91.dc.subtract.1_0, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_3:
    target_device: 0
    input_count: 128
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_6_0, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_97: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_94],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_104: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_100, e2e__fused_op_6_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_7: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4, layernorm_105.dc.subtract.1, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_108: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_7, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_114: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_7, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_120: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_4:
    target_device: 0
    input_count: 128
    _fused_op_8: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_120_0, input_1_multiply_122_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_128: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_7_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_9: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_8],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_9, matmul_128],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_139: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_143: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_139, e2e__fused_op_7_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_10: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4, layernorm_144.dc.subtract.1, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_147: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_150: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_147],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_5:
    target_device: 0
    input_count: 128
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_157: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_153, e2e__fused_op_10_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_11: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4, layernorm_158.dc.subtract.1, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_161: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_167: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_173: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_161, matmul_167],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_12: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_12, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_181: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_11, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_13: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_12],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_13, matmul_181],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_192: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_11_add_196: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_11],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_196: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_192, buffer_0__fused_op_11_add_196],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_6:
    target_device: 0
    input_count: 128
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_197.dc.subtract.1_0, e2e_layernorm_197.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_14: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4, e2e_layernorm_197.dc.subtract.1_0, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_7:
    target_device: 0
    input_count: 128
    matmul_200: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_14_0, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_203: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_200],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_210: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_206, e2e__fused_op_14_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_15: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4, layernorm_211.dc.subtract.1, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_214: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_15, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_220: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_15, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_226: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_214, matmul_220],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_8:
    target_device: 0
    input_count: 128
    _fused_op_16: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_226_0, input_1_multiply_228_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_234: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_15_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_17: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_16],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_17, matmul_234],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_245: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_249: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_245, e2e__fused_op_15_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_18: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4, layernorm_250.dc.subtract.1, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_253: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_18, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_256: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_253],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_9:
    target_device: 0
    input_count: 128
    matmul_259: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_256_0, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_263: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_259, e2e__fused_op_18_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_19: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4, layernorm_264.dc.subtract.1, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_267: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_273: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_279: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_267, matmul_273],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_20: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_20, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_287: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_19, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_20],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_21, matmul_287],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_298: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_19_add_302: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_19],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_302: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_298, buffer_0__fused_op_19_add_302],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_10:
    target_device: 0
    input_count: 128
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_303.dc.subtract.1_0, e2e_layernorm_303.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_22: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4, e2e_layernorm_303.dc.subtract.1_0, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_11:
    target_device: 0
    input_count: 128
    matmul_306: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_22_0, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_309: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_306],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_316: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_312, e2e__fused_op_22_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_23: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4, layernorm_317.dc.subtract.1, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_320: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_23, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_326: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_23, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_332: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_320, matmul_326],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_12:
    target_device: 0
    input_count: 128
    _fused_op_24: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_332_0, input_1_multiply_334_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_340: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_23_0, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_25: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_24],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_25, matmul_340],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_351: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_355: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_351, e2e__fused_op_23_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_26: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4, layernorm_356.dc.subtract.1, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_359: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_26, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_362: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_359],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_13:
    target_device: 0
    input_count: 128
    matmul_365: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_362_0, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_369: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_365, e2e__fused_op_26_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_27: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4, layernorm_370.dc.subtract.1, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_373: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_379: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_385: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_373, matmul_379],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_28: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_28, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_393: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_27, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_29: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_28],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_29, matmul_393],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_404: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_27_add_408: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_27],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_408: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_404, buffer_0__fused_op_27_add_408],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_14:
    target_device: 0
    input_count: 128
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_409.dc.subtract.1_0, e2e_layernorm_409.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_30: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4, e2e_layernorm_409.dc.subtract.1_0, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_15:
    target_device: 0
    input_count: 128
    matmul_412: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_30_0, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_415: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_412],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_422: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_418, e2e__fused_op_30_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_31: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4, layernorm_423.dc.subtract.1, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_426: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_31, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_432: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_31, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_438: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_426, matmul_432],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_16:
    target_device: 0
    input_count: 128
    _fused_op_32: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_438_0, input_1_multiply_440_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_446: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_31_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_32],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_33, matmul_446],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_457: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_461: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_457, e2e__fused_op_31_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_34: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4, layernorm_462.dc.subtract.1, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_465: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_34, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_468: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_465],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_17:
    target_device: 0
    input_count: 128
    matmul_471: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_468_0, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_475: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_471, e2e__fused_op_34_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_35: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4, layernorm_476.dc.subtract.1, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_479: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_485: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_491: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_479, matmul_485],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_499: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_35, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_37: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_36],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_37, matmul_499],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_510: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_35_add_514: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_35],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_514: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_510, buffer_0__fused_op_35_add_514],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_18:
    target_device: 0
    input_count: 128
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_515.dc.subtract.1_0, e2e_layernorm_515.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_38: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4, e2e_layernorm_515.dc.subtract.1_0, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_19:
    target_device: 0
    input_count: 128
    matmul_518: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_38_0, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_521: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_518],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_528: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_524, e2e__fused_op_38_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_39: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4, layernorm_529.dc.subtract.1, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_532: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_39, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_538: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_39, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_544: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_532, matmul_538],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_20:
    target_device: 0
    input_count: 128
    _fused_op_40: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_544_0, input_1_multiply_546_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_552: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_39_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_41: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_40],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_41, matmul_552],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_563: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_567: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_563, e2e__fused_op_39_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_42: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4, layernorm_568.dc.subtract.1, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_571: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_42, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_574: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_571],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_21:
    target_device: 0
    input_count: 128
    matmul_577: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_574_0, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_581: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_577, e2e__fused_op_42_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_43: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4, layernorm_582.dc.subtract.1, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_585: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_591: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_597: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_585, matmul_591],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_44: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_44, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_605: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_43, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_45: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_44],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_45, matmul_605],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_616: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_43_add_620: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_43],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_620: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_616, buffer_0__fused_op_43_add_620],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_22:
    target_device: 0
    input_count: 128
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_621.dc.subtract.1_0, e2e_layernorm_621.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_46: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4, e2e_layernorm_621.dc.subtract.1_0, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_23:
    target_device: 0
    input_count: 128
    matmul_624: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_46_0, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_627: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_624],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_634: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_630, e2e__fused_op_46_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_47: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4, layernorm_635.dc.subtract.1, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_638: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_47, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_644: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_47, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_650: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_638, matmul_644],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_24:
    target_device: 0
    input_count: 128
    _fused_op_48: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_650_0, input_1_multiply_652_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_654.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_48, lc.input_tensor.softmax_654.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_658: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_47_0, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_654.dc.reduce_sum.1.lc1, _fused_op_48],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_665: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_49, matmul_658],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_669: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_665, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_673: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_669, e2e__fused_op_47_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_673, lc.input_tensor.layernorm_674.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_674.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_673, layernorm_674.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_674.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_674.dc.subtract.1, layernorm_674.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_674.dc.multiply.2, lc.input_tensor.layernorm_674.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_50: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_674.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_674.4, layernorm_674.dc.subtract.1, layer.12.attention.output.LayerNorm.weight, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_677: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_50, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_680: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_677],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_25:
    target_device: 0
    input_count: 128
    matmul_683: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_680_0, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_687: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_683, e2e__fused_op_50_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_687, lc.input_tensor.layernorm_688.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_688.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_687, layernorm_688.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_688.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_688.dc.subtract.1, layernorm_688.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_688.dc.multiply.2, lc.input_tensor.layernorm_688.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_51: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_688.4, layernorm_688.dc.subtract.1, layer.12.output.LayerNorm.weight, layer.12.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_691: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_697: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_703: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_691, matmul_697],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_52: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_703, input_1_multiply_705_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_707.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_52, lc.input_tensor.softmax_707.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_711: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_51, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_53: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_707.dc.reduce_sum.1.lc1, _fused_op_52],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_718: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_53, matmul_711],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_722: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_718, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_51_add_726: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_51],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_726: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_722, buffer_0__fused_op_51_add_726],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_726, lc.input_tensor.layernorm_727.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_727.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_726, layernorm_727.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_26:
    target_device: 0
    input_count: 128
    layernorm_727.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_727.dc.subtract.1_0, e2e_layernorm_727.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_727.dc.multiply.2, lc.input_tensor.layernorm_727.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_54: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_727.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_727.4, e2e_layernorm_727.dc.subtract.1_0, layer.13.attention.output.LayerNorm.weight, layer.13.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_27:
    target_device: 0
    input_count: 128
    matmul_730: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_54_0, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_733: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_730],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_736: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_733, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_740: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_736, e2e__fused_op_54_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_740, lc.input_tensor.layernorm_741.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_741.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_740, layernorm_741.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_741.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_741.dc.subtract.1, layernorm_741.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_741.dc.multiply.2, lc.input_tensor.layernorm_741.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_55: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_741.4, layernorm_741.dc.subtract.1, layer.13.output.LayerNorm.weight, layer.13.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_744: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_55, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_750: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_55, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_756: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_744, matmul_750],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_28:
    target_device: 0
    input_count: 128
    _fused_op_56: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_756_0, input_1_multiply_758_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_760.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_56, lc.input_tensor.softmax_760.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_764: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_55_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_57: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_760.dc.reduce_sum.1.lc1, _fused_op_56],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_771: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_57, matmul_764],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_775: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_771, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_779: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_775, e2e__fused_op_55_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_779, lc.input_tensor.layernorm_780.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_780.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_779, layernorm_780.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_780.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_780.dc.subtract.1, layernorm_780.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_780.dc.multiply.2, lc.input_tensor.layernorm_780.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_58: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_780.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_780.4, layernorm_780.dc.subtract.1, layer.14.attention.output.LayerNorm.weight, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_783: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_58, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_786: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_783],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_29:
    target_device: 0
    input_count: 128
    matmul_789: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_786_0, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_793: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_789, e2e__fused_op_58_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_793, lc.input_tensor.layernorm_794.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_794.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_793, layernorm_794.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_794.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_794.dc.subtract.1, layernorm_794.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_794.dc.multiply.2, lc.input_tensor.layernorm_794.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_59: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_794.4, layernorm_794.dc.subtract.1, layer.14.output.LayerNorm.weight, layer.14.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_797: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_803: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_809: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_797, matmul_803],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_60: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_809, input_1_multiply_811_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_813.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_60, lc.input_tensor.softmax_813.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_817: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_59, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_61: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_813.dc.reduce_sum.1.lc1, _fused_op_60],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_824: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_61, matmul_817],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_828: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_824, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_59_add_832: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_59],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_832: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_828, buffer_0__fused_op_59_add_832],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_832, lc.input_tensor.layernorm_833.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_833.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_832, layernorm_833.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_30:
    target_device: 0
    input_count: 128
    layernorm_833.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_833.dc.subtract.1_0, e2e_layernorm_833.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_833.dc.multiply.2, lc.input_tensor.layernorm_833.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_62: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_833.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_833.4, e2e_layernorm_833.dc.subtract.1_0, layer.15.attention.output.LayerNorm.weight, layer.15.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_31:
    target_device: 0
    input_count: 128
    matmul_836: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_62_0, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_839: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_836],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_842: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_839, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_846: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_842, e2e__fused_op_62_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_846, lc.input_tensor.layernorm_847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_847.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_846, layernorm_847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_847.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_847.dc.subtract.1, layernorm_847.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_847.dc.multiply.2, lc.input_tensor.layernorm_847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_63: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_847.4, layernorm_847.dc.subtract.1, layer.15.output.LayerNorm.weight, layer.15.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_850: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_63, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_856: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_63, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_862: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_850, matmul_856],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_32:
    target_device: 0
    input_count: 128
    _fused_op_64: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_862_0, input_1_multiply_864_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_866.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_64, lc.input_tensor.softmax_866.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_870: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_63_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_65: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_866.dc.reduce_sum.1.lc1, _fused_op_64],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_877: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_65, matmul_870],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_881: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_877, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_885: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_881, e2e__fused_op_63_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_885, lc.input_tensor.layernorm_886.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_886.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_885, layernorm_886.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_886.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_886.dc.subtract.1, layernorm_886.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_886.dc.multiply.2, lc.input_tensor.layernorm_886.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_66: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_886.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_886.4, layernorm_886.dc.subtract.1, layer.16.attention.output.LayerNorm.weight, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_889: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_66, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_892: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_889],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_33:
    target_device: 0
    input_count: 128
    matmul_895: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_892_0, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_899: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_895, e2e__fused_op_66_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_899, lc.input_tensor.layernorm_900.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_900.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_899, layernorm_900.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_900.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_900.dc.subtract.1, layernorm_900.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_900.dc.multiply.2, lc.input_tensor.layernorm_900.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_67: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_900.4, layernorm_900.dc.subtract.1, layer.16.output.LayerNorm.weight, layer.16.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_903: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_909: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_915: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_903, matmul_909],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_68: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_915, input_1_multiply_917_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_919.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_68, lc.input_tensor.softmax_919.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_923: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_67, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_69: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_919.dc.reduce_sum.1.lc1, _fused_op_68],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_930: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_69, matmul_923],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_934: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_930, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_67_add_938: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_67],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_938: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_934, buffer_0__fused_op_67_add_938],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_938, lc.input_tensor.layernorm_939.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_939.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_938, layernorm_939.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_34:
    target_device: 0
    input_count: 128
    layernorm_939.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_939.dc.subtract.1_0, e2e_layernorm_939.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_939.dc.multiply.2, lc.input_tensor.layernorm_939.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_70: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_939.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_939.4, e2e_layernorm_939.dc.subtract.1_0, layer.17.attention.output.LayerNorm.weight, layer.17.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_35:
    target_device: 0
    input_count: 128
    matmul_942: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_70_0, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_945: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_942],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_948: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_945, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_952: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_948, e2e__fused_op_70_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_952, lc.input_tensor.layernorm_953.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_953.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_952, layernorm_953.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_953.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_953.dc.subtract.1, layernorm_953.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_953.dc.multiply.2, lc.input_tensor.layernorm_953.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_71: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_953.4, layernorm_953.dc.subtract.1, layer.17.output.LayerNorm.weight, layer.17.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_956: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_71, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_962: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_71, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_968: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_956, matmul_962],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_36:
    target_device: 0
    input_count: 128
    _fused_op_72: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_968_0, input_1_multiply_970_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_972.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_72, lc.input_tensor.softmax_972.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_976: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_71_0, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_73: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_972.dc.reduce_sum.1.lc1, _fused_op_72],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_983: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_73, matmul_976],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_987: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_983, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_991: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_987, e2e__fused_op_71_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_991, lc.input_tensor.layernorm_992.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_992.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_991, layernorm_992.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_992.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_992.dc.subtract.1, layernorm_992.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_992.dc.multiply.2, lc.input_tensor.layernorm_992.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_74: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_992.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_992.4, layernorm_992.dc.subtract.1, layer.18.attention.output.LayerNorm.weight, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_995: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_74, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_998: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_995],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_37:
    target_device: 0
    input_count: 128
    matmul_1001: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_998_0, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1005: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1001, e2e__fused_op_74_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1005, lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1006.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1005, layernorm_1006.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1006.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1006.dc.subtract.1, layernorm_1006.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1006.dc.multiply.2, lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_75: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_1006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1006.4, layernorm_1006.dc.subtract.1, layer.18.output.LayerNorm.weight, layer.18.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1009: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1015: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1021: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_1009, matmul_1015],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_76: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_1021, input_1_multiply_1023_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1025.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_76, lc.input_tensor.softmax_1025.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1029: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_75, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_77: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_1025.dc.reduce_sum.1.lc1, _fused_op_76],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1036: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_77, matmul_1029],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1040: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1036, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_75_add_1044: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_75],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1044: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_1040, buffer_0__fused_op_75_add_1044],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_1044, lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1045.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_1044, layernorm_1045.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_38:
    target_device: 0
    input_count: 128
    layernorm_1045.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1045.dc.subtract.1_0, e2e_layernorm_1045.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1045.dc.multiply.2, lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_78: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_1045.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1045.4, e2e_layernorm_1045.dc.subtract.1_0, layer.19.attention.output.LayerNorm.weight, layer.19.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_39:
    target_device: 0
    input_count: 128
    matmul_1048: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_78_0, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1051: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_1048],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1054: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_1051, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1058: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_1054, e2e__fused_op_78_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_1058, lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1059.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_1058, layernorm_1059.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1059.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_1059.dc.subtract.1, layernorm_1059.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1059.dc.multiply.2, lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_79: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_1059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1059.4, layernorm_1059.dc.subtract.1, layer.19.output.LayerNorm.weight, layer.19.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1062: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_79, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1068: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_79, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1074: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_1062, matmul_1068],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_40:
    target_device: 0
    input_count: 128
    _fused_op_80: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_1074_0, input_1_multiply_1076_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1078.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_80, lc.input_tensor.softmax_1078.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1082: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_79_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_81: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_1078.dc.reduce_sum.1.lc1, _fused_op_80],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1089: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_81, matmul_1082],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1093: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_1089, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_1097: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_1093, e2e__fused_op_79_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_1097, lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1098.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_1097, layernorm_1098.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1098.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_1098.dc.subtract.1, layernorm_1098.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1098.dc.multiply.2, lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_82: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_1098.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1098.4, layernorm_1098.dc.subtract.1, layer.20.attention.output.LayerNorm.weight, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1101: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_82, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1104: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_1101],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_41:
    target_device: 0
    input_count: 128
    matmul_1107: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_1104_0, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1111: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1107, e2e__fused_op_82_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1111, lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1112.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1111, layernorm_1112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1112.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1112.dc.subtract.1, layernorm_1112.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1112.dc.multiply.2, lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_83: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_1112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1112.4, layernorm_1112.dc.subtract.1, layer.20.output.LayerNorm.weight, layer.20.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1115: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1121: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1127: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_1115, matmul_1121],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_84: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_1127, input_1_multiply_1129_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1131.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [_fused_op_84, lc.input_tensor.softmax_1131.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1135: {type: matmul, grid_loc: [6, 7], grid_size: [2, 4], inputs: [_fused_op_83, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_85: {type: fused_op, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_1131.dc.reduce_sum.1.lc1, _fused_op_84],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1142: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_85, matmul_1135],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1146: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1142, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_83_add_1150: {type: nop, grid_loc: [8, 6], grid_size: [2, 2], inputs: [_fused_op_83],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1150: {type: add, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_1146, buffer_0__fused_op_83_add_1150],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [add_1150, lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1151.dc.subtract.1: {type: subtract, grid_loc: [8, 10], grid_size: [2, 2], inputs: [add_1150, layernorm_1151.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_42:
    target_device: 0
    input_count: 128
    layernorm_1151.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1151.dc.subtract.1_0, e2e_layernorm_1151.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1151.dc.multiply.2, lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_86: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_1151.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1151.4, e2e_layernorm_1151.dc.subtract.1_0, layer.21.attention.output.LayerNorm.weight, layer.21.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_43:
    target_device: 0
    input_count: 128
    matmul_1154: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_86_0, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1157: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_1154],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1160: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_1157, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1164: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_1160, e2e__fused_op_86_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_1164, lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1165.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_1164, layernorm_1165.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1165.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_1165.dc.subtract.1, layernorm_1165.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1165.dc.multiply.2, lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_87: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_1165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1165.4, layernorm_1165.dc.subtract.1, layer.21.output.LayerNorm.weight, layer.21.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1168: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [_fused_op_87, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1174: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [_fused_op_87, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1180: {type: matmul, grid_loc: [8, 8], grid_size: [2, 2], inputs: [matmul_1168, matmul_1174],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}

  fwd_44:
    target_device: 0
    input_count: 128
    _fused_op_88: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 3], inputs: [e2e_matmul_1180_0, input_1_multiply_1182_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1184.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_88, lc.input_tensor.softmax_1184.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1188: {type: matmul, grid_loc: [0, 5], grid_size: [2, 4], inputs: [e2e__fused_op_87_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_89: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_1184.dc.reduce_sum.1.lc1, _fused_op_88],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1195: {type: matmul, grid_loc: [0, 9], grid_size: [2, 2], inputs: [_fused_op_89, matmul_1188],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1199: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_1195, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_1203: {type: add, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_1199, e2e__fused_op_87_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [add_1203, lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1204.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_1203, layernorm_1204.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1204.dc.multiply.2: {type: multiply, grid_loc: [2, 8], grid_size: [2, 2], inputs: [layernorm_1204.dc.subtract.1, layernorm_1204.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [layernorm_1204.dc.multiply.2, lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_90: {type: fused_op, grid_loc: [2, 11], grid_size: [4, 1], inputs: [layernorm_1204.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1204.4, layernorm_1204.dc.subtract.1, layer.22.attention.output.LayerNorm.weight, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1207: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_90, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1210: {type: gelu, grid_loc: [4, 8], grid_size: [6, 2], inputs: [matmul_1207],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_45:
    target_device: 0
    input_count: 128
    matmul_1213: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_gelu_1210_0, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1217: {type: add, grid_loc: [0, 8], grid_size: [2, 2], inputs: [matmul_1213, e2e__fused_op_90_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [2, 1], inputs: [add_1217, lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1218.dc.subtract.1: {type: subtract, grid_loc: [2, 8], grid_size: [2, 2], inputs: [add_1217, layernorm_1218.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1218.dc.multiply.2: {type: multiply, grid_loc: [2, 10], grid_size: [2, 2], inputs: [layernorm_1218.dc.subtract.1, layernorm_1218.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [2, 1], inputs: [layernorm_1218.dc.multiply.2, lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_91: {type: fused_op, grid_loc: [4, 0], grid_size: [4, 1], inputs: [layernorm_1218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1218.4, layernorm_1218.dc.subtract.1, layer.22.output.LayerNorm.weight, layer.22.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1221: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1227: {type: matmul, grid_loc: [4, 8], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_1233: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [matmul_1221, matmul_1227],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_92: {type: fused_op, grid_loc: [6, 2], grid_size: [2, 3], inputs: [matmul_1233, input_1_multiply_1235_tile_bcast_tile_bcast, attention_mask],
         t: 16, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 16}, broadcast: {r: 12}], input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_1237.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [2, 1], inputs: [_fused_op_92, lc.input_tensor.softmax_1237.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_1241: {type: matmul, grid_loc: [6, 5], grid_size: [2, 4], inputs: [_fused_op_91, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    _fused_op_93: {type: fused_op, grid_loc: [5, 2], grid_size: [2, 1], inputs: [softmax_1237.dc.reduce_sum.1.lc1, _fused_op_92], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 128], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_1248: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [_fused_op_93, matmul_1241],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_1252: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [matmul_1248, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0__fused_op_91_add_1256: {type: nop, grid_loc: [6, 9], grid_size: [2, 2], inputs: [_fused_op_91],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1256: {type: add, grid_loc: [8, 6], grid_size: [2, 2], inputs: [matmul_1252, buffer_0__fused_op_91_add_1256],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [2, 1], inputs: [add_1256, lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1257.dc.subtract.1: {type: subtract, grid_loc: [8, 9], grid_size: [2, 2], inputs: [add_1256, layernorm_1257.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_46:
    target_device: 0
    input_count: 128
    layernorm_1257.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 2], inputs: [e2e_layernorm_1257.dc.subtract.1_0, e2e_layernorm_1257.dc.subtract.1_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1257.dc.multiply.2, lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_94: {type: fused_op, grid_loc: [0, 3], grid_size: [4, 1], inputs: [layernorm_1257.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1257.4, e2e_layernorm_1257.dc.subtract.1_0, layer.23.attention.output.LayerNorm.weight, layer.23.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}

  fwd_47:
    target_device: 0
    input_count: 128
    matmul_1260: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e__fused_op_94_0, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1263: {type: gelu, grid_loc: [0, 8], grid_size: [6, 2], inputs: [matmul_1260],
         t: 1, mblock: [1, 16], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1266: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [gelu_1263, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_1270: {type: add, grid_loc: [0, 10], grid_size: [2, 2], inputs: [matmul_1266, e2e__fused_op_94_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [2, 1], inputs: [add_1270, lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    layernorm_1271.dc.subtract.1: {type: subtract, grid_loc: [4, 10], grid_size: [2, 2], inputs: [add_1270, layernorm_1271.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [80, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_1271.dc.multiply.2: {type: multiply, grid_loc: [6, 8], grid_size: [2, 2], inputs: [layernorm_1271.dc.subtract.1, layernorm_1271.dc.subtract.1],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [2, 1], inputs: [layernorm_1271.dc.multiply.2, lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 2, u_kt: 16}}
    _fused_op_95: {type: fused_op, grid_loc: [6, 10], grid_size: [4, 1], inputs: [layernorm_1271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1271.4, layernorm_1271.dc.subtract.1, layer.23.output.LayerNorm.weight, layer.23.output.LayerNorm.bias],
         t: 1, mblock: [3, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 12}], input_3_tms: [broadcast: {r: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_95_output_nop_0: {type: nop, grid_loc: [8, 0], grid_size: [2, 2], inputs: [_fused_op_95], untilize_output: true,
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_zero: 0, $gptr_q43: 0, $gptr_q40: 0, $gptr_q37: 0, $gptr_q35: 0, $lptr_q35: 0, $lptr_q34: 0, $gptr_q33: 0, $lptr_q33: 0, $gptr_q39: 0, $gptr_q31: 0, $lptr_q31: 0, $lptr_q29: 0, $gptr_q28: 0, $gptr_q27: 0, $gptr_q25: 0, $lptr_q25: 0, $lptr_q23: 0, $gptr_q22: 0, $lptr_q21: 0, $lptr_q3: 0, $gptr_q57: 0, $lptr_q19: 0, $lptr_q59: 0, $lptr_q55: 0, $lptr_q58: 0, $lptr_q37: 0, $gptr_q15: 0, $gptr_q52: 0, $lptr_q40: 0, $gptr_q55: 0, $gptr_q61: 0, $gptr_q63: 0, $lptr_q71: 0, $gptr_q69: 0, $lptr_q43: 0, $gptr_q4: 0, $lptr_q70: 0, $lptr_q69: 0, $gptr_q34: 0, $lptr_q64: 0, $gptr_q58: 0, $gptr_q70: 0, $gptr_q3: 0, $gptr_q59: 0, $gptr_q41: 0, $gptr_q65: 0, $gptr_q45: 0, $gptr_q17: 0, $lptr_q53: 0, $lptr_q57: 0, $lptr_q67: 0, $gptr_q71: 0, $lptr_q63: 0, $gptr_q64: 0, $gptr_q67: 0, $gptr_q19: 0, $lptr_q51: 0, $gptr_q29: 0, $lptr_q65: 0, $lptr_q52: 0, $lptr_q39: 0, $gptr_q51: 0, $lptr_q45: 0, $lptr_q49: 0, $lptr_q16: 0, $gptr_q49: 0, $lptr_q47: 0, $gptr_q53: 0, $gptr_q47: 0, $gptr_q21: 0, $lptr_q46: 0, $lptr_q22: 0, $gptr_q46: 0, $lptr_q41: 0, $gptr_q13: 0, $lptr_q17: 0, $lptr_q61: 0, $lptr_q7: 0, $gptr_q7: 0, $gptr_q16: 0, $lptr_q13: 0, $gptr_q23: 0, $lptr_q15: 0, $lptr_q28: 0, $lptr_q27: 0, $lptr_q11: 0, $gptr_q11: 0, $lptr_q9: 0, $lptr_q10: 0, $gptr_q10: 0, $gptr_q5: 0, $c_one: 1, $gptr_q9: 0, $lptr_q5: 0, $lptr_q4: 0}
    - staticvar: {$gptr_q42_shadow: 0, $gptr_q38_shadow: 0, $lptr_q42: 0, $gptr_q38: 0, $gptr_q36: 0, $lptr_q36: 0, $lptr_q32: 0, $gptr_q30_shadow: 0, $gptr_q30: 0, $gptr_q18_shadow: 0, $lptr_q12: 0, $lptr_q44: 0, $gptr_q8_shadow: 0, $gptr_q18: 0, $gptr_q42: 0, $lptr_q14: 0, $gptr_q6_shadow: 0, $gptr_q44: 0, $lptr_q0: 0, $lptr_q38: 0, $gptr_q48: 0, $gptr_q32_shadow: 0, $gptr_q50_shadow: 0, $gptr_q66: 0, $gptr_q56_shadow: 0, $gptr_q60: 0, $lptr_q26: 0, $lptr_q30: 0, $gptr_q68: 0, $lptr_q48: 0, $lptr_q54: 0, $lptr_q66: 0, $gptr_q32: 0, $gptr_q66_shadow: 0, $gptr_q60_shadow: 0, $lptr_q62: 0, $gptr_q2: 0, $gptr_q62_shadow: 0, $gptr_q12_shadow: 0, $lptr_q56: 0, $gptr_q6: 0, $gptr_q48_shadow: 0, $gptr_q54_shadow: 0, $lptr_q68: 0, $gptr_q0: 0, $lptr_q60: 0, $gptr_q50: 0, $gptr_q56: 0, $gptr_q2_shadow: 0, $gptr_q62: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q12: 0, $lptr_q8: 0, $gptr_q14_shadow: 0, $lptr_q6: 0, $lptr_q2: 0, $lptr_q1: 0, $gptr_q24_shadow: 0, $gptr_q1: 0, $gptr_q1_shadow: 0, $gptr_q20: 0, $gptr_q36_shadow: 0, $gptr_q44_shadow: 0, $gptr_q54: 0, $gptr_q20_shadow: 0, $lptr_q24: 0, $lptr_q18: 0, $lptr_q20: 0, $gptr_q24: 0, $gptr_q26: 0, $lptr_q50: 0, $gptr_q26_shadow: 0}
    - varinst: [$gptr_q66, set, $gptr_q66_shadow]
    - varinst: [$gptr_q62, set, $gptr_q62_shadow]
    - varinst: [$gptr_q60, set, $gptr_q60_shadow]
    - varinst: [$gptr_q56, set, $gptr_q56_shadow]
    - varinst: [$gptr_q54, set, $gptr_q54_shadow]
    - varinst: [$gptr_q50, set, $gptr_q50_shadow]
    - varinst: [$gptr_q48, set, $gptr_q48_shadow]
    - varinst: [$gptr_q44, set, $gptr_q44_shadow]
    - varinst: [$gptr_q18, set, $gptr_q18_shadow]
    - varinst: [$gptr_q14, set, $gptr_q14_shadow]
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q8, set, $gptr_q8_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q20, set, $gptr_q20_shadow]
    - varinst: [$gptr_q24, set, $gptr_q24_shadow]
    - varinst: [$gptr_q26, set, $gptr_q26_shadow]
    - varinst: [$gptr_q30, set, $gptr_q30_shadow]
    - varinst: [$gptr_q32, set, $gptr_q32_shadow]
    - varinst: [$gptr_q36, set, $gptr_q36_shadow]
    - varinst: [$gptr_q38, set, $gptr_q38_shadow]
    - varinst: [$gptr_q42, set, $gptr_q42_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_2_0, e2e_gelu_44_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_91.dc.subtract.1_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_2_0, e2e_gelu_44_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_6_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               e2e_layernorm_91.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_91.dc.subtract.1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_7_0, e2e_matmul_120_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_6_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_10_0, e2e_gelu_150_0]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_matmul_120_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_7_0, e2e_matmul_120_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_197.dc.subtract.1_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_10_0, e2e_gelu_150_0]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_14_0]
    -   execute: {graph_name: fwd_6, queue_settings: {
               e2e_layernorm_197.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_197.dc.subtract.1_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_15_0, e2e_matmul_226_0]
    -   execute: {graph_name: fwd_7, queue_settings: {
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_14_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_18_0, e2e_gelu_256_0]
    -   execute: {graph_name: fwd_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_226_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_15_0, e2e_matmul_226_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_303.dc.subtract.1_0]
    -   execute: {graph_name: fwd_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_gelu_256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_18_0, e2e_gelu_256_0]
    -   varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_22_0]
    -   execute: {graph_name: fwd_10, queue_settings: {
               e2e_layernorm_303.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_303.dc.subtract.1_0]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_23_0, e2e_matmul_332_0]
    -   execute: {graph_name: fwd_11, queue_settings: {
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_22_0]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_26_0, e2e_gelu_362_0]
    -   execute: {graph_name: fwd_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_matmul_332_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_23_0, e2e_matmul_332_0]
    -   varinst: [$gptr_q18_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_409.dc.subtract.1_0]
    -   execute: {graph_name: fwd_13, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_gelu_362_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_26_0, e2e_gelu_362_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_30_0]
    -   execute: {graph_name: fwd_14, queue_settings: {
               e2e_layernorm_409.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_409.dc.subtract.1_0]
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_31_0, e2e_matmul_438_0]
    -   execute: {graph_name: fwd_15, queue_settings: {
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_30_0]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_34_0, e2e_gelu_468_0]
    -   execute: {graph_name: fwd_16, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_matmul_438_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_31_0, e2e_matmul_438_0]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_515.dc.subtract.1_0]
    -   execute: {graph_name: fwd_17, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_34_0, e2e_gelu_468_0]
    -   varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_38_0]
    -   execute: {graph_name: fwd_18, queue_settings: {
               e2e_layernorm_515.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_515.dc.subtract.1_0]
    -   varinst: [$gptr_q28, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_39_0, e2e_matmul_544_0]
    -   execute: {graph_name: fwd_19, queue_settings: {
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_38_0]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_42_0, e2e_gelu_574_0]
    -   execute: {graph_name: fwd_20, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_matmul_544_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_39_0, e2e_matmul_544_0]
    -   varinst: [$gptr_q30_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_621.dc.subtract.1_0]
    -   execute: {graph_name: fwd_21, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_42_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_gelu_574_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_42_0, e2e_gelu_574_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_46_0]
    -   execute: {graph_name: fwd_22, queue_settings: {
               e2e_layernorm_621.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_621.dc.subtract.1_0]
    -   varinst: [$gptr_q34, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_47_0, e2e_matmul_650_0]
    -   execute: {graph_name: fwd_23, queue_settings: {
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_46_0]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_50_0, e2e_gelu_680_0]
    -   execute: {graph_name: fwd_24, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e__fused_op_47_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               e2e_matmul_650_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               input_1_multiply_652_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_47_0, e2e_matmul_650_0]
    -   varinst: [$gptr_q36_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_727.dc.subtract.1_0]
    -   execute: {graph_name: fwd_25, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e__fused_op_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               e2e_gelu_680_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_50_0, e2e_gelu_680_0]
    -   varinst: [$gptr_q38_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_54_0]
    -   execute: {graph_name: fwd_26, queue_settings: {
               e2e_layernorm_727.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               lc.input_tensor.layernorm_727.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_727.dc.subtract.1_0]
    -   varinst: [$gptr_q40, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_55_0, e2e_matmul_756_0]
    -   execute: {graph_name: fwd_27, queue_settings: {
               e2e__fused_op_54_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_54_0]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_58_0, e2e_gelu_786_0]
    -   execute: {graph_name: fwd_28, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e__fused_op_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_matmul_756_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               input_1_multiply_758_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_55_0, e2e_matmul_756_0]
    -   varinst: [$gptr_q42_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_833.dc.subtract.1_0]
    -   execute: {graph_name: fwd_29, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               e2e_gelu_786_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_58_0, e2e_gelu_786_0]
    -   varinst: [$gptr_q44_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q45, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_62_0]
    -   execute: {graph_name: fwd_30, queue_settings: {
               e2e_layernorm_833.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               lc.input_tensor.layernorm_833.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_833.dc.subtract.1_0]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_63_0, e2e_matmul_862_0]
    -   execute: {graph_name: fwd_31, queue_settings: {
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_62_0]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_66_0, e2e_gelu_892_0]
    -   execute: {graph_name: fwd_32, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               e2e__fused_op_63_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_matmul_862_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               input_1_multiply_864_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_63_0, e2e_matmul_862_0]
    -   varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_939.dc.subtract.1_0]
    -   execute: {graph_name: fwd_33, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               e2e_gelu_892_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_66_0, e2e_gelu_892_0]
    -   varinst: [$gptr_q50_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_70_0]
    -   execute: {graph_name: fwd_34, queue_settings: {
               e2e_layernorm_939.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               lc.input_tensor.layernorm_939.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_939.dc.subtract.1_0]
    -   varinst: [$gptr_q52, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q52, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_71_0, e2e_matmul_968_0]
    -   execute: {graph_name: fwd_35, queue_settings: {
               e2e__fused_op_70_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_70_0]
    -   varinst: [$gptr_q53, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q53, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_74_0, e2e_gelu_998_0]
    -   execute: {graph_name: fwd_36, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               e2e__fused_op_71_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e_matmul_968_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               input_1_multiply_970_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_71_0, e2e_matmul_968_0]
    -   varinst: [$gptr_q54_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q55, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q54, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q55, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1045.dc.subtract.1_0]
    -   execute: {graph_name: fwd_37, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               e2e__fused_op_74_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               e2e_gelu_998_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_74_0, e2e_gelu_998_0]
    -   varinst: [$gptr_q56_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q57, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q56, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q57, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_78_0]
    -   execute: {graph_name: fwd_38, queue_settings: {
               e2e_layernorm_1045.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               lc.input_tensor.layernorm_1045.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1045.dc.subtract.1_0]
    -   varinst: [$gptr_q58, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q58, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_79_0, e2e_matmul_1074_0]
    -   execute: {graph_name: fwd_39, queue_settings: {
               e2e__fused_op_78_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_78_0]
    -   varinst: [$gptr_q59, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q59, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_82_0, e2e_gelu_1104_0]
    -   execute: {graph_name: fwd_40, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               e2e__fused_op_79_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               e2e_matmul_1074_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               input_1_multiply_1076_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_79_0, e2e_matmul_1074_0]
    -   varinst: [$gptr_q60_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q61, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q60, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q61, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1151.dc.subtract.1_0]
    -   execute: {graph_name: fwd_41, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               e2e__fused_op_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               e2e_gelu_1104_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_82_0, e2e_gelu_1104_0]
    -   varinst: [$gptr_q62_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q63, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q62, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q63, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_86_0]
    -   execute: {graph_name: fwd_42, queue_settings: {
               e2e_layernorm_1151.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               lc.input_tensor.layernorm_1151.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1151.dc.subtract.1_0]
    -   varinst: [$gptr_q64, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q64, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_87_0, e2e_matmul_1180_0]
    -   execute: {graph_name: fwd_43, queue_settings: {
               e2e__fused_op_86_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_86_0]
    -   varinst: [$gptr_q65, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q65, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_90_0, e2e_gelu_1210_0]
    -   execute: {graph_name: fwd_44, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               e2e__fused_op_87_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               e2e_matmul_1180_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               input_1_multiply_1182_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_87_0, e2e_matmul_1180_0]
    -   varinst: [$gptr_q66_shadow, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q67, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q66, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q67, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_1257.dc.subtract.1_0]
    -   execute: {graph_name: fwd_45, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               e2e__fused_op_90_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               e2e_gelu_1210_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_90_0, e2e_gelu_1210_0]
    -   varinst: [$gptr_q68, incwrap, $c_microbatch_size, 512]
    -   varinst: [$gptr_q69, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q68, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q69, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e__fused_op_94_0]
    -   execute: {graph_name: fwd_46, queue_settings: {
               e2e_layernorm_1257.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               lc.input_tensor.layernorm_1257.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1257.dc.subtract.1_0]
    -   varinst: [$gptr_q70, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q70, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_47, queue_settings: {
               e2e__fused_op_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_94_0]
    -   varinst: [$gptr_q71, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q71, incwrap, $c_microbatch_size, 256]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [3, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [3, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 12}], pop_last: [intermed0], mblock: [3, 3], ublock: [2, 4], output: output}
  2: 
    inputs: 5
    intermediates: 1
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [1, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [3, 1], ublock: [1, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [1, 1], output: intermed0}
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 32}, tile_broadcast: c], pop_last: [intermed0], mblock: [3, 8], ublock: [1, 4], output: dest}
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [dest, input3], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [1, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input4], input_1_tms: [tile_broadcast: r], mblock: [3, 8], ublock: [1, 4], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_1271]
