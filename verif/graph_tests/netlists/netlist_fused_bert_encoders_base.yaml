# git checkout 82b4084a
# pybuda/test/benchmark/benchmark.py -m bert -c base -opt 3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1 --loop_count 100 --layers 1

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                               {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                              {input: HOST, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x31a40020]]}

  # output
  bert_encoders.output_layernorm_53:                                           {input: _fused_op_7_output_nop_0, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x50560c0], [3, 0x50ae600]]}
  layer.0.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x50a1cc0], [1, 0x5062e80]]}
  layer.0.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x5057b00], [7, 0x50a1840]]}
  layer.0.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x50a13c0], [5, 0x50560c0]]}
  layer.0.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x5052c20], [0, 0x50530a0]]}
  layer.0.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5052c20], [6, 0x5054660]]}
  layer.0.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x505f560], [4, 0x50527a0]]}
  layer.0.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x505f560], [2, 0x5052c20]]}
  layer.0.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x5058c40]]}
  layer.0.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5058c40]]}
  layer.0.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x500a020], [2, 0x5003b80], [3, 0x500a020], [4, 0x5003b80], [5, 0x5003b80], [6, 0x50048a0], [7, 0x5003b80], [0, 0x5004000]]}
  layer.0.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5006b80], [2, 0x50006e0], [3, 0x5006b80], [4, 0x50006e0], [5, 0x50006e0], [6, 0x5001400], [7, 0x50006e0], [0, 0x5000b60]]}
  layer.0.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb7f60], [2, 0x4fb1ac0], [3, 0x4fb7f60], [4, 0x4fb1ac0], [5, 0x4fb1ac0], [6, 0x4fb27e0], [7, 0x4fb1ac0], [0, 0x4fb1f40]]}
  layer.0.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb7220], [2, 0x4fb0d80], [3, 0x4fb7220], [4, 0x4fb0d80], [5, 0x4fb0d80], [6, 0x4fb1aa0], [7, 0x4fb0d80], [0, 0x4fb1200]]}
  layer.0.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  layer.0.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900]]}

  # constant
  constant_1_multiply_17:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x50ae180]]}
  lc.input_tensor.softmax_19.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5062a00]]}
  lc.input_tensor.layernorm_39.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x5052c20]]}
  lc.input_tensor.layernorm_39.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x50527a0]]}
  dc.input_tensor.layernorm_39.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x50534c0]]}
  lc.input_tensor.layernorm_39.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x50527a0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x50527a0]]}
  lc.input_tensor.layernorm_53.dc.reduce_avg.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0d80]]}
  lc.input_tensor.layernorm_53.dc.reduce_avg.3.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x4fb0900]]}
  dc.input_tensor.layernorm_53.4:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x4fb0900]]}
  lc.input_tensor.layernorm_53.dc.reciprocal.7_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fb0900]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    matmul_23: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 2], inputs: [matmul_14, constant_1_multiply_17, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 9], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_19.dc.reduce_sum.1.0, _fused_op_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_30: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_1, matmul_23],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_34: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_30, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_2: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_34, hidden_states, lc.input_tensor.layernorm_39.dc.reduce_avg.0.0, matmul_34, hidden_states],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_3: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_2, _fused_op_2, lc.input_tensor.layernorm_39.dc.reduce_avg.3.0, dc.input_tensor.layernorm_39.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_39.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_3, lc.input_tensor.layernorm_39.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_4: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_2, layernorm_39.dc.reciprocal.7_s_brcst_m1_0_0.lc1, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {r: 4}], input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_42: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [_fused_op_4, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    gelu_45: {type: gelu, grid_loc: [1, 7], grid_size: [1, 2], inputs: [matmul_42],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: false}}
    matmul_48: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_45, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 16}}
    _fused_op_5: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_48, _fused_op_4, lc.input_tensor.layernorm_53.dc.reduce_avg.0.0, matmul_48, _fused_op_4],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_6: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_5, _fused_op_5, lc.input_tensor.layernorm_53.dc.reduce_avg.3.0, dc.input_tensor.layernorm_53.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_53.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [_fused_op_6, lc.input_tensor.layernorm_53.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_5, layernorm_53.dc.reciprocal.7_s_brcst_m1_0_0.lc1, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [192, 0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_3_tms: [broadcast: {r: 4}], input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_7_output_nop_0: {type: nop, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_7], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_17: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_19.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_39.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_39.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_39.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_39.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_53.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_53.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_53.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_53.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 512]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_17: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 2], output: dest}
        - add_18: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 2], output: dest}
        - softmax_19.dc.exp.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 3
    intermediates: 2
    schedules: 
      -
        - softmax_19.dc.reduce_sum.1.lc1: { type: matmul, inputs: [input0, input1], attributes: {m_k: 1, u_kt: 4}, mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_19.dc.reciprocal.2: { type: reciprocal, inputs: [intermed0], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 1], output: intermed1}
      -
        - softmax_19.dc.multiply.3: { type: multiply, inputs: [input2, intermed1], input_1_tms: [broadcast: {c: 4}], pop_last: [intermed1], mblock: [2, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 5
    intermediates: 2
    schedules: 
      -
        - add_38: { type: add, inputs: [input0, input1], mblock: [2, 6], ublock: [2, 4], output: intermed0}
        - layernorm_39.dc.reduce_avg.0.lc1: { type: matmul, inputs: [intermed0, input2], attributes: {m_k: 6, u_kt: 4}, pop: [intermed0], mblock: [2, 1], ublock: [2, 1], output: intermed1}
      -
        - add_38: { type: add, inputs: [input3, input4], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_39.dc.subtract.1: { type: subtract, inputs: [dest, intermed1], input_1_tms: [broadcast: {c: 24}], pop_last: [intermed1], mblock: [2, 6], ublock: [2, 4], output: output}
  3: 
    inputs: 4
    intermediates: 2
    schedules: 
      -
        - layernorm_39.dc.multiply.2: { type: multiply, inputs: [input0, input1], mblock: [2, 6], ublock: [2, 4], output: intermed0}
        - layernorm_39.dc.reduce_avg.3.lc1: { type: matmul, inputs: [intermed0, input2], attributes: {m_k: 6, u_kt: 4}, pop: [intermed0], mblock: [2, 1], ublock: [2, 1], output: intermed1}
      -
        - layernorm_39.dc.add.5: { type: add, inputs: [intermed1, input3], pop_last: [intermed1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_39.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_39.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  4: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - layernorm_39.dc.multiply.8: { type: multiply, inputs: [input0, input1], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_39.dc.multiply.9: { type: multiply, inputs: [dest, input2], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_39.dc.add.10: { type: add, inputs: [dest, input3], mblock: [2, 6], ublock: [2, 4], output: output}


test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.25
    normal_stddev: 0.2
  io-config:
    inputs: [attention_mask, hidden_states]
    outputs: [bert_encoders.output_layernorm_53]