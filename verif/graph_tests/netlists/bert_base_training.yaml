# git checkout b5c18644
# pybuda/test/benchmark/benchmark.py -m bert -c base -opt 4 -df Fp16_b -mf HiFi3 -o perf.json --env PYBUDA_EXP_APPROX=1 PYBUDA_FUSE_OPS=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1 PYBUDA_NO_TRIPLET_PLACEMENT=1 PYBUDA_NO_PLACER_BWD_GROUPS=1 --training --microbatch 64

devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                          {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                         {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31860020]]}

  # output
  bert_encoders.output_layernorm_635:                                     {input: _fused_op_95_output_nop_0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  output_grad_hidden_states:                                              {input: _fused_op_179_output_nop_0, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x1800020]}

  # parameter
  layer.0.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fca9c0], [4, 0x6bba9c0], [5, 0x7d809c0], [6, 0x658abe0]]}
  layer.0.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8a00620], [2, 0x88e3800], [3, 0x76ab3e0], [4, 0x73be380]]}
  layer.0.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x86de8c0], [6, 0x70040a0], [7, 0x7c403a0], [0, 0x9396220]]}
  layer.0.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8a03700], [2, 0x88e68e0], [3, 0x76ae4c0], [4, 0x73c1460]]}
  layer.0.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x93df440], [1, 0x8a067e0], [2, 0x88e99c0], [3, 0x76b15a0]]}
  layer.0.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x73c4540], [5, 0x8728320], [6, 0x704d2c0], [7, 0x7c89e00]]}
  layer.0.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9428660], [1, 0x8a4fa00], [2, 0x8932be0], [3, 0x76fa7c0]]}
  layer.0.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x73c7620], [5, 0x872b400], [6, 0x70503a0], [7, 0x7c8cee0]]}
  layer.0.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x77439e0]]}
  layer.0.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x73ca700]]}
  layer.0.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x872e4e0], [6, 0x7053480], [7, 0x7c8ffc0], [0, 0x9473920], [1, 0x8a99460], [2, 0x897dea0], [3, 0x774fd00], [4, 0x73d6a20], [5, 0x8777700], [6, 0x709c6a0], [7, 0x7cd91e0], [0, 0x94bcb40], [1, 0x8ae2680], [2, 0x89c70c0], [3, 0x7798f20], [4, 0x741fc40]]}
  layer.0.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x87c0920], [6, 0x70e58c0], [7, 0x7d22400], [0, 0x9505d60]]}
  layer.0.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8b2b8a0], [2, 0x8a102e0], [3, 0x77e2140], [4, 0x7468e60], [5, 0x87ccc40], [6, 0x70f1be0], [7, 0x7d2e720], [0, 0x9512080], [1, 0x8b8d0c0], [2, 0x8a71b00], [3, 0x7843960], [4, 0x74ca680]]}
  layer.0.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x882e460], [6, 0x7153400], [7, 0x7d8ff40], [0, 0x95738a0], [1, 0x8bee8e0], [2, 0x8ad3320], [3, 0x78a5180], [4, 0x752bea0], [5, 0x882f4c0], [6, 0x7154460], [7, 0x7d90fa0], [0, 0x9574900]]}
  layer.0.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7591d40]]}
  layer.0.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x72a6540]]}
  layer.1.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x84f1600], [6, 0x6e15580], [7, 0x79b8ae0], [0, 0x910f1a0]]}
  layer.1.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8865fe0], [2, 0x87491c0], [3, 0x759e060], [4, 0x72b2860]]}
  layer.1.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x853a820], [6, 0x6e5e7a0], [7, 0x7a01d00], [0, 0x91583c0]]}
  layer.1.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x88690c0], [2, 0x874c2a0], [3, 0x75a1140], [4, 0x72b5940]]}
  layer.1.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7a4af20], [0, 0x91a15e0], [1, 0x886c1a0], [2, 0x874f380]]}
  layer.1.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75a4220], [4, 0x72b8a20], [5, 0x8584280], [6, 0x6ea8200]]}
  layer.1.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7a94140], [0, 0x91ea800], [1, 0x88b53c0], [2, 0x87985a0]]}
  layer.1.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x79b5a00], [0, 0x910c0c0], [1, 0x8862f00], [2, 0x87460e0]]}
  layer.1.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7add360]]}
  layer.1.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9233a20]]}
  layer.1.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x88fe5e0], [2, 0x87e17c0], [3, 0x75a93a0], [4, 0x72bc340], [5, 0x8587ba0], [6, 0x6ead380], [7, 0x7ae9680], [0, 0x923fd40], [1, 0x8947800], [2, 0x882a9e0], [3, 0x75f25c0], [4, 0x7305560], [5, 0x85d0dc0], [6, 0x6ef65a0], [7, 0x7b328a0], [0, 0x9288f60]]}
  layer.1.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8990a20], [2, 0x8873c00], [3, 0x763b7e0], [4, 0x734e780]]}
  layer.1.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8619fe0], [6, 0x6f3f7c0], [7, 0x7b7bac0], [0, 0x92d2180], [1, 0x899cd40], [2, 0x887ff20], [3, 0x7647b00], [4, 0x735aaa0], [5, 0x867b800], [6, 0x6fa0fe0], [7, 0x7bdd2e0], [0, 0x93339a0]]}
  layer.1.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x89fe560], [2, 0x88e1740], [3, 0x76a9320], [4, 0x73bc2c0], [5, 0x86dd020], [6, 0x7002800], [7, 0x7c3eb00], [0, 0x93951c0], [1, 0x89ff5c0], [2, 0x88e27a0], [3, 0x76aa380], [4, 0x73bd320]]}
  layer.1.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x72f2be0]]}
  layer.1.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7ee85a0]]}
  layer.2.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9690040], [1, 0x8d07fc0], [2, 0x8cc4720], [3, 0x7ae07c0]]}
  layer.2.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77b78a0], [5, 0x8abd760], [6, 0x72fef00], [7, 0x7ef48c0]]}
  layer.2.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x96d9260], [1, 0x8d511e0], [2, 0x8d0d940], [3, 0x7b299e0]]}
  layer.2.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x77ba980], [5, 0x8ac0840], [6, 0x7301fe0], [7, 0x7ef79a0]]}
  layer.2.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d56b60], [3, 0x7b72c00], [4, 0x77bda60], [5, 0x8ac3920]]}
  layer.2.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x73050c0], [7, 0x7efaa80], [0, 0x9722cc0], [1, 0x8d9ac40]]}
  layer.2.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d9fd80], [3, 0x7bbbe20], [4, 0x7806c80], [5, 0x8b0cb40]]}
  layer.2.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8cc1640], [3, 0x7add6e0], [4, 0x77b47c0], [5, 0x8aba680]]}
  layer.2.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8de8fa0]]}
  layer.2.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c05040]]}
  layer.2.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x784fea0], [5, 0x8b55d60], [6, 0x73089e0], [7, 0x7efe3a0], [0, 0x97265e0], [1, 0x8d9fdc0], [2, 0x8df52c0], [3, 0x7c11360], [4, 0x78990c0], [5, 0x8b9ef80], [6, 0x7351c00], [7, 0x7f475c0], [0, 0x976f800], [1, 0x8de8fe0], [2, 0x8e3e4e0], [3, 0x7c5a580]]}
  layer.2.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x78e22e0], [5, 0x8be81a0], [6, 0x739ae20], [7, 0x7f907e0]]}
  layer.2.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x97b8a20], [1, 0x8e32200], [2, 0x8e87700], [3, 0x7ca37a0], [4, 0x78ee600], [5, 0x8bf44c0], [6, 0x73a7140], [7, 0x7f9cb00], [0, 0x981a240], [1, 0x8e93a20], [2, 0x8ee8f20], [3, 0x7d04fc0]]}
  layer.2.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x794fe20], [5, 0x8c55ce0], [6, 0x7408960], [7, 0x7ffe320], [0, 0x987ba60], [1, 0x8ef5240], [2, 0x8f4a740], [3, 0x7d667e0], [4, 0x7950e80], [5, 0x8c56d40], [6, 0x74099c0], [7, 0x7fff380]]}
  layer.2.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9575960]]}
  layer.2.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8bf0180]]}
  layer.3.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ad4bc0], [3, 0x78ef400], [4, 0x7576120], [5, 0x8879740]]}
  layer.3.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x719e6e0], [7, 0x7d940a0], [0, 0x9581c80], [1, 0x8bfc4a0]]}
  layer.3.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b1dde0], [3, 0x7938620], [4, 0x75bf340], [5, 0x88c2960]]}
  layer.3.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x71a17c0], [7, 0x7d97180], [0, 0x9584d60], [1, 0x8bff580]]}
  layer.3.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7608560], [5, 0x890bb80], [6, 0x71a48a0], [7, 0x7d9a260]]}
  layer.3.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9587e40], [1, 0x8c02660], [2, 0x8b67840], [3, 0x7982080]]}
  layer.3.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x78a61e0], [4, 0x752cf00], [5, 0x8830520], [6, 0x71554c0]]}
  layer.3.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8954da0], [6, 0x71edac0], [7, 0x7de3480], [0, 0x958af20]]}
  layer.3.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7651fc0]]}
  layer.3.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8957e80]]}
  layer.3.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x71f0ba0], [7, 0x7de6560], [0, 0x958e000], [1, 0x8c05f80], [2, 0x8b6b160], [3, 0x7987200], [4, 0x765e2e0], [5, 0x89641a0], [6, 0x7239dc0], [7, 0x7e2f780], [0, 0x95d7220], [1, 0x8c4f1a0], [2, 0x8bb4380], [3, 0x79d0420], [4, 0x76a7500], [5, 0x89ad3c0]]}
  layer.3.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7282fe0], [7, 0x7e789a0], [0, 0x9620440], [1, 0x8c983c0]]}
  layer.3.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8bfd5a0], [3, 0x7a19640], [4, 0x76f0720], [5, 0x89f65e0], [6, 0x728f300], [7, 0x7e84cc0], [0, 0x962c760], [1, 0x8ca46e0], [2, 0x8c5edc0], [3, 0x7a7ae60], [4, 0x7751f40], [5, 0x8a57e00]]}
  layer.3.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x72f0b20], [7, 0x7ee64e0], [0, 0x968df80], [1, 0x8d05f00], [2, 0x8cc05e0], [3, 0x7adc680], [4, 0x77b3760], [5, 0x8ab9620], [6, 0x72f1b80], [7, 0x7ee7540], [0, 0x968efe0], [1, 0x8d06f60]]}
  layer.3.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8396d20]]}
  layer.3.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x835cee0]]}
  layer.4.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x71b25a0], [4, 0x6e3ab20], [5, 0x8018900], [6, 0x6866be0]]}
  layer.4.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x741de20], [0, 0x8bf4c80], [1, 0x83a3040], [2, 0x8369200]]}
  layer.4.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x71fb7c0], [4, 0x6e83d40], [5, 0x8061b20], [6, 0x68afe00]]}
  layer.4.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7420f00], [0, 0x8bf7d60], [1, 0x83a6120], [2, 0x836c2e0]]}
  layer.4.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x80aad40], [6, 0x68f9020], [7, 0x7423fe0], [0, 0x8bfae40]]}
  layer.4.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x71af4c0], [4, 0x6e37a40], [5, 0x8015820], [6, 0x6863b00]]}
  layer.4.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x80f4fc0], [6, 0x69432a0], [7, 0x746e260], [0, 0x8c450c0]]}
  layer.4.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x83ab2c0], [2, 0x8371480], [3, 0x72472e0], [4, 0x6ecf860]]}
  layer.4.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8c8e2e0]]}
  layer.4.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x83ae3a0]]}
  layer.4.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8374560], [3, 0x724a3c0], [4, 0x6ed2940], [5, 0x813ea20], [6, 0x698cd00], [7, 0x74b9520], [0, 0x8c9a600], [1, 0x83ba6c0], [2, 0x83bd780], [3, 0x72935e0], [4, 0x6f1bb60], [5, 0x8187c40], [6, 0x69d5f20], [7, 0x7502740], [0, 0x8ce3820], [1, 0x84038e0]]}
  layer.4.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x84069a0], [3, 0x72dc800], [4, 0x6f64d80], [5, 0x81d0e60]]}
  layer.4.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6a1f140], [7, 0x754b960], [0, 0x8d2ca40], [1, 0x844cb00], [2, 0x8412cc0], [3, 0x72e8b20], [4, 0x6f710a0], [5, 0x81dd180], [6, 0x6a80960], [7, 0x75ad180], [0, 0x8d8e260], [1, 0x84ae320]]}
  layer.4.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6718060], [7, 0x727c640], [0, 0x8a503e0], [1, 0x81ea2a0], [2, 0x81f4520], [3, 0x70a8320], [4, 0x6d308a0], [5, 0x7f0e680], [6, 0x67190c0], [7, 0x727d6a0], [0, 0x8a51440], [1, 0x81eb300]]}
  layer.4.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81e2040]]}
  layer.4.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7095e40]]}
  layer.5.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c9dc20], [5, 0x7e7ba00], [6, 0x6685c20], [7, 0x71ea200]]}
  layer.5.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a4a220], [1, 0x81e40e0], [2, 0x81ee360], [3, 0x70a2160]]}
  layer.5.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ce6e40], [5, 0x7ec4c20], [6, 0x66cee40], [7, 0x7233420]]}
  layer.5.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a4d300], [1, 0x81e71c0], [2, 0x81f1440], [3, 0x70a5240]]}
  layer.5.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x704cc20], [4, 0x6c54a00], [5, 0x7e327e0], [6, 0x663ca00]]}
  layer.5.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81f5580], [3, 0x70a9380], [4, 0x6d31900], [5, 0x7f0f6e0]]}
  layer.5.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x671a120], [7, 0x727e700], [0, 0x8a524a0], [1, 0x81ec360]]}
  layer.5.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81f8660], [3, 0x70ac460], [4, 0x6d349e0], [5, 0x7f127c0]]}
  layer.5.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8235580]]}
  layer.5.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81fb740]]}
  layer.5.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x70af540], [4, 0x6d37ac0], [5, 0x7f158a0], [6, 0x6763b80], [7, 0x72c8160], [0, 0x8a9d760], [1, 0x82418a0], [2, 0x8207a60], [3, 0x70f8760], [4, 0x6d80ce0], [5, 0x7f5eac0], [6, 0x67acda0], [7, 0x7311380], [0, 0x8ae6980], [1, 0x828aac0], [2, 0x8250c80]]}
  layer.5.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7141980], [4, 0x6dc9f00], [5, 0x7fa7ce0], [6, 0x67f5fc0]]}
  layer.5.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x735a5a0], [0, 0x8b2fba0], [1, 0x82d3ce0], [2, 0x8299ea0], [3, 0x714dca0], [4, 0x6dd6220], [5, 0x7fb4000], [6, 0x68022e0], [7, 0x73bbdc0], [0, 0x8b913c0], [1, 0x8335500], [2, 0x82fb6c0]]}
  layer.5.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x83a9200], [2, 0x836f3c0], [3, 0x7245220], [4, 0x6ecd7a0], [5, 0x80f3f60], [6, 0x6942240], [7, 0x746d200], [0, 0x8c44060], [1, 0x83aa260], [2, 0x8370420], [3, 0x7246280], [4, 0x6ece800]]}
  layer.5.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x843a800]]}
  layer.5.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d1aee0]]}
  layer.6.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7846ee0], [0, 0x8f9bd40], [1, 0x86e8900], [2, 0x860f380]]}
  layer.6.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x74e7260], [4, 0x71fba60], [5, 0x8446b20], [6, 0x6d27200]]}
  layer.6.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7890100], [0, 0x8fe4f60], [1, 0x8731b20], [2, 0x86585a0]]}
  layer.6.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x74ea340], [4, 0x71feb40], [5, 0x8449c00], [6, 0x6d2a2e0]]}
  layer.6.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6cd1cc0], [7, 0x77fdcc0], [0, 0x8f52b20], [1, 0x869f6e0]]}
  layer.6.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x86a17c0], [3, 0x74ed420], [4, 0x7201c20], [5, 0x844cce0]]}
  layer.6.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d2d3c0], [7, 0x78d9b60], [0, 0x902e9c0], [1, 0x877b580]]}
  layer.6.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x86a48a0], [3, 0x74f0500], [4, 0x7204d00], [5, 0x844fdc0]]}
  layer.6.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x87c47a0]]}
  layer.6.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x86a7980]]}
  layer.6.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x74f35e0], [4, 0x7207de0], [5, 0x8452ea0], [6, 0x6d76e20], [7, 0x79235c0], [0, 0x9079c80], [1, 0x87d0ac0], [2, 0x86b3ca0], [3, 0x753c800], [4, 0x7251000], [5, 0x849c0c0], [6, 0x6dc0040], [7, 0x796c7e0], [0, 0x90c2ea0], [1, 0x8819ce0], [2, 0x86fcec0]]}
  layer.6.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7585a20], [4, 0x729a220], [5, 0x84e52e0], [6, 0x6e09260]]}
  layer.6.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82d2ea0], [6, 0x6b76680], [7, 0x76a2680], [0, 0x8df74e0], [1, 0x8518e00], [2, 0x8488260], [3, 0x735e8e0], [4, 0x7067600], [5, 0x83346c0], [6, 0x6bd7ea0], [7, 0x7703ea0], [0, 0x8e58d00]]}
  layer.6.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x734a340], [4, 0x6fd28c0], [5, 0x823e9a0], [6, 0x6ae2180], [7, 0x760e9a0], [0, 0x8defa80], [1, 0x850fb40], [2, 0x8474d20], [3, 0x734b3a0], [4, 0x6fd3920], [5, 0x823fa00], [6, 0x6ae31e0]]}
  layer.6.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8475d80]]}
  layer.6.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x734c400]]}
  layer.7.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6fd4980], [5, 0x8240a60], [6, 0x6ae4240], [7, 0x7610240]]}
  layer.7.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8df1320], [1, 0x8512c40], [2, 0x84820a0], [3, 0x7358720]]}
  layer.7.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x701dba0], [5, 0x8289c80], [6, 0x6b2d460], [7, 0x7659460]]}
  layer.7.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8df4400], [1, 0x8515d20], [2, 0x8485180], [3, 0x735b800]]}
  layer.7.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x857a620], [2, 0x84e9a80], [3, 0x73c0100], [4, 0x70c8e20]]}
  layer.7.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8395ee0], [6, 0x6c396c0], [7, 0x77656c0], [0, 0x8eba520]]}
  layer.7.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x85c3840], [2, 0x8532ca0], [3, 0x7409320], [4, 0x7112040]]}
  layer.7.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8398fc0], [6, 0x6c3c7a0], [7, 0x77687a0], [0, 0x8ebd600]]}
  layer.7.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x715b260]]}
  layer.7.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x839c0a0]]}
  layer.7.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6c3f880], [7, 0x776b880], [0, 0x8ec06e0], [1, 0x860d2a0], [2, 0x857c700], [3, 0x74545e0], [4, 0x7167580], [5, 0x83a83c0], [6, 0x6c88aa0], [7, 0x77b4aa0], [0, 0x8f09900], [1, 0x86564c0], [2, 0x85c5920], [3, 0x749d800], [4, 0x71b07a0], [5, 0x83f15e0]]}
  layer.7.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e21f40], [4, 0x7a00b00], [5, 0x8cee3a0], [6, 0x74c5120]]}
  layer.7.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bead80], [5, 0x8ec20a0], [6, 0x772f320], [7, 0x8322440], [0, 0x9c2de80], [1, 0x92c0ca0], [2, 0x927c3c0], [3, 0x809b520], [4, 0x7c4c5a0], [5, 0x8f238c0], [6, 0x7790b40], [7, 0x8383c60]]}
  layer.7.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9c8f6a0], [1, 0x93224c0], [2, 0x92ddbe0], [3, 0x80fcd40], [4, 0x7caddc0], [5, 0x8f850e0], [6, 0x77f2360], [7, 0x83e5480], [0, 0x9c90700], [1, 0x9323520], [2, 0x92dec40], [3, 0x80fdda0]]}
  layer.7.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x83e64e0]]}
  layer.7.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9c91760]]}
  layer.8.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9324580], [2, 0x92dfca0], [3, 0x80fee00], [4, 0x7caf660]]}
  layer.8.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8f86980], [6, 0x77f5460], [7, 0x83f2800], [0, 0x9c9da80]]}
  layer.8.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x936d7a0], [2, 0x9328ec0], [3, 0x8148020], [4, 0x7cf8880]]}
  layer.8.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8f89a60], [6, 0x77f8540], [7, 0x83f58e0], [0, 0x9ca0b60]]}
  layer.8.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8191240], [4, 0x7d41aa0], [5, 0x8f8cb40], [6, 0x77fb620]]}
  layer.8.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x83f89c0], [0, 0x9ca3c40], [1, 0x93b8a60], [2, 0x9372920]]}
  layer.8.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81da460], [4, 0x7d8acc0], [5, 0x8fd5d60], [6, 0x7844840]]}
  layer.8.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x83fbaa0], [0, 0x9ca6d20], [1, 0x93bbb40], [2, 0x9375a00]]}
  layer.8.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x788da60]]}
  layer.8.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x83feb80]]}
  layer.8.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b51f40], [5, 0x8e29260], [6, 0x760a260], [7, 0x81fd380], [0, 0x9afd2e0], [1, 0x9190100], [2, 0x91d7aa0], [3, 0x80026e0], [4, 0x7b9b160], [5, 0x8e72480], [6, 0x7653480], [7, 0x82465a0], [0, 0x9b46500], [1, 0x91d9320], [2, 0x9220cc0], [3, 0x804b900]]}
  layer.8.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x817e380], [0, 0x99fb2a0], [1, 0x908e0c0], [2, 0x90d5a60]]}
  layer.8.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7eab9c0], [4, 0x7a8a580], [5, 0x8d60040], [6, 0x7536dc0], [7, 0x818a6a0], [0, 0x9a075c0], [1, 0x909a3e0], [2, 0x90e1d80], [3, 0x7f0d1e0], [4, 0x7aebda0], [5, 0x8dc1860], [6, 0x75985e0]]}
  layer.8.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x81ebec0], [0, 0x9a68de0], [1, 0x90fbc00], [2, 0x91435a0], [3, 0x7f6ea00], [4, 0x7b4d5c0], [5, 0x8e23080], [6, 0x75f9e00], [7, 0x81ecf20], [0, 0x9a69e40], [1, 0x90fcc60], [2, 0x9144600]]}
  layer.8.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x75fae60]]}
  layer.8.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x81edf80]]}
  layer.9.attention.self.query.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9a6aea0], [1, 0x90fdcc0], [2, 0x9145660], [3, 0x7f702a0]]}
  layer.9.attention.self.query.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b4ee60], [5, 0x8e26180], [6, 0x7607180], [7, 0x81fa2a0]]}
  layer.9.attention.self.key.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9ab40c0], [1, 0x9146ee0], [2, 0x918e880], [3, 0x7fb94c0]]}
  layer.9.attention.self.key.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ea88e0], [4, 0x7a874a0], [5, 0x8d5cf60], [6, 0x7533ce0]]}
  layer.9.attention.self.value.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x769c6a0], [7, 0x828f7c0], [0, 0x9b8f720], [1, 0x9222540]]}
  layer.9.attention.self.value.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9269ee0], [3, 0x8094b20], [4, 0x7be4bc0], [5, 0x8ebbee0]]}
  layer.9.attention.output.dense.weight:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x76e58c0], [7, 0x82d89e0], [0, 0x9bd8940], [1, 0x926b760]]}
  layer.9.attention.output.dense.bias:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x926cfc0], [3, 0x8097c00], [4, 0x7be7ca0], [5, 0x8ebefc0]]}
  layer.9.attention.output.LayerNorm.weight:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x92b4980]]}
  layer.9.attention.output.LayerNorm.bias:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x92700a0]]}
  layer.9.intermediate.dense.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ed46a0], [5, 0x9120fa0], [6, 0x7999d00], [7, 0x850ae20], [0, 0x9e484a0], [1, 0x955d2c0], [2, 0x9517180], [3, 0x83c2560], [4, 0x7f1d8c0], [5, 0x916a1c0], [6, 0x79e2f20], [7, 0x8554040], [0, 0x9e916c0], [1, 0x95a64e0], [2, 0x95603a0], [3, 0x840b780]]}
  layer.9.intermediate.dense.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x85b6960], [0, 0x9ef3fe0], [1, 0x96413e0], [2, 0x95fb2a0]]}
  layer.9.output.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8532900], [4, 0x8045280], [5, 0x924f320], [6, 0x7ad3b60], [7, 0x85c2c80], [0, 0x9f00300], [1, 0x964d700], [2, 0x96075c0], [3, 0x8594120], [4, 0x80a6aa0], [5, 0x92b0b40], [6, 0x7b35380]]}
  layer.9.output.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x86244a0], [0, 0x9f61b20], [1, 0x96aef20], [2, 0x9668de0], [3, 0x85f5940], [4, 0x81082c0], [5, 0x9312360], [6, 0x7b96ba0], [7, 0x8625500], [0, 0x9f62b80], [1, 0x96aff80], [2, 0x9669e40]]}
  layer.9.output.LayerNorm.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7b97c00]]}
  layer.9.output.LayerNorm.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7ac7840]]}
  layer.10.attention.self.query.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9f63be0], [1, 0x96b0fe0], [2, 0x966aea0], [3, 0x85f71e0]]}
  layer.10.attention.self.query.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8109b60], [5, 0x9315460], [6, 0x7ba3f20], [7, 0x8632880]]}
  layer.10.attention.self.key.weight:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9face00], [1, 0x96fa200], [2, 0x96b40c0], [3, 0x8640400]]}
  layer.10.attention.self.key.bias:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8635960], [0, 0x9ff6020], [1, 0x9743420], [2, 0x96fd2e0]]}
  layer.10.attention.self.value.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xa042320], [1, 0x978f720], [2, 0x97495e0], [3, 0x86d3080]]}
  layer.10.attention.self.value.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8110560], [5, 0x9327940], [6, 0x7bac180], [7, 0x863bb20]]}
  layer.10.attention.output.dense.weight:                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9ff9100], [1, 0x9746500], [2, 0x97003c0], [3, 0x8689e60]]}
  layer.10.attention.output.dense.bias:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x810d480], [5, 0x9324860], [6, 0x7ba90a0], [7, 0x8638a40]]}
  layer.10.attention.output.LayerNorm.weight:                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9318540]]}
  layer.10.attention.output.LayerNorm.bias:                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8626560]]}
  layer.10.intermediate.dense.weight:                                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7dd4720], [5, 0x9021020], [6, 0x7899d80], [7, 0x840aea0], [0, 0x9cf3020], [1, 0x9407e40], [2, 0x93c1d00], [3, 0x826d0e0], [4, 0x7e1d940], [5, 0x906a240], [6, 0x78e2fa0], [7, 0x84540c0], [0, 0x9d3c240], [1, 0x9451060], [2, 0x940af20], [3, 0x82b6300]]}
  layer.10.intermediate.dense.bias:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e66b60], [5, 0x90b3460], [6, 0x792c1c0], [7, 0x849d2e0]]}
  layer.10.output.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9d85460], [1, 0x949a280], [2, 0x9454140], [3, 0x82ff520], [4, 0x7e72e80], [5, 0x90bf780], [6, 0x79384e0], [7, 0x84a9600], [0, 0x9de6c80], [1, 0x94fbaa0], [2, 0x94b5960], [3, 0x8360d40]]}
  layer.10.output.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9638920], [2, 0x95f27e0], [3, 0x849dbc0], [4, 0x7fb0540], [5, 0x91b6d00], [6, 0x7a312c0], [7, 0x85ac660], [0, 0x9ee9ce0], [1, 0x9639980], [2, 0x95f3840], [3, 0x849ec20], [4, 0x7fb15a0]]}
  layer.10.output.LayerNorm.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x859d260]]}
  layer.10.output.LayerNorm.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9eda8e0]]}
  layer.11.attention.self.query.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x95ef700], [2, 0x95a95c0], [3, 0x84549a0], [4, 0x7f67320]]}
  layer.11.attention.self.query.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x91b3c20], [6, 0x7a2e1e0], [7, 0x85a9580], [0, 0x9ee6c00]]}
  layer.11.attention.self.key.weight:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9ca9e00], [1, 0x93bec20], [2, 0x9378ae0], [3, 0x8223ec0]]}
  layer.11.attention.self.key.bias:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x91b7d60], [6, 0x7a32320], [7, 0x85ad6c0], [0, 0x9eead40]]}
  layer.11.attention.self.value.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [6, 3], ublock: [4, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x849fc80], [4, 0x7fb2600], [5, 0x91bae40], [6, 0x7a35400]]}
  layer.11.attention.self.value.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x85b07a0], [0, 0x9eede20], [1, 0x963b220], [2, 0x95f50e0]]}
  layer.11.attention.output.dense.weight:                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [12, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x84e8ea0], [4, 0x7ffb820], [5, 0x9204060], [6, 0x7a7e620]]}
  layer.11.attention.output.dense.bias:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 3], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x85b3880], [0, 0x9ef0f00], [1, 0x963e300], [2, 0x95f81c0]]}
  layer.11.attention.output.LayerNorm.weight:                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9c21b60]]}
  layer.11.attention.output.LayerNorm.bias:                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7423040]]}
  layer.11.intermediate.dense.weight:                                     {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [6, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8018a00], [0, 0x987db40], [1, 0x8ef7320], [2, 0x8f4c820], [3, 0x7d688c0], [4, 0x7952f60], [5, 0x8c58e20], [6, 0x742f360], [7, 0x8061c20], [0, 0x98c6d60], [1, 0x8f40540], [2, 0x8f95a40], [3, 0x7db1ae0], [4, 0x799c180], [5, 0x8ca2040], [6, 0x7478580]]}
  layer.11.intermediate.dense.bias:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80aae40], [0, 0x990ff80], [1, 0x8f89760], [2, 0x8fdec60]]}
  layer.11.output.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [16, 1], ublock: [6, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80bab00], [0, 0x991fc40], [1, 0x8fb2a60], [2, 0x90121e0], [3, 0x7e46880], [4, 0x7a25440], [5, 0x8cfaf00], [6, 0x74d1c80], [7, 0x811c320], [0, 0x9981460], [1, 0x9014280], [2, 0x9073a00]]}
  layer.11.output.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 12], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7dfad00], [4, 0x79e53a0], [5, 0x8ceb260], [6, 0x74c17a0], [7, 0x80b7160], [0, 0x991c2a0], [1, 0x8f95a80], [2, 0x8feaf80], [3, 0x7dfbd60], [4, 0x79e6400], [5, 0x8cec2c0], [6, 0x74c2800]]}
  layer.11.output.LayerNorm.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8febfe0]]}
  layer.11.output.LayerNorm.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7dfcdc0]]}

  # constant
  input_1_multiply_16_fork_clone4377_tile_bcast_tile_bcast:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8727ae0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7c895c0]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7c3fb60]]}
  lc.input_tensor.layernorm_38.dc.reduce_avg.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8a98c20]]}
  dc.input_tensor.layernorm_38.4:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x897be00]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8bef940]]}
  lc.input_tensor.layernorm_52.dc.reduce_avg.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ad4380]]}
  dc.input_tensor.layernorm_52.4:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x75a7300]]}
  input_1_multiply_69_fork_clone4406_tile_bcast_tile_bcast:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8583a40]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6ea79c0]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x72bbb00]]}
  lc.input_tensor.layernorm_91.dc.reduce_avg.3.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8587360]]}
  dc.input_tensor.layernorm_91.4:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6eab2e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x86de080]]}
  lc.input_tensor.layernorm_105.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7003860]]}
  dc.input_tensor.layernorm_105.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9471880]]}
  input_1_multiply_122_fork_clone4423_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9722480]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d9a400]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7efdb60]]}
  lc.input_tensor.layernorm_144.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9725da0]]}
  dc.input_tensor.layernorm_144.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d9dd20]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x987cac0]]}
  lc.input_tensor.layernorm_158.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7651780]]}
  dc.input_tensor.layernorm_158.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7d92000]]}
  input_1_multiply_175_fork_clone4440_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b67000]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7981840]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8c05740]]}
  lc.input_tensor.layernorm_197.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8b6a920]]}
  dc.input_tensor.layernorm_197.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7985160]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x877ad40]]}
  lc.input_tensor.layernorm_211.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x741d5e0]]}
  dc.input_tensor.layernorm_211.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8bf2be0]]}
  input_1_multiply_228_fork_clone4457_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x72449e0]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6eccf60]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x813e1e0]]}
  lc.input_tensor.layernorm_250.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x698c4c0]]}
  dc.input_tensor.layernorm_250.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x74b7480]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e99c0]]}
  lc.input_tensor.layernorm_264.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a499e0]]}
  dc.input_tensor.layernorm_264.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81e2040]]}
  input_1_multiply_281_fork_clone4474_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6d30060]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f0de40]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6763340]]}
  lc.input_tensor.layernorm_303.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x72c7920]]}
  dc.input_tensor.layernorm_303.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a9b6c0]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x860eb40]]}
  lc.input_tensor.layernorm_317.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x74e6a20]]}
  dc.input_tensor.layernorm_317.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x71f99c0]]}
  input_1_multiply_334_fork_clone4491_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x78d9320]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x902e180]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6d765e0]]}
  lc.input_tensor.layernorm_356.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7922d80]]}
  dc.input_tensor.layernorm_356.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9077be0]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x760fa00]]}
  lc.input_tensor.layernorm_370.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8df0ae0]]}
  dc.input_tensor.layernorm_370.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8510ba0]]}
  input_1_multiply_387_fork_clone4508_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7066dc0]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x84744e0]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x860ca60]]}
  lc.input_tensor.layernorm_409.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x857bec0]]}
  dc.input_tensor.layernorm_409.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7452540]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7caee20]]}
  lc.input_tensor.layernorm_423.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8f86140]]}
  dc.input_tensor.layernorm_423.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x77f33c0]]}
  input_1_multiply_440_fork_clone4525_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x809ace0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x93720e0]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8223680]]}
  lc.input_tensor.layernorm_462.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7dd3ee0]]}
  dc.input_tensor.layernorm_462.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x901ef80]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f6fa60]]}
  lc.input_tensor.layernorm_476.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b4e620]]}
  dc.input_tensor.layernorm_476.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8e240e0]]}
  input_1_multiply_493_fork_clone4542_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7be4380]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8ebb6a0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x772eae0]]}
  lc.input_tensor.layernorm_515.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x8321c00]]}
  dc.input_tensor.layernorm_515.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x93b69c0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85f69a0]]}
  lc.input_tensor.layernorm_529.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8109320]]}
  dc.input_tensor.layernorm_529.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x93133c0]]}
  input_1_multiply_546_fork_clone4559_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x932aa20]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8113640]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8689620]]}
  lc.input_tensor.layernorm_568.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x810cc40]]}
  dc.input_tensor.layernorm_568.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7ba7000]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f66ae0]]}
  lc.input_tensor.layernorm_582.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x91b33e0]]}
  dc.input_tensor.layernorm_582.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7a2c140]]}
  input_1_multiply_599_fork_clone4576_tile_bcast_tile_bcast:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x963a9e0]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x95f48a0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85320c0]]}
  lc.input_tensor.layernorm_621.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8044a40]]}
  dc.input_tensor.layernorm_621.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x924d280]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80b81c0]]}
  lc.input_tensor.layernorm_635.dc.reduce_avg.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x991d300]]}
  dc.input_tensor.layernorm_635.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8f96ae0]]}
  lc.input_tensor.bw_in2_layernorm_635_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x79e7460]]}
  lc.input_tensor.bw_in1_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80b8a00]]}
  lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x991db40]]}
  lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8f98b80]]}
  lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ff8300]]}
  dc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e090e0], [4, 0x79e7ca0]]}
  lc.input_tensor.bw_in1_add_632_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8ced320]]}
  lc.input_tensor.bw_in1_add_626_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74c40a0]]}
  lc.input_tensor.bw_in2_layernorm_621_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8ef62a0]]}
  lc.input_tensor.bw_in1_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8f4b7a0]]}
  lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d67840]]}
  lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7951ee0]]}
  lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8c57da0]]}
  dc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x740aa20], [7, 0x80003e0]]}
  lc.input_tensor.bw_in1_add_618_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x987d300]]}
  lc.input_tensor.bw_in1_add_607_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8ef6ae0]]}
  lc.input_tensor.bw_in0_softmax_601_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8f4bfe0]]}
  input_1_multiply_599_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d68080]]}
  lc.input_tensor.bw_in1_add_593_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7952720]]}
  lc.input_tensor.bw_in1_add_587_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8c585e0]]}
  lc.input_tensor.bw_in2_layernorm_582_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74c3860]]}
  lc.input_tensor.bw_in1_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80ba2c0]]}
  lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x991f400]]}
  lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8fb2220]]}
  lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x90119a0]]}
  dc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e2e260], [4, 0x7a0ce20]]}
  lc.input_tensor.bw_in1_add_579_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8cfa6c0]]}
  lc.input_tensor.bw_in1_add_573_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74d1440]]}
  lc.input_tensor.bw_in2_layernorm_568_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ea80a0]]}
  lc.input_tensor.bw_in1_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7a86c60]]}
  lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8d5c720]]}
  lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x75334a0]]}
  lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x817db40]]}
  dc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x99e2c80], [1, 0x9075aa0]]}
  lc.input_tensor.bw_in1_add_565_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x90d5220]]}
  lc.input_tensor.bw_in1_add_554_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80b9240]]}
  lc.input_tensor.bw_in0_softmax_548_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x991e380]]}
  input_1_multiply_546_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8f993c0]]}
  lc.input_tensor.bw_in1_add_540_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ff8b40]]}
  lc.input_tensor.bw_in1_add_534_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e21700]]}
  lc.input_tensor.bw_in2_layernorm_529_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7a002c0]]}
  lc.input_tensor.bw_in1_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8cedb60]]}
  lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x74c48e0]]}
  lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x80b9a80]]}
  lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x991ebc0]]}
  dc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8f99c00], [2, 0x8ff9380]]}
  lc.input_tensor.bw_in1_add_526_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x73081a0]]}
  lc.input_tensor.bw_in1_add_520_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71687a0]]}
  lc.input_tensor.bw_in2_layernorm_515_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89e05a0]]}
  lc.input_tensor.bw_in1_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81909e0]]}
  lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81909e0]]}
  lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7013be0]]}
  lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81901a0]]}
  dc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c03be0], [5, 0x7dc9be0]]}
  lc.input_tensor.bw_in1_add_512_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65d3e00]]}
  lc.input_tensor.bw_in1_add_501_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7168fe0]]}
  lc.input_tensor.bw_in0_softmax_495_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89e0de0]]}
  input_1_multiply_493_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8191220]]}
  lc.input_tensor.bw_in1_add_487_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8191220]]}
  lc.input_tensor.bw_in1_add_481_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7167720]]}
  lc.input_tensor.bw_in2_layernorm_476_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89df520]]}
  lc.input_tensor.bw_in1_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818f960]]}
  lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x818f960]]}
  lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fca180]]}
  lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6589b60]]}
  dc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ba23a0], [5, 0x7d683a0]]}
  lc.input_tensor.bw_in1_add_473_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x658a3a0]]}
  lc.input_tensor.bw_in1_add_467_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7167f60]]}
  lc.input_tensor.bw_in2_layernorm_462_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89dfd60]]}
  lc.input_tensor.bw_in1_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81901a0]]}
  lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x70154a0]]}
  lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1d280]]}
  lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de3280]]}
  dc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65d56c0], [7, 0x719a460]]}
  lc.input_tensor.bw_in1_add_459_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a12260]]}
  lc.input_tensor.bw_in1_add_448_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8192ae0]]}
  lc.input_tensor.bw_in0_softmax_442_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81922a0]]}
  input_1_multiply_440_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7015ce0]]}
  lc.input_tensor.bw_in1_add_434_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1dac0]]}
  lc.input_tensor.bw_in1_add_428_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de3ac0]]}
  lc.input_tensor.bw_in2_layernorm_423_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65edce0]]}
  lc.input_tensor.bw_in1_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71b2a80]]}
  lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1c200]]}
  lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de2200]]}
  lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65d4640]]}
  dc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7169820], [0, 0x89e1620]]}
  lc.input_tensor.bw_in1_add_420_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8191a60]]}
  lc.input_tensor.bw_in1_add_414_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8191a60]]}
  lc.input_tensor.bw_in2_layernorm_409_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7014420]]}
  lc.input_tensor.bw_in1_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7014c60]]}
  lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1ca40]]}
  lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de2a40]]}
  lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65d4e80]]}
  dc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7181e40], [0, 0x89f9c40]]}
  lc.input_tensor.bw_in1_add_406_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81922a0]]}
  lc.input_tensor.bw_in1_add_395_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8192ae0]]}
  lc.input_tensor.bw_in0_softmax_389_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818d020]]}
  input_1_multiply_387_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8175240]]}
  lc.input_tensor.bw_in1_add_381_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fafa60]]}
  lc.input_tensor.bw_in1_add_375_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6b9fa60]]}
  lc.input_tensor.bw_in2_layernorm_370_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7135220]]}
  lc.input_tensor.bw_in1_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6587a60]]}
  lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x7135a60]]}
  lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89ad860]]}
  lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818d860]]}
  dc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8175a80], [3, 0x6fb02a0]]}
  lc.input_tensor.bw_in1_add_367_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ba02a0]]}
  lc.input_tensor.bw_in1_add_361_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d65a60]]}
  lc.input_tensor.bw_in2_layernorm_356_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fae9e0]]}
  lc.input_tensor.bw_in1_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6b9e9e0]]}
  lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d649e0]]}
  lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65869e0]]}
  lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71349e0]]}
  dc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8994a00], [1, 0x8174a00]]}
  lc.input_tensor.bw_in1_add_353_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8174a00]]}
  lc.input_tensor.bw_in1_add_342_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6faf220]]}
  lc.input_tensor.bw_in0_softmax_336_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6b9f220]]}
  input_1_multiply_334_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d65220]]}
  lc.input_tensor.bw_in1_add_328_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6587220]]}
  lc.input_tensor.bw_in1_add_322_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818f120]]}
  lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x818e8e0]]}
  lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fc9100]]}
  lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ba1320]]}
  lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d67320]]}
  lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6589320]]}
  dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x714f100], [0, 0x89c6f00]]}
  lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818e8e0]]}
  lc.input_tensor.bw_in1_add_308_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x818f120]]}
  lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fc9940]]}
  lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ba1b60]]}
  lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d67b60]]}
  lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d66ae0]]}
  lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65882a0]]}
  dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71362a0], [0, 0x89ae0a0]]}
  lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x818e0a0]]}
  lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x818e0a0]]}
  lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x6fc88c0]]}
  input_1_multiply_281_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6ba0ae0]]}
  lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7d662a0]]}
  lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6588ae0]]}
  lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x714e8c0]]}
  lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89c66c0]]}
  lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x89ad020]]}
  lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e31fa0]]}
  lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7032500]]}
  dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c22500], [5, 0x7de8500]]}
  lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x66222e0]]}
  lc.input_tensor.bw_in1_add_255_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e7080]]}
  lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a470a0]]}
  lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c70e0]]}
  lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81df700]]}
  lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c7920]]}
  lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7032d40]]}
  dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c3ab20], [5, 0x7e00b20]]}
  lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6622b20]]}
  lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e78c0]]}
  lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6609480]]}
  input_1_multiply_228_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71ce220]]}
  lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a46020]]}
  lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81de680]]}
  lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c68a0]]}
  lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de7480]]}
  lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7031cc0]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c21cc0]]}
  lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de7cc0]]}
  dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6609cc0], [7, 0x71cea60]]}
  lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a46860]]}
  lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81deec0]]}
  lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e31760]]}
  lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x663b980]]}
  lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e8940]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a48960]]}
  lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81e0fc0]]}
  dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c91e0], [3, 0x7034600]]}
  lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c541c0]]}
  lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x663c1c0]]}
  lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e9180]]}
  input_1_multiply_175_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a491a0]]}
  lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81e1800]]}
  lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81e1800]]}
  lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a478e0]]}
  lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81dff40]]}
  lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c8160]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7033580]]}
  lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c53140]]}
  dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e19140], [6, 0x6623360]]}
  lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71e8100]]}
  lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a48120]]}
  lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81e0780]]}
  lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81c89a0]]}
  lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7033dc0]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c53980]]}
  lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de6c40]]}
  dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65eed60], [7, 0x71b3b00]]}
  lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a2b900]]}
  lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81ac180]]}
  lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81ac180]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x702f380]]}
  lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1f380]]}
  lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6607380]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71cc120]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a2c140]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81ac9c0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81ac9c0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65ee520]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a12aa0], [1, 0x8193320]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8193320]]}
  lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7016520]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1e300]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de4300]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71b32c0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a2b0c0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81ab940]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8193b60], [3, 0x7016d60]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1eb40]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de4b40]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de5380]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a44fa0]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81c5820]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81ada40]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7030c40]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c20c40]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6608c40]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71cd9e0]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a457e0]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x81c6060], [2, 0x81ae280]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7031480]]}
  lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c21480]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7030400]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c1fbc0]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de5bc0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6607bc0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71cc960]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8a2c980], [1, 0x81ad200]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x81ad200]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x702fbc0]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x6c20400]]}
  input_1_multiply_16_tile_bcast_tile_bcast:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7de6400]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x6608400]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x71cd1a0]]}

  # epoch_to_epoch
  e2e__fused_op_12_0:                                                     {input: _fused_op_12, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fb0900]]}
  e2e_gelu_150_0:                                                         {input: gelu_150, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x7baf260], [7, 0x863ec00], [0, 0xa08b540]]}
  e2e__fused_op_20_0:                                                     {input: _fused_op_20, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x99432a0]]}
  e2e_matmul_245_0:                                                       {input: matmul_245, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19b61380], [7, 0x1c82ba20], [0, 0x1e0e1f60], [1, 0x1cfadb60]]}
  e2e__fused_op_31_0:                                                     {input: _fused_op_31, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9c70380]]}
  e2e__fused_op_44_0:                                                     {input: _fused_op_44, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xe7bede0]]}
  e2e_gelu_362_0:                                                         {input: gelu_362, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10952a80], [3, 0xecac520], [4, 0xe726100]]}
  e2e__fused_op_52_0:                                                     {input: _fused_op_52, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x10cd0f20]]}
  e2e_matmul_457_0:                                                       {input: matmul_457, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1dd6ee20], [3, 0x1cdb77c0], [4, 0x18d452a0], [5, 0x1d6cec80]]}
  e2e__fused_op_63_0:                                                     {input: _fused_op_63, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xfb255e0]]}
  e2e__fused_op_76_0:                                                     {input: _fused_op_76, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14688da0]]}
  e2e_gelu_574_0:                                                         {input: gelu_574, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1423d780], [7, 0x16019120], [0, 0x18181a60]]}
  e2e__fused_op_84_0:                                                     {input: _fused_op_84, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x174a37c0]]}
  e2e__fused_op_94_0:                                                     {input: _fused_op_94, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1a740e00], [3, 0x1974c8a0]]}
  e2e__fused_op_93_0:                                                     {input: _fused_op_93, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x168a4480]]}
  e2e_gelu_627_0:                                                         {input: gelu_627, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1900f860], [6, 0x16853880], [7, 0x195ed220]]}
  e2e_matmul_624_0:                                                       {input: matmul_624, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1b1bfb60], [1, 0x1a704f60], [2, 0x1ad58e20], [3, 0x19d648c0], [4, 0x169264a0], [5, 0x1a04f880], [6, 0x178938a0], [7, 0x1a62d240], [0, 0x1b4cbb80], [1, 0x1aa10f80], [2, 0x1b064e40], [3, 0x1a0708e0], [4, 0x16c324c0], [5, 0x1a35b8a0], [6, 0x17b9f8c0], [7, 0x1a939260]]}
  e2e__fused_op_92_0:                                                     {input: _fused_op_92, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1b7d7ba0]]}
  e2e__fused_op_91_0:                                                     {input: _fused_op_91, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ad1cfa0], [2, 0x1b370e60]]}
  e2e__fused_op_90_0:                                                     {input: _fused_op_90, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x16f3e4e0]]}
  e2e_matmul_612_0:                                                       {input: matmul_612, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1a37c900]]}
  e2e_bw_in1_matmul_616_transpose_0_0:                                    {input: bw_in1_matmul_616_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x190512c0], [5, 0x1d9daca0], [6, 0x19b61380], [7, 0x1c82ba20]]}
  e2e__fused_op_100_0:                                                    {input: _fused_op_100, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19e6d3a0], [7, 0x1cb37a40]]}
  e2e_bw_in0_matmul_616_matmul_1_0:                                       {input: bw_in0_matmul_616_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e0e1f60], [1, 0x1cfadb60], [2, 0x1e07ae40], [3, 0x1d0c37e0]]}
  e2e_matmul_605_0:                                                       {input: matmul_605, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x17eab8e0], [7, 0x1ac45280], [0, 0x1c407bc0], [1, 0x1b334fc0]]}
  e2e__fused_op_89_0:                                                     {input: _fused_op_89, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1a9f5900]]}
  e2e__fused_op_87_0:                                                     {input: _fused_op_87, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1bc94ea0]]}
  e2e_matmul_591_0:                                                       {input: matmul_591, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1b988e80], [3, 0x1afac920], [4, 0x16fc0500], [5, 0x1a6e98e0]]}
  e2e_matmul_585_0:                                                       {input: matmul_585, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x181b7900], [7, 0x1af512a0], [0, 0x1c713be0], [1, 0x1b640fe0]]}
  e2e__fused_op_86_0:                                                     {input: _fused_op_86, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1b2b8940], [4, 0x172cc520]]}
  e2e__fused_op_85_0:                                                     {input: _fused_op_85, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1a6678c0]]}
  e2e_matmul_571_0:                                                       {input: matmul_571, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1784ce60], [2, 0x17c98d20], [3, 0x178d47c0], [4, 0x150443a0], [5, 0x16e8b780], [6, 0x1527d7a0], [7, 0x17059140], [0, 0x191c1a80], [1, 0x17b58e80], [2, 0x17fa4d40], [3, 0x17be07e0], [4, 0x153503c0], [5, 0x171977a0], [6, 0x155897c0], [7, 0x17365160], [0, 0x194cdaa0]]}
  e2e__fused_op_105_0:                                                    {input: _fused_op_105, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1e386e60], [3, 0x1d3cf800], [4, 0x1935d2e0], [5, 0x1dce6cc0], [6, 0x1a4853c0], [7, 0x1d14fa60]]}
  e2e__fused_op_104_0:                                                    {input: _fused_op_104, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e3edf80], [1, 0x1d2b9b80]]}
  e2e__fused_op_83_0:                                                     {input: _fused_op_83, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x158957e0], [7, 0x17671180]]}
  e2e__fused_op_82_0:                                                     {input: _fused_op_82, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x197d9ac0]]}
  e2e_matmul_559_0:                                                       {input: matmul_559, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x18170ec0]]}
  e2e_matmul_552_0:                                                       {input: matmul_552, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x181f8820], [4, 0x15968400], [5, 0x180d37e0], [6, 0x15ead800]]}
  e2e__fused_op_81_0:                                                     {input: _fused_op_81, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x185bcd80]]}
  e2e__fused_op_79_0:                                                     {input: _fused_op_79, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x18504840]]}
  e2e_matmul_538_0:                                                       {input: matmul_538, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x17e64ea0], [2, 0x182b0d60], [3, 0x17eec800], [4, 0x1565c3e0]]}
  e2e_matmul_532_0:                                                       {input: matmul_532, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x17c891a0], [0, 0x1985bae0], [1, 0x18da0ee0], [2, 0x19e1cda0]]}
  e2e_bw_in1_matmul_532_transpose_0_0:                                    {input: bw_in1_matmul_532_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1cb37a40], [0, 0x1f01dfc0], [1, 0x1dee9bc0], [2, 0x1eba6e80]]}
  e2e_bw_in0_matmul_544_matmul_1_0:                                       {input: bw_in0_matmul_544_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1aca53e0]]}
  e2e_bw_in0_matmul_552_matmul_1_0:                                       {input: bw_in0_matmul_552_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1dd6ee20], [3, 0x1cdb77c0], [4, 0x18d452a0], [5, 0x1d6cec80]]}
  e2e_bw_in0_matmul_538_matmul_1_0:                                       {input: bw_in0_matmul_538_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c82ba20], [0, 0x1e0e1f60], [1, 0x1cfadb60], [2, 0x1e07ae40]]}
  e2e_bw_in0_matmul_532_matmul_1_0:                                       {input: bw_in0_matmul_532_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1d0c37e0], [4, 0x190512c0], [5, 0x1d9daca0], [6, 0x19b61380]]}
  e2e__fused_op_107_0:                                                    {input: _fused_op_107, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ea05fa0], [1, 0x1d8d1ba0]]}
  e2e__fused_op_78_0:                                                     {input: _fused_op_78, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x15c74420], [5, 0x183df800]]}
  e2e__fused_op_77_0:                                                     {input: _fused_op_77, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x161b9820]]}
  e2e_gelu_521_0:                                                         {input: gelu_521, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x17f951c0], [0, 0x19b67b00], [1, 0x190acf00]]}
  e2e_matmul_518_0:                                                       {input: matmul_518, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1a128dc0], [3, 0x19134860], [4, 0x1628c440], [5, 0x189f7820], [6, 0x1623b840], [7, 0x18fd51e0], [0, 0x1aba7b20], [1, 0x1a0ecf20], [2, 0x1a434de0], [3, 0x19440880], [4, 0x16598460], [5, 0x18d03840], [6, 0x16547860], [7, 0x192e1200], [0, 0x1aeb3b40], [1, 0x1a3f8f40]]}
  e2e__fused_op_75_0:                                                     {input: _fused_op_75, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16438c60], [3, 0x16074700]]}
  e2e__fused_op_74_0:                                                     {input: _fused_op_74, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x143102e0]]}
  e2e_matmul_506_0:                                                       {input: matmul_506, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x149fb6c0]]}
  e2e_bw_in1_matmul_510_transpose_0_0:                                    {input: bw_in1_matmul_510_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1dce6cc0], [6, 0x19e6d3a0], [7, 0x1ce43a60], [0, 0x1e3edf80]]}
  e2e__fused_op_114_0:                                                    {input: _fused_op_114, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1dbef820], [4, 0x19b7d300]]}
  e2e_bw_in0_matmul_510_matmul_1_0:                                       {input: bw_in0_matmul_510_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d2b9b80], [2, 0x1e386e60], [3, 0x1d3cf800], [4, 0x1935d2e0]]}
  e2e_matmul_499_0:                                                       {input: matmul_499, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16d5cca0], [3, 0x16998740], [4, 0x1469e320], [5, 0x15937700]]}
  e2e__fused_op_73_0:                                                     {input: _fused_op_73, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x123c5720]]}
  e2e__fused_op_71_0:                                                     {input: _fused_op_71, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x152b8dc0]]}
  e2e_matmul_485_0:                                                       {input: matmul_485, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x120b9700], [7, 0x146b50a0], [0, 0x1681d9e0], [1, 0x15ee8de0]]}
  e2e_matmul_479_0:                                                       {input: matmul_479, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16a50c80], [3, 0x1668c720], [4, 0x14392300], [5, 0x1562b6e0]]}
  e2e__fused_op_70_0:                                                     {input: _fused_op_70, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1409d080], [0, 0x162059c0]]}
  e2e__fused_op_69_0:                                                     {input: _fused_op_69, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x120376e0]]}
  e2e_gelu_468_0:                                                         {input: gelu_468, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x14fd9100], [0, 0x17141a40], [1, 0x1680ce40]]}
  e2e_matmul_465_0:                                                       {input: matmul_465, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x149c10c0], [0, 0x16b29a00], [1, 0x161f4e00], [2, 0x17068cc0], [3, 0x16ca4760], [4, 0x149aa340], [5, 0x15c43720], [6, 0x13c25740], [7, 0x14ccd0e0], [0, 0x16e35a20], [1, 0x16500e20], [2, 0x17374ce0], [3, 0x16fb0780], [4, 0x14cb6360], [5, 0x15f4f740], [6, 0x13f31760]]}
  e2e__fused_op_119_0:                                                    {input: _fused_op_119, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1d14fa80], [0, 0x1e6f9fa0], [1, 0x1d5c5ba0], [2, 0x1e692e80], [3, 0x1e207840], [4, 0x1a195320]]}
  e2e__fused_op_68_0:                                                     {input: _fused_op_68, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1625b760]]}
  e2e__fused_op_118_0:                                                    {input: _fused_op_118, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1dff2ce0], [6, 0x1a1793c0]]}
  e2e__fused_op_67_0:                                                     {input: _fused_op_67, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x17680d00], [3, 0x172bc7a0]]}
  e2e__fused_op_66_0:                                                     {input: _fused_op_66, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x14fc2380]]}
  e2e_matmul_453_0:                                                       {input: matmul_453, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a70220]]}
  e2e_matmul_446_0:                                                       {input: matmul_446, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x12224fa0], [0, 0x136db8e0], [1, 0x12b1cce0], [2, 0x144bcba0]]}
  e2e__fused_op_65_0:                                                     {input: _fused_op_65, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12ba4640]]}
  e2e_matmul_432_0:                                                       {input: matmul_432, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12898620], [4, 0x11764200], [5, 0x124e95e0], [6, 0x10755600]]}
  e2e_matmul_426_0:                                                       {input: matmul_426, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x11f18f80], [0, 0x133cf8c0], [1, 0x12810cc0], [2, 0x141b0b80]]}
  e2e_bw_in1_matmul_426_transpose_0_0:                                    {input: bw_in1_matmul_426_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d6cec80], [6, 0x19b61380], [7, 0x1c82ba20], [0, 0x1e0e1f60]]}
  e2e_bw_in0_matmul_438_matmul_1_0:                                       {input: bw_in0_matmul_438_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1dde5bc0]]}
  e2e_bw_in0_matmul_446_matmul_1_0:                                       {input: bw_in0_matmul_446_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1e07ae40], [3, 0x1d0c37e0], [4, 0x190512c0], [5, 0x1dce6cc0]]}
  e2e_bw_in0_matmul_432_matmul_1_0:                                       {input: bw_in0_matmul_432_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d9daca0], [6, 0x19e6d3a0], [7, 0x1cb37a40], [0, 0x1e3edf80]]}
  e2e_bw_in0_matmul_426_matmul_1_0:                                       {input: bw_in0_matmul_426_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1cfadb60], [2, 0x1dd6ee20], [3, 0x1cdb77c0], [4, 0x18d452a0]]}
  e2e__fused_op_121_0:                                                    {input: _fused_op_121, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1a7913e0], [7, 0x1d96faa0]]}
  e2e__fused_op_62_0:                                                     {input: _fused_op_62, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x110fb680], [7, 0x13161020]]}
  e2e__fused_op_61_0:                                                     {input: _fused_op_61, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13a3d660]]}
  e2e_gelu_415_0:                                                         {input: gelu_415, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14de0c00], [3, 0x14a1c6a0], [4, 0x12cb8280]]}
  e2e_matmul_412_0:                                                       {input: matmul_412, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x147c8bc0], [3, 0x14404660], [4, 0x126a0240], [5, 0x13425620], [6, 0x10ae3640], [7, 0x12b48fe0], [0, 0x13fff920], [1, 0x13a58d20], [2, 0x14ad4be0], [3, 0x14710680], [4, 0x129ac260], [5, 0x13731640], [6, 0x10def660], [7, 0x12e55000], [0, 0x1430b940], [1, 0x13d64d40]]}
  e2e__fused_op_60_0:                                                     {input: _fused_op_60, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12e28d00]]}
  e2e__fused_op_59_0:                                                     {input: _fused_op_59, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x12530fc0], [0, 0x139e7900]]}
  e2e__fused_op_58_0:                                                     {input: _fused_op_58, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x10a61620]]}
  e2e_matmul_400_0:                                                       {input: matmul_400, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x127f5600]]}
  e2e_bw_in1_matmul_404_transpose_0_0:                                    {input: bw_in1_matmul_404_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1a1793c0], [7, 0x1ce43a60], [0, 0x1e6f9fa0], [1, 0x1d2b9b80]]}
  e2e__fused_op_128_0:                                                    {input: _fused_op_128, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ef19fc0], [1, 0x1ea15be0]]}
  e2e_bw_in0_matmul_404_matmul_1_0:                                       {input: bw_in0_matmul_404_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1e386e60], [3, 0x1d3cf800], [4, 0x1935d2e0], [5, 0x1dff2ce0]]}
  e2e_matmul_393_0:                                                       {input: matmul_393, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14070d60], [2, 0x15e20c20], [3, 0x15a5c6c0], [4, 0x13cf82a0]]}
  e2e__fused_op_57_0:                                                     {input: _fused_op_57, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x14617960]]}
  e2e__fused_op_55_0:                                                     {input: _fused_op_55, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13dcb6a0]]}
  e2e_matmul_379_0:                                                       {input: matmul_379, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13abf680], [6, 0x117136a0], [7, 0x13779040], [0, 0x15e77980]]}
  e2e_matmul_373_0:                                                       {input: matmul_373, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1437cd80], [2, 0x1612cc40], [3, 0x15d686e0], [4, 0x140042c0]]}
  e2e__fused_op_54_0:                                                     {input: _fused_op_54, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x11a1f6c0], [7, 0x13a85060]]}
  e2e__fused_op_53_0:                                                     {input: _fused_op_53, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x161839a0]]}
  e2e_matmul_359_0:                                                       {input: matmul_359, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xf3254e0], [6, 0xe2c5500], [7, 0x100a0ea0], [0, 0x11b6f7e0], [1, 0x10fb0be0], [2, 0x11992aa0], [3, 0xfcec540], [4, 0xf766120], [5, 0xf631500], [6, 0xe5d1520], [7, 0x103acec0], [0, 0x11e7b800], [1, 0x112bcc00], [2, 0x11c9eac0], [3, 0xfff8560], [4, 0xfa72140]]}
  e2e__fused_op_133_0:                                                    {input: _fused_op_133, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x19669300], [5, 0x1d6cec80], [6, 0x1a4853e0], [7, 0x1d14fa80], [0, 0x1f531fe0], [1, 0x1d5c5ba0]]}
  e2e__fused_op_132_0:                                                    {input: _fused_op_132, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1e692e80], [3, 0x1d6db820]]}
  e2e__fused_op_51_0:                                                     {input: _fused_op_51, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10879560], [6, 0xeef5580]]}
  e2e__fused_op_50_0:                                                     {input: _fused_op_50, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1008a180]]}
  e2e_matmul_347_0:                                                       {input: matmul_347, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x106105a0]]}
  e2e_matmul_340_0:                                                       {input: matmul_340, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xebe9560], [7, 0x109c4f00], [0, 0x12493840], [1, 0x118d4c40]]}
  e2e__fused_op_49_0:                                                     {input: _fused_op_49, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x122b6b00]]}
  e2e__fused_op_47_0:                                                     {input: _fused_op_47, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xf93d520]]}
  e2e_matmul_326_0:                                                       {input: matmul_326, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11faaae0], [3, 0x10304580], [4, 0xfd7e160], [5, 0x1056d540]]}
  e2e_matmul_320_0:                                                       {input: matmul_320, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xe8dd540], [7, 0x106b8ee0], [0, 0x12187820], [1, 0x115c8c20]]}
  e2e_bw_in1_matmul_320_transpose_0_0:                                    {input: bw_in1_matmul_320_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18d452a0], [5, 0x1deeeca0], [6, 0x19b61380], [7, 0x1c82ba20]]}
  e2e_bw_in0_matmul_332_matmul_1_0:                                       {input: bw_in0_matmul_332_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e3edf80]]}
  e2e_bw_in0_matmul_340_matmul_1_0:                                       {input: bw_in0_matmul_340_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d2b9b80], [2, 0x1ecaaea0], [3, 0x1dcf3840], [4, 0x1935d2e0]]}
  e2e_bw_in0_matmul_326_matmul_1_0:                                       {input: bw_in0_matmul_326_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x190512c0], [5, 0x1e1facc0], [6, 0x19e6d3a0], [7, 0x1cb37a40]]}
  e2e_bw_in0_matmul_320_matmul_1_0:                                       {input: bw_in0_matmul_320_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e0e1f60], [1, 0x1cfadb60], [2, 0x1e386e40], [3, 0x1d3cf7e0]]}
  e2e__fused_op_135_0:                                                    {input: _fused_op_135, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1dd6ee20], [3, 0x1cdb77c0]]}
  e2e__fused_op_46_0:                                                     {input: _fused_op_46, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1279f860], [1, 0x11be0c60]]}
  e2e__fused_op_45_0:                                                     {input: _fused_op_45, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13b16b20]]}
  e2e_gelu_309_0:                                                         {input: gelu_309, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11858600], [4, 0x107241e0], [5, 0x114a95c0]]}
  e2e_matmul_306_0:                                                       {input: matmul_306, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x112405c0], [4, 0x1010c1a0], [5, 0x10e91580], [6, 0xf50d5a0], [7, 0x11900f40], [0, 0x12db7880], [1, 0x121f8c80], [2, 0x13b98b40], [3, 0x1154c5e0], [4, 0x104181c0], [5, 0x1119d5a0], [6, 0xf8195c0], [7, 0x11c0cf60], [0, 0x130c38a0], [1, 0x12504ca0], [2, 0x13ea4b60]]}
  e2e__fused_op_43_0:                                                     {input: _fused_op_43, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xc779420], [6, 0xc44d440]]}
  e2e__fused_op_42_0:                                                     {input: _fused_op_42, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x108a5720]]}
  e2e_matmul_294_0:                                                       {input: matmul_294, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xf138b20]]}
  e2e_bw_in1_matmul_298_transpose_0_0:                                    {input: bw_in1_matmul_298_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1d6db800], [4, 0x19669300], [5, 0x1d6cec80], [6, 0x1a1793c0]]}
  e2e__fused_op_142_0:                                                    {input: _fused_op_142, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1e506ce0], [6, 0x1aca5400]]}
  e2e_bw_in0_matmul_298_matmul_1_0:                                       {input: bw_in0_matmul_298_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1ce43a60], [0, 0x1f01dfa0], [1, 0x1d5c5ba0], [2, 0x1e692e60]]}
  e2e_matmul_287_0:                                                       {input: matmul_287, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xcd71480], [7, 0xf6fae20], [0, 0x10c33760], [1, 0x10074b60]]}
  e2e__fused_op_41_0:                                                     {input: _fused_op_41, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xeadaa20]]}
  e2e__fused_op_39_0:                                                     {input: _fused_op_39, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xcd91440]]}
  e2e_matmul_273_0:                                                       {input: matmul_273, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xe7cea00], [3, 0xd3484a0], [4, 0xcdc2080], [5, 0xd9c1460]]}
  e2e_matmul_267_0:                                                       {input: matmul_267, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xca65460], [7, 0xf3eee00], [0, 0x10927740], [1, 0xfd68b40]]}
  e2e__fused_op_38_0:                                                     {input: _fused_op_38, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xcd30480], [4, 0xc7aa060]]}
  e2e__fused_op_37_0:                                                     {input: _fused_op_37, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xe74c9e0]]}
  e2e_gelu_256_0:                                                         {input: gelu_256, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xdc6c500], [4, 0xd6e60e0], [5, 0xe2e54c0]]}
  e2e_matmul_253_0:                                                       {input: matmul_253, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd6544c0], [4, 0xd0ce0a0], [5, 0xdccd480], [6, 0xd07d4a0], [7, 0xfa06e40], [0, 0x10f3f780], [1, 0x10380b80], [2, 0x1033aa40], [3, 0xd9604e0], [4, 0xd3da0c0], [5, 0xdfd94a0], [6, 0xd3894c0], [7, 0xfd12e60], [0, 0x1124b7a0], [1, 0x1068cba0], [2, 0x10646a60]]}
  e2e__fused_op_147_0:                                                    {input: _fused_op_147, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d8d1bc0], [2, 0x1dd6ee20], [3, 0x1cdb77c0], [4, 0x18d452a0], [5, 0x1d9daca0], [6, 0x1a4853e0]]}
  e2e__fused_op_36_0:                                                     {input: _fused_op_36, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xd6954e0]]}
  e2e__fused_op_146_0:                                                    {input: _fused_op_146, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1d14fa80], [0, 0x1f329fc0]]}
  e2e__fused_op_35_0:                                                     {input: _fused_op_35, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x115577c0], [1, 0x10998bc0]]}
  e2e__fused_op_34_0:                                                     {input: _fused_op_34, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1001ee80]]}
  e2e_matmul_241_0:                                                       {input: matmul_241, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xc23a8e0]]}
  e2e_matmul_234_0:                                                       {input: matmul_234, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9c7ff80], [5, 0xb7bb360], [6, 0xa4d1380], [7, 0xc842d20]]}
  e2e__fused_op_33_0:                                                     {input: _fused_op_33, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xd3d5660]]}
  e2e_matmul_220_0:                                                       {input: matmul_220, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xd0c9640], [1, 0xc690a40], [2, 0xce6a900], [3, 0xa8a03a0]]}
  e2e_matmul_214_0:                                                       {input: matmul_214, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9973f60], [5, 0xb4af340], [6, 0xa1c5360], [7, 0xc536d00]]}
  e2e_bw_in1_matmul_214_transpose_0_0:                                    {input: bw_in1_matmul_214_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1cfadb60], [2, 0x1e58ee40], [3, 0x1d5d77e0], [4, 0x195652c0]]}
  e2e_bw_in0_matmul_226_matmul_1_0:                                       {input: bw_in0_matmul_226_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1e0f1be0]]}
  e2e_bw_in0_matmul_234_matmul_1_0:                                       {input: bw_in0_matmul_234_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1e89ae60], [3, 0x1d8e3800], [4, 0x198712e0], [5, 0x1e1facc0]]}
  e2e_bw_in0_matmul_220_matmul_1_0:                                       {input: bw_in0_matmul_220_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19e6d3a0], [7, 0x1d767aa0], [0, 0x1ea05fa0], [1, 0x1d2b9b80]]}
  e2e_bw_in0_matmul_214_matmul_1_0:                                       {input: bw_in0_matmul_214_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d6cec80], [6, 0x19b61380], [7, 0x1ce43a40], [0, 0x1e6f9f80]]}
  e2e__fused_op_149_0:                                                    {input: _fused_op_149, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c82ba20], [0, 0x1e0e1f60]]}
  e2e__fused_op_30_0:                                                     {input: _fused_op_30, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xabac3c0], [4, 0x9f8bfa0]]}
  e2e__fused_op_29_0:                                                     {input: _fused_op_29, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xbac7380]]}
  e2e_gelu_203_0:                                                         {input: gelu_203, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0xa7dd3a0], [7, 0xcb4ed40], [0, 0xec35680]]}
  e2e_matmul_200_0:                                                       {input: matmul_200, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xcfb4a80], [2, 0xd78e940], [3, 0xb1c43e0], [4, 0xa5a3fc0], [5, 0xbb493a0], [6, 0xb81d3c0], [7, 0xdb8ed60], [0, 0xfc756a0], [1, 0xd2c0aa0], [2, 0xda9a960], [3, 0xb4d0400], [4, 0xa8affe0], [5, 0xbe553c0], [6, 0xbb293e0], [7, 0xde9ad80], [0, 0xff816c0]]}
  e2e__fused_op_28_0:                                                     {input: _fused_op_28, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xd5ccac0]]}
  e2e__fused_op_27_0:                                                     {input: _fused_op_27, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xc99ca60], [2, 0xd176920]]}
  e2e__fused_op_26_0:                                                     {input: _fused_op_26, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xdda6980]]}
  e2e_matmul_188_0:                                                       {input: matmul_188, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb7dc420]]}
  e2e_bw_in1_matmul_192_transpose_0_0:                                    {input: bw_in1_matmul_192_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ed11fc0], [1, 0x1d5c5ba0], [2, 0x1dd6ee20], [3, 0x1cdb77c0]]}
  e2e__fused_op_156_0:                                                    {input: _fused_op_156, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1eba6e80], [3, 0x1dbef820]]}
  e2e_bw_in0_matmul_192_matmul_1_0:                                       {input: bw_in0_matmul_192_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18d452a0], [5, 0x1d9daca0], [6, 0x1a1793c0], [7, 0x1d14fa60]]}
  e2e_matmul_181_0:                                                       {input: matmul_181, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xc1613e0], [6, 0xbe35400], [7, 0xe1a6da0], [0, 0x1028d6e0]]}
  e2e__fused_op_25_0:                                                     {input: _fused_op_25, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xabbc000]]}
  e2e__fused_op_23_0:                                                     {input: _fused_op_23, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xe508b00]]}
  e2e_matmul_167_0:                                                       {input: matmul_167, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xe1fcae0], [2, 0xde289a0], [3, 0xc40c440], [4, 0xc41c020]]}
  e2e_matmul_161_0:                                                       {input: matmul_161, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0xc46d400], [6, 0xc141420], [7, 0xe4b2dc0], [0, 0x10599700]]}
  e2e__fused_op_22_0:                                                     {input: _fused_op_22, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xe1349c0], [3, 0xc718460]]}
  e2e__fused_op_21_0:                                                     {input: _fused_op_21, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc728040]]}
  e2e_matmul_147_0:                                                       {input: matmul_147, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x97d8940], [2, 0x9792800], [3, 0x871c2a0], [4, 0x8113e80], [5, 0x932b260], [6, 0x8bef280], [7, 0x967ec20], [0, 0xb0cb560], [1, 0x9ae4960], [2, 0x9a9e820], [3, 0x8a282c0], [4, 0x841fea0], [5, 0x9637280], [6, 0x8efb2a0], [7, 0x998ac40], [0, 0xb3d7580]]}
  e2e__fused_op_161_0:                                                    {input: _fused_op_161, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1a4853e0], [7, 0x1c82ba20], [0, 0x1e0e1f60], [1, 0x1d8d1bc0], [2, 0x1e07ae40], [3, 0x1d0c37e0]]}
  e2e__fused_op_160_0:                                                    {input: _fused_op_160, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x19b7d300], [5, 0x1e506ce0]]}
  e2e__fused_op_19_0:                                                     {input: _fused_op_19, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x92072c0], [7, 0x9c96c60]]}
  e2e__fused_op_18_0:                                                     {input: _fused_op_18, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xb6e35a0]]}
  e2e_matmul_135_0:                                                       {input: matmul_135, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0xa0fc9a0]]}
  e2e_matmul_128_0:                                                       {input: matmul_128, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9df0980], [2, 0x9daa840], [3, 0x8d342e0], [4, 0x872bec0]]}
  e2e__fused_op_17_0:                                                     {input: _fused_op_17, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xa0b6860]]}
  e2e__fused_op_15_0:                                                     {input: _fused_op_15, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xa2aec80]]}
  e2e_matmul_114_0:                                                       {input: matmul_114, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0xb7655c0], [1, 0xad2c9c0], [2, 0xb916880], [3, 0x934c320]]}
  e2e_matmul_108_0:                                                       {input: matmul_108, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9040300], [4, 0x8a37ee0], [5, 0xa5732c0], [6, 0x981f2e0]]}
  e2e_bw_in1_matmul_108_transpose_0_0:                                    {input: bw_in1_matmul_108_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e901f80], [1, 0x1cfadb60], [2, 0x1dd6ee20], [3, 0x1cdb77c0]]}
  e2e_bw_in0_matmul_120_matmul_1_0:                                       {input: bw_in0_matmul_120_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ec0dfa0]]}
  e2e_bw_in0_matmul_128_matmul_1_0:                                       {input: bw_in0_matmul_128_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d2b9b80], [2, 0x1e89ae60], [3, 0x1d8e3800], [4, 0x1935d2e0]]}
  e2e_bw_in0_matmul_114_matmul_1_0:                                       {input: bw_in0_matmul_114_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x190512c0], [5, 0x1d6cec80], [6, 0x19b61380], [7, 0x1d45ba80]]}
  e2e_bw_in0_matmul_108_matmul_1_0:                                       {input: bw_in0_matmul_108_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18d452a0], [5, 0x1d9daca0], [6, 0x19e6d3a0], [7, 0x1d04ba40]]}
  e2e__fused_op_163_0:                                                    {input: _fused_op_163, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1dce6cc0], [6, 0x1aca5400]]}
  e2e__fused_op_14_0:                                                     {input: _fused_op_14, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8d43f00], [5, 0xa87f2e0]]}
  e2e__fused_op_13_0:                                                     {input: _fused_op_13, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x9b2b300]]}
  e2e_gelu_97_0:                                                          {input: gelu_97, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0xaedeca0], [0, 0xba715e0], [1, 0xb0389e0]]}
  e2e_matmul_94_0:                                                        {input: matmul_94, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0xbc228a0], [3, 0x9658340], [4, 0x935bf20], [5, 0xae97300], [6, 0x9bad320], [7, 0xbf1ecc0], [0, 0xcab1600], [1, 0xc078a00], [2, 0xbf2e8c0], [3, 0x9964360], [4, 0x9667f40], [5, 0xb1a3320], [6, 0x9eb9340], [7, 0xc22ace0], [0, 0xcdbd620], [1, 0xc384a20]]}
  e2e__fused_op_11_0:                                                     {input: _fused_op_11, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4fb0900], [2, 0x4fb0900]]}
  e2e__fused_op_10_0:                                                     {input: _fused_op_10, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4fb0900]]}
  e2e_matmul_82_0:                                                        {input: matmul_82, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4fb0900]]}
  e2e_bw_in1_matmul_86_transpose_0_0:                                     {input: bw_in1_matmul_86_transpose_0, type: queue, entries: 64, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1e2fece0], [6, 0x1a1793c0], [7, 0x1c82ba20], [0, 0x1e0e1f60]]}
  e2e__fused_op_170_0:                                                    {input: _fused_op_170, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1d767aa0], [0, 0x1f83dfc0]]}
  e2e_bw_in0_matmul_86_matmul_1_0:                                        {input: bw_in0_matmul_86_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d5c5ba0], [2, 0x1e07ae40], [3, 0x1d0c37e0], [4, 0x19669300]]}
  e2e_matmul_75_0:                                                        {input: matmul_75, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x627a980], [5, 0x5eec980], [6, 0x5eec980], [7, 0x6504980]]}
  e2e__fused_op_9_0:                                                      {input: _fused_op_9, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x68109a0]]}
  e2e__fused_op_7_0:                                                      {input: _fused_op_7, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x58d4960]]}
  e2e_matmul_61_0:                                                        {input: matmul_61, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6504980], [1, 0x6c20980], [2, 0x6c20980], [3, 0x668a980]]}
  e2e_matmul_55_0:                                                        {input: matmul_55, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fb0900], [6, 0x4fb0900], [7, 0x4fb0900], [0, 0x5be0920]]}
  e2e__fused_op_6_0:                                                      {input: _fused_op_6, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x58d4960], [6, 0x58d4960]]}
  e2e__fused_op_5_0:                                                      {input: _fused_op_5, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x61f8960]]}
  e2e_gelu_44_0:                                                          {input: gelu_44, type: queue, entries: 64, grid_size: [1, 3], t: 1, mblock: [2, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5be0960], [2, 0x5be0960], [3, 0x564a960]]}
  e2e_matmul_41_0:                                                        {input: matmul_41, type: queue, entries: 64, grid_size: [4, 4], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55c8920], [2, 0x55c8920], [3, 0x5032920], [4, 0x5be0920], [5, 0x52bc920], [6, 0x52bc920], [7, 0x52bc920], [0, 0x5eec940], [1, 0x58d4940], [2, 0x58d4940], [3, 0x533e940], [4, 0x5eec940], [5, 0x55c8940], [6, 0x55c8940], [7, 0x55c8940], [0, 0x61f8960]]}
  e2e__fused_op_175_0:                                                    {input: _fused_op_175, type: queue, entries: 64, grid_size: [1, 6], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1d3cf800], [4, 0x18d452a0], [5, 0x1d6cec80], [6, 0x1a4853e0], [7, 0x1cb37a40], [0, 0x1e3edf80]]}
  e2e__fused_op_4_0:                                                      {input: _fused_op_4, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x75449e0]]}
  e2e__fused_op_174_0:                                                    {input: _fused_op_174, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1d8d1bc0], [2, 0x1eba6e80]]}
  e2e__fused_op_3_0:                                                      {input: _fused_op_3, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x6b1c9c0], [0, 0x837c9e0]]}
  e2e__fused_op_2_0:                                                      {input: _fused_op_2, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x65049c0]]}
  e2e_matmul_29_0:                                                        {input: matmul_29, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x75449e0]]}
  e2e_matmul_22_0:                                                        {input: matmul_22, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x72389c0], [2, 0x72389c0], [3, 0x6ca29c0], [4, 0x68929c0]]}
  e2e__fused_op_1_0:                                                      {input: _fused_op_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x65049c0]]}
  e2e_matmul_8_0:                                                         {input: matmul_8, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61f89a0], [6, 0x61f89a0], [7, 0x68109a0], [0, 0x80709c0]]}
  e2e_matmul_2_0:                                                         {input: matmul_2, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6f2c9a0], [2, 0x6f2c9a0], [3, 0x69969a0], [4, 0x65869a0]]}
  e2e_bw_in0_matmul_14_matmul_1_0:                                        {input: bw_in0_matmul_14_matmul_1, type: queue, entries: 64, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1d357a60]]}
  e2e_bw_in0_matmul_22_matmul_1_0:                                        {input: bw_in0_matmul_22_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1cdb77c0], [4, 0x195652c0], [5, 0x1deeeca0], [6, 0x19b61380]]}
  e2e_bw_in0_matmul_8_matmul_1_0:                                         {input: bw_in0_matmul_8_matmul_1, type: queue, entries: 64, grid_size: [1, 4], t: 1, mblock: [2, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e0e1f60], [1, 0x1d5c5b80], [2, 0x1dd6ee20], [3, 0x1d0c37e0]]}
  e2e__fused_op_177_0:                                                    {input: _fused_op_177, type: queue, entries: 64, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1cfadb60], [2, 0x1e386e60]]}

  # loss
  loss_bert_encoders.output_layernorm_635:                                {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31964040]]}

  # grad_accumulator
  grad_acc_layer.11.output.LayerNorm.bias:                                {input: bw_in2_layernorm_635_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1daf5100]]}
  grad_acc_layer.11.output.LayerNorm.weight:                              {input: bw_in1_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1cb7a9a0]]}
  grad_acc_layer.11.output.dense.bias:                                    {input: bw_in1_add_632_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18b08480]]}
  grad_acc_layer.11.output.dense.weight:                                  {input: bw_in1_matmul_630_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d43c960], [6, 0x19918280], [7, 0x1c5a5a20], [0, 0x1de37660], [1, 0x1cd7d060], [2, 0x1db01420], [3, 0x1cb86cc0], [4, 0x18b147a0], [5, 0x1d49e180], [6, 0x19979aa0], [7, 0x1c607240], [0, 0x1de98e80]]}
  grad_acc_layer.11.intermediate.dense.bias:                              {input: bw_in1_add_626_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1da7b2c0]]}
  grad_acc_layer.11.intermediate.dense.weight:                            {input: bw_in1_matmul_624_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1ca62420], [4, 0x189b3000], [5, 0x1d3243e0], [6, 0x197c2e00], [7, 0x1c4a5aa0], [0, 0x1dd745e0], [1, 0x1cc7d0e0], [2, 0x1da19aa0], [3, 0x1cac3c40], [4, 0x18a14820], [5, 0x1d385c00], [6, 0x19824620]]}
  grad_acc_layer.11.attention.output.LayerNorm.bias:                      {input: bw_in2_layernorm_621_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c5072c0]]}
  grad_acc_layer.11.attention.output.LayerNorm.weight:                    {input: bw_in1_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ddd5e00]]}
  grad_acc_layer.11.attention.output.dense.bias:                          {input: bw_in1_add_618_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ccde900]]}
  grad_acc_layer.11.attention.output.dense.weight:                        {input: bw_in1_matmul_616_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c45c880], [0, 0x1dd2b3c0], [1, 0x1cc33ec0], [2, 0x1d9d0880]]}
  grad_acc_layer.11.attention.self.value.bias:                            {input: bw_in1_add_607_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1cb25460]]}
  grad_acc_layer.11.attention.self.value.weight:                          {input: bw_in1_matmul_605_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18a76040], [5, 0x1d3e7420], [6, 0x19885e40], [7, 0x1c5135e0]]}
  grad_acc_layer.11.attention.self.key.bias:                              {input: bw_in1_add_593_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1dde2120]]}
  grad_acc_layer.11.attention.self.key.weight:                            {input: bw_in1_matmul_591_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1cceac20], [2, 0x1daabee0], [3, 0x1cb31780], [4, 0x18abf260]]}
  grad_acc_layer.11.attention.self.query.bias:                            {input: bw_in1_add_587_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d430640]]}
  grad_acc_layer.11.attention.self.query.weight:                          {input: bw_in1_matmul_585_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x198cf060], [7, 0x1c55c800], [0, 0x1ddee440], [1, 0x1cd33e40]]}
  grad_acc_layer.10.output.LayerNorm.bias:                                {input: bw_in2_layernorm_582_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ce7cfe0]]}
  grad_acc_layer.10.output.LayerNorm.weight:                              {input: bw_in1_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1dc3e2a0]]}
  grad_acc_layer.10.output.dense.bias:                                    {input: bw_in1_add_579_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1cc86c40]]}
  grad_acc_layer.10.output.dense.weight:                                  {input: bw_in1_matmul_577_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18c14720], [5, 0x1d59e100], [6, 0x19a3cb20], [7, 0x1c7071c0], [0, 0x1dfbd700], [1, 0x1ce89300], [2, 0x1dc4a5c0], [3, 0x1cc92f60], [4, 0x18c75f40], [5, 0x1d5ff920], [6, 0x19a9e340], [7, 0x1c7689e0]]}
  grad_acc_layer.10.intermediate.dense.bias:                              {input: bw_in1_add_573_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1df8cae0]]}
  grad_acc_layer.10.intermediate.dense.weight:                            {input: bw_in1_matmul_571_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1e01ef20], [1, 0x1ceeab20], [2, 0x1dcabde0], [3, 0x1ccf4780], [4, 0x18cd7760], [5, 0x1d661140], [6, 0x19affb60], [7, 0x1c7ca200], [0, 0x1e080740], [1, 0x1cf4c340], [2, 0x1dd0d600], [3, 0x1cd55fa0]]}
  grad_acc_layer.10.attention.output.LayerNorm.bias:                      {input: bw_in2_layernorm_568_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18d38f80]]}
  grad_acc_layer.10.attention.output.LayerNorm.weight:                    {input: bw_in1_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d6c2960]]}
  grad_acc_layer.10.attention.output.dense.bias:                          {input: bw_in1_add_565_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18c08400]]}
  grad_acc_layer.10.attention.output.dense.weight:                        {input: bw_in1_matmul_563_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d4ff9a0], [6, 0x199db2c0], [7, 0x1c668a60], [0, 0x1defa6a0]]}
  grad_acc_layer.10.attention.self.value.bias:                            {input: bw_in1_add_554_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ce27aa0]]}
  grad_acc_layer.10.attention.self.value.weight:                          {input: bw_in1_matmul_552_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1dbabe60], [3, 0x1cc31700], [4, 0x18bbf1e0], [5, 0x1d548bc0]]}
  grad_acc_layer.10.attention.self.key.bias:                              {input: bw_in1_add_540_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19a244e0]]}
  grad_acc_layer.10.attention.self.key.weight:                            {input: bw_in1_matmul_538_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c6b1c80], [0, 0x1df438c0], [1, 0x1ce33dc0], [2, 0x1dbf5080]]}
  grad_acc_layer.10.attention.self.query.bias:                            {input: bw_in1_add_534_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1cc7a920]]}
  grad_acc_layer.10.attention.self.query.weight:                          {input: bw_in1_matmul_532_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1cdde880], [2, 0x1db62c40], [3, 0x1cbe84e0], [4, 0x18b75fc0]]}
  grad_acc_layer.9.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_529_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d591de0]]}
  grad_acc_layer.9.output.LayerNorm.weight:                               {input: bw_in1_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19a30800]]}
  grad_acc_layer.9.output.dense.bias:                                     {input: bw_in1_add_526_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c6faea0]]}
  grad_acc_layer.9.output.dense.weight:                                   {input: bw_in1_matmul_524_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1cd1fa40], [1, 0x1bc0ff40], [2, 0x1cb6f800], [3, 0x1bb62ca0], [4, 0x17b39980], [5, 0x1c4e7c60], [6, 0x1879ee80], [7, 0x1b52c520], [0, 0x1cd81260], [1, 0x1bc71760], [2, 0x1cbd1020], [3, 0x1bbc44c0]]}
  grad_acc_layer.9.intermediate.dense.bias:                               {input: bw_in1_add_520_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18ae7ec0]]}
  grad_acc_layer.9.intermediate.dense.weight:                             {input: bw_in1_matmul_518_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b8b2460], [0, 0x1d068aa0], [1, 0x1bf7d8a0], [2, 0x1cedd160], [3, 0x1be93700], [4, 0x17efc7e0], [5, 0x1c83cfc0], [6, 0x18b18ae0], [7, 0x1b913c80], [0, 0x1d0ca2c0], [1, 0x1bfdf0c0], [2, 0x1cf3e980]]}
  grad_acc_layer.9.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_515_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1bef4f20]]}
  grad_acc_layer.9.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17f5e000]]}
  grad_acc_layer.9.attention.output.dense.bias:                           {input: bw_in1_add_512_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18b7a300]]}
  grad_acc_layer.9.attention.output.dense.weight:                         {input: bw_in1_matmul_510_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b9754a0], [0, 0x1d12bae0], [1, 0x1c0408e0], [2, 0x1cfa01a0]]}
  grad_acc_layer.9.attention.self.value.bias:                             {input: bw_in1_add_501_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1bf01240]]}
  grad_acc_layer.9.attention.self.value.weight:                           {input: bw_in1_matmul_499_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17f6a320], [5, 0x1c8aab00], [6, 0x18b86620], [7, 0x1b9be6c0]]}
  grad_acc_layer.9.attention.self.key.bias:                               {input: bw_in1_add_487_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1cdac5e0]]}
  grad_acc_layer.9.attention.self.key.weight:                             {input: bw_in1_matmul_485_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b6fb7c0], [0, 0x1cf1f900], [1, 0x1be03b00], [2, 0x1cd633c0]]}
  grad_acc_layer.9.attention.self.query.bias:                             {input: bw_in1_add_481_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1bd56860]]}
  grad_acc_layer.9.attention.self.query.weight:                           {input: bw_in1_matmul_479_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17d2d540], [5, 0x1c6c3220], [6, 0x1897a440], [7, 0x1b7449e0]]}
  grad_acc_layer.8.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_476_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1cf68b20]]}
  grad_acc_layer.8.output.LayerNorm.weight:                               {input: bw_in1_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1be4cd20]]}
  grad_acc_layer.8.output.dense.bias:                                     {input: bw_in1_add_473_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1bd62b80]]}
  grad_acc_layer.8.output.dense.weight:                                   {input: bw_in1_matmul_471_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17d76760], [5, 0x1c70c440], [6, 0x189c3660], [7, 0x1b78dc00], [0, 0x1cf74e40], [1, 0x1be59040], [2, 0x1cdb8900], [3, 0x1bd6eea0], [4, 0x17dd7f80], [5, 0x1c76dc60], [6, 0x18a24e80], [7, 0x1b7ef420]]}
  grad_acc_layer.8.intermediate.dense.bias:                               {input: bw_in1_add_467_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1cfd6660]]}
  grad_acc_layer.8.intermediate.dense.weight:                             {input: bw_in1_matmul_465_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1beba860], [2, 0x1ce1a120], [3, 0x1bdd06c0], [4, 0x17e397a0], [5, 0x1c7cf480], [6, 0x18a866a0], [7, 0x1b850c40], [0, 0x1d007280], [1, 0x1bf1c080], [2, 0x1ce7b940], [3, 0x1be31ee0], [4, 0x17e9afc0]]}
  grad_acc_layer.8.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_462_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c830ca0]]}
  grad_acc_layer.8.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c89e7e0]]}
  grad_acc_layer.8.attention.output.dense.bias:                           {input: bw_in1_add_459_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d119f40]]}
  grad_acc_layer.8.attention.output.dense.weight:                         {input: bw_in1_matmul_457_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c03e0e0], [4, 0x180e40c0], [5, 0x1c9f3ca0], [6, 0x18d3d2c0]]}
  grad_acc_layer.8.attention.self.value.bias:                             {input: bw_in1_add_448_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1bb75360]]}
  grad_acc_layer.8.attention.self.value.weight:                           {input: bw_in1_matmul_446_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d2eeaa0], [1, 0x1c258da0], [2, 0x1d126260], [3, 0x1c087300]]}
  grad_acc_layer.8.attention.self.key.bias:                               {input: bw_in1_add_434_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1812d2e0]]}
  grad_acc_layer.8.attention.self.key.weight:                             {input: bw_in1_matmul_432_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1ca3cec0], [6, 0x18d864e0], [7, 0x1bb81680], [0, 0x1d337cc0]]}
  grad_acc_layer.8.attention.self.query.bias:                             {input: bw_in1_add_428_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c2a1fc0]]}
  grad_acc_layer.8.attention.self.query.weight:                           {input: bw_in1_matmul_426_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d16f480], [3, 0x1c0d0520], [4, 0x18139600], [5, 0x1ca860e0]]}
  grad_acc_layer.7.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_423_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18dcf700]]}
  grad_acc_layer.7.output.LayerNorm.weight:                               {input: bw_in1_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d10dc20]]}
  grad_acc_layer.7.output.dense.bias:                                     {input: bw_in1_add_420_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d174d00]]}
  grad_acc_layer.7.output.dense.weight:                                   {input: bw_in1_matmul_418_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c089b00], [2, 0x1cfe93c0], [3, 0x1bf0d560], [4, 0x17fb3540], [5, 0x1c8f3d20], [6, 0x18bcf840], [7, 0x1ba078e0], [0, 0x1d181020], [1, 0x1c0eb320], [2, 0x1d04abe0], [3, 0x1bf6ed80], [4, 0x18014d60]]}
  grad_acc_layer.7.intermediate.dense.bias:                               {input: bw_in1_add_414_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c955540]]}
  grad_acc_layer.7.intermediate.dense.weight:                             {input: bw_in1_matmul_412_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18c31060], [7, 0x1ba69100], [0, 0x1d1e2840], [1, 0x1c14cb40], [2, 0x1d0ac400], [3, 0x1bfd05a0], [4, 0x18076580], [5, 0x1c986160], [6, 0x18c92880], [7, 0x1baca920], [0, 0x1d244060], [1, 0x1c1ae360]]}
  grad_acc_layer.7.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_409_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c031dc0]]}
  grad_acc_layer.7.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x180d7da0]]}
  grad_acc_layer.7.attention.output.dense.bias:                           {input: bw_in1_add_406_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c9e7980]]}
  grad_acc_layer.7.attention.output.dense.weight:                         {input: bw_in1_matmul_404_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18cf40a0], [7, 0x1bb2c140], [0, 0x1d2a5880], [1, 0x1c20fb80]]}
  grad_acc_layer.7.attention.self.value.bias:                             {input: bw_in1_add_395_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1cb2be60]]}
  grad_acc_layer.7.attention.self.value.weight:                           {input: bw_in1_matmul_393_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c30c680], [6, 0x185e81a0], [7, 0x1b375840], [0, 0x1cb38180]]}
  grad_acc_layer.7.attention.self.key.bias:                               {input: bw_in1_add_381_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1baa2480]]}
  grad_acc_layer.7.attention.self.key.weight:                             {input: bw_in1_matmul_379_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1c9dd440], [3, 0x1b9d08e0], [4, 0x179e44c0], [5, 0x1c3558a0]]}
  grad_acc_layer.7.attention.self.query.bias:                             {input: bw_in1_add_375_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x186313c0]]}
  grad_acc_layer.7.attention.self.query.weight:                           {input: bw_in1_matmul_373_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b3bea60], [0, 0x1cb813a0], [1, 0x1baae7a0], [2, 0x1ca26660]]}
  grad_acc_layer.6.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_370_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1ba19b00]]}
  grad_acc_layer.6.output.LayerNorm.weight:                               {input: bw_in1_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17a2d6e0]]}
  grad_acc_layer.6.output.dense.bias:                                     {input: bw_in1_add_367_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c39eac0]]}
  grad_acc_layer.6.output.dense.weight:                                   {input: bw_in1_matmul_365_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1863d6e0], [7, 0x1b407c80], [0, 0x1cbca5c0], [1, 0x1baf79c0], [2, 0x1ca6f880], [3, 0x1ba25e20], [4, 0x17a39a00], [5, 0x1c3aade0], [6, 0x1869ef00], [7, 0x1b4694a0], [0, 0x1cc2bde0], [1, 0x1bb591e0]]}
  grad_acc_layer.6.intermediate.dense.bias:                               {input: bw_in1_add_361_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x184c3920]]}
  grad_acc_layer.6.intermediate.dense.weight:                             {input: bw_in1_matmul_359_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b25d2c0], [0, 0x1ca1fc00], [1, 0x1b94d000], [2, 0x1c8c4ec0], [3, 0x1b8d0960], [4, 0x178e4540], [5, 0x1c255920], [6, 0x184f4540], [7, 0x1b2beae0], [0, 0x1ca81420], [1, 0x1b9ae820], [2, 0x1c9266e0]]}
  grad_acc_layer.6.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_356_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1b932180]]}
  grad_acc_layer.6.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17945d60]]}
  grad_acc_layer.6.attention.output.dense.bias:                           {input: bw_in1_add_353_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c2b7140]]}
  grad_acc_layer.6.attention.output.dense.weight:                         {input: bw_in1_matmul_351_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18555d60], [7, 0x1b320300], [0, 0x1cae2c40], [1, 0x1ba10040]]}
  grad_acc_layer.6.attention.self.value.bias:                             {input: bw_in1_add_342_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1c987f00]]}
  grad_acc_layer.6.attention.self.value.weight:                           {input: bw_in1_matmul_340_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1b93e4a0], [4, 0x17952080], [5, 0x1c2c3460], [6, 0x1859ef80]]}
  grad_acc_layer.6.attention.self.key.bias:                               {input: bw_in1_add_328_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b369520]]}
  grad_acc_layer.6.attention.self.key.weight:                             {input: bw_in1_matmul_326_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ba59260], [2, 0x1c994220], [3, 0x1b9876c0], [4, 0x1799b2a0]]}
  grad_acc_layer.6.attention.self.query.bias:                             {input: bw_in1_add_322_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17b9b1a0]]}
  grad_acc_layer.6.attention.self.query.weight:                           {input: bw_in1_matmul_320_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c549480], [6, 0x188006a0], [7, 0x1b58dd40], [0, 0x1cde2a80]]}
  grad_acc_layer.5.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1bcd2f80]]}
  grad_acc_layer.5.output.LayerNorm.weight:                               {input: bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1cc32840]]}
  grad_acc_layer.5.output.dense.bias:                                     {input: bw_in1_add_314_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1bc25ce0]]}
  grad_acc_layer.5.output.dense.weight:                                   {input: bw_in1_matmul_312_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17ba74c0], [5, 0x1c5926a0], [6, 0x188498c0], [7, 0x1b5d6f60], [0, 0x1ce2bca0], [1, 0x1bcdf2a0], [2, 0x1cc3eb60], [3, 0x1bc32000], [4, 0x17c08ce0], [5, 0x1c5f3ec0], [6, 0x188ab0e0], [7, 0x1b638780]]}
  grad_acc_layer.5.intermediate.dense.bias:                               {input: bw_in1_add_308_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ce8d4c0]]}
  grad_acc_layer.5.intermediate.dense.weight:                             {input: bw_in1_matmul_306_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1bd40ac0], [2, 0x1cca0380], [3, 0x1bc93820], [4, 0x17c6a500], [5, 0x1c6556e0], [6, 0x1890c900], [7, 0x1b699fa0], [0, 0x1cebe0e0], [1, 0x1bda22e0], [2, 0x1cd01ba0], [3, 0x1bcf5040], [4, 0x17ccbd20]]}
  grad_acc_layer.5.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c6b6f00]]}
  grad_acc_layer.5.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1896e120]]}
  grad_acc_layer.5.attention.output.dense.bias:                           {input: bw_in1_add_300_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1cad10a0]]}
  grad_acc_layer.5.attention.output.dense.weight:                         {input: bw_in1_matmul_298_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1ba87640], [4, 0x17a9b220], [5, 0x1c40c600], [6, 0x18700720]]}
  grad_acc_layer.5.attention.self.value.bias:                             {input: bw_in1_add_289_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b4cacc0]]}
  grad_acc_layer.5.attention.self.value.weight:                           {input: bw_in1_matmul_287_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1cc8d600], [1, 0x1bbbaa00], [2, 0x1cadd3c0], [3, 0x1bad0860]]}
  grad_acc_layer.5.attention.self.key.bias:                               {input: bw_in1_add_275_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x17ae4440]]}
  grad_acc_layer.5.attention.self.key.weight:                             {input: bw_in1_matmul_273_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1c455820], [6, 0x18749940], [7, 0x1b4d6fe0], [0, 0x1ccd6820]]}
  grad_acc_layer.5.attention.self.query.bias:                             {input: bw_in1_add_269_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1bc03c20]]}
  grad_acc_layer.5.attention.self.query.weight:                           {input: bw_in1_matmul_267_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1cb265e0], [3, 0x1bb19a80], [4, 0x17af0760], [5, 0x1c49ea40]]}
  grad_acc_layer.4.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18792b60]]}
  grad_acc_layer.4.output.LayerNorm.weight:                               {input: bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1b520200]]}
  grad_acc_layer.4.output.dense.bias:                                     {input: bw_in1_add_261_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cfb6a40]]}
  grad_acc_layer.4.output.dense.weight:                                   {input: bw_in1_matmul_259_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19349260], [7, 0x1c0b2000], [0, 0x1d937940], [1, 0x1c840440], [2, 0x1d6c4700], [3, 0x1c6932a0], [4, 0x18651980], [5, 0x1cfc2d60], [6, 0x193aaa80], [7, 0x1c113820], [0, 0x1d999160], [1, 0x1c8a1c60]]}
  grad_acc_layer.4.intermediate.dense.bias:                               {input: bw_in1_add_255_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d725f20]]}
  grad_acc_layer.4.intermediate.dense.weight:                             {input: bw_in1_matmul_253_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c6f4ac0], [4, 0x186b31a0], [5, 0x1d024580], [6, 0x1940c2a0], [7, 0x1c175040], [0, 0x1d9fa980], [1, 0x1c903480], [2, 0x1d756b40], [3, 0x1c7562e0], [4, 0x187149c0], [5, 0x1d085da0], [6, 0x1946dac0]]}
  grad_acc_layer.4.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c1d6860]]}
  grad_acc_layer.4.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x187d7a00]]}
  grad_acc_layer.4.attention.output.dense.bias:                           {input: bw_in1_add_247_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d148de0]]}
  grad_acc_layer.4.attention.output.dense.weight:                         {input: bw_in1_matmul_245_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x19530b00], [7, 0x1c2443a0], [0, 0x1db1f1e0], [1, 0x1ca27ce0]]}
  grad_acc_layer.4.attention.self.value.bias:                             {input: bw_in1_add_236_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d87b3a0]]}
  grad_acc_layer.4.attention.self.value.weight:                           {input: bw_in1_matmul_234_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d5b84a0], [3, 0x1c587040], [4, 0x18576320], [5, 0x1cf0c000]]}
  grad_acc_layer.4.attention.self.key.bias:                               {input: bw_in1_add_222_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1bfeef80]]}
  grad_acc_layer.4.attention.self.key.weight:                             {input: bw_in1_matmul_220_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d7e24c0], [1, 0x1c727ec0], [2, 0x1d56f280], [3, 0x1c53de20]]}
  grad_acc_layer.4.attention.self.query.bias:                             {input: bw_in1_add_216_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1856a000]]}
  grad_acc_layer.4.attention.self.query.weight:                           {input: bw_in1_matmul_214_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cec2de0], [6, 0x19292500], [7, 0x1bffb2a0], [0, 0x1d82b6e0]]}
  grad_acc_layer.3.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c7710e0]]}
  grad_acc_layer.3.output.LayerNorm.weight:                               {input: bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x192db720]]}
  grad_acc_layer.3.output.dense.bias:                                     {input: bw_in1_add_208_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c0444c0]]}
  grad_acc_layer.3.output.dense.weight:                                   {input: bw_in1_matmul_206_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d874900], [1, 0x1c77d400], [2, 0x1d6016c0], [3, 0x1c5d0260], [4, 0x185bf540], [5, 0x1cf55220], [6, 0x192e7a40], [7, 0x1c0507e0], [0, 0x1d8d6120], [1, 0x1c7dec20], [2, 0x1d662ee0], [3, 0x1c631a80]]}
  grad_acc_layer.3.intermediate.dense.bias:                               {input: bw_in1_add_202_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18620d60]]}
  grad_acc_layer.3.intermediate.dense.weight:                             {input: bw_in1_matmul_200_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1da5c1a0], [1, 0x1c964ca0], [2, 0x1d7b8360], [3, 0x1c7b7b00], [4, 0x187761e0], [5, 0x1d0e75c0], [6, 0x194cf2e0], [7, 0x1c1e2b80], [0, 0x1dabd9c0], [1, 0x1c9c64c0], [2, 0x1d819b80], [3, 0x1c819320]]}
  grad_acc_layer.3.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d9b8240]]}
  grad_acc_layer.3.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1ca0cee0]]}
  grad_acc_layer.3.attention.output.dense.bias:                           {input: bw_in1_add_194_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1895dac0]]}
  grad_acc_layer.3.attention.output.dense.weight:                         {input: bw_in1_matmul_192_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x196e77a0], [7, 0x1c3ca440], [0, 0x1dc98f80], [1, 0x1cba1a80]]}
  grad_acc_layer.3.attention.self.value.bias:                             {input: bw_in1_add_183_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d2ceea0]]}
  grad_acc_layer.3.attention.self.value.weight:                           {input: bw_in1_matmul_181_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x197309c0], [7, 0x1c413660], [0, 0x1dce21a0], [1, 0x1cbeaca0]]}
  grad_acc_layer.3.attention.self.key.bias:                               {input: bw_in1_add_169_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d9c4560]]}
  grad_acc_layer.3.attention.self.key.weight:                             {input: bw_in1_matmul_167_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1ca19200], [4, 0x18969de0], [5, 0x1d2db1c0], [6, 0x19779be0]]}
  grad_acc_layer.3.attention.self.query.bias:                             {input: bw_in1_add_163_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d8876c0]]}
  grad_acc_layer.3.attention.self.query.weight:                           {input: bw_in1_matmul_161_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x187e3d20], [5, 0x1d155100], [6, 0x19579d20], [7, 0x1c28d5c0]]}
  grad_acc_layer.2.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1db68400]]}
  grad_acc_layer.2.output.LayerNorm.weight:                               {input: bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1ca70f00]]}
  grad_acc_layer.2.output.dense.bias:                                     {input: bw_in1_add_155_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c87ab40]]}
  grad_acc_layer.2.output.dense.weight:                                   {input: bw_in1_matmul_153_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c886e60], [4, 0x1882cf40], [5, 0x1d19e320], [6, 0x195c2f40], [7, 0x1c2d67e0], [0, 0x1db74720], [1, 0x1ca7d220], [2, 0x1d8939e0], [3, 0x1c8e8680], [4, 0x1888e760], [5, 0x1d1ffb40], [6, 0x19624760]]}
  grad_acc_layer.2.intermediate.dense.bias:                               {input: bw_in1_add_149_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1c338000]]}
  grad_acc_layer.2.intermediate.dense.weight:                             {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1dbd5f40], [1, 0x1cadea40], [2, 0x1d8f5200], [3, 0x1c949ea0], [4, 0x188eff80], [5, 0x1d261360], [6, 0x19685f80], [7, 0x1c368c20], [0, 0x1dc37760], [1, 0x1cb40260], [2, 0x1d956a20], [3, 0x1c9ab6c0]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x189517a0]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1d2c2b80]]}
  grad_acc_layer.2.attention.output.dense.bias:                           {input: bw_in1_add_141_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18f494a0]]}
  grad_acc_layer.2.attention.output.dense.weight:                         {input: bw_in1_matmul_139_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1bd44640], [0, 0x1d4bdd80], [1, 0x1c47d580], [2, 0x1d332440]]}
  grad_acc_layer.2.attention.self.value.bias:                             {input: bw_in1_add_130_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c2934e0]]}
  grad_acc_layer.2.attention.self.value.weight:                           {input: bw_in1_matmul_128_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x183394c0], [5, 0x1cc184a0], [6, 0x18f557c0], [7, 0x1bd8d860]]}
  grad_acc_layer.2.attention.self.key.bias:                               {input: bw_in1_add_116_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cc0c180]]}
  grad_acc_layer.2.attention.self.key.weight:                             {input: bw_in1_matmul_114_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d506fa0], [1, 0x1c4c67a0], [2, 0x1d37b660], [3, 0x1c29f800]]}
  grad_acc_layer.2.attention.self.query.bias:                             {input: bw_in1_add_110_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x183826e0]]}
  grad_acc_layer.2.attention.self.query.weight:                           {input: bw_in1_matmul_108_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cc616c0], [6, 0x18f9e9e0], [7, 0x1bdd6a80], [0, 0x1d5501c0]]}
  grad_acc_layer.1.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c50f9c0]]}
  grad_acc_layer.1.output.LayerNorm.weight:                               {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d3c4880]]}
  grad_acc_layer.1.output.dense.bias:                                     {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d380ee0]]}
  grad_acc_layer.1.output.dense.weight:                                   {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c2ae2e0], [2, 0x1d1b86a0], [3, 0x1c119740], [4, 0x18182820], [5, 0x1cacf300], [6, 0x18ddba20], [7, 0x1bbd6bc0], [0, 0x1d38d200], [1, 0x1c30fb00], [2, 0x1d219ec0], [3, 0x1c17af60], [4, 0x181e4040]]}
  grad_acc_layer.1.intermediate.dense.bias:                               {input: bw_in1_add_96_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cb30b20]]}
  grad_acc_layer.1.intermediate.dense.weight:                             {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18e3d240], [7, 0x1bc383e0], [0, 0x1d3eea20], [1, 0x1c371320], [2, 0x1d27b6e0], [3, 0x1c1dc780], [4, 0x18245860], [5, 0x1cb61740], [6, 0x18e9ea60], [7, 0x1bc99c00], [0, 0x1d450240], [1, 0x1c3d2b40]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d2dcf00]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1bbca8a0]]}
  grad_acc_layer.1.attention.output.dense.bias:                           {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c23dfa0]]}
  grad_acc_layer.1.attention.output.dense.weight:                         {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x182a7080], [5, 0x1cbc2f60], [6, 0x18f00280], [7, 0x1bcfb420]]}
  grad_acc_layer.1.attention.self.value.bias:                             {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d4b1a60]]}
  grad_acc_layer.1.attention.self.value.weight:                           {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c434360], [2, 0x1d2e9220], [3, 0x1c24a2c0], [4, 0x182f02a0]]}
  grad_acc_layer.1.attention.self.key.bias:                               {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d4260e0]]}
  grad_acc_layer.1.attention.self.key.weight:                             {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c387180], [4, 0x183f0260], [5, 0x1cd49040], [6, 0x190c3260]]}
  grad_acc_layer.1.attention.self.query.bias:                             {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1bebe400]]}
  grad_acc_layer.1.attention.self.query.weight:                           {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d674a40], [1, 0x1c5ba440], [2, 0x1d432400], [3, 0x1c3d03a0]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                 {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18439480]]}
  grad_acc_layer.0.output.LayerNorm.weight:                               {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1c5ae120]]}
  grad_acc_layer.0.output.dense.bias:                                     {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1cd92260]]}
  grad_acc_layer.0.output.dense.weight:                                   {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [3, 4], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x1910c480], [7, 0x1beca720], [0, 0x1d6bdc60], [1, 0x1c603660], [2, 0x1d47b620], [3, 0x1c4195c0], [4, 0x184457a0], [5, 0x1cd9e580], [6, 0x1916dca0], [7, 0x1bf2bf40], [0, 0x1d71f480], [1, 0x1c664e80]]}
  grad_acc_layer.0.intermediate.dense.bias:                               {input: bw_in1_add_43_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d4dce40]]}
  grad_acc_layer.0.intermediate.dense.weight:                             {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [4, 3], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c47ade0], [4, 0x184a6fc0], [5, 0x1cdffda0], [6, 0x191cf4c0], [7, 0x1bf8d760], [0, 0x1d780ca0], [1, 0x1c6c66a0], [2, 0x1d50da60], [3, 0x1c4dc600], [4, 0x185087e0], [5, 0x1ce615c0], [6, 0x19230ce0]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                       {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c2e8a20]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                     {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1838ea00]]}
  grad_acc_layer.0.attention.output.dense.bias:                           {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1ccaa8e0]]}
  grad_acc_layer.0.attention.output.dense.weight:                         {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x18fe7c00], [7, 0x1be1fca0], [0, 0x1d5993e0], [1, 0x1c51bce0]]}
  grad_acc_layer.0.attention.self.value.bias:                             {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1d3d0ba0]]}
  grad_acc_layer.0.attention.self.value.weight:                           {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1c2f4d40], [4, 0x1839ad20], [5, 0x1ccb6c00], [6, 0x19030e20]]}
  grad_acc_layer.0.attention.self.key.bias:                               {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x1be68ec0]]}
  grad_acc_layer.0.attention.self.key.weight:                             {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1d5e2600], [1, 0x1c564f00], [2, 0x1d3dcec0], [3, 0x1c33df60]]}
  grad_acc_layer.0.attention.self.query.bias:                             {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x183e3f40]]}
  grad_acc_layer.0.attention.self.query.weight:                           {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1ccffe20], [6, 0x1907a040], [7, 0x1be751e0], [0, 0x1d62b820]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 64
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone4377_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_22: {type: matmul, grid_loc: [0, 8], grid_size: [1, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, _fused_op_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_29: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_1, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 5], grid_size: [1, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_37: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_33, hidden_states],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_38.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [1, 2], inputs: [add_37, layernorm_38.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.1, layernorm_38.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.2, lc.input_tensor.layernorm_38.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_2: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_38.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 2], inputs: [layernorm_38.dc.subtract.1, _fused_op_2],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_4: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_3, layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_41: {type: matmul, grid_loc: [2, 7], grid_size: [4, 4], inputs: [_fused_op_4, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_44: {type: gelu, grid_loc: [3, 0], grid_size: [1, 3], inputs: [matmul_41],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_51: {type: add, grid_loc: [3, 3], grid_size: [1, 2], inputs: [matmul_47, _fused_op_4],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_52.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [1, 2], inputs: [add_51, layernorm_52.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.1, layernorm_52.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.2, lc.input_tensor.layernorm_52.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_5: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_52.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_52.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_6: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 2], inputs: [layernorm_52.dc.subtract.1, _fused_op_5],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_7: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_6, layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_55: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [_fused_op_7, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_61: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [_fused_op_7, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_67: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_8: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone4406_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [_fused_op_8, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_75: {type: matmul, grid_loc: [7, 4], grid_size: [1, 4], inputs: [_fused_op_7, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_9: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, _fused_op_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_82: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_9, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [7, 8], grid_size: [1, 4], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_90: {type: add, grid_loc: [8, 0], grid_size: [1, 2], inputs: [matmul_86, _fused_op_7],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_91.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [1, 2], inputs: [add_90, layernorm_91.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.1, layernorm_91.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.2, lc.input_tensor.layernorm_91.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_10: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_91.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_91.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_11: {type: fused_op, grid_loc: [8, 6], grid_size: [1, 2], inputs: [layernorm_91.dc.subtract.1, _fused_op_10],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_12: {type: fused_op, grid_loc: [8, 8], grid_size: [1, 1], inputs: [_fused_op_11, layer.1.attention.output.LayerNorm.weight, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}

  fwd_1:
    target_device: 0
    input_count: 64
    matmul_94: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_12_0, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_97: {type: gelu, grid_loc: [0, 4], grid_size: [1, 3], inputs: [matmul_94],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_104: {type: add, grid_loc: [0, 7], grid_size: [1, 2], inputs: [matmul_100, e2e__fused_op_12_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_105.dc.subtract.1: {type: subtract, grid_loc: [0, 10], grid_size: [1, 2], inputs: [add_104, layernorm_105.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.1, layernorm_105.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.2, lc.input_tensor.layernorm_105.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_13: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_105.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_105.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_14: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 2], inputs: [layernorm_105.dc.subtract.1, _fused_op_13],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_15: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [_fused_op_14, layer.1.output.LayerNorm.weight, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_108: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_15, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_114: {type: matmul, grid_loc: [2, 8], grid_size: [1, 4], inputs: [_fused_op_15, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_120: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_16: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_fork_clone4423_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_16, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_128: {type: matmul, grid_loc: [3, 6], grid_size: [1, 4], inputs: [_fused_op_15, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, _fused_op_16],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_135: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_17, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_143: {type: add, grid_loc: [5, 4], grid_size: [1, 2], inputs: [matmul_139, _fused_op_15],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_144.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [1, 2], inputs: [add_143, layernorm_144.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.1, layernorm_144.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.2, lc.input_tensor.layernorm_144.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_18: {type: fused_op, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_144.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_144.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_19: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 2], inputs: [layernorm_144.dc.subtract.1, _fused_op_18],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_20: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [_fused_op_19, layer.2.attention.output.LayerNorm.weight, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_147: {type: matmul, grid_loc: [6, 2], grid_size: [4, 4], inputs: [_fused_op_20, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_150: {type: gelu, grid_loc: [6, 6], grid_size: [1, 3], inputs: [matmul_147],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_2:
    target_device: 0
    input_count: 64
    matmul_153: {type: matmul, grid_loc: [0, 0], grid_size: [1, 12], inputs: [e2e_gelu_150_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_157: {type: add, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_153, e2e__fused_op_20_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_158.dc.subtract.1: {type: subtract, grid_loc: [1, 3], grid_size: [1, 2], inputs: [add_157, layernorm_158.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.1, layernorm_158.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.2, lc.input_tensor.layernorm_158.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_21: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_158.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_22: {type: fused_op, grid_loc: [1, 8], grid_size: [1, 2], inputs: [layernorm_158.dc.subtract.1, _fused_op_21],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_23: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_22, layer.2.output.LayerNorm.weight, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_161: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [_fused_op_23, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_167: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_23, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_173: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_24: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175_fork_clone4440_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_24, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_181: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [_fused_op_23, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_25: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, _fused_op_24],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_188: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_25, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_196: {type: add, grid_loc: [3, 8], grid_size: [1, 2], inputs: [matmul_192, _fused_op_23],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_197.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 2], inputs: [add_196, layernorm_197.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_197.dc.subtract.1, layernorm_197.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.2, lc.input_tensor.layernorm_197.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_26: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_197.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_197.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_27: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 2], inputs: [layernorm_197.dc.subtract.1, _fused_op_26],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_28: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_27, layer.3.attention.output.LayerNorm.weight, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_200: {type: matmul, grid_loc: [4, 7], grid_size: [4, 4], inputs: [_fused_op_28, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_203: {type: gelu, grid_loc: [5, 0], grid_size: [1, 3], inputs: [matmul_200],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [8, 0], grid_size: [1, 12], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_210: {type: add, grid_loc: [5, 3], grid_size: [1, 2], inputs: [matmul_206, _fused_op_28],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_211.dc.subtract.1: {type: subtract, grid_loc: [5, 5], grid_size: [1, 2], inputs: [add_210, layernorm_211.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_211.dc.subtract.1, layernorm_211.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.2, lc.input_tensor.layernorm_211.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_29: {type: fused_op, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_211.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_211.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_30: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_211.dc.subtract.1, _fused_op_29],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_31: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_30, layer.3.output.LayerNorm.weight, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_214: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_220: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [_fused_op_31, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_226: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_32: {type: fused_op, grid_loc: [6, 6], grid_size: [1, 1], inputs: [matmul_226, input_1_multiply_228_fork_clone4457_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [_fused_op_32, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_234: {type: matmul, grid_loc: [9, 4], grid_size: [1, 4], inputs: [_fused_op_31, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_33: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, _fused_op_32],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_241: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_33, matmul_234],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [9, 8], grid_size: [1, 4], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}

  fwd_3:
    target_device: 0
    input_count: 64
    add_249: {type: add, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_245_0, e2e__fused_op_31_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [1, 2], inputs: [add_249, layernorm_250.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_250.dc.subtract.1, layernorm_250.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.2, lc.input_tensor.layernorm_250.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_34: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_250.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_250.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_35: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 2], inputs: [layernorm_250.dc.subtract.1, _fused_op_34],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_36: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_35, layer.4.attention.output.LayerNorm.weight, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_253: {type: matmul, grid_loc: [1, 0], grid_size: [4, 4], inputs: [_fused_op_36, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_256: {type: gelu, grid_loc: [1, 4], grid_size: [1, 3], inputs: [matmul_253],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [5, 0], grid_size: [1, 12], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_263: {type: add, grid_loc: [1, 7], grid_size: [1, 2], inputs: [matmul_259, _fused_op_36],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_264.dc.subtract.1: {type: subtract, grid_loc: [1, 9], grid_size: [1, 2], inputs: [add_263, layernorm_264.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_264.dc.subtract.1, layernorm_264.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.2, lc.input_tensor.layernorm_264.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_37: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_264.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_264.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_38: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 2], inputs: [layernorm_264.dc.subtract.1, _fused_op_37],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_39: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [_fused_op_38, layer.4.output.LayerNorm.weight, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_267: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_39, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_273: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_39, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_279: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_40: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [matmul_279, input_1_multiply_281_fork_clone4474_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_40, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_287: {type: matmul, grid_loc: [4, 5], grid_size: [1, 4], inputs: [_fused_op_39, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, _fused_op_40],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_294: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_41, matmul_287],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_302: {type: add, grid_loc: [4, 10], grid_size: [1, 2], inputs: [matmul_298, _fused_op_39],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_303.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [1, 2], inputs: [add_302, layernorm_303.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_303.dc.subtract.1, layernorm_303.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.2, lc.input_tensor.layernorm_303.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_42: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_303.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_303.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_43: {type: fused_op, grid_loc: [6, 10], grid_size: [1, 2], inputs: [layernorm_303.dc.subtract.1, _fused_op_42],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_44: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [_fused_op_43, layer.5.attention.output.LayerNorm.weight, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}

  fwd_4:
    target_device: 0
    input_count: 64
    matmul_306: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_44_0, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_309: {type: gelu, grid_loc: [0, 4], grid_size: [1, 3], inputs: [matmul_306],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_316: {type: add, grid_loc: [0, 7], grid_size: [1, 2], inputs: [matmul_312, e2e__fused_op_44_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_317.dc.subtract.1: {type: subtract, grid_loc: [0, 10], grid_size: [1, 2], inputs: [add_316, layernorm_317.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_317.dc.subtract.1, layernorm_317.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.2, lc.input_tensor.layernorm_317.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_45: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_317.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_317.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_46: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 2], inputs: [layernorm_317.dc.subtract.1, _fused_op_45],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_47: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [_fused_op_46, layer.5.output.LayerNorm.weight, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_320: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_47, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_326: {type: matmul, grid_loc: [2, 8], grid_size: [1, 4], inputs: [_fused_op_47, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_332: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_48: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_332, input_1_multiply_334_fork_clone4491_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_48, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_340: {type: matmul, grid_loc: [3, 6], grid_size: [1, 4], inputs: [_fused_op_47, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_49: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, _fused_op_48],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_347: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_49, matmul_340],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_355: {type: add, grid_loc: [5, 4], grid_size: [1, 2], inputs: [matmul_351, _fused_op_47],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_356.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [1, 2], inputs: [add_355, layernorm_356.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.1, layernorm_356.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.2, lc.input_tensor.layernorm_356.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_50: {type: fused_op, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_356.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_356.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_51: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 2], inputs: [layernorm_356.dc.subtract.1, _fused_op_50],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_52: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [_fused_op_51, layer.6.attention.output.LayerNorm.weight, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_359: {type: matmul, grid_loc: [6, 2], grid_size: [4, 4], inputs: [_fused_op_52, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_362: {type: gelu, grid_loc: [6, 6], grid_size: [1, 3], inputs: [matmul_359],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_5:
    target_device: 0
    input_count: 64
    matmul_365: {type: matmul, grid_loc: [0, 0], grid_size: [1, 12], inputs: [e2e_gelu_362_0, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_369: {type: add, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_365, e2e__fused_op_52_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_370.dc.subtract.1: {type: subtract, grid_loc: [1, 3], grid_size: [1, 2], inputs: [add_369, layernorm_370.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_370.dc.subtract.1, layernorm_370.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.2, lc.input_tensor.layernorm_370.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_53: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_370.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_370.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_54: {type: fused_op, grid_loc: [1, 8], grid_size: [1, 2], inputs: [layernorm_370.dc.subtract.1, _fused_op_53],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_55: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_54, layer.6.output.LayerNorm.weight, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_373: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [_fused_op_55, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_379: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_55, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_385: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_56: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [matmul_385, input_1_multiply_387_fork_clone4508_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_56, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_393: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [_fused_op_55, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_57: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, _fused_op_56],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_400: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_57, matmul_393],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_404: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_408: {type: add, grid_loc: [3, 8], grid_size: [1, 2], inputs: [matmul_404, _fused_op_55],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_409.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 2], inputs: [add_408, layernorm_409.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_409.dc.subtract.1, layernorm_409.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.2, lc.input_tensor.layernorm_409.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_58: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_409.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_409.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_59: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 2], inputs: [layernorm_409.dc.subtract.1, _fused_op_58],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_60: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_59, layer.7.attention.output.LayerNorm.weight, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_412: {type: matmul, grid_loc: [4, 7], grid_size: [4, 4], inputs: [_fused_op_60, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_415: {type: gelu, grid_loc: [5, 0], grid_size: [1, 3], inputs: [matmul_412],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [8, 0], grid_size: [1, 12], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_422: {type: add, grid_loc: [5, 3], grid_size: [1, 2], inputs: [matmul_418, _fused_op_60],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_423.dc.subtract.1: {type: subtract, grid_loc: [5, 5], grid_size: [1, 2], inputs: [add_422, layernorm_423.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_423.dc.subtract.1, layernorm_423.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.2, lc.input_tensor.layernorm_423.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_61: {type: fused_op, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_423.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_423.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_62: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_423.dc.subtract.1, _fused_op_61],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_63: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_62, layer.7.output.LayerNorm.weight, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_426: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [_fused_op_63, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_432: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [_fused_op_63, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_438: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_64: {type: fused_op, grid_loc: [6, 6], grid_size: [1, 1], inputs: [matmul_438, input_1_multiply_440_fork_clone4525_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [_fused_op_64, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_446: {type: matmul, grid_loc: [9, 4], grid_size: [1, 4], inputs: [_fused_op_63, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_65: {type: fused_op, grid_loc: [7, 4], grid_size: [1, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, _fused_op_64],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_453: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_65, matmul_446],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_457: {type: matmul, grid_loc: [9, 8], grid_size: [1, 4], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}

  fwd_6:
    target_device: 0
    input_count: 64
    add_461: {type: add, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_457_0, e2e__fused_op_63_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_462.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [1, 2], inputs: [add_461, layernorm_462.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_462.dc.subtract.1, layernorm_462.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.2, lc.input_tensor.layernorm_462.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_66: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_462.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_462.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_67: {type: fused_op, grid_loc: [0, 8], grid_size: [1, 2], inputs: [layernorm_462.dc.subtract.1, _fused_op_66],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_68: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 1], inputs: [_fused_op_67, layer.8.attention.output.LayerNorm.weight, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_465: {type: matmul, grid_loc: [1, 0], grid_size: [4, 4], inputs: [_fused_op_68, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_468: {type: gelu, grid_loc: [1, 4], grid_size: [1, 3], inputs: [matmul_465],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_471: {type: matmul, grid_loc: [5, 0], grid_size: [1, 12], inputs: [gelu_468, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_475: {type: add, grid_loc: [1, 7], grid_size: [1, 2], inputs: [matmul_471, _fused_op_68],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_476.dc.subtract.1: {type: subtract, grid_loc: [1, 9], grid_size: [1, 2], inputs: [add_475, layernorm_476.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_476.dc.subtract.1, layernorm_476.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.2, lc.input_tensor.layernorm_476.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_69: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_476.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_476.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_70: {type: fused_op, grid_loc: [2, 6], grid_size: [1, 2], inputs: [layernorm_476.dc.subtract.1, _fused_op_69],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_71: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [_fused_op_70, layer.8.output.LayerNorm.weight, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_479: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [_fused_op_71, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_485: {type: matmul, grid_loc: [3, 8], grid_size: [1, 4], inputs: [_fused_op_71, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_491: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_72: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [matmul_491, input_1_multiply_493_fork_clone4542_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_72, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_499: {type: matmul, grid_loc: [4, 5], grid_size: [1, 4], inputs: [_fused_op_71, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_73: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, _fused_op_72],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_506: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_73, matmul_499],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [6, 0], grid_size: [1, 4], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_514: {type: add, grid_loc: [4, 10], grid_size: [1, 2], inputs: [matmul_510, _fused_op_71],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_515.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [1, 2], inputs: [add_514, layernorm_515.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_515.dc.subtract.1, layernorm_515.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.2, lc.input_tensor.layernorm_515.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_74: {type: fused_op, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_515.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_515.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_75: {type: fused_op, grid_loc: [6, 10], grid_size: [1, 2], inputs: [layernorm_515.dc.subtract.1, _fused_op_74],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_76: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [_fused_op_75, layer.9.attention.output.LayerNorm.weight, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}

  fwd_7:
    target_device: 0
    input_count: 64
    matmul_518: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_76_0, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_521: {type: gelu, grid_loc: [0, 4], grid_size: [1, 3], inputs: [matmul_518],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_528: {type: add, grid_loc: [0, 7], grid_size: [1, 2], inputs: [matmul_524, e2e__fused_op_76_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_529.dc.subtract.1: {type: subtract, grid_loc: [0, 10], grid_size: [1, 2], inputs: [add_528, layernorm_529.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_529.dc.subtract.1, layernorm_529.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.2, lc.input_tensor.layernorm_529.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_77: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_529.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_529.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_78: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 2], inputs: [layernorm_529.dc.subtract.1, _fused_op_77],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_79: {type: fused_op, grid_loc: [1, 9], grid_size: [1, 1], inputs: [_fused_op_78, layer.9.output.LayerNorm.weight, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_532: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_79, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_538: {type: matmul, grid_loc: [2, 8], grid_size: [1, 4], inputs: [_fused_op_79, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_544: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_80: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_544, input_1_multiply_546_fork_clone4559_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_80, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_552: {type: matmul, grid_loc: [3, 6], grid_size: [1, 4], inputs: [_fused_op_79, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_81: {type: fused_op, grid_loc: [3, 5], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, _fused_op_80],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_559: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [_fused_op_81, matmul_552],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_563: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_567: {type: add, grid_loc: [5, 4], grid_size: [1, 2], inputs: [matmul_563, _fused_op_79],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_568.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [1, 2], inputs: [add_567, layernorm_568.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_568.dc.subtract.1, layernorm_568.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.2, lc.input_tensor.layernorm_568.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_82: {type: fused_op, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_568.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_568.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_83: {type: fused_op, grid_loc: [6, 0], grid_size: [1, 2], inputs: [layernorm_568.dc.subtract.1, _fused_op_82],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_84: {type: fused_op, grid_loc: [5, 11], grid_size: [1, 1], inputs: [_fused_op_83, layer.10.attention.output.LayerNorm.weight, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_571: {type: matmul, grid_loc: [6, 2], grid_size: [4, 4], inputs: [_fused_op_84, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_574: {type: gelu, grid_loc: [6, 6], grid_size: [1, 3], inputs: [matmul_571],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_8:
    target_device: 0
    input_count: 64
    matmul_577: {type: matmul, grid_loc: [0, 0], grid_size: [1, 12], inputs: [e2e_gelu_574_0, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_581: {type: add, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_577, e2e__fused_op_84_0],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_582.dc.subtract.1: {type: subtract, grid_loc: [1, 3], grid_size: [1, 2], inputs: [add_581, layernorm_582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_582.dc.subtract.1, layernorm_582.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.2, lc.input_tensor.layernorm_582.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_85: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_582.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_86: {type: fused_op, grid_loc: [1, 8], grid_size: [1, 2], inputs: [layernorm_582.dc.subtract.1, _fused_op_85],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_87: {type: fused_op, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_86, layer.10.output.LayerNorm.weight, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_585: {type: matmul, grid_loc: [2, 0], grid_size: [1, 4], inputs: [_fused_op_87, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_591: {type: matmul, grid_loc: [2, 4], grid_size: [1, 4], inputs: [_fused_op_87, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    matmul_597: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    _fused_op_88: {type: fused_op, grid_loc: [2, 8], grid_size: [1, 1], inputs: [matmul_597, input_1_multiply_599_fork_clone4576_tile_bcast_tile_bcast, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {z: 12}, broadcast: {r: 4}], input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 0}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [_fused_op_88, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_605: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [_fused_op_87, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, u_kt: 4}}
    _fused_op_89: {type: fused_op, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, _fused_op_88],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 32], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    matmul_612: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [_fused_op_89, matmul_605],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, input_buf_min_size_tiles: [0, 16], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [3, 4], grid_size: [1, 4], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_620: {type: add, grid_loc: [3, 8], grid_size: [1, 2], inputs: [matmul_616, _fused_op_87],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_621.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 2], inputs: [add_620, layernorm_621.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_621.dc.subtract.1, layernorm_621.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.2, lc.input_tensor.layernorm_621.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_90: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_621.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_621.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_91: {type: fused_op, grid_loc: [4, 4], grid_size: [1, 2], inputs: [layernorm_621.dc.subtract.1, _fused_op_90],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_92: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_91, layer.11.attention.output.LayerNorm.weight, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_624: {type: matmul, grid_loc: [4, 7], grid_size: [4, 4], inputs: [_fused_op_92, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 24, u_kt: 1}}
    gelu_627: {type: gelu, grid_loc: [5, 0], grid_size: [1, 3], inputs: [matmul_624],
         t: 1, mblock: [2, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [8, 0], grid_size: [1, 12], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 16, u_kt: 6}}
    add_634: {type: add, grid_loc: [5, 3], grid_size: [1, 2], inputs: [matmul_630, _fused_op_92],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_635.dc.subtract.1: {type: subtract, grid_loc: [5, 5], grid_size: [1, 2], inputs: [add_634, layernorm_635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_635.dc.subtract.1, layernorm_635.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.2, lc.input_tensor.layernorm_635.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_93: {type: fused_op, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_635.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 2}}
    _fused_op_94: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_635.dc.subtract.1, _fused_op_93],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    _fused_op_95: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_94, layer.11.output.LayerNorm.weight, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_95_output_nop_0: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_95], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  bwd_9:
    target_device: 0
    input_count: 64
    bw_in2_layernorm_635_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_635_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoders.output_layernorm_635], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_635_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e__fused_op_94_0, loss_bert_encoders.output_layernorm_635],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_635_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e__fused_op_93_0, lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_96: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [loss_bert_encoders.output_layernorm_635, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_96, lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_96, e2e__fused_op_94_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_97: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 2], inputs: [e2e__fused_op_94_0, bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.6, _fused_op_96, layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_632_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_632_brcst_reduce_sum_0.0, _fused_op_97], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_630_matmul_1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 12], inputs: [_fused_op_97, layer.11.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_630_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [3, 1], inputs: [e2e_gelu_627_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_630_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_630_transpose_0, _fused_op_97], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_98: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 6], inputs: [e2e_matmul_624_0, bw_in0_matmul_630_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_626_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_626_brcst_reduce_sum_0.0, _fused_op_98], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_624_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [4, 4], inputs: [_fused_op_98, layer.11.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_624_transpose_0: {type: nop, grid_loc: [2, 11], grid_size: [4, 1], inputs: [e2e__fused_op_92_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_624_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_624_transpose_0, _fused_op_98], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_621_combine_add_0: {type: add, grid_loc: [3, 9], grid_size: [1, 2], inputs: [_fused_op_97, bw_in0_matmul_624_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_621_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_621_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_621_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_621_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e__fused_op_91_0, bw_in0_layernorm_621_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_621_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [e2e__fused_op_90_0, lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_99: {type: fused_op, grid_loc: [4, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_621_combine_add_0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [_fused_op_99, lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_621_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 10], grid_size: [1, 1], inputs: [_fused_op_99, e2e__fused_op_91_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_621_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_100: {type: fused_op, grid_loc: [6, 3], grid_size: [1, 2], inputs: [e2e__fused_op_91_0, bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.6, _fused_op_99, layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_618_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_618_brcst_reduce_sum_0.0, _fused_op_100], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_616_matmul_1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 4], inputs: [_fused_op_100, layer.11.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_616_transpose_0: {type: nop, grid_loc: [6, 11], grid_size: [4, 1], inputs: [e2e_matmul_612_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_10:
    target_device: 0
    input_count: 64
    bw_in1_matmul_616_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_616_transpose_0_0, e2e__fused_op_100_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_612_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_616_matmul_1_0, e2e_matmul_605_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_612_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_89_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_612_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_612_transpose_0, e2e_bw_in0_matmul_616_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_607_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_607_brcst_reduce_sum_0.0, bw_in1_matmul_612_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_605_add_607_unsqueeze3_2282_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_612_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_605_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_605_add_607_unsqueeze3_2282_squeeze_0, layer.11.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_605_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_87_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_605_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_605_transpose_0, bw_in0_matmul_605_add_607_unsqueeze3_2282_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_601_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_612_matmul_1, e2e__fused_op_89_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_601_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_601_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_601_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_101: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_612_matmul_1, bw_in0_softmax_601_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_89_0, input_1_multiply_599_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_597_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_101, e2e_matmul_591_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_597_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_585_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_597_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_597_transpose_0, _fused_op_101],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_593_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_593_brcst_reduce_sum_0.0, bw_in1_matmul_597_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_591_add_593_unsqueeze3_2270_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_597_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_591_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_591_add_593_unsqueeze3_2270_squeeze_0, layer.11.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_591_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_87_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_591_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_591_transpose_0, bw_in0_matmul_591_add_593_unsqueeze3_2270_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_587_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_587_brcst_reduce_sum_0.0, bw_in0_matmul_597_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_585_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_597_matmul_1, layer.11.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_585_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_87_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_585_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_585_transpose_0, bw_in0_matmul_597_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_102: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_605_matmul_1, bw_in0_matmul_591_matmul_1, bw_in0_matmul_585_matmul_1, e2e__fused_op_100_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_582_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_582_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_102], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_582_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_86_0, _fused_op_102],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_582_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_85_0, lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_103: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_102, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_103, lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_582_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_103, e2e__fused_op_86_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_582_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_104: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_86_0, bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.6, _fused_op_103, layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_579_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_579_brcst_reduce_sum_0.0, _fused_op_104], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_577_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_104, layer.10.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_577_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_574_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_577_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_577_transpose_0, _fused_op_104], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_105: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_571_0, bw_in0_matmul_577_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_11:
    target_device: 0
    input_count: 64
    bw_in1_add_573_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_573_brcst_reduce_sum_0.0, e2e__fused_op_105_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_571_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_105_0, layer.10.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_571_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_84_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_571_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_571_transpose_0, e2e__fused_op_105_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_568_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_104_0, bw_in0_matmul_571_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_568_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_568_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_568_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_568_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_83_0, bw_in0_layernorm_568_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_568_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_82_0, lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_106: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_568_combine_add_0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_106, lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_568_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_106, e2e__fused_op_83_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_568_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_107: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_83_0, bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.6, _fused_op_106, layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_565_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_565_brcst_reduce_sum_0.0, _fused_op_107], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_563_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_107, layer.10.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_563_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_559_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_563_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_563_transpose_0, _fused_op_107], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_559_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_563_matmul_1, e2e_matmul_552_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_559_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e__fused_op_81_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_559_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_559_transpose_0, bw_in0_matmul_563_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_554_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_554_brcst_reduce_sum_0.0, bw_in1_matmul_559_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_552_add_554_unsqueeze3_2240_squeeze_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_559_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_552_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_552_add_554_unsqueeze3_2240_squeeze_0, layer.10.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_552_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [e2e__fused_op_79_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_552_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_552_transpose_0, bw_in0_matmul_552_add_554_unsqueeze3_2240_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_548_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_559_matmul_1, e2e__fused_op_81_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_548_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_548_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_548_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_108: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_559_matmul_1, bw_in0_softmax_548_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_81_0, input_1_multiply_546_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_544_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_108, e2e_matmul_538_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_544_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_matmul_532_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_544_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_544_transpose_0, _fused_op_108],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_540_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_540_brcst_reduce_sum_0.0, bw_in1_matmul_544_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_538_add_540_unsqueeze3_2228_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_544_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_538_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_538_add_540_unsqueeze3_2228_squeeze_0, layer.10.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_538_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [4, 1], inputs: [e2e__fused_op_79_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_538_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_538_transpose_0, bw_in0_matmul_538_add_540_unsqueeze3_2228_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_534_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_534_brcst_reduce_sum_0.0, bw_in0_matmul_544_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_532_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_544_matmul_1, layer.10.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_532_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [e2e__fused_op_79_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_12:
    target_device: 0
    input_count: 64
    bw_in1_matmul_532_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_532_transpose_0_0, e2e_bw_in0_matmul_544_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_109: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_552_matmul_1_0, e2e_bw_in0_matmul_538_matmul_1_0, e2e_bw_in0_matmul_532_matmul_1_0, e2e__fused_op_107_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_529_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_529_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_109], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_529_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_78_0, _fused_op_109],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_529_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_77_0, lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_110: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_109, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_110, lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_529_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_110, e2e__fused_op_78_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_529_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_111: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_78_0, bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.6, _fused_op_110, layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_526_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_526_brcst_reduce_sum_0.0, _fused_op_111], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_524_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [_fused_op_111, layer.9.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_524_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [3, 1], inputs: [e2e_gelu_521_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_524_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [3, 4], inputs: [bw_in1_matmul_524_transpose_0, _fused_op_111], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_112: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 6], inputs: [e2e_matmul_518_0, bw_in0_matmul_524_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_520_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_520_brcst_reduce_sum_0.0, _fused_op_112], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_518_matmul_1: {type: matmul, grid_loc: [5, 6], grid_size: [4, 4], inputs: [_fused_op_112, layer.9.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_518_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [4, 1], inputs: [e2e__fused_op_76_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_518_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_518_transpose_0, _fused_op_112], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_515_combine_add_0: {type: add, grid_loc: [2, 1], grid_size: [1, 2], inputs: [_fused_op_111, bw_in0_matmul_518_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_515_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_515_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_515_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_515_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e__fused_op_75_0, bw_in0_layernorm_515_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_515_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_74_0, lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_113: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_515_combine_add_0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_113, lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_515_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_113, e2e__fused_op_75_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_515_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_114: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 2], inputs: [e2e__fused_op_75_0, bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.6, _fused_op_113, layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_512_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_512_brcst_reduce_sum_0.0, _fused_op_114], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_510_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [_fused_op_114, layer.9.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_510_transpose_0: {type: nop, grid_loc: [5, 11], grid_size: [4, 1], inputs: [e2e_matmul_506_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_13:
    target_device: 0
    input_count: 64
    bw_in1_matmul_510_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_510_transpose_0_0, e2e__fused_op_114_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_506_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_510_matmul_1_0, e2e_matmul_499_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_506_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_73_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_506_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_506_transpose_0, e2e_bw_in0_matmul_510_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_501_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_501_brcst_reduce_sum_0.0, bw_in1_matmul_506_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_499_add_501_unsqueeze3_2198_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_506_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_499_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_499_add_501_unsqueeze3_2198_squeeze_0, layer.9.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_499_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_71_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_499_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_499_transpose_0, bw_in0_matmul_499_add_501_unsqueeze3_2198_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_495_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_506_matmul_1, e2e__fused_op_73_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_495_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_495_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_495_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_115: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_506_matmul_1, bw_in0_softmax_495_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_73_0, input_1_multiply_493_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_491_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_115, e2e_matmul_485_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_491_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_479_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_491_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_491_transpose_0, _fused_op_115],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_487_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_487_brcst_reduce_sum_0.0, bw_in1_matmul_491_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_485_add_487_unsqueeze3_2186_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_491_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_485_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_485_add_487_unsqueeze3_2186_squeeze_0, layer.9.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_485_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_71_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_485_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_485_transpose_0, bw_in0_matmul_485_add_487_unsqueeze3_2186_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_481_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_481_brcst_reduce_sum_0.0, bw_in0_matmul_491_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_479_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_491_matmul_1, layer.9.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_479_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_71_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_479_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_479_transpose_0, bw_in0_matmul_491_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_116: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_499_matmul_1, bw_in0_matmul_485_matmul_1, bw_in0_matmul_479_matmul_1, e2e__fused_op_114_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_476_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_476_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_116], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_476_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_70_0, _fused_op_116],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_476_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_69_0, lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_117: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_116, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_117, lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_476_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_117, e2e__fused_op_70_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_476_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_118: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_70_0, bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.6, _fused_op_117, layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_473_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_473_brcst_reduce_sum_0.0, _fused_op_118], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_471_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_118, layer.8.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_471_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_468_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_471_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_471_transpose_0, _fused_op_118], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_119: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_465_0, bw_in0_matmul_471_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_14:
    target_device: 0
    input_count: 64
    bw_in1_add_467_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_467_brcst_reduce_sum_0.0, e2e__fused_op_119_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_465_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_119_0, layer.8.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_465_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_68_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_465_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_465_transpose_0, e2e__fused_op_119_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_462_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_118_0, bw_in0_matmul_465_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_462_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_462_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_462_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_462_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_67_0, bw_in0_layernorm_462_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_462_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_66_0, lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_120: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_462_combine_add_0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_120, lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_462_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_120, e2e__fused_op_67_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_462_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_121: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_67_0, bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.6, _fused_op_120, layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_459_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_459_brcst_reduce_sum_0.0, _fused_op_121], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_457_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_121, layer.8.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_457_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_453_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_457_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_457_transpose_0, _fused_op_121], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_453_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_457_matmul_1, e2e_matmul_446_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_453_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e__fused_op_65_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_453_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_453_transpose_0, bw_in0_matmul_457_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_448_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_448_brcst_reduce_sum_0.0, bw_in1_matmul_453_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_446_add_448_unsqueeze3_2156_squeeze_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_453_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_446_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_446_add_448_unsqueeze3_2156_squeeze_0, layer.8.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_446_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [e2e__fused_op_63_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_446_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_446_transpose_0, bw_in0_matmul_446_add_448_unsqueeze3_2156_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_442_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_453_matmul_1, e2e__fused_op_65_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_442_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_442_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_442_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_122: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_453_matmul_1, bw_in0_softmax_442_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_65_0, input_1_multiply_440_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_438_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_122, e2e_matmul_432_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_438_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_matmul_426_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_438_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_438_transpose_0, _fused_op_122],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_434_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_434_brcst_reduce_sum_0.0, bw_in1_matmul_438_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_432_add_434_unsqueeze3_2144_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_438_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_432_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_432_add_434_unsqueeze3_2144_squeeze_0, layer.8.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_432_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [4, 1], inputs: [e2e__fused_op_63_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_432_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_432_transpose_0, bw_in0_matmul_432_add_434_unsqueeze3_2144_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_428_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_428_brcst_reduce_sum_0.0, bw_in0_matmul_438_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_426_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_438_matmul_1, layer.8.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_426_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [e2e__fused_op_63_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_15:
    target_device: 0
    input_count: 64
    bw_in1_matmul_426_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_426_transpose_0_0, e2e_bw_in0_matmul_438_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_123: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_446_matmul_1_0, e2e_bw_in0_matmul_432_matmul_1_0, e2e_bw_in0_matmul_426_matmul_1_0, e2e__fused_op_121_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_423_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_423_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_123], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_423_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_62_0, _fused_op_123],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_423_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_61_0, lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_124: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_123, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_124, lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_423_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_124, e2e__fused_op_62_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_423_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_125: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_62_0, bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.6, _fused_op_124, layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_420_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_420_brcst_reduce_sum_0.0, _fused_op_125], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_418_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [_fused_op_125, layer.7.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_418_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [3, 1], inputs: [e2e_gelu_415_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_418_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [3, 4], inputs: [bw_in1_matmul_418_transpose_0, _fused_op_125], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_126: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 6], inputs: [e2e_matmul_412_0, bw_in0_matmul_418_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_414_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_414_brcst_reduce_sum_0.0, _fused_op_126], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_412_matmul_1: {type: matmul, grid_loc: [5, 6], grid_size: [4, 4], inputs: [_fused_op_126, layer.7.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_412_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [4, 1], inputs: [e2e__fused_op_60_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_412_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_412_transpose_0, _fused_op_126], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_409_combine_add_0: {type: add, grid_loc: [2, 1], grid_size: [1, 2], inputs: [_fused_op_125, bw_in0_matmul_412_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_409_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_409_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_409_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_409_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e__fused_op_59_0, bw_in0_layernorm_409_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_409_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_58_0, lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_127: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_409_combine_add_0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_127, lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_409_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_127, e2e__fused_op_59_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_409_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_128: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 2], inputs: [e2e__fused_op_59_0, bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.6, _fused_op_127, layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_406_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_406_brcst_reduce_sum_0.0, _fused_op_128], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_404_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [_fused_op_128, layer.7.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_404_transpose_0: {type: nop, grid_loc: [5, 11], grid_size: [4, 1], inputs: [e2e_matmul_400_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_16:
    target_device: 0
    input_count: 64
    bw_in1_matmul_404_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_404_transpose_0_0, e2e__fused_op_128_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_400_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_404_matmul_1_0, e2e_matmul_393_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_400_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_57_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_400_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_400_transpose_0, e2e_bw_in0_matmul_404_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_395_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_395_brcst_reduce_sum_0.0, bw_in1_matmul_400_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_393_add_395_unsqueeze3_2114_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_400_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_393_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_393_add_395_unsqueeze3_2114_squeeze_0, layer.7.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_393_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_55_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_393_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_393_transpose_0, bw_in0_matmul_393_add_395_unsqueeze3_2114_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_389_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_400_matmul_1, e2e__fused_op_57_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_389_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_389_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_389_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_129: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_400_matmul_1, bw_in0_softmax_389_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_57_0, input_1_multiply_387_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_385_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_129, e2e_matmul_379_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_385_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_373_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_385_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_385_transpose_0, _fused_op_129],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_381_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_381_brcst_reduce_sum_0.0, bw_in1_matmul_385_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_379_add_381_unsqueeze3_2102_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_385_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_379_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_379_add_381_unsqueeze3_2102_squeeze_0, layer.7.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_379_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_55_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_379_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_379_transpose_0, bw_in0_matmul_379_add_381_unsqueeze3_2102_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_375_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_375_brcst_reduce_sum_0.0, bw_in0_matmul_385_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_373_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_385_matmul_1, layer.7.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_373_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_55_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_373_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_373_transpose_0, bw_in0_matmul_385_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_130: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_393_matmul_1, bw_in0_matmul_379_matmul_1, bw_in0_matmul_373_matmul_1, e2e__fused_op_128_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_370_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_370_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_130], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_370_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_54_0, _fused_op_130],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_370_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_53_0, lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_131: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_130, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_131, lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_370_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_131, e2e__fused_op_54_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_370_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_132: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_54_0, bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.6, _fused_op_131, layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_367_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_367_brcst_reduce_sum_0.0, _fused_op_132], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_365_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_132, layer.6.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_365_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_362_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_365_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_365_transpose_0, _fused_op_132], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_133: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_359_0, bw_in0_matmul_365_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_17:
    target_device: 0
    input_count: 64
    bw_in1_add_361_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_361_brcst_reduce_sum_0.0, e2e__fused_op_133_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_359_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_133_0, layer.6.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_359_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_52_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_359_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_359_transpose_0, e2e__fused_op_133_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_356_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_132_0, bw_in0_matmul_359_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_356_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_356_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_356_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_356_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_51_0, bw_in0_layernorm_356_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_356_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_50_0, lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_134: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_356_combine_add_0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_134, lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_356_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_134, e2e__fused_op_51_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_356_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_135: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_51_0, bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.6, _fused_op_134, layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_353_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_353_brcst_reduce_sum_0.0, _fused_op_135], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_351_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_135, layer.6.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_351_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_347_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_351_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_351_transpose_0, _fused_op_135], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_347_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_351_matmul_1, e2e_matmul_340_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_347_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e__fused_op_49_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_347_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_347_transpose_0, bw_in0_matmul_351_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_342_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_342_brcst_reduce_sum_0.0, bw_in1_matmul_347_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_340_add_342_unsqueeze3_2072_squeeze_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_347_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_340_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_340_add_342_unsqueeze3_2072_squeeze_0, layer.6.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_340_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_340_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_340_transpose_0, bw_in0_matmul_340_add_342_unsqueeze3_2072_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_336_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_347_matmul_1, e2e__fused_op_49_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_336_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_336_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_336_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_136: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_347_matmul_1, bw_in0_softmax_336_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_49_0, input_1_multiply_334_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_332_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_136, e2e_matmul_326_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_332_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_matmul_320_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_332_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_332_transpose_0, _fused_op_136],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_328_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_328_brcst_reduce_sum_0.0, bw_in1_matmul_332_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_326_add_328_unsqueeze3_2060_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_332_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_326_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_326_add_328_unsqueeze3_2060_squeeze_0, layer.6.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_326_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [4, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_326_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_326_transpose_0, bw_in0_matmul_326_add_328_unsqueeze3_2060_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_322_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_322_brcst_reduce_sum_0.0, bw_in0_matmul_332_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_320_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_332_matmul_1, layer.6.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_320_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [e2e__fused_op_47_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_18:
    target_device: 0
    input_count: 64
    bw_in1_matmul_320_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_320_transpose_0_0, e2e_bw_in0_matmul_332_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_137: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_340_matmul_1_0, e2e_bw_in0_matmul_326_matmul_1_0, e2e_bw_in0_matmul_320_matmul_1_0, e2e__fused_op_135_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_137], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_317_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_46_0, _fused_op_137],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_317_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_45_0, lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_138: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_137, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_138, lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_138, e2e__fused_op_46_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_317_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_139: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_46_0, bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6, _fused_op_138, layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_314_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0, _fused_op_139], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_312_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [_fused_op_139, layer.5.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_312_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [3, 1], inputs: [e2e_gelu_309_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_312_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [3, 4], inputs: [bw_in1_matmul_312_transpose_0, _fused_op_139], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_140: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 6], inputs: [e2e_matmul_306_0, bw_in0_matmul_312_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_308_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_308_brcst_reduce_sum_0.0, _fused_op_140], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_306_matmul_1: {type: matmul, grid_loc: [5, 6], grid_size: [4, 4], inputs: [_fused_op_140, layer.5.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_306_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [4, 1], inputs: [e2e__fused_op_44_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_306_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_306_transpose_0, _fused_op_140], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_303_combine_add_0: {type: add, grid_loc: [2, 1], grid_size: [1, 2], inputs: [_fused_op_139, bw_in0_matmul_306_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_303_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_303_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e__fused_op_43_0, bw_in0_layernorm_303_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_303_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_42_0, lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_141: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_303_combine_add_0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_141, lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_303_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_141, e2e__fused_op_43_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_303_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_142: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 2], inputs: [e2e__fused_op_43_0, bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6, _fused_op_141, layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_300_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0, _fused_op_142], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_298_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [_fused_op_142, layer.5.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_298_transpose_0: {type: nop, grid_loc: [5, 11], grid_size: [4, 1], inputs: [e2e_matmul_294_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_19:
    target_device: 0
    input_count: 64
    bw_in1_matmul_298_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_298_transpose_0_0, e2e__fused_op_142_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_294_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_298_matmul_1_0, e2e_matmul_287_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_294_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_41_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_294_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_294_transpose_0, e2e_bw_in0_matmul_298_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_289_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0, bw_in1_matmul_294_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_287_add_289_unsqueeze3_2030_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_294_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_287_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_287_add_289_unsqueeze3_2030_squeeze_0, layer.5.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_287_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_39_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_287_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_287_transpose_0, bw_in0_matmul_287_add_289_unsqueeze3_2030_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_283_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_294_matmul_1, e2e__fused_op_41_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_283_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_143: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_294_matmul_1, bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_41_0, input_1_multiply_281_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_279_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_143, e2e_matmul_273_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_279_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_267_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_279_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_279_transpose_0, _fused_op_143],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_275_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0, bw_in1_matmul_279_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_273_add_275_unsqueeze3_2018_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_279_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_273_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_273_add_275_unsqueeze3_2018_squeeze_0, layer.5.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_273_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_39_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_273_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_273_transpose_0, bw_in0_matmul_273_add_275_unsqueeze3_2018_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_269_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0, bw_in0_matmul_279_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_267_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_279_matmul_1, layer.5.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_267_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_39_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_267_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_267_transpose_0, bw_in0_matmul_279_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_144: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_287_matmul_1, bw_in0_matmul_273_matmul_1, bw_in0_matmul_267_matmul_1, e2e__fused_op_142_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_144], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_264_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_38_0, _fused_op_144],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_264_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_37_0, lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_145: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_144, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_145, lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_264_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_145, e2e__fused_op_38_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_264_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_146: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_38_0, bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6, _fused_op_145, layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_261_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0, _fused_op_146], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_259_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_146, layer.4.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_259_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_256_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_259_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_259_transpose_0, _fused_op_146], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_147: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_253_0, bw_in0_matmul_259_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_20:
    target_device: 0
    input_count: 64
    bw_in1_add_255_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_255_brcst_reduce_sum_0.0, e2e__fused_op_147_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_253_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_147_0, layer.4.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_253_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_36_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_253_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_253_transpose_0, e2e__fused_op_147_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_250_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_146_0, bw_in0_matmul_253_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_250_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_250_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_35_0, bw_in0_layernorm_250_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_250_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_34_0, lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_148: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_250_combine_add_0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_148, lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_250_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_148, e2e__fused_op_35_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_250_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_149: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_35_0, bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6, _fused_op_148, layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_247_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0, _fused_op_149], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_245_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_149, layer.4.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_245_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_241_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_245_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_245_transpose_0, _fused_op_149], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_241_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_245_matmul_1, e2e_matmul_234_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_241_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e__fused_op_33_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_241_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_241_transpose_0, bw_in0_matmul_245_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_236_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0, bw_in1_matmul_241_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_234_add_236_unsqueeze3_1988_squeeze_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_241_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_234_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_234_add_236_unsqueeze3_1988_squeeze_0, layer.4.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_234_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [e2e__fused_op_31_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_234_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_234_transpose_0, bw_in0_matmul_234_add_236_unsqueeze3_1988_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_230_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_241_matmul_1, e2e__fused_op_33_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_230_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_150: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_241_matmul_1, bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_33_0, input_1_multiply_228_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_226_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_150, e2e_matmul_220_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_226_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_matmul_214_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_226_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_226_transpose_0, _fused_op_150],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_222_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0, bw_in1_matmul_226_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_220_add_222_unsqueeze3_1976_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_226_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_220_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_220_add_222_unsqueeze3_1976_squeeze_0, layer.4.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_220_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [4, 1], inputs: [e2e__fused_op_31_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_220_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_220_transpose_0, bw_in0_matmul_220_add_222_unsqueeze3_1976_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_216_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0, bw_in0_matmul_226_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_214_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_226_matmul_1, layer.4.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_214_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [e2e__fused_op_31_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_21:
    target_device: 0
    input_count: 64
    bw_in1_matmul_214_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_214_transpose_0_0, e2e_bw_in0_matmul_226_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_151: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_234_matmul_1_0, e2e_bw_in0_matmul_220_matmul_1_0, e2e_bw_in0_matmul_214_matmul_1_0, e2e__fused_op_149_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_151], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_30_0, _fused_op_151],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_211_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_29_0, lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_152: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_151, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_152, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_152, e2e__fused_op_30_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_211_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_153: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_30_0, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6, _fused_op_152, layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_208_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0, _fused_op_153], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_206_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [_fused_op_153, layer.3.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_206_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [3, 1], inputs: [e2e_gelu_203_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_206_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [3, 4], inputs: [bw_in1_matmul_206_transpose_0, _fused_op_153], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_154: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 6], inputs: [e2e_matmul_200_0, bw_in0_matmul_206_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_202_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0, _fused_op_154], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_200_matmul_1: {type: matmul, grid_loc: [5, 6], grid_size: [4, 4], inputs: [_fused_op_154, layer.3.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_200_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [4, 1], inputs: [e2e__fused_op_28_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_200_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_200_transpose_0, _fused_op_154], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_197_combine_add_0: {type: add, grid_loc: [2, 1], grid_size: [1, 2], inputs: [_fused_op_153, bw_in0_matmul_200_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_197_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e__fused_op_27_0, bw_in0_layernorm_197_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_197_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_26_0, lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_155: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_combine_add_0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_155, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_155, e2e__fused_op_27_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_197_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_156: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 2], inputs: [e2e__fused_op_27_0, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6, _fused_op_155, layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_194_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0, _fused_op_156], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_192_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [_fused_op_156, layer.3.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_192_transpose_0: {type: nop, grid_loc: [5, 11], grid_size: [4, 1], inputs: [e2e_matmul_188_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_22:
    target_device: 0
    input_count: 64
    bw_in1_matmul_192_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_192_transpose_0_0, e2e__fused_op_156_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_188_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_192_matmul_1_0, e2e_matmul_181_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_188_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_25_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_188_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_188_transpose_0, e2e_bw_in0_matmul_192_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_183_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0, bw_in1_matmul_188_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_181_add_183_unsqueeze3_1946_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_188_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_181_add_183_unsqueeze3_1946_squeeze_0, layer.3.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_181_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_181_transpose_0, bw_in0_matmul_181_add_183_unsqueeze3_1946_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_177_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_188_matmul_1, e2e__fused_op_25_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_177_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_157: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_188_matmul_1, bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_25_0, input_1_multiply_175_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_173_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_157, e2e_matmul_167_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_173_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_161_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_173_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_173_transpose_0, _fused_op_157],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_169_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0, bw_in1_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_167_add_169_unsqueeze3_1934_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_173_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_167_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_167_add_169_unsqueeze3_1934_squeeze_0, layer.3.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_167_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_167_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_167_transpose_0, bw_in0_matmul_167_add_169_unsqueeze3_1934_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_163_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0, bw_in0_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_161_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_173_matmul_1, layer.3.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_161_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_23_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_161_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_161_transpose_0, bw_in0_matmul_173_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_158: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_181_matmul_1, bw_in0_matmul_167_matmul_1, bw_in0_matmul_161_matmul_1, e2e__fused_op_156_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_158], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_22_0, _fused_op_158],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_158_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_21_0, lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_159: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_158, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_159, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_159, e2e__fused_op_22_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_158_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_160: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_22_0, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6, _fused_op_159, layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_155_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0, _fused_op_160], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_153_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_160, layer.2.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_153_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_150_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_153_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_153_transpose_0, _fused_op_160], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_161: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_147_0, bw_in0_matmul_153_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_23:
    target_device: 0
    input_count: 64
    bw_in1_add_149_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0, e2e__fused_op_161_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_161_0, layer.2.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_20_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_147_transpose_0, e2e__fused_op_161_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_144_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_160_0, bw_in0_matmul_147_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_144_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_19_0, bw_in0_layernorm_144_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_144_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_18_0, lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_162: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_combine_add_0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_162, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_162, e2e__fused_op_19_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_144_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_163: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_19_0, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6, _fused_op_162, layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_141_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0, _fused_op_163], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_139_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_163, layer.2.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_139_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_135_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_139_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_139_transpose_0, _fused_op_163], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_135_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_139_matmul_1, e2e_matmul_128_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_135_transpose_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [e2e__fused_op_17_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_135_matmul_1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_135_transpose_0, bw_in0_matmul_139_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_130_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0, bw_in1_matmul_135_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_128_add_130_unsqueeze3_1904_squeeze_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_135_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_128_add_130_unsqueeze3_1904_squeeze_0, layer.2.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [4, 8], grid_size: [4, 1], inputs: [e2e__fused_op_15_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [4, 1], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_matmul_128_add_130_unsqueeze3_1904_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_124_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, e2e__fused_op_17_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_124_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_164: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_135_matmul_1, bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_17_0, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_120_matmul_1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_164, e2e_matmul_114_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_120_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [1, 1], inputs: [e2e_matmul_108_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_120_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_120_transpose_0, _fused_op_164],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_116_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0, bw_in1_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_114_add_116_unsqueeze3_1892_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_120_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_114_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_114_add_116_unsqueeze3_1892_squeeze_0, layer.2.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_114_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [4, 1], inputs: [e2e__fused_op_15_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_114_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [4, 1], inputs: [bw_in1_matmul_114_transpose_0, bw_in0_matmul_114_add_116_unsqueeze3_1892_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_110_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0, bw_in0_matmul_120_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_108_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_120_matmul_1, layer.2.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_108_transpose_0: {type: nop, grid_loc: [6, 7], grid_size: [4, 1], inputs: [e2e__fused_op_15_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_24:
    target_device: 0
    input_count: 64
    bw_in1_matmul_108_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_108_transpose_0_0, e2e_bw_in0_matmul_120_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_165: {type: fused_op, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in0_matmul_114_matmul_1_0, e2e_bw_in0_matmul_108_matmul_1_0, e2e__fused_op_163_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_165], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [e2e__fused_op_14_0, _fused_op_165],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [e2e__fused_op_13_0, lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_166: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_165, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [_fused_op_166, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_166, e2e__fused_op_14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_167: {type: fused_op, grid_loc: [0, 10], grid_size: [1, 2], inputs: [e2e__fused_op_14_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, _fused_op_166, layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, _fused_op_167], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 12], inputs: [_fused_op_167, layer.1.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [3, 1], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [3, 4], inputs: [bw_in1_matmul_100_transpose_0, _fused_op_167], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_168: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 6], inputs: [e2e_matmul_94_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}
    bw_in1_add_96_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0, _fused_op_168], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [5, 6], grid_size: [4, 4], inputs: [_fused_op_168, layer.1.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [5, 10], grid_size: [4, 1], inputs: [e2e__fused_op_12_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [4, 3], inputs: [bw_in1_matmul_94_transpose_0, _fused_op_168], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [2, 1], grid_size: [1, 2], inputs: [_fused_op_167, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [e2e__fused_op_11_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_169: {type: fused_op, grid_loc: [1, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [_fused_op_169, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [_fused_op_169, e2e__fused_op_11_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_170: {type: fused_op, grid_loc: [3, 1], grid_size: [1, 2], inputs: [e2e__fused_op_11_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, _fused_op_169, layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, _fused_op_170], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 4], inputs: [_fused_op_170, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [5, 11], grid_size: [4, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}

  bwd_25:
    target_device: 0
    input_count: 64
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [e2e_bw_in1_matmul_86_transpose_0_0, e2e__fused_op_170_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_bw_in0_matmul_86_matmul_1_0, e2e_matmul_75_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_9_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, e2e_bw_in0_matmul_86_matmul_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_75_add_77_unsqueeze3_1862_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_75_add_77_unsqueeze3_1862_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [0, 9], grid_size: [4, 1], inputs: [e2e__fused_op_7_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [0, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_75_transpose_0, bw_in0_matmul_75_add_77_unsqueeze3_1862_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e__fused_op_9_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_171: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_9_0, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_171, e2e_matmul_61_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, _fused_op_171],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_61_add_63_unsqueeze3_1850_squeeze_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_61_add_63_unsqueeze3_1850_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [1, 8], grid_size: [4, 1], inputs: [e2e__fused_op_7_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_matmul_61_add_63_unsqueeze3_1850_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [2, 6], grid_size: [4, 1], inputs: [e2e__fused_op_7_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [2, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_172: {type: fused_op, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1, bw_in0_matmul_55_matmul_1, e2e__fused_op_170_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, _fused_op_172], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [e2e__fused_op_6_0, _fused_op_172],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [e2e__fused_op_5_0, lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_173: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_172, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [_fused_op_173, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_173, e2e__fused_op_6_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_174: {type: fused_op, grid_loc: [5, 0], grid_size: [1, 2], inputs: [e2e__fused_op_6_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, _fused_op_173, layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, _fused_op_174], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 12], inputs: [_fused_op_174, layer.0.output.dense.weight],
         t: 1, mblock: [2, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 24, u_kt: 1}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [7, 0], grid_size: [3, 1], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [7, 1], grid_size: [3, 4], inputs: [bw_in1_matmul_47_transpose_0, _fused_op_174], gradient_op: true,
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    _fused_op_175: {type: fused_op, grid_loc: [7, 5], grid_size: [1, 6], inputs: [e2e_matmul_41_0, bw_in0_matmul_47_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 98}}

  bwd_26:
    target_device: 0
    input_count: 64
    bw_in1_add_43_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0, e2e__fused_op_175_0], gradient_op: true,
         t: 1, mblock: [1, 24], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e__fused_op_175_0, layer.0.intermediate.dense.weight],
         t: 1, mblock: [1, 3], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 12, u_kt: 8}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 1], inputs: [e2e__fused_op_4_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [4, 3], inputs: [bw_in1_matmul_41_transpose_0, e2e__fused_op_175_0], gradient_op: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 1}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [e2e__fused_op_174_0, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [e2e__fused_op_3_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [e2e__fused_op_2_0, lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    _fused_op_176: {type: fused_op, grid_loc: [0, 11], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {approximate_mode: true, fused_op_id: 96}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [_fused_op_176, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_176, e2e__fused_op_3_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    _fused_op_177: {type: fused_op, grid_loc: [2, 9], grid_size: [1, 2], inputs: [e2e__fused_op_3_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, _fused_op_176, layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 0, 0, 96, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_5_tms: [broadcast: {c: 24}], input_2_tms: [broadcast: {c: 24}], input_1_tms: [broadcast: {c: 24}],
         attributes: {approximate_mode: true, fused_op_id: 97}}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, _fused_op_177], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [_fused_op_177, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [3, 9], grid_size: [4, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 12]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [3, 11], grid_size: [4, 1], inputs: [bw_in1_matmul_33_transpose_0, _fused_op_177], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [e2e__fused_op_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_22_add_24_unsqueeze3_1820_squeeze_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_29_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 12]}
    bw_in0_matmul_22_matmul_1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 4], inputs: [bw_in0_matmul_22_add_24_unsqueeze3_1820_squeeze_0, layer.0.attention.self.value.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [5, 3], grid_size: [4, 1], inputs: [hidden_states],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [5, 10], grid_size: [4, 1], inputs: [bw_in1_matmul_22_transpose_0, bw_in0_matmul_22_add_24_unsqueeze3_1820_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e__fused_op_1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    _fused_op_178: {type: fused_op, grid_loc: [4, 10], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1, e2e__fused_op_1_0, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [32, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}], input_1_tms: [broadcast: {c: 4}],
         attributes: {approximate_mode: true, fused_op_id: 101}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [_fused_op_178, e2e_matmul_8_0],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 12, transpose]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, _fused_op_178],
         t: 12, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_8_add_10_unsqueeze3_1808_squeeze_0: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_14_matmul_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 12]}
    bw_in0_matmul_8_matmul_1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 4], inputs: [bw_in0_matmul_8_add_10_unsqueeze3_1808_squeeze_0, layer.0.attention.self.key.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 6, u_kt: 4}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [6, 1], grid_size: [4, 1], inputs: [hidden_states],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [6, 8], grid_size: [4, 1], inputs: [bw_in1_matmul_8_transpose_0, bw_in0_matmul_8_add_10_unsqueeze3_1808_squeeze_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}

  bwd_27:
    target_device: 0
    input_count: 64
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_matmul_2_matmul_1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_14_matmul_1_0, layer.0.attention.self.query.weight],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 12],
         attributes: {m_k: 12, u_kt: 2}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [4, 1], inputs: [hidden_states],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [4, 1], inputs: [bw_in1_matmul_2_transpose_0, e2e_bw_in0_matmul_14_matmul_1_0], gradient_op: true,
         t: 1, mblock: [3, 6], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 12],
         attributes: {m_k: 2, u_kt: 2}}
    _fused_op_179: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 4], inputs: [e2e_bw_in0_matmul_22_matmul_1_0, e2e_bw_in0_matmul_8_matmul_1_0, bw_in0_matmul_2_matmul_1, e2e__fused_op_177_0],
         t: 1, mblock: [2, 3], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 48, 120], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 102}}
    _fused_op_179_output_nop_0: {type: nop, grid_loc: [0, 11], grid_size: [1, 1], inputs: [_fused_op_179], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 64, $c_zero: 0, $gptr_q7: 0, $c_one: 1, $lptr_q7: 0, $gptr_q14: 0, $lptr_q14: 0}
    - staticvar: {$gptr_q0: 0, $gptr_q1_shadow: 0, $lptr_q1: 0, $gptr_q2_shadow: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q4: 0, $gptr_q5_shadow: 0, $lptr_q5: 0, $lptr_q6: 0, $gptr_q13: 0, $gptr_q16_shadow: 0, $lptr_q13: 0, $gptr_q16: 0, $lptr_q8: 0, $gptr_q0_shadow: 0, $lptr_q16: 0, $gptr_q8: 0, $lptr_q17: 0, $gptr_q18: 0, $gptr_q6_shadow: 0, $lptr_q18: 0, $gptr_q13_shadow: 0, $gptr_q15: 0, $gptr_q18_shadow: 0, $lptr_q12: 0, $gptr_q8_shadow: 0, $gptr_q12: 0, $lptr_q0: 0, $lptr_q3: 0, $lptr_q10: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q17: 0, $gptr_q12_shadow: 0, $gptr_q15_shadow: 0, $lptr_q15: 0, $lptr_q11: 0, $gptr_q3_shadow: 0, $gptr_q11: 0, $gptr_q1: 0, $lptr_q9: 0, $gptr_q11_shadow: 0, $gptr_q4_shadow: 0, $gptr_q5: 0, $gptr_q10: 0, $gptr_q10_shadow: 0, $gptr_q9: 0, $gptr_q2: 0, $gptr_q9_shadow: 0}
    - varinst: [$gptr_q18, set, $gptr_q18_shadow]
    - varinst: [$gptr_q16, set, $gptr_q16_shadow]
    - varinst: [$gptr_q15, set, $gptr_q15_shadow]
    - varinst: [$gptr_q13, set, $gptr_q13_shadow]
    - varinst: [$gptr_q12, set, $gptr_q12_shadow]
    - varinst: [$gptr_q11, set, $gptr_q11_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q9, set, $gptr_q9_shadow]
    - varinst: [$gptr_q8, set, $gptr_q8_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q5, set, $gptr_q5_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone4377_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone4406_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_fork_clone4423_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_matmul_245_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_20_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_fork_clone4440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_fork_clone4457_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_245_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.layernorm_250.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_fork_clone4474_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_245_0]
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_fork_clone4491_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q9_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e_matmul_457_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_52_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_gelu_362_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_fork_clone4508_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_fork_clone4525_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q11_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_63_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_matmul_457_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               lc.input_tensor.layernorm_462.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_fork_clone4542_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_457_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_fork_clone4559_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q15_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_84_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_gelu_574_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_fork_clone4576_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q18_shadow, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 128]
    - endloop

  - run_bwd:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_one: 1, $c_zero: 0, $lptr_q25: 0, $gptr_q23: 0, $lptr_q23: 0, $gptr_q19: 0, $gptr_q31: 0, $lptr_q27: 0, $lptr_q29: 0, $lptr_q38: 0, $lptr_q31: 0, $lptr_q36: 0, $gptr_q5: 0, $lptr_q3: 0, $gptr_q36: 0, $gptr_q27: 0, $gptr_q38: 0, $lptr_q21: 0, $gptr_q3: 0, $gptr_q17: 0, $gptr_q21: 0, $lptr_q5: 0, $gptr_q25: 0, $lptr_q9: 0, $gptr_q11: 0, $gptr_q9: 0, $lptr_q13: 0, $gptr_q29: 0, $gptr_q33: 0, $gptr_q13: 0, $lptr_q11: 0, $lptr_q15: 0, $c_microbatch_size: 64, $gptr_q15: 0, $lptr_q33: 0, $lptr_q7: 0, $gptr_q7: 0, $lptr_q17: 0, $lptr_q19: 0}
    - staticvar: {$lptr_q24: 0, $gptr_q24: 0, $gptr_q22: 0, $gptr_q20: 0, $lptr_q18: 0, $gptr_q16: 0, $gptr_q12: 0, $lptr_q12: 0, $gptr_q28: 0, $lptr_q28: 0, $lptr_q30: 0, $lptr_q32: 0, $gptr_q30: 0, $lptr_q22: 0, $gptr_q34: 0, $lptr_q4: 0, $gptr_q32: 0, $gptr_q35: 0, $gptr_q26: 0, $lptr_q1: 0, $lptr_q10: 0, $lptr_q0: 0, $lptr_q34: 0, $gptr_q18: 0, $lptr_q14: 0, $gptr_q37: 0, $lptr_q6: 0, $lptr_q37: 0, $lptr_q16: 0, $gptr_q34_shadow: 0, $lptr_q26: 0, $gptr_q1: 0, $lptr_q20: 0, $lptr_q8: 0, $gptr_q0: 0, $lptr_q2: 0, $gptr_q2: 0, $lptr_q35: 0, $gptr_q6: 0, $gptr_q4: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q10: 0}
    - varinst: [$gptr_q34, set, $gptr_q34_shadow]
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_100_0, e2e_bw_in0_matmul_616_matmul_1_0, e2e_bw_in1_matmul_616_transpose_0_0]
    -   execute: {graph_name: bwd_9, queue_settings: {
               loss_bert_encoders.output_layernorm_635: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_matmul_612_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_90_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_91_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_92_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_624_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_gelu_627_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_93_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e__fused_op_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_635_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_635_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_632_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_626_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_621_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_621_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_618_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.11.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_104_0, e2e__fused_op_105_0]
    -   execute: {graph_name: bwd_10, queue_settings: {
               e2e_matmul_571_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_gelu_574_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_85_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_86_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_87_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_585_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_591_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_605_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_89_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_100_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_bw_in0_matmul_616_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_bw_in1_matmul_616_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_607_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_601_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_593_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_587_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_582_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_582_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_579_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.11.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.11.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_100_0, e2e_bw_in0_matmul_616_matmul_1_0, e2e_bw_in1_matmul_616_transpose_0_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_107_0, e2e_bw_in0_matmul_552_matmul_1_0, e2e_bw_in0_matmul_544_matmul_1_0, e2e_bw_in0_matmul_538_matmul_1_0, e2e_bw_in0_matmul_532_matmul_1_0, e2e_bw_in1_matmul_532_transpose_0_0]
    -   execute: {graph_name: bwd_11, queue_settings: {
               e2e__fused_op_79_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_532_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_538_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_552_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_81_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_559_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_83_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_84_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_104_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e__fused_op_105_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_573_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_568_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_568_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_565_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_554_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_548_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_540_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_534_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.10.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_104_0, e2e__fused_op_105_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_114_0, e2e_bw_in0_matmul_510_matmul_1_0, e2e_bw_in1_matmul_510_transpose_0_0]
    -   execute: {graph_name: bwd_12, queue_settings: {
               e2e_matmul_506_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_74_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_matmul_518_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_gelu_521_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_77_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_78_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_107_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_matmul_552_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_matmul_544_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_matmul_538_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_matmul_532_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in1_matmul_532_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_529_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_529_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_526_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_520_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_515_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_515_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_512_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.10.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_107_0, e2e_bw_in0_matmul_552_matmul_1_0, e2e_bw_in0_matmul_544_matmul_1_0, e2e_bw_in0_matmul_538_matmul_1_0, e2e_bw_in0_matmul_532_matmul_1_0, e2e_bw_in1_matmul_532_transpose_0_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_118_0, e2e__fused_op_119_0]
    -   execute: {graph_name: bwd_13, queue_settings: {
               e2e_matmul_465_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_gelu_468_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_69_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_70_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_71_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_479_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_485_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_499_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_73_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_matmul_510_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in1_matmul_510_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_501_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_495_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_487_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_481_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_476_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_476_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_473_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.9.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.9.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_114_0, e2e_bw_in0_matmul_510_matmul_1_0, e2e_bw_in1_matmul_510_transpose_0_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_121_0, e2e_bw_in0_matmul_446_matmul_1_0, e2e_bw_in0_matmul_438_matmul_1_0, e2e_bw_in0_matmul_432_matmul_1_0, e2e_bw_in0_matmul_426_matmul_1_0, e2e_bw_in1_matmul_426_transpose_0_0]
    -   execute: {graph_name: bwd_14, queue_settings: {
               e2e__fused_op_63_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_426_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_432_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_446_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_65_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_matmul_453_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_67_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_68_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_118_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e__fused_op_119_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_467_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_462_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_462_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_459_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_448_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_442_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_434_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_428_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.8.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_118_0, e2e__fused_op_119_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_128_0, e2e_bw_in0_matmul_404_matmul_1_0, e2e_bw_in1_matmul_404_transpose_0_0]
    -   execute: {graph_name: bwd_15, queue_settings: {
               e2e_matmul_400_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_59_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_60_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_412_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_gelu_415_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_121_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_matmul_446_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_matmul_438_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_matmul_432_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_matmul_426_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in1_matmul_426_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_423_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_423_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_420_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_414_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_409_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_409_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_406_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.8.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_121_0, e2e_bw_in0_matmul_446_matmul_1_0, e2e_bw_in0_matmul_438_matmul_1_0, e2e_bw_in0_matmul_432_matmul_1_0, e2e_bw_in0_matmul_426_matmul_1_0, e2e_bw_in1_matmul_426_transpose_0_0]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_132_0, e2e__fused_op_133_0]
    -   execute: {graph_name: bwd_16, queue_settings: {
               e2e_matmul_359_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_gelu_362_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_53_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_54_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_matmul_373_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_matmul_379_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_matmul_393_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_57_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_bw_in0_matmul_404_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_bw_in1_matmul_404_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_395_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_389_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_381_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_375_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_370_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_370_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_367_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.7.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.7.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_128_0, e2e_bw_in0_matmul_404_matmul_1_0, e2e_bw_in1_matmul_404_transpose_0_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_135_0, e2e_bw_in0_matmul_340_matmul_1_0, e2e_bw_in0_matmul_332_matmul_1_0, e2e_bw_in0_matmul_326_matmul_1_0, e2e_bw_in0_matmul_320_matmul_1_0, e2e_bw_in1_matmul_320_transpose_0_0]
    -   execute: {graph_name: bwd_17, queue_settings: {
               e2e__fused_op_47_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_320_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_326_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_340_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_49_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_347_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_51_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_52_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_132_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_133_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_361_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_356_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_356_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_353_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_342_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_336_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_328_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_322_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.6.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_132_0, e2e__fused_op_133_0]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_142_0, e2e_bw_in0_matmul_298_matmul_1_0, e2e_bw_in1_matmul_298_transpose_0_0]
    -   execute: {graph_name: bwd_18, queue_settings: {
               e2e_matmul_294_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_42_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_43_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_matmul_306_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_gelu_309_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_45_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in0_matmul_340_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in0_matmul_332_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in0_matmul_326_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in0_matmul_320_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in1_matmul_320_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_317_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_317_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_314_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_308_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_303_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_303_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_300_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.6.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_135_0, e2e_bw_in0_matmul_340_matmul_1_0, e2e_bw_in0_matmul_332_matmul_1_0, e2e_bw_in0_matmul_326_matmul_1_0, e2e_bw_in0_matmul_320_matmul_1_0, e2e_bw_in1_matmul_320_transpose_0_0]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_146_0, e2e__fused_op_147_0]
    -   execute: {graph_name: bwd_19, queue_settings: {
               e2e_matmul_253_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_gelu_256_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_37_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_267_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_273_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_287_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_142_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_bw_in0_matmul_298_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_bw_in1_matmul_298_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_289_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_283_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_275_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_269_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_264_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_264_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_261_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.5.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.5.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_142_0, e2e_bw_in0_matmul_298_matmul_1_0, e2e_bw_in1_matmul_298_transpose_0_0]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_149_0, e2e_bw_in0_matmul_234_matmul_1_0, e2e_bw_in0_matmul_226_matmul_1_0, e2e_bw_in0_matmul_220_matmul_1_0, e2e_bw_in0_matmul_214_matmul_1_0, e2e_bw_in1_matmul_214_transpose_0_0]
    -   execute: {graph_name: bwd_20, queue_settings: {
               e2e__fused_op_31_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_214_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_220_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_234_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_241_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_36_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_146_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e__fused_op_147_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_255_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_250_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_250_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_247_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_236_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_230_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_222_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.4.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_146_0, e2e__fused_op_147_0]
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_156_0, e2e_bw_in0_matmul_192_matmul_1_0, e2e_bw_in1_matmul_192_transpose_0_0]
    -   execute: {graph_name: bwd_21, queue_settings: {
               e2e_matmul_188_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_27_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_matmul_200_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_gelu_203_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_149_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_bw_in0_matmul_234_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_bw_in0_matmul_226_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_bw_in0_matmul_220_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_bw_in0_matmul_214_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_bw_in1_matmul_214_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_211_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_211_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_208_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_197_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_197_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_194_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.4.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_149_0, e2e_bw_in0_matmul_234_matmul_1_0, e2e_bw_in0_matmul_226_matmul_1_0, e2e_bw_in0_matmul_220_matmul_1_0, e2e_bw_in0_matmul_214_matmul_1_0, e2e_bw_in1_matmul_214_transpose_0_0]
    -   varinst: [$gptr_q24, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_160_0, e2e__fused_op_161_0]
    -   execute: {graph_name: bwd_22, queue_settings: {
               e2e_matmul_147_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_gelu_150_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_matmul_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_matmul_167_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_matmul_181_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_25_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_156_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_bw_in0_matmul_192_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_bw_in1_matmul_192_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_183_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_177_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_169_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_158_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_158_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_155_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.3.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.3.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_156_0, e2e_bw_in0_matmul_192_matmul_1_0, e2e_bw_in1_matmul_192_transpose_0_0]
    -   varinst: [$gptr_q26, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_163_0, e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in0_matmul_120_matmul_1_0, e2e_bw_in0_matmul_114_matmul_1_0, e2e_bw_in0_matmul_108_matmul_1_0, e2e_bw_in1_matmul_108_transpose_0_0]
    -   execute: {graph_name: bwd_23, queue_settings: {
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_matmul_108_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_matmul_114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_matmul_128_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_matmul_135_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_20_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_160_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e__fused_op_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_144_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_144_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_141_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_130_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_124_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_116_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_110_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_160_0, e2e__fused_op_161_0]
    -   varinst: [$gptr_q28, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_170_0, e2e_bw_in0_matmul_86_matmul_1_0, e2e_bw_in1_matmul_86_transpose_0_0]
    -   execute: {graph_name: bwd_24, queue_settings: {
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_13_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_163_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_bw_in0_matmul_128_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_bw_in0_matmul_120_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_bw_in0_matmul_114_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_bw_in0_matmul_108_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_bw_in1_matmul_108_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.2.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_163_0, e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in0_matmul_120_matmul_1_0, e2e_bw_in0_matmul_114_matmul_1_0, e2e_bw_in0_matmul_108_matmul_1_0, e2e_bw_in1_matmul_108_transpose_0_0]
    -   varinst: [$gptr_q30, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_174_0, e2e__fused_op_175_0]
    -   execute: {graph_name: bwd_25, queue_settings: {
               e2e_matmul_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_170_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_bw_in0_matmul_86_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_bw_in1_matmul_86_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_170_0, e2e_bw_in0_matmul_86_matmul_1_0, e2e_bw_in1_matmul_86_transpose_0_0]
    -   varinst: [$gptr_q32, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 128]
    -   allocate_queue: [e2e__fused_op_177_0, e2e_bw_in0_matmul_22_matmul_1_0, e2e_bw_in0_matmul_14_matmul_1_0, e2e_bw_in0_matmul_8_matmul_1_0]
    -   execute: {graph_name: bwd_26, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e__fused_op_174_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e__fused_op_175_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_174_0, e2e__fused_op_175_0]
    -   varinst: [$gptr_q34_shadow, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 128]
    -   varinst: [$gptr_q36, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: bwd_27, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               e2e__fused_op_177_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_bw_in0_matmul_22_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_bw_in0_matmul_14_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_bw_in0_matmul_8_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_177_0, e2e_bw_in0_matmul_22_matmul_1_0, e2e_bw_in0_matmul_14_matmul_1_0, e2e_bw_in0_matmul_8_matmul_1_0]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 256]
    -   varinst: [$gptr_q38, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 128]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt:
    - var: {$c_microbatch_size: 64, $c_one: 1, $c_zero: 0}


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 1
    schedules: 
      -
        - softmax_18.dc.reciprocal.2: { type: reciprocal, inputs: [input0], mblock: [2, 1], ublock: [2, 1], output: intermed0}
      -
        - softmax_18.dc.multiply.3: { type: multiply, inputs: [input1, intermed0], input_1_tms: [broadcast: {c: 4}], pop_last: [intermed0], mblock: [2, 1], ublock: [2, 4], output: output}
  2: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.add.5: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.6: { type: sqrt, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.7: { type: reciprocal, inputs: [dest], mblock: [2, 1], ublock: [2, 1], output: output}
  3: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.8: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [2, 3], ublock: [2, 4], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.9: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.10: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  96: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: r], mblock: [2, 6], ublock: [2, 4], output: output}
  97: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.4: { type: multiply, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_635_layernorm_bw_0.dc.add.5: { type: add, inputs: [input2, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.7: { type: multiply, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_635_layernorm_bw_0.dc.subtract.8: { type: subtract, inputs: [input4, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: intermed0}
        - bw_in0_layernorm_635_layernorm_bw_0.dc.multiply.9: { type: multiply, inputs: [input5, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 4], output: output}
  98: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - bw_in0_gelu_627_gelu_derivative_0: { type: gelu_derivative, inputs: [input0], mblock: [2, 4], ublock: [2, 4], output: dest}
        - bw_in0_gelu_627_multiply_1: { type: multiply, inputs: [dest, input1], mblock: [2, 4], ublock: [2, 4], output: output}
  101: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - bw_in0_softmax_601_softmax_bw_0.dc.subtract.2: { type: subtract, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_softmax_601_softmax_bw_0.dc.multiply.3: { type: multiply, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: dest}
        - bw_in0_multiply_599_multiply_0: { type: multiply, inputs: [dest, input3], mblock: [2, 1], ublock: [2, 4], output: output}
  102: 
    inputs: 4
    intermediates: 1
    schedules: 
      -
        - bw_in0_reshape_583_combine_add_0: { type: add, inputs: [input0, input1], mblock: [2, 3], ublock: [2, 2], output: dest}
        - bw_in0_reshape_583_combine_add_1: { type: add, inputs: [dest, input2], mblock: [2, 3], ublock: [2, 2], output: intermed0}
        - bw_in0_layernorm_582_combine_add_0: { type: add, inputs: [input3, intermed0], pop: [intermed0], mblock: [2, 3], ublock: [2, 2], output: output}

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.01
    uniform_upper_bound: 0.25
  io-config:
    inputs: [hidden_states, attention_mask, loss_bert_encoders.output_layernorm_635]
    outputs: [bert_encoders.output_layernorm_635, output_grad_hidden_states]
