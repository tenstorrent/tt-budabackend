# git checkout d27daa7a
# pytest pybuda/test/test_bert.py::test_ff[training-no_recompute]

devices:
  arch: grayskull

queues:

  # input
  encoder_input:                                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}

  # output
  ff.output_ff_0_ff2.bias:                                                           {input: ff_0_ff2.bias, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  ff.bert.encoder.layer.0.intermediate.dense.weight:                                 {input: opt_in1_ff.bert.encoder.layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x20000000]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias:                                   {input: opt_in1_ff.bert.encoder.layer.0.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x20000000]]}
  ff.bert.encoder.layer.0.output.dense.weight:                                       {input: opt_in0_ff.bert.encoder.layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x20000000], [5, 0x20000000]]}
  ff.bert.encoder.layer.0.output.dense.bias:                                         {input: opt_in1_ff.bert.encoder.layer.0.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x20000000]]}

  # constant
  lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[7, 0x31000000]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_1_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x31000000]]}
  lc.input_tensor.bw_in1_ff_0_ff2.bias_reduce_sum_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31000000]]}
  lc.input_tensor.bw_in1_ff_0_ff1.bias_reduce_sum_0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[6, 0x31000000]]}

  # epoch_to_epoch
  e2e_ff0_gelu_0:                                                                    {input: ff0_gelu, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10000000]]}
  e2e_ff_0_ff1.bias_0:                                                               {input: ff_0_ff1.bias, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10000000]]}

  # optimizer_parameter
  input_opt_ff.bert.encoder.layer.0.intermediate.dense.weight_0.lr:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  input_opt_ff.bert.encoder.layer.0.intermediate.dense.bias_0.lr:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10000000]]}
  input_opt_ff.bert.encoder.layer.0.output.dense.weight_0.lr:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10000000]]}
  input_opt_ff.bert.encoder.layer.0.output.dense.bias_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10000000]]}

  # loss
  loss_ff.output_ff_0_ff2.bias:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30008220]]}

  # grad_accumulator
  grad_acc_ff.bert.encoder.layer.0.output.dense.bias:                                {input: bw_in1_ff_0_ff2.bias_brcst_reduce_sum_0, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x30000000]]}
  grad_acc_ff.bert.encoder.layer.0.output.dense.weight:                              {input: bw_in1_ff_0_ff2_matmul_1, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x30000000], [2, 0x30000000]]}
  grad_acc_ff.bert.encoder.layer.0.intermediate.dense.bias:                          {input: bw_in1_ff_0_ff1.bias_brcst_reduce_sum_0, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x30000000]]}
  grad_acc_ff.bert.encoder.layer.0.intermediate.dense.weight:                        {input: bw_in1_ff_0_ff1_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x30000000]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 1
    ff_0_ff1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [encoder_input, ff.bert.encoder.layer.0.intermediate.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0, ff.bert.encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff_0_ff1.bias: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [ff_0_ff1, ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff0_gelu: {type: gelu, grid_loc: [0, 3], grid_size: [1, 1], inputs: [ff_0_ff1.bias],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    ff_0_ff2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 8}}
    ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_1_0.0, ff.bert.encoder.layer.0.output.dense.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff_0_ff2.bias: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [ff_0_ff2, ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_1_0.lc1], untilize_output: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  bwd_1:
    target_device: 0
    input_count: 1
    bw_in1_ff_0_ff2.bias_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_ff_0_ff2.bias_reduce_sum_0.0, loss_ff.output_ff_0_ff2.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_ff_0_ff2.bias_brcst_reduce_sum_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [bw_in1_ff_0_ff2.bias_reduce_sum_0.lc1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_ff_0_ff2_matmul_1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [loss_ff.output_ff_0_ff2.bias, ff.bert.encoder.layer.0.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 2, u_kt: 2}}
    bw_in1_ff_0_ff2_matmul_1_transpose_nop_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_ff0_gelu_0],
         t: 1, mblock: [8, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_ff_0_ff2_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [2, 1], inputs: [bw_in1_ff_0_ff2_matmul_1_transpose_nop_0, loss_ff.output_ff_0_ff2.bias], gradient_op: true,
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}
    bw_in0_ff0_gelu_gelu_derivative_0: {type: gelu_derivative, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_ff_0_ff1.bias_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_ff0_gelu_multiply_1: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_ff0_gelu_gelu_derivative_0, bw_in0_ff_0_ff2_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_ff_0_ff1.bias_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_ff_0_ff1.bias_reduce_sum_0.0, bw_in0_ff0_gelu_multiply_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, u_kt: 4}}
    bw_in1_ff_0_ff1.bias_brcst_reduce_sum_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in1_ff_0_ff1.bias_reduce_sum_0.lc1], gradient_op: true,
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_ff_0_ff1_matmul_1_transpose_nop_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [encoder_input],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_ff_0_ff1_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [bw_in1_ff_0_ff1_matmul_1_transpose_nop_0, bw_in0_ff0_gelu_multiply_1], gradient_op: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 4}}

  opt_2:
    target_device: 0
    input_count: 1
    opt_in1_ff.bert.encoder.layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [grad_acc_ff.bert.encoder.layer.0.intermediate.dense.weight, input_opt_ff.bert.encoder.layer.0.intermediate.dense.weight_0.lr],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}, broadcast: {r: 4}]}
    opt_in1_ff.bert.encoder.layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [0, 1], grid_size: [1, 1], inputs: [ff.bert.encoder.layer.0.intermediate.dense.weight, opt_in1_ff.bert.encoder.layer.0.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_ff.bert.encoder.layer.0.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [grad_acc_ff.bert.encoder.layer.0.intermediate.dense.bias, input_opt_ff.bert.encoder.layer.0.intermediate.dense.bias_0.lr],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}]}
    opt_in1_ff.bert.encoder.layer.0.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [ff.bert.encoder.layer.0.intermediate.dense.bias, opt_in1_ff.bert.encoder.layer.0.intermediate.dense.bias_multiply_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_ff.bert.encoder.layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [grad_acc_ff.bert.encoder.layer.0.output.dense.weight, input_opt_ff.bert.encoder.layer.0.output.dense.weight_0.lr],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 16}]}
    opt_in0_ff.bert.encoder.layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [0, 5], grid_size: [2, 1], inputs: [ff.bert.encoder.layer.0.output.dense.weight, opt_in0_ff.bert.encoder.layer.0.output.dense.weight_multiply_1],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_ff.bert.encoder.layer.0.output.dense.bias_multiply_1: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [grad_acc_ff.bert.encoder.layer.0.output.dense.bias, input_opt_ff.bert.encoder.layer.0.output.dense.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    opt_in1_ff.bert.encoder.layer.0.output.dense.bias_subtract_2: {type: subtract, grid_loc: [0, 7], grid_size: [1, 1], inputs: [ff.bert.encoder.layer.0.output.dense.bias, opt_in1_ff.bert.encoder.layer.0.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_microbatch_count, $p_microbatch_size]
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $gptr_q0_shadow: 0}
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - var: {$c_zero: 0, $c_wrap: 2}
    - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_size]
    - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_count]
    - loop: $p_microbatch_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               encoder_input: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$lptr_q0, incwrap, $p_microbatch_size, $c_wrap]
    - endloop

  - run_bwd:
    - param: [$p_zero_grad, $p_microbatch_count, $p_microbatch_size]
    - staticvar: {$lptr_q0: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q1: 0, $lptr_q1: 0}
    - var: {$c_zero: 0, $c_wrap: 2}
    - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_size]
    - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_count]
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_microbatch_count
    -   execute: {graph_name: bwd_1, queue_settings: {
               encoder_input: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               loss_ff.output_ff_0_ff2.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_ff_0_ff1.bias_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_ff0_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_ff_0_ff2.bias_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_ff_0_ff1.bias_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_ff.bert.encoder.layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$gptr_q1, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$gptr_q2, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$lptr_q0, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$lptr_q1, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$lptr_q2, incwrap, $p_microbatch_size, $c_wrap]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt:
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
    - execute: {graph_name: opt_2, queue_settings: {
             ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             ff.bert.encoder.layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             ff.bert.encoder.layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_ff.bert.encoder.layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_ff.bert.encoder.layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }

test-config:
  test-args:
    minibatch_count: 5
    microbatch_size: 1 # input_count
    microbatch_count: 1 # entries / input_count
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.80
    check_pcc: 0.99
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: -1.0
    uniform_upper_bound: 1.0
    overrides:
      .*\.weight:
        type: Normal
        normal_mean: 0.0
        normal_stddev: 0.01
      .*\.bias:
        type: Constant
        constant_value: 1.0
        set_tile_rows: [1, 1]
      .*\.bias_s_brcst.*:
        type: Constant
        constant_value: 1.0
        set_tile_cols: [1, 1]
      .*bias_reduce_sum_0.0:
        type: Constant
        constant_value: 1.0
      loss_ff.output_ff_0_ff2.bias:
        type: Constant
        constant_value: 0.001