devices:
  arch: grayskull

queues:

  # input
  input_0_add_mha_0:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30035100]]}
  input_0_mask_copy_0:                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30045d80]]}

  # output
  encoder1.output_norm_ff_1_bias:                                                              {input: norm_ff_1_bias, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  encoder1.output_mask_copy_1:                                                                 {input: mask_copy_1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: host, host: [0x8020]}

  # parameter
  ff.bert.encoder.layer.1.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000840]]}
  ff.bert.encoder.layer.1.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30003120]]}
  ff.bert.encoder.layer.1.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30005a00]]}
  ff.bert.encoder.layer.1.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300082e0]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30010d40]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30013620]]}
  ff.bert.encoder.layer.1.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30015f00]]}
  ff.bert.encoder.layer.1.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300187e0]]}
  ff.bert.encoder.layer.0.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3001b0c0]]}
  ff.bert.encoder.layer.0.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3001d9a0]]}
  ff.bert.encoder.layer.0.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30020280]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30022b60]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3002b5c0]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3002dea0]]}
  ff.bert.encoder.layer.0.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30030780]]}
  ff.bert.encoder.layer.0.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30033060]]}
  ff.bert.encoder.layer.0.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3003d320]]}
  ff.reciprocal_of_sqrt_of_head_size_0:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30048660]]}
  ff.bert.encoder.layer.0.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30049f20]]}
  ff.bert.encoder.layer.0.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3004bfc0]]}
  ff.bert.encoder.layer.0.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30054a20]]}
  ff.bert.encoder.layer.0.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30056ac0]]}
  ff.bert.encoder.layer.0.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3005f520]]}
  ff.bert.encoder.layer.0.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30069840]]}
  ff.bert.encoder.layer.0.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3008a060]]}
  ff.bert.encoder.layer.1.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300ac980]]}
  ff.reciprocal_of_sqrt_of_head_size_1:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b5c20]]}
  ff.bert.encoder.layer.1.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b74e0]]}
  ff.bert.encoder.layer.1.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b9580]]}
  ff.bert.encoder.layer.1.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300c1fe0]]}
  ff.bert.encoder.layer.1.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300c4080]]}
  ff.bert.encoder.layer.1.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300ccae0]]}
  ff.bert.encoder.layer.1.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300d6e00]]}
  ff.bert.encoder.layer.1.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300f7620]]}

  # constant
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300028e0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300051c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30007aa0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30010500]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30012de0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300156c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30017fa0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3001a880]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3001d160]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3001fa40]]}
  lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30022320]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3002ad80]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3002d660]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3002ff40]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30032820]]}
  lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30045540]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30047e20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30048ea0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300496e0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300541e0]]}
  lc.input_tensor.mha_0_as_softmax_sum.0:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3005ece0]]}
  lc.input_tensor.norm_mha_0_mean.0:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30067740]]}
  lc.input_tensor.norm_mha_0_var.0:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30067f80]]}
  constant_1_norm_mha_0_var_plus_eps:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300687c0]]}
  lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30069000]]}
  lc.input_tensor.norm_ff_0_mean.0:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300aa880]]}
  lc.input_tensor.norm_ff_0_var.0:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300ab0c0]]}
  constant_1_norm_ff_0_var_plus_eps:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300ab900]]}
  lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300ac140]]}
  lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b4ba0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b53e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b6460]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300b6ca0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300c17a0]]}
  lc.input_tensor.mha_1_as_softmax_sum.0:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300cc2a0]]}
  lc.input_tensor.norm_mha_1_mean.0:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300d4d00]]}
  lc.input_tensor.norm_mha_1_var.0:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300d5540]]}
  constant_1_norm_mha_1_var_plus_eps:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300d5d80]]}
  lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x300d65c0]]}
  lc.input_tensor.norm_ff_1_mean.0:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30117e40]]}
  lc.input_tensor.norm_ff_1_var.0:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30118680]]}
  constant_1_norm_ff_1_var_plus_eps:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30118ec0]]}
  lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0:                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30119700]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 1
    ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_value: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.value.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_0_value.bias: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [mha_0_value, ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    input_0_mask_copy_0_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0, input_0_mask_copy_0],
         t: 4, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_0],
         t: 4, mblock: [1, 1], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0],
         t: 4, mblock: [1, 1], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_key: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.key.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_0_key.bias: {type: add, grid_loc: [1, 11], grid_size: [1, 1], inputs: [mha_0_key, ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_query: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.query.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_0_query.bias: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [mha_0_query, ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mha_0_as: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [mha_0_query.bias, mha_0_key.bias],
         t: 4, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_div: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [mha_0_as, ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1],
         t: 4, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_0_as_mask: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [mha_0_as_div, input_0_mask_copy_0_s_brcst_m2_1_1.lc1],
         t: 4, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mha_0_as_softmax_exp: {type: exp, grid_loc: [2, 6], grid_size: [1, 1], inputs: [mha_0_as_mask],
         t: 4, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    mha_0_as_softmax_sum.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [mha_0_as_softmax_exp, lc.input_tensor.mha_0_as_softmax_sum.0],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    mha_0_as_softmax_recip: {type: reciprocal, grid_loc: [2, 8], grid_size: [1, 1], inputs: [mha_0_as_softmax_sum.lc1],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    mha_0_as_softmax_mult: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [mha_0_as_softmax_exp, mha_0_as_softmax_recip],
         t: 4, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    mha_0_ac: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [mha_0_as_softmax_mult, mha_0_value.bias],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4],
         attributes: {m_k: 4, u_kt: 1}}
    mha_0_output: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [mha_0_ac, ff.bert.encoder.layer.0.attention.output.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4],
         attributes: {m_k: 4, u_kt: 1}}
    mha_0_output.bias: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [mha_0_output, ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_mha_0: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [input_0_add_mha_0, mha_0_output.bias],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_0_mean.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_mha_0, lc.input_tensor.norm_mha_0_mean.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    norm_mha_0_sub: {type: subtract, grid_loc: [3, 3], grid_size: [1, 1], inputs: [add_mha_0, norm_mha_0_mean.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_mha_0_sq: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [norm_mha_0_sub, norm_mha_0_sub],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_0_var.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [norm_mha_0_sq, lc.input_tensor.norm_mha_0_var.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, u_kt: 2}}
    norm_mha_0_var_plus_eps: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [norm_mha_0_var.lc1, constant_1_norm_mha_0_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_mha_0_sqrt: {type: sqrt, grid_loc: [3, 7], grid_size: [1, 1], inputs: [norm_mha_0_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_0_recip: {type: reciprocal, grid_loc: [3, 8], grid_size: [1, 1], inputs: [norm_mha_0_sqrt],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_0_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [norm_mha_0_recip, lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0_output: {type: multiply, grid_loc: [3, 10], grid_size: [1, 1], inputs: [norm_mha_0_sub, norm_mha_0_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_mha_0_weights: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [norm_mha_0_output, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_mha_0_bias: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [norm_mha_0_weights, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff_0_ff1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [norm_mha_0_bias, ff.bert.encoder.layer.0.intermediate.dense.weight],
         t: 1, mblock: [4, 8], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    ff_0_ff1.bias: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [ff_0_ff1, ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff0_gelu: {type: gelu, grid_loc: [4, 3], grid_size: [1, 1], inputs: [ff_0_ff1.bias],
         t: 1, mblock: [2, 8], ublock: [2, 2], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    ff_0_ff2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 8, u_kt: 2}}
    ff_0_ff2.bias: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [ff_0_ff2, ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_ff_0: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [norm_mha_0_bias, ff_0_ff2.bias],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_0_mean.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_ff_0, lc.input_tensor.norm_ff_0_mean.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    norm_ff_0_sub: {type: subtract, grid_loc: [4, 8], grid_size: [1, 1], inputs: [add_ff_0, norm_ff_0_mean.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_ff_0_sq: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [norm_ff_0_sub, norm_ff_0_sub],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_0_var.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [norm_ff_0_sq, lc.input_tensor.norm_ff_0_var.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, u_kt: 2}}
    norm_ff_0_var_plus_eps: {type: add, grid_loc: [4, 11], grid_size: [1, 1], inputs: [norm_ff_0_var.lc1, constant_1_norm_ff_0_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_ff_0_sqrt: {type: sqrt, grid_loc: [5, 0], grid_size: [1, 1], inputs: [norm_ff_0_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_0_recip: {type: reciprocal, grid_loc: [5, 1], grid_size: [1, 1], inputs: [norm_ff_0_sqrt],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_0_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [norm_ff_0_recip, lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0_output: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [norm_ff_0_sub, norm_ff_0_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_ff_0_weights: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [norm_ff_0_output, ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_ff_0_bias: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [norm_ff_0_weights, ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mha_1_value: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.value.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_1_value.bias: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [mha_1_value, ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mask_copy_0_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [5, 8], grid_size: [1, 1], inputs: [lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0, input_0_mask_copy_0],
         t: 4, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_1],
         t: 4, mblock: [1, 1], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0],
         t: 4, mblock: [1, 1], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 4}],
         attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_key: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.key.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_1_key.bias: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [mha_1_key, ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_query: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.query.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    mha_1_query.bias: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [mha_1_query, ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mha_1_as: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [mha_1_query.bias, mha_1_key.bias],
         t: 4, mblock: [4, 4], ublock: [1, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4],
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_as_div: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [mha_1_as, ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1],
         t: 4, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_1_as_mask: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [mha_1_as_div, mask_copy_0_s_brcst_m2_1_1.lc1],
         t: 4, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mha_1_as_softmax_exp: {type: exp, grid_loc: [6, 8], grid_size: [1, 1], inputs: [mha_1_as_mask],
         t: 4, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    mha_1_as_softmax_sum.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [mha_1_as_softmax_exp, lc.input_tensor.mha_1_as_softmax_sum.0],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    mha_1_as_softmax_recip: {type: reciprocal, grid_loc: [6, 10], grid_size: [1, 1], inputs: [mha_1_as_softmax_sum.lc1],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    mha_1_as_softmax_mult: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [mha_1_as_softmax_exp, mha_1_as_softmax_recip],
         t: 4, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    mha_1_ac: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [mha_1_as_softmax_mult, mha_1_value.bias],
         t: 4, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4],
         attributes: {m_k: 4, u_kt: 1}}
    mha_1_output: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [mha_1_ac, ff.bert.encoder.layer.1.attention.output.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4],
         attributes: {m_k: 4, u_kt: 1}}
    mha_1_output.bias: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [mha_1_output, ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_mha_1: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [norm_ff_0_bias, mha_1_output.bias],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_1_mean.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_mha_1, lc.input_tensor.norm_mha_1_mean.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    norm_mha_1_sub: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_mha_1, norm_mha_1_mean.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_mha_1_sq: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [norm_mha_1_sub, norm_mha_1_sub],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_1_var.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [norm_mha_1_sq, lc.input_tensor.norm_mha_1_var.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, u_kt: 2}}
    norm_mha_1_var_plus_eps: {type: add, grid_loc: [7, 8], grid_size: [1, 1], inputs: [norm_mha_1_var.lc1, constant_1_norm_mha_1_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_mha_1_sqrt: {type: sqrt, grid_loc: [7, 9], grid_size: [1, 1], inputs: [norm_mha_1_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_1_recip: {type: reciprocal, grid_loc: [7, 10], grid_size: [1, 1], inputs: [norm_mha_1_sqrt],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_mha_1_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [norm_mha_1_recip, lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1_output: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [norm_mha_1_sub, norm_mha_1_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_mha_1_weights: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [norm_mha_1_output, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_mha_1_bias: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [norm_mha_1_weights, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff_1_ff1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [norm_mha_1_bias, ff.bert.encoder.layer.1.intermediate.dense.weight],
         t: 1, mblock: [4, 8], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 2}}
    ff_1_ff1.bias: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [ff_1_ff1, ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 8], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    ff1_gelu: {type: gelu, grid_loc: [8, 5], grid_size: [1, 1], inputs: [ff_1_ff1.bias],
         t: 1, mblock: [2, 8], ublock: [2, 2], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    ff_1_ff2: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [ff1_gelu, ff.bert.encoder.layer.1.output.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 8, u_kt: 2}}
    ff_1_ff2.bias: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [ff_1_ff2, ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    add_ff_1: {type: add, grid_loc: [8, 8], grid_size: [1, 1], inputs: [norm_mha_1_bias, ff_1_ff2.bias],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_1_mean.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [add_ff_1, lc.input_tensor.norm_ff_1_mean.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 4, u_kt: 1}}
    norm_ff_1_sub: {type: subtract, grid_loc: [8, 10], grid_size: [1, 1], inputs: [add_ff_1, norm_ff_1_mean.lc1],
         t: 1, mblock: [2, 4], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_ff_1_sq: {type: multiply, grid_loc: [8, 11], grid_size: [1, 1], inputs: [norm_ff_1_sub, norm_ff_1_sub],
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_1_var.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [norm_ff_1_sq, lc.input_tensor.norm_ff_1_var.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 2, u_kt: 2}}
    norm_ff_1_var_plus_eps: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [norm_ff_1_var.lc1, constant_1_norm_ff_1_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_ff_1_sqrt: {type: sqrt, grid_loc: [9, 2], grid_size: [1, 1], inputs: [norm_ff_1_var_plus_eps],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_1_recip: {type: reciprocal, grid_loc: [9, 3], grid_size: [1, 1], inputs: [norm_ff_1_sqrt],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}
    norm_ff_1_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [norm_ff_1_recip, lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1_output: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [norm_ff_1_sub, norm_ff_1_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    norm_ff_1_weights: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [norm_ff_1_output, ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [1, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    norm_ff_1_bias: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [norm_ff_1_weights, ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [2, 2], ublock: [2, 2], in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    mask_copy_1: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [input_0_mask_copy_0], untilize_output: true,
         t: 1, mblock: [1, 2], ublock: [1, 2], in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, ublock_order: r, buf_size_mb: 2, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_microbatch_count]
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_microbatch_count
    -   execute: {graph_name: fwd_0, queue_settings: {
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_0: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax_sum.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_norm_mha_0_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_norm_ff_0_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_1: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax_sum.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_norm_mha_1_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_norm_ff_1_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_0_add_mha_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               input_0_mask_copy_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}} }
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 2]
    - endloop

test-config:
  test-args:
    microbatch_count: 1
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [input_0_add_mha_0, input_0_mask_copy_0]
    outputs: [encoder1.output_norm_ff_1_bias, encoder1.output_mask_copy_1]
