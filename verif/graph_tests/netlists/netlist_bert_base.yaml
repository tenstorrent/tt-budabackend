devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x33480020]]}

  # output
  run_bert.output_layernorm_841_bias:                                           {input: layernorm_841_bias, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x997ef80]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9ea3ce0]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9db9320]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9917d60]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa40a920]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e9d3c0]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9db2a00]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9911440]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa404000]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e96aa0]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dac0e0]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x990ab20]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa3fd6e0]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e90180]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafe8ba0]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb0034a0]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x970bf60]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3fd320]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1f2640]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaffcb80]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9705640]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96f8400]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1ebd20]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaff6260]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96fed20]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3591e0]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1e5400]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafef940]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e893e0]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa09bf80]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafd4740]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96dcd80]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa344000]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa095660]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafcde20]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9864ee0]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa33d6e0]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa08ed40]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafc7500]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96d5fe0]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa336dc0]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa088420]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafc0be0]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafe1e00]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96f0d60]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3516c0]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1dd8e0]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafdb4e0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96ea440]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa34ada0]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1400c0]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96e3b20]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa35f5c0]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e82ac0]]}
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa0a28a0]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x986c580]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0dcae0]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c19b00]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa6e4c40], [5, 0xa44f9a0], [6, 0x98ed4e0]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa00aac0], [2, 0xa4e68a0], [3, 0xa06aec0]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa0645a0]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0041a0]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb1bc6c0]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa448c00]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa646fa0]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa4dfb00]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9f66980]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x984f3c0]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa854900]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa528760]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9a5d1a0], [7, 0x9c34820], [0, 0xb2f8480]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa13dc60], [4, 0xa85b220], [5, 0xa542b80]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5bf660]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa180c20]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb25ac60]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa521e40]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa7b70e0]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5b8d40]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa0e3400]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x99bf500]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9a959e0]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa4131c0]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa29d520], [3, 0x9eb25a0], [4, 0xa4a1d60]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x99c39c0], [0, 0xb010b60], [1, 0x9dc1740]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1f8f60]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x99bd0a0]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9712d00]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eab800]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1ff880]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb009dc0]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x991f880]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa403c40]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c12460]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa42e360]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa40d660], [3, 0x9f91c80], [4, 0xa574b00]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9b40440], [0, 0xb0e9da0], [1, 0x9e944e0]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9f845c0]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9b39b20]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x97b12a0]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9f8aee0]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa36f9c0]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb0e3000]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9a9c300]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa35fb00]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9313e00]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dff800]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x99ef120], [3, 0x98acf60], [4, 0x9d7d580]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9241de0], [0, 0xab12ec0], [1, 0x9978660]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9971d40]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x923b4c0]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9235020]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x98a61c0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9951480]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xab0c120]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x919dca0]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9b79680]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9f28360]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9b5f260]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x93b83c0], [0, 0xabec100], [1, 0x9a4b400]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9e56340], [5, 0x9e209c0], [6, 0x92d9ee0]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92d35c0]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9e4fa20]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x99858a0]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xabe5360]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x931a720]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9e19c20]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9ac1a40]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9801300]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9d18a80]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97e6ee0]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa960a40], [1, 0x97e6ee0], [2, 0x987dde0]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9c46a60], [6, 0x8fbddc0], [7, 0x8f20a20]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97e0140]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9c40140]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8f205a0]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x97e05c0]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x97e05c0]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xab05380]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9cdf460]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x98d40a0]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x989f420]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x98b9c80]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9090260], [7, 0x8ff2ec0], [0, 0xaa32ee0]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9d2c5e0], [6, 0x9162700], [7, 0x90cb800]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x90c4ee0]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9d25cc0]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c41340]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9d1f3a0]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9f4d3e0]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x95f6480]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa150a60]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9ddd720]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa25d280]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9dc3300]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x96da500], [0, 0xaeed540], [1, 0x9bf8160]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa18b260], [5, 0xa07e5c0], [6, 0x9523fe0]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x951d6c0]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa184940]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9d03c40]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96cf6c0]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa2c0ba0]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e7afa0]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9feac00]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x97c76c0]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9da2f40]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x97ad2a0]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa2644a0], [5, 0xa1ee700], [6, 0x95fd220]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cd0f20], [2, 0x9e7b3c0], [3, 0x9da8b00]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9da21e0]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cca600]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa0e0800]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x955d180]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x947eca0]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa007a40]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9af62e0]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xacd8e40]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9ce8ec0]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xacbea20]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9ef32e0], [6, 0x93ac800], [7, 0x948ace0]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9c16ea0], [3, 0x9a23e40], [4, 0x9f355a0]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9f2ec80]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaee67a0]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x963c860]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa17e020]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9c66420]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9bf0f40]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xae48f80]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9fe04a0]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9635ac0]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9fc6080]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [24, 32], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9cf00e0], [3, 0x9b93f80], [4, 0xa00e7e0]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [96, 8], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9563aa0], [0, 0xad76ae0], [1, 0x9b1eaa0]]}

  # constant
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa411240]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa358d60]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1e4f80]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafef4c0]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96f7f80]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3588e0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1e4b00]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dabc60]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96f7b00]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa358460]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1e4680]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafe8720]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96f7680]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa357fe0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa1e4200]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dc09c0]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x991f400]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa411fc0]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eaaf00]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dc0540]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x991ef80]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa411b40]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eaaa80]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dc00c0]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x991eb00]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa4116c0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eaa600]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dbfc40]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x991e680]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e82640]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9daaa60]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x986c100]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa35ecc0]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e821c0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9daa5e0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x986bc80]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa35e840]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e81d40]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9daa160]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x986b800]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa35e3c0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e818c0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9da9ce0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96dc900]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x990a6a0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa3fd260]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e8fd00]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dab7e0]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x990a220]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa3fcde0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dab360]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9909da0]]}
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa34a920]]}
  constant_1_multiply_17:                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9daaee0]]}
  lc.input_tensor.softmax_19.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafdb060]]}
  lc.input_tensor.layernorm_37_mean.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96e36a0]]}
  lc.input_tensor.layernorm_37_var.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa35f140]]}
  constant_1_layernorm_37_var_plus_eps:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa13cee0]]}
  lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e94060]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5b88c0]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb25a360]]}
  lc.input_tensor.layernorm_60_mean.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb259ee0]]}
  lc.input_tensor.layernorm_60_var.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c19680]]}
  constant_1_layernorm_60_var_plus_eps:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x98ed060]]}
  lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa44f520]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa6e47c0]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa063ca0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c19200]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x98ecbe0]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa064120]]}
  constant_1_multiply_88:                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb1bc240]]}
  lc.input_tensor.softmax_90.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c18d80]]}
  lc.input_tensor.layernorm_108_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa448780]]}
  lc.input_tensor.layernorm_108_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa646b20]]}
  constant_1_layernorm_108_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa20fc80]]}
  lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa7b6c60]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5c5f80]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa13d7e0]]}
  lc.input_tensor.layernorm_131_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9b2f1c0]]}
  lc.input_tensor.layernorm_131_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9d06840]]}
  constant_1_layernorm_131_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb3ca4a0]]}
  lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa1879c0]]}
  lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5c6400]]}
  lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa187540]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c343a0]]}
  lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9a5cd20]]}
  lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa13d360]]}
  constant_1_multiply_159:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb25a7e0]]}
  lc.input_tensor.softmax_161.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9c33f20]]}
  lc.input_tensor.layernorm_179_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa4e6420]]}
  lc.input_tensor.layernorm_179_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa5219c0]]}
  constant_1_layernorm_179_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa36f540]]}
  lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e93760]]}
  lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb0e2b80]]}
  lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x97b09a0]]}
  lc.input_tensor.layernorm_202_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x97b0520]]}
  lc.input_tensor.layernorm_202_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa412d40]]}
  constant_1_layernorm_202_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa4a18e0]]}
  lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eb2120]]}
  lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa29d0a0]]}
  lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb0106e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa4128c0]]}
  lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa4a1460]]}
  lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dc0e40]]}
  constant_1_multiply_230:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9712880]]}
  lc.input_tensor.softmax_232.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa412440]]}
  lc.input_tensor.layernorm_250_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9eab380]]}
  lc.input_tensor.layernorm_250_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9dc12c0]]}
  constant_1_layernorm_250_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa4df680]]}
  lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9f66500]]}
  lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb1bbdc0]]}
  lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x984ef40]]}
  lc.input_tensor.layernorm_273_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x984eac0]]}
  lc.input_tensor.layernorm_273_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa42dee0]]}
  constant_1_layernorm_273_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa574680]]}
  lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9f91800]]}
  lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa40d1e0]]}
  lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xb0e9920]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa42da60]]}
  lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa574200]]}
  lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e93be0]]}
  constant_1_multiply_301:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x97b0e20]]}
  lc.input_tensor.softmax_303.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa42d5e0]]}
  lc.input_tensor.layernorm_321_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa573d80]]}
  lc.input_tensor.layernorm_321_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x98b9380]]}
  constant_1_layernorm_321_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9ac1140]]}
  lc.input_tensor.layernorm_321_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9a4a680]]}
  lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xabe4ee0]]}
  lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92d2cc0]]}
  lc.input_tensor.layernorm_344_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92d2840]]}
  lc.input_tensor.layernorm_344_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dff380]]}
  constant_1_layernorm_344_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9d7d100]]}
  lc.input_tensor.layernorm_344_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dfea80]]}
  lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x99eeca0]]}
  lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xab12a40]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dfef00]]}
  lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9d7cc80]]}
  lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x99718c0]]}
  constant_1_multiply_372:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9234ba0]]}
  lc.input_tensor.softmax_374.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x98acae0]]}
  lc.input_tensor.layernorm_392_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9b1d420]]}
  lc.input_tensor.layernorm_392_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xacbe120]]}
  constant_1_layernorm_392_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x948a3e0]]}
  lc.input_tensor.layernorm_392_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x93abf00]]}
  lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9ef29e0]]}
  lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9a23540]]}
  lc.input_tensor.layernorm_415_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9a230c0]]}
  lc.input_tensor.layernorm_415_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9e4f5a0]]}
  constant_1_layernorm_415_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9a4af80]]}
  lc.input_tensor.layernorm_415_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xabebc80]]}
  lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x93b7f40]]}
  lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9e20540]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9ac15c0]]}
  lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9a4ab00]]}
  lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92d3140]]}
  constant_1_multiply_443:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97e6a60]]}
  lc.input_tensor.softmax_445.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c40ec0]]}
  lc.input_tensor.layernorm_463_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x994fe00]]}
  lc.input_tensor.layernorm_463_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x98b8f00]]}
  constant_1_layernorm_463_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaa32a60]]}
  lc.input_tensor.layernorm_463_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8ff2a40]]}
  lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x908fde0]]}
  lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c40a40]]}
  lc.input_tensor.layernorm_486_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c405c0]]}
  lc.input_tensor.layernorm_486_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8f20120]]}
  constant_1_layernorm_486_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa960140]]}
  lc.input_tensor.layernorm_486_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x97e0140]]}
  lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x97e0140]]}
  lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c40140]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f205a0]]}
  lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xa9605c0]]}
  lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f20120]]}
  constant_1_multiply_514:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x98a5d40]]}
  lc.input_tensor.softmax_516.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9951000]]}
  lc.input_tensor.layernorm_534_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xab0bca0]]}
  lc.input_tensor.layernorm_534_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x919d820]]}
  constant_1_layernorm_534_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9234720]]}
  lc.input_tensor.layernorm_534_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9dfe600]]}
  lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9cdefe0]]}
  lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9950b80]]}
  lc.input_tensor.layernorm_557_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9cdeb60]]}
  lc.input_tensor.layernorm_557_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x989efa0]]}
  constant_1_layernorm_557_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9950700]]}
  lc.input_tensor.layernorm_557_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x98b9800]]}
  lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xab04f00]]}
  lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9162280]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x989eb20]]}
  lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9950280]]}
  lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x97ac9a0]]}
  constant_1_multiply_585:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa263ba0]]}
  lc.input_tensor.softmax_587.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9da1d60]]}
  lc.input_tensor.layernorm_605_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cca180]]}
  lc.input_tensor.layernorm_605_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafbf560]]}
  constant_1_layernorm_605_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x97ac520]]}
  lc.input_tensor.layernorm_605_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x95f6000]]}
  lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa1505e0]]}
  lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9bf7860]]}
  lc.input_tensor.layernorm_628_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9da1460]]}
  lc.input_tensor.layernorm_628_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9dc2e80]]}
  constant_1_layernorm_628_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9bf7ce0]]}
  lc.input_tensor.layernorm_628_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xaeed0c0]]}
  lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x96da080]]}
  lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa07e140]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9dc2a00]]}
  lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9da18e0]]}
  lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa336940]]}
  constant_1_multiply_656:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9da9860]]}
  lc.input_tensor.softmax_658.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafc0760]]}
  lc.input_tensor.layernorm_676_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x96cf240]]}
  lc.input_tensor.layernorm_676_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa2c0720]]}
  constant_1_layernorm_676_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa3364c0]]}
  lc.input_tensor.layernorm_676_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9e7ab20]]}
  lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafbf9e0]]}
  lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafc02e0]]}
  lc.input_tensor.layernorm_699_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xafbfe60]]}
  lc.input_tensor.layernorm_699_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x97ace20]]}
  constant_1_layernorm_699_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x95fcda0]]}
  lc.input_tensor.layernorm_699_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa1ee280]]}
  lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa264020]]}
  lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9e7af40]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9b1e620]]}
  lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xad76660]]}
  lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9fc5780]]}
  constant_1_multiply_727:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9cef7e0]]}
  lc.input_tensor.softmax_729.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9b1e1a0]]}
  lc.input_tensor.layernorm_747_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x955cd00]]}
  lc.input_tensor.layernorm_747_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x947e820]]}
  constant_1_layernorm_747_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9a239c0]]}
  lc.input_tensor.layernorm_747_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa0075c0]]}
  lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9af5e60]]}
  lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9b1dd20]]}
  lc.input_tensor.layernorm_770_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9b1d8a0]]}
  lc.input_tensor.layernorm_770_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xacbe5a0]]}
  constant_1_layernorm_770_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x948a860]]}
  lc.input_tensor.layernorm_770_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x93ac380]]}
  lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9ef2e60]]}
  lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9fc5300]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x951d240]]}
  lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa07dcc0]]}
  lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9dc2580]]}
  constant_1_multiply_798:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x963c3e0]]}
  lc.input_tensor.softmax_800.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x951cdc0]]}
  lc.input_tensor.layernorm_818_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9cefc60]]}
  lc.input_tensor.layernorm_818_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9c65fa0]]}
  constant_1_layernorm_818_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x9dc2100]]}
  lc.input_tensor.layernorm_818_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9bf0ac0]]}
  lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0xae48b00]]}
  lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x951c940]]}
  lc.input_tensor.layernorm_841_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x951c4c0]]}
  lc.input_tensor.layernorm_841_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x9fc5c00]]}
  constant_1_layernorm_841_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa00e360]]}
  lc.input_tensor.layernorm_841_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9b93b00]]}

  # epoch_to_epoch
  e2e_matmul_127_0:                                                             {input: matmul_127, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [4, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x8200100], [2, 0x8200100], [3, 0x8200100]]}
  e2e_layernorm_108_bias_0:                                                     {input: layernorm_108_bias, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8200100]]}
  e2e_attention_mask_s_brcst_m2_9_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_9_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8200100]]}
  e2e_attention_mask_s_brcst_m2_8_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_8_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8200100]]}
  e2e_add_265_0:                                                                {input: add_265, type: queue, entries: 256, grid_size: [1, 2], t: 1, mblock: [4, 6], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa92d240], [5, 0xa614ba0]]}
  e2e_layernorm_250_bias_0:                                                     {input: layernorm_250_bias, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9b2f640]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8200100]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8200100]]}
  e2e_matmul_405_0:                                                             {input: matmul_405, type: queue, entries: 256, grid_size: [1, 3], t: 1, mblock: [4, 4], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9d06cc0], [0, 0xb3ca920], [1, 0xa187e40]]}
  e2e_layernorm_392_bias_0:                                                     {input: layernorm_392_bias, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0xa5c6880]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x8ac0120]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9c40120]]}
  e2e_layernorm_534_weights_0:                                                  {input: layernorm_534_weights, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa210100]]}
  e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8200100]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8ac0120]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8ac0120]]}
  e2e_layernorm_676_recip_s_brcst_m1_0_0.lc1_0:                                 {input: layernorm_676_recip_s_brcst_m1_0_0.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8200100]]}
  e2e_layernorm_676_sub_0:                                                      {input: layernorm_676_sub, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0xa614ba0]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8f20120]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8f20120]]}
  e2e_layernorm_818_var.lc1_0:                                                  {input: layernorm_818_var.lc1, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8200100]]}
  e2e_layernorm_818_sub_0:                                                      {input: layernorm_818_sub, type: queue, entries: 256, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9d06cc0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 256
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_22: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [hidden_states, layer.0.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_24: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_22, layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_8: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [hidden_states, layer.0.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_10: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_8, layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [hidden_states, layer.0.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_4: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [matmul_2, layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_14: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_4, add_10],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_17: {type: multiply, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_14, constant_1_multiply_17],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_18: {type: add, grid_loc: [0, 10], grid_size: [1, 1], inputs: [multiply_17, attention_mask_s_brcst_m2_11_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_19.dc.exp.0: {type: exp, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_18],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_19.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0, lc.input_tensor.softmax_19.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_19.dc.reciprocal.2: {type: reciprocal, grid_loc: [1, 1], grid_size: [1, 1], inputs: [softmax_19.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_19.dc.multiply.3: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0, softmax_19.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_29: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_19.dc.multiply.3, add_24],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_29, layer.0.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_35: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_33, layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_36: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_35, hidden_states],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_mean.lc1: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [add_36, lc.input_tensor.layernorm_37_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_37_sub: {type: subtract, grid_loc: [2, 10], grid_size: [1, 1], inputs: [add_36, layernorm_37_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_37_sq: {type: multiply, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_37_sub, layernorm_37_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_var.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_37_sq, lc.input_tensor.layernorm_37_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_37_var_plus_eps: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_37_var.lc1, constant_1_layernorm_37_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_37_sqrt: {type: sqrt, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_37_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_recip: {type: reciprocal, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_37_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_37_recip, lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_37_output: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_37_sub, layernorm_37_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_37_weights: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_37_output, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_37_bias: {type: add, grid_loc: [3, 9], grid_size: [1, 1], inputs: [layernorm_37_weights, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0, layer.0.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_50: {type: matmul, grid_loc: [4, 0], grid_size: [1, 3], inputs: [layernorm_37_bias, layer.0.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_52: {type: add, grid_loc: [4, 4], grid_size: [1, 2], inputs: [matmul_50, layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_53: {type: gelu, grid_loc: [4, 6], grid_size: [1, 2], inputs: [add_52],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_56: {type: matmul, grid_loc: [4, 8], grid_size: [1, 3], inputs: [gelu_53, layer.0.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_58: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [matmul_56, layer.0.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_59: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [add_58, layernorm_37_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_mean.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [add_59, lc.input_tensor.layernorm_60_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_60_sub: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_59, layernorm_60_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_60_sq: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_60_sub, layernorm_60_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_var.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_60_sq, lc.input_tensor.layernorm_60_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_60_var_plus_eps: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_60_var.lc1, constant_1_layernorm_60_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_60_sqrt: {type: sqrt, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_60_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_recip: {type: reciprocal, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_60_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_60_recip, lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_60_output: {type: multiply, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_60_sub, layernorm_60_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_60_weights: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_60_output, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_60_bias: {type: add, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_60_weights, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_93: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_60_bias, layer.1.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_95: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_93, layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_79: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_60_bias, layer.1.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_81: {type: add, grid_loc: [6, 8], grid_size: [1, 1], inputs: [matmul_79, layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_73: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_60_bias, layer.1.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_75: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_73, layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_85: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [add_75, add_81],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_88: {type: multiply, grid_loc: [6, 10], grid_size: [1, 1], inputs: [matmul_85, constant_1_multiply_88],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_89: {type: add, grid_loc: [6, 11], grid_size: [1, 1], inputs: [multiply_88, attention_mask_s_brcst_m2_10_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_90.dc.exp.0: {type: exp, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_89],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_90.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [softmax_90.dc.exp.0, lc.input_tensor.softmax_90.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_90.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_90.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_90.dc.multiply.3: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_90.dc.exp.0, softmax_90.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_100: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [softmax_90.dc.multiply.3, add_95],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_104: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [matmul_100, layer.1.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_106: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [matmul_104, layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_107: {type: add, grid_loc: [7, 11], grid_size: [1, 1], inputs: [add_106, layernorm_60_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_mean.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_107, lc.input_tensor.layernorm_108_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_108_sub: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_107, layernorm_108_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_108_sq: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_108_sub, layernorm_108_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_var.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_108_sq, lc.input_tensor.layernorm_108_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_108_var_plus_eps: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_108_var.lc1, constant_1_layernorm_108_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_108_sqrt: {type: sqrt, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_108_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_recip: {type: reciprocal, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_108_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_108_recip, lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_108_output: {type: multiply, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_108_sub, layernorm_108_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_108_weights: {type: multiply, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_108_output, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_108_bias: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_108_weights, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_121: {type: matmul, grid_loc: [9, 1], grid_size: [1, 3], inputs: [layernorm_108_bias, layer.1.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_123: {type: add, grid_loc: [9, 5], grid_size: [1, 2], inputs: [matmul_121, layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_124: {type: gelu, grid_loc: [9, 7], grid_size: [1, 2], inputs: [add_123],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_127: {type: matmul, grid_loc: [9, 9], grid_size: [1, 3], inputs: [gelu_124, layer.1.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}], input_0_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 256
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0, layer.1.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_129: {type: add, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_matmul_127_0, layer.1.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_130: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_129, e2e_layernorm_108_bias_0],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_mean.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [add_130, lc.input_tensor.layernorm_131_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_131_sub: {type: subtract, grid_loc: [0, 4], grid_size: [1, 1], inputs: [add_130, layernorm_131_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_131_sq: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_131_sub, layernorm_131_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_var.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_131_sq, lc.input_tensor.layernorm_131_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_131_var_plus_eps: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_131_var.lc1, constant_1_layernorm_131_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_131_sqrt: {type: sqrt, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_131_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_recip: {type: reciprocal, grid_loc: [0, 9], grid_size: [1, 1], inputs: [layernorm_131_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_131_recip, lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_131_output: {type: multiply, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_131_sub, layernorm_131_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_131_weights: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_131_output, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_131_bias: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_131_weights, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.2.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0, layer.2.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_164: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_131_bias, layer.2.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_166: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_164, layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0, layer.2.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_150: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_131_bias, layer.2.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_152: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_150, layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0, layer.2.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_144: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_131_bias, layer.2.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_146: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_144, layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_156: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_146, add_152],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_159: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_156, constant_1_multiply_159],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_160: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [multiply_159, e2e_attention_mask_s_brcst_m2_9_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_161.dc.exp.0: {type: exp, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_160],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_161.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [softmax_161.dc.exp.0, lc.input_tensor.softmax_161.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_161.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [1, 1], inputs: [softmax_161.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_161.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_161.dc.exp.0, softmax_161.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_171: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_161.dc.multiply.3, add_166],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_175: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_171, layer.2.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_177: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_175, layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_178: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_177, layernorm_131_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_mean.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_178, lc.input_tensor.layernorm_179_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_179_sub: {type: subtract, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_178, layernorm_179_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_179_sq: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_179_sub, layernorm_179_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_var.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_179_sq, lc.input_tensor.layernorm_179_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_179_var_plus_eps: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_179_var.lc1, constant_1_layernorm_179_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_179_sqrt: {type: sqrt, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_179_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_recip: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_179_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [layernorm_179_recip, lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_179_output: {type: multiply, grid_loc: [3, 9], grid_size: [1, 1], inputs: [layernorm_179_sub, layernorm_179_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_179_weights: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_179_output, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_179_bias: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_179_weights, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0, layer.2.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_192: {type: matmul, grid_loc: [4, 2], grid_size: [1, 3], inputs: [layernorm_179_bias, layer.2.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_194: {type: add, grid_loc: [4, 6], grid_size: [1, 2], inputs: [matmul_192, layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_195: {type: gelu, grid_loc: [4, 8], grid_size: [1, 2], inputs: [add_194],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_198: {type: matmul, grid_loc: [5, 0], grid_size: [1, 3], inputs: [gelu_195, layer.2.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_200: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_198, layer.2.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_201: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [add_200, layernorm_179_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_mean.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_201, lc.input_tensor.layernorm_202_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_202_sub: {type: subtract, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_201, layernorm_202_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_202_sq: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_202_sub, layernorm_202_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_var.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_202_sq, lc.input_tensor.layernorm_202_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_202_var_plus_eps: {type: add, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_202_var.lc1, constant_1_layernorm_202_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_202_sqrt: {type: sqrt, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_202_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_recip: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_202_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_202_recip, lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_202_output: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_202_sub, layernorm_202_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_202_weights: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_202_output, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_202_bias: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_202_weights, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.3.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0, layer.3.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_235: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [layernorm_202_bias, layer.3.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_237: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [matmul_235, layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0, layer.3.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_221: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_202_bias, layer.3.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_223: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_221, layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0, layer.3.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_215: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_202_bias, layer.3.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_217: {type: add, grid_loc: [6, 9], grid_size: [1, 1], inputs: [matmul_215, layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_227: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_217, add_223],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_230: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [matmul_227, constant_1_multiply_230],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_231: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [multiply_230, e2e_attention_mask_s_brcst_m2_8_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_232.dc.exp.0: {type: exp, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_231],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_232.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [softmax_232.dc.exp.0, lc.input_tensor.softmax_232.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_232.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [1, 1], inputs: [softmax_232.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_232.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [softmax_232.dc.exp.0, softmax_232.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_242: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [softmax_232.dc.multiply.3, add_237],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_246: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_242, layer.3.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_248: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [matmul_246, layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_249: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [add_248, layernorm_202_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_mean.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250_sub: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [add_249, layernorm_250_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250_sq: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_250_sub, layernorm_250_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_var.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_250_sq, lc.input_tensor.layernorm_250_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250_var_plus_eps: {type: add, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_250_var.lc1, constant_1_layernorm_250_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_250_sqrt: {type: sqrt, grid_loc: [8, 9], grid_size: [1, 1], inputs: [layernorm_250_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_recip: {type: reciprocal, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_250_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_250_recip, lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_250_output: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_250_sub, layernorm_250_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250_weights: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_250_output, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_250_bias: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_250_weights, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_263: {type: matmul, grid_loc: [9, 5], grid_size: [1, 3], inputs: [layernorm_250_bias, layer.3.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_265: {type: add, grid_loc: [9, 9], grid_size: [1, 2], inputs: [matmul_263, layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_2:
    target_device: 0
    input_count: 256
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0, layer.3.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    gelu_266: {type: gelu, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_add_265_0],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_269: {type: matmul, grid_loc: [0, 2], grid_size: [1, 3], inputs: [gelu_266, layer.3.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_271: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_269, layer.3.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_272: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_271, e2e_layernorm_250_bias_0],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_mean.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [add_272, lc.input_tensor.layernorm_273_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_273_sub: {type: subtract, grid_loc: [0, 9], grid_size: [1, 1], inputs: [add_272, layernorm_273_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_273_sq: {type: multiply, grid_loc: [0, 10], grid_size: [1, 1], inputs: [layernorm_273_sub, layernorm_273_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_var.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [layernorm_273_sq, lc.input_tensor.layernorm_273_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_273_var_plus_eps: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_273_var.lc1, constant_1_layernorm_273_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_273_sqrt: {type: sqrt, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_273_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_recip: {type: reciprocal, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_273_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_273_recip, lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_273_output: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_273_sub, layernorm_273_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_273_weights: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_273_output, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_273_bias: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_273_weights, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.4.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0, layer.4.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_306: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [layernorm_273_bias, layer.4.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_308: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [matmul_306, layer.4.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0, layer.4.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_292: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_273_bias, layer.4.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_294: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_292, layer.4.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0, layer.4.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_286: {type: matmul, grid_loc: [1, 9], grid_size: [1, 1], inputs: [layernorm_273_bias, layer.4.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_288: {type: add, grid_loc: [1, 11], grid_size: [1, 1], inputs: [matmul_286, layer.4.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_298: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_288, add_294],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_301: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_298, constant_1_multiply_301],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_302: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [multiply_301, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_303.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_302],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_303.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_303.dc.exp.0, lc.input_tensor.softmax_303.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_303.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 8], grid_size: [1, 1], inputs: [softmax_303.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_303.dc.multiply.3: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [softmax_303.dc.exp.0, softmax_303.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_313: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [softmax_303.dc.multiply.3, add_308],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_317: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_313, layer.4.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_319: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_317, layer.4.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_320: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_319, layernorm_273_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_321_mean.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_320, lc.input_tensor.layernorm_321_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_321_sub: {type: subtract, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_320, layernorm_321_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_321_sq: {type: multiply, grid_loc: [3, 8], grid_size: [1, 1], inputs: [layernorm_321_sub, layernorm_321_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_321_var.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [layernorm_321_sq, lc.input_tensor.layernorm_321_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_321_var_plus_eps: {type: add, grid_loc: [3, 10], grid_size: [1, 1], inputs: [layernorm_321_var.lc1, constant_1_layernorm_321_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_321_sqrt: {type: sqrt, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_321_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_321_recip: {type: reciprocal, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_321_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_321_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_321_recip, lc.input_tensor.layernorm_321_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_321_output: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_321_sub, layernorm_321_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_321_weights: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_321_output, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_321_bias: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_321_weights, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0, layer.4.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_334: {type: matmul, grid_loc: [4, 7], grid_size: [1, 3], inputs: [layernorm_321_bias, layer.4.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_336: {type: add, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_334, layer.4.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_337: {type: gelu, grid_loc: [5, 2], grid_size: [1, 2], inputs: [add_336],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_340: {type: matmul, grid_loc: [5, 4], grid_size: [1, 3], inputs: [gelu_337, layer.4.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_342: {type: add, grid_loc: [5, 8], grid_size: [1, 1], inputs: [matmul_340, layer.4.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_343: {type: add, grid_loc: [5, 9], grid_size: [1, 1], inputs: [add_342, layernorm_321_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_344_mean.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [add_343, lc.input_tensor.layernorm_344_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_344_sub: {type: subtract, grid_loc: [5, 11], grid_size: [1, 1], inputs: [add_343, layernorm_344_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_344_sq: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_344_sub, layernorm_344_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_344_var.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_344_sq, lc.input_tensor.layernorm_344_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_344_var_plus_eps: {type: add, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_344_var.lc1, constant_1_layernorm_344_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_344_sqrt: {type: sqrt, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_344_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_344_recip: {type: reciprocal, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_344_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_344_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_344_recip, lc.input_tensor.layernorm_344_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_344_output: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_344_sub, layernorm_344_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_344_weights: {type: multiply, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_344_output, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_344_bias: {type: add, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_344_weights, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.5.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0, layer.5.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_377: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_344_bias, layer.5.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_379: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [matmul_377, layer.5.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0, layer.5.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_363: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_344_bias, layer.5.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_365: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_363, layer.5.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0, layer.5.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_357: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_344_bias, layer.5.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_359: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [matmul_357, layer.5.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_369: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_359, add_365],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_372: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_369, constant_1_multiply_372],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_373: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [multiply_372, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_374.dc.exp.0: {type: exp, grid_loc: [7, 8], grid_size: [1, 1], inputs: [add_373],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_374.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [softmax_374.dc.exp.0, lc.input_tensor.softmax_374.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_374.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 10], grid_size: [1, 1], inputs: [softmax_374.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_374.dc.multiply.3: {type: multiply, grid_loc: [7, 11], grid_size: [1, 1], inputs: [softmax_374.dc.exp.0, softmax_374.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_384: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_374.dc.multiply.3, add_379],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_388: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [matmul_384, layer.5.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_390: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [matmul_388, layer.5.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_391: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [add_390, layernorm_344_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_392_mean.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [add_391, lc.input_tensor.layernorm_392_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_392_sub: {type: subtract, grid_loc: [8, 9], grid_size: [1, 1], inputs: [add_391, layernorm_392_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_392_sq: {type: multiply, grid_loc: [8, 10], grid_size: [1, 1], inputs: [layernorm_392_sub, layernorm_392_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_392_var.lc1: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_392_sq, lc.input_tensor.layernorm_392_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_392_var_plus_eps: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_392_var.lc1, constant_1_layernorm_392_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_392_sqrt: {type: sqrt, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_392_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_392_recip: {type: reciprocal, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_392_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_392_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_392_recip, lc.input_tensor.layernorm_392_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_392_output: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_392_sub, layernorm_392_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_392_weights: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_392_output, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_392_bias: {type: add, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_392_weights, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_405: {type: matmul, grid_loc: [9, 9], grid_size: [1, 3], inputs: [layernorm_392_bias, layer.5.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}

  fwd_3:
    target_device: 0
    input_count: 256
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0, layer.5.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.5.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    add_407: {type: add, grid_loc: [0, 1], grid_size: [1, 2], inputs: [e2e_matmul_405_0, layer.5.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_408: {type: gelu, grid_loc: [0, 3], grid_size: [1, 2], inputs: [add_407],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_411: {type: matmul, grid_loc: [0, 5], grid_size: [1, 3], inputs: [gelu_408, layer.5.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_413: {type: add, grid_loc: [0, 9], grid_size: [1, 1], inputs: [matmul_411, layer.5.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_414: {type: add, grid_loc: [0, 10], grid_size: [1, 1], inputs: [add_413, e2e_layernorm_392_bias_0],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_415_mean.lc1: {type: matmul, grid_loc: [0, 11], grid_size: [1, 1], inputs: [add_414, lc.input_tensor.layernorm_415_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_415_sub: {type: subtract, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_414, layernorm_415_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_415_sq: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_415_sub, layernorm_415_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_415_var.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_415_sq, lc.input_tensor.layernorm_415_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_415_var_plus_eps: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_415_var.lc1, constant_1_layernorm_415_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_415_sqrt: {type: sqrt, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_415_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_415_recip: {type: reciprocal, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_415_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_415_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_415_recip, lc.input_tensor.layernorm_415_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_415_output: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_415_sub, layernorm_415_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_415_weights: {type: multiply, grid_loc: [1, 9], grid_size: [1, 1], inputs: [layernorm_415_output, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_415_bias: {type: add, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_415_weights, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.6.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0, layer.6.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_448: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_415_bias, layer.6.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_450: {type: add, grid_loc: [3, 3], grid_size: [1, 1], inputs: [matmul_448, layer.6.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0, layer.6.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_434: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_415_bias, layer.6.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_436: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_434, layer.6.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0, layer.6.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_428: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_415_bias, layer.6.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_430: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_428, layer.6.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_440: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_430, add_436],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_443: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_440, constant_1_multiply_443],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_444: {type: add, grid_loc: [2, 8], grid_size: [1, 1], inputs: [multiply_443, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_445.dc.exp.0: {type: exp, grid_loc: [2, 9], grid_size: [1, 1], inputs: [add_444],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_445.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_445.dc.exp.0, lc.input_tensor.softmax_445.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_445.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 11], grid_size: [1, 1], inputs: [softmax_445.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_445.dc.multiply.3: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_445.dc.exp.0, softmax_445.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_455: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [softmax_445.dc.multiply.3, add_450],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_459: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [matmul_455, layer.6.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_461: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_459, layer.6.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_462: {type: add, grid_loc: [3, 8], grid_size: [1, 1], inputs: [add_461, layernorm_415_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_463_mean.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [add_462, lc.input_tensor.layernorm_463_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_463_sub: {type: subtract, grid_loc: [3, 10], grid_size: [1, 1], inputs: [add_462, layernorm_463_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_463_sq: {type: multiply, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_463_sub, layernorm_463_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_463_var.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_463_sq, lc.input_tensor.layernorm_463_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_463_var_plus_eps: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_463_var.lc1, constant_1_layernorm_463_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_463_sqrt: {type: sqrt, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_463_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_463_recip: {type: reciprocal, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_463_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_463_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_463_recip, lc.input_tensor.layernorm_463_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_463_output: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_463_sub, layernorm_463_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_463_weights: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_463_output, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_463_bias: {type: add, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_463_weights, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0, layer.6.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.6.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_476: {type: matmul, grid_loc: [5, 0], grid_size: [1, 3], inputs: [layernorm_463_bias, layer.6.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_478: {type: add, grid_loc: [5, 4], grid_size: [1, 2], inputs: [matmul_476, layer.6.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_479: {type: gelu, grid_loc: [5, 6], grid_size: [1, 2], inputs: [add_478],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_482: {type: matmul, grid_loc: [5, 8], grid_size: [1, 3], inputs: [gelu_479, layer.6.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_484: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_482, layer.6.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_485: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_484, layernorm_463_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_486_mean.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_485, lc.input_tensor.layernorm_486_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_486_sub: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [add_485, layernorm_486_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_486_sq: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_486_sub, layernorm_486_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_486_var.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_486_sq, lc.input_tensor.layernorm_486_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_486_var_plus_eps: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_486_var.lc1, constant_1_layernorm_486_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_486_sqrt: {type: sqrt, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_486_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_486_recip: {type: reciprocal, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_486_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_486_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_486_recip, lc.input_tensor.layernorm_486_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_486_output: {type: multiply, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_486_sub, layernorm_486_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_486_weights: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_486_output, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_486_bias: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_486_weights, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.7.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.7.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0, layer.7.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_519: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_486_bias, layer.7.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_521: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [matmul_519, layer.7.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0, layer.7.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_505: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_486_bias, layer.7.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_507: {type: add, grid_loc: [7, 8], grid_size: [1, 1], inputs: [matmul_505, layer.7.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0, layer.7.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_499: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_486_bias, layer.7.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_501: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_499, layer.7.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_511: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [add_501, add_507],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_514: {type: multiply, grid_loc: [7, 10], grid_size: [1, 1], inputs: [matmul_511, constant_1_multiply_514],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_515: {type: add, grid_loc: [7, 11], grid_size: [1, 1], inputs: [multiply_514, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_516.dc.exp.0: {type: exp, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_515],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_516.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [softmax_516.dc.exp.0, lc.input_tensor.softmax_516.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_516.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 2], grid_size: [1, 1], inputs: [softmax_516.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_516.dc.multiply.3: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_516.dc.exp.0, softmax_516.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_526: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_516.dc.multiply.3, add_521],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_530: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [matmul_526, layer.7.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_532: {type: add, grid_loc: [8, 10], grid_size: [1, 1], inputs: [matmul_530, layer.7.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_533: {type: add, grid_loc: [8, 11], grid_size: [1, 1], inputs: [add_532, layernorm_486_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_534_mean.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [add_533, lc.input_tensor.layernorm_534_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_534_sub: {type: subtract, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_533, layernorm_534_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_534_sq: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_534_sub, layernorm_534_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_534_var.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_534_sq, lc.input_tensor.layernorm_534_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_534_var_plus_eps: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_534_var.lc1, constant_1_layernorm_534_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_534_sqrt: {type: sqrt, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_534_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_534_recip: {type: reciprocal, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_534_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_534_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_534_recip, lc.input_tensor.layernorm_534_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_534_output: {type: multiply, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_534_sub, layernorm_534_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_534_weights: {type: multiply, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_534_output, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_4:
    target_device: 0
    input_count: 256
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_534_bias: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_534_weights_0, e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0, layer.7.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.7.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_547: {type: matmul, grid_loc: [0, 1], grid_size: [1, 3], inputs: [layernorm_534_bias, layer.7.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_549: {type: add, grid_loc: [0, 5], grid_size: [1, 2], inputs: [matmul_547, layer.7.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_550: {type: gelu, grid_loc: [0, 7], grid_size: [1, 2], inputs: [add_549],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_553: {type: matmul, grid_loc: [0, 9], grid_size: [1, 3], inputs: [gelu_550, layer.7.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_555: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_553, layer.7.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_556: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_555, layernorm_534_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_557_mean.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_556, lc.input_tensor.layernorm_557_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_557_sub: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_556, layernorm_557_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_557_sq: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_557_sub, layernorm_557_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_557_var.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_557_sq, lc.input_tensor.layernorm_557_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_557_var_plus_eps: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_557_var.lc1, constant_1_layernorm_557_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_557_sqrt: {type: sqrt, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_557_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_557_recip: {type: reciprocal, grid_loc: [1, 9], grid_size: [1, 1], inputs: [layernorm_557_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_557_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 10], grid_size: [1, 1], inputs: [layernorm_557_recip, lc.input_tensor.layernorm_557_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_557_output: {type: multiply, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_557_sub, layernorm_557_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_557_weights: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_557_output, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_557_bias: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_557_weights, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.8.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.8.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0, layer.8.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_590: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_557_bias, layer.8.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_592: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_590, layer.8.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0, layer.8.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_576: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_557_bias, layer.8.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_578: {type: add, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_576, layer.8.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0, layer.8.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_570: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_557_bias, layer.8.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_572: {type: add, grid_loc: [2, 6], grid_size: [1, 1], inputs: [matmul_570, layer.8.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_582: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [add_572, add_578],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_585: {type: multiply, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_582, constant_1_multiply_585],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_586: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [multiply_585, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_587.dc.exp.0: {type: exp, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_586],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_587.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [softmax_587.dc.exp.0, lc.input_tensor.softmax_587.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_587.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_587.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_587.dc.multiply.3: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [softmax_587.dc.exp.0, softmax_587.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_597: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [softmax_587.dc.multiply.3, add_592],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_601: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [matmul_597, layer.8.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_603: {type: add, grid_loc: [3, 11], grid_size: [1, 1], inputs: [matmul_601, layer.8.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_604: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_603, layernorm_557_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_605_mean.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_604, lc.input_tensor.layernorm_605_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_605_sub: {type: subtract, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_604, layernorm_605_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_605_sq: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_605_sub, layernorm_605_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_605_var.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_605_sq, lc.input_tensor.layernorm_605_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_605_var_plus_eps: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_605_var.lc1, constant_1_layernorm_605_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_605_sqrt: {type: sqrt, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_605_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_605_recip: {type: reciprocal, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_605_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_605_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_605_recip, lc.input_tensor.layernorm_605_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_605_output: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_605_sub, layernorm_605_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_605_weights: {type: multiply, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_605_output, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_605_bias: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_605_weights, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0, layer.8.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.8.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_618: {type: matmul, grid_loc: [5, 2], grid_size: [1, 3], inputs: [layernorm_605_bias, layer.8.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_620: {type: add, grid_loc: [5, 6], grid_size: [1, 2], inputs: [matmul_618, layer.8.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_621: {type: gelu, grid_loc: [5, 8], grid_size: [1, 2], inputs: [add_620],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_624: {type: matmul, grid_loc: [6, 0], grid_size: [1, 3], inputs: [gelu_621, layer.8.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_626: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [matmul_624, layer.8.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_627: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [add_626, layernorm_605_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_628_mean.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [add_627, lc.input_tensor.layernorm_628_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_628_sub: {type: subtract, grid_loc: [6, 7], grid_size: [1, 1], inputs: [add_627, layernorm_628_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_628_sq: {type: multiply, grid_loc: [6, 8], grid_size: [1, 1], inputs: [layernorm_628_sub, layernorm_628_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_628_var.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [layernorm_628_sq, lc.input_tensor.layernorm_628_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_628_var_plus_eps: {type: add, grid_loc: [6, 10], grid_size: [1, 1], inputs: [layernorm_628_var.lc1, constant_1_layernorm_628_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_628_sqrt: {type: sqrt, grid_loc: [6, 11], grid_size: [1, 1], inputs: [layernorm_628_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_628_recip: {type: reciprocal, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_628_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_628_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_628_recip, lc.input_tensor.layernorm_628_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_628_output: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_628_sub, layernorm_628_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_628_weights: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_628_output, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_628_bias: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_628_weights, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.9.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0, layer.9.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_661: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_628_bias, layer.9.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_663: {type: add, grid_loc: [8, 10], grid_size: [1, 1], inputs: [matmul_661, layer.9.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0, layer.9.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_647: {type: matmul, grid_loc: [7, 10], grid_size: [1, 1], inputs: [layernorm_628_bias, layer.9.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_649: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_647, layer.9.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0, layer.9.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_641: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_628_bias, layer.9.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_643: {type: add, grid_loc: [7, 9], grid_size: [1, 1], inputs: [matmul_641, layer.9.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_653: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_643, add_649],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_656: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [matmul_653, constant_1_multiply_656],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_657: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [multiply_656, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_658.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [1, 1], inputs: [add_657],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_658.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_658.dc.exp.0, lc.input_tensor.softmax_658.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_658.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_658.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_658.dc.multiply.3: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_658.dc.exp.0, softmax_658.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_668: {type: matmul, grid_loc: [8, 11], grid_size: [1, 1], inputs: [softmax_658.dc.multiply.3, add_663],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_672: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_668, layer.9.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_674: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_672, layer.9.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_675: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_674, layernorm_628_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_676_mean.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_675, lc.input_tensor.layernorm_676_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_676_sub: {type: subtract, grid_loc: [9, 5], grid_size: [1, 1], inputs: [add_675, layernorm_676_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_676_sq: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_676_sub, layernorm_676_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_676_var.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_676_sq, lc.input_tensor.layernorm_676_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_676_var_plus_eps: {type: add, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_676_var.lc1, constant_1_layernorm_676_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_676_sqrt: {type: sqrt, grid_loc: [9, 9], grid_size: [1, 1], inputs: [layernorm_676_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_676_recip: {type: reciprocal, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_676_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_676_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [layernorm_676_recip, lc.input_tensor.layernorm_676_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_5:
    target_device: 0
    input_count: 256
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_676_output: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_676_sub_0, e2e_layernorm_676_recip_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_676_weights: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_676_output, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_676_bias: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_676_weights, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0, layer.9.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_689: {type: matmul, grid_loc: [0, 5], grid_size: [1, 3], inputs: [layernorm_676_bias, layer.9.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_691: {type: add, grid_loc: [0, 9], grid_size: [1, 2], inputs: [matmul_689, layer.9.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_692: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [add_691],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_695: {type: matmul, grid_loc: [1, 2], grid_size: [1, 3], inputs: [gelu_692, layer.9.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_697: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_695, layer.9.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_698: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_697, layernorm_676_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_699_mean.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [add_698, lc.input_tensor.layernorm_699_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_699_sub: {type: subtract, grid_loc: [1, 9], grid_size: [1, 1], inputs: [add_698, layernorm_699_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_699_sq: {type: multiply, grid_loc: [1, 10], grid_size: [1, 1], inputs: [layernorm_699_sub, layernorm_699_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_699_var.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_699_sq, lc.input_tensor.layernorm_699_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_699_var_plus_eps: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_699_var.lc1, constant_1_layernorm_699_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_699_sqrt: {type: sqrt, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_699_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_699_recip: {type: reciprocal, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_699_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_699_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_699_recip, lc.input_tensor.layernorm_699_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_699_output: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_699_sub, layernorm_699_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_699_weights: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_699_output, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_699_bias: {type: add, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_699_weights, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.10.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.10.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 11], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0, layer.10.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_732: {type: matmul, grid_loc: [3, 10], grid_size: [1, 1], inputs: [layernorm_699_bias, layer.10.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_734: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [matmul_732, layer.10.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0, layer.10.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_718: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_699_bias, layer.10.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_720: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_718, layer.10.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0, layer.10.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_712: {type: matmul, grid_loc: [2, 9], grid_size: [1, 1], inputs: [layernorm_699_bias, layer.10.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_714: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_712, layer.10.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_724: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [add_714, add_720],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_727: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_724, constant_1_multiply_727],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_728: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [multiply_727, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_729.dc.exp.0: {type: exp, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_728],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_729.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_729.dc.exp.0, lc.input_tensor.softmax_729.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_729.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 8], grid_size: [1, 1], inputs: [softmax_729.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_729.dc.multiply.3: {type: multiply, grid_loc: [3, 9], grid_size: [1, 1], inputs: [softmax_729.dc.exp.0, softmax_729.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_739: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [softmax_729.dc.multiply.3, add_734],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_743: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [matmul_739, layer.10.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_745: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_743, layer.10.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_746: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [add_745, layernorm_699_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_747_mean.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_746, lc.input_tensor.layernorm_747_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_747_sub: {type: subtract, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_746, layernorm_747_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_747_sq: {type: multiply, grid_loc: [4, 8], grid_size: [1, 1], inputs: [layernorm_747_sub, layernorm_747_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_747_var.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_747_sq, lc.input_tensor.layernorm_747_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_747_var_plus_eps: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [layernorm_747_var.lc1, constant_1_layernorm_747_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_747_sqrt: {type: sqrt, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_747_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_747_recip: {type: reciprocal, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_747_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_747_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_747_recip, lc.input_tensor.layernorm_747_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_747_output: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_747_sub, layernorm_747_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_747_weights: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_747_output, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_747_bias: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_747_weights, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0, layer.10.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.10.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_760: {type: matmul, grid_loc: [5, 7], grid_size: [1, 3], inputs: [layernorm_747_bias, layer.10.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_762: {type: add, grid_loc: [6, 0], grid_size: [1, 2], inputs: [matmul_760, layer.10.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_763: {type: gelu, grid_loc: [6, 2], grid_size: [1, 2], inputs: [add_762],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_766: {type: matmul, grid_loc: [6, 4], grid_size: [1, 3], inputs: [gelu_763, layer.10.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_768: {type: add, grid_loc: [6, 8], grid_size: [1, 1], inputs: [matmul_766, layer.10.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_769: {type: add, grid_loc: [6, 9], grid_size: [1, 1], inputs: [add_768, layernorm_747_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_770_mean.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [add_769, lc.input_tensor.layernorm_770_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_770_sub: {type: subtract, grid_loc: [6, 11], grid_size: [1, 1], inputs: [add_769, layernorm_770_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_770_sq: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_770_sub, layernorm_770_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_770_var.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_770_sq, lc.input_tensor.layernorm_770_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_770_var_plus_eps: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_770_var.lc1, constant_1_layernorm_770_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_770_sqrt: {type: sqrt, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_770_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_770_recip: {type: reciprocal, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_770_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_770_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_770_recip, lc.input_tensor.layernorm_770_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_770_output: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_770_sub, layernorm_770_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_770_weights: {type: multiply, grid_loc: [7, 8], grid_size: [1, 1], inputs: [layernorm_770_output, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_770_bias: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [layernorm_770_weights, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.11.attention.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.11.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0, layer.11.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_803: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [layernorm_770_bias, layer.11.attention.self.value.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_805: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_803, layer.11.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0, layer.11.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_789: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_770_bias, layer.11.attention.self.key.weight],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_791: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [matmul_789, layer.11.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0, layer.11.attention.self.query.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_783: {type: matmul, grid_loc: [7, 11], grid_size: [1, 1], inputs: [layernorm_770_bias, layer.11.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_785: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [matmul_783, layer.11.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_795: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [add_785, add_791],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_798: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [matmul_795, constant_1_multiply_798],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}, broadcast: {c: 4}, broadcast: {r: 4}]}
    add_799: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [multiply_798, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_800.dc.exp.0: {type: exp, grid_loc: [8, 9], grid_size: [1, 1], inputs: [add_799],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_800.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [softmax_800.dc.exp.0, lc.input_tensor.softmax_800.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_800.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 11], grid_size: [1, 1], inputs: [softmax_800.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_800.dc.multiply.3: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_800.dc.exp.0, softmax_800.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_810: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_800.dc.multiply.3, add_805],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_814: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [matmul_810, layer.11.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_816: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [matmul_814, layer.11.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_817: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [add_816, layernorm_770_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_818_mean.lc1: {type: matmul, grid_loc: [9, 8], grid_size: [1, 1], inputs: [add_817, lc.input_tensor.layernorm_818_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_818_sub: {type: subtract, grid_loc: [9, 9], grid_size: [1, 1], inputs: [add_817, layernorm_818_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_818_sq: {type: multiply, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_818_sub, layernorm_818_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_818_var.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [layernorm_818_sq, lc.input_tensor.layernorm_818_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}

  fwd_6:
    target_device: 0
    input_count: 256
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_818_var_plus_eps: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_818_var.lc1_0, constant_1_layernorm_818_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_818_sqrt: {type: sqrt, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_818_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_818_recip: {type: reciprocal, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_818_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_818_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_818_recip, lc.input_tensor.layernorm_818_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_818_output: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_layernorm_818_sub_0, layernorm_818_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_818_weights: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_818_output, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_818_bias: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_818_weights, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0, layer.11.output.dense.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.11.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_831: {type: matmul, grid_loc: [0, 9], grid_size: [1, 3], inputs: [layernorm_818_bias, layer.11.intermediate.dense.weight],
         t: 1, mblock: [4, 4], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 8, u_kt: 3}}
    add_833: {type: add, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_831, layer.11.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_834: {type: gelu, grid_loc: [1, 3], grid_size: [1, 2], inputs: [add_833],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_837: {type: matmul, grid_loc: [1, 5], grid_size: [1, 3], inputs: [gelu_834, layer.11.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 16}}
    add_839: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [matmul_837, layer.11.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_840: {type: add, grid_loc: [1, 10], grid_size: [1, 1], inputs: [add_839, layernorm_818_bias],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_841_mean.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [add_840, lc.input_tensor.layernorm_841_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_841_sub: {type: subtract, grid_loc: [2, 0], grid_size: [1, 1], inputs: [add_840, layernorm_841_mean.lc1],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_841_sq: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_841_sub, layernorm_841_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_841_var.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_841_sq, lc.input_tensor.layernorm_841_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_841_var_plus_eps: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_841_var.lc1, constant_1_layernorm_841_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_841_sqrt: {type: sqrt, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_841_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_841_recip: {type: reciprocal, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_841_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_841_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_841_recip, lc.input_tensor.layernorm_841_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_841_output: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_841_sub, layernorm_841_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_841_weights: {type: multiply, grid_loc: [2, 9], grid_size: [1, 1], inputs: [layernorm_841_output, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_841_bias: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [layernorm_841_weights, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 256, $lptr_q6: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0, $gptr_q3: 0, $gptr_q2: 0, $c_one: 1, $lptr_q1: 0, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_108_bias_0, e2e_matmul_127_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_17: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_19.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_37_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_60_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_88: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_90.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_108_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 1024]
    -   allocate_queue: [e2e_layernorm_250_bias_0, e2e_add_265_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_131_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_159: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_161.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_179_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_202_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_230: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_232.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_250_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               e2e_layernorm_108_bias_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_127_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}} }
    -   deallocate_queue: [e2e_layernorm_108_bias_0, e2e_matmul_127_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_392_bias_0, e2e_matmul_405_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_273_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_301: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_303.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_321_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_321_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_321_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_321_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_344_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_344_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_344_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_344_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_372: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_374.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_392_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_392_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_392_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_392_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               e2e_layernorm_250_bias_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_add_265_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}} }
    -   deallocate_queue: [e2e_layernorm_250_bias_0, e2e_add_265_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0, e2e_layernorm_534_weights_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_415_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_415_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_415_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_415_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_443: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_445.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_463_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_463_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_463_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_463_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_486_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_486_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_486_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_486_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_514: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_516.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_534_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_534_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_534_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_534_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_layernorm_392_bias_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_405_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}} }
    -   deallocate_queue: [e2e_layernorm_392_bias_0, e2e_matmul_405_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_676_sub_0, e2e_layernorm_676_recip_s_brcst_m1_0_0.lc1_0]
    -   execute: {graph_name: fwd_4, queue_settings: {
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_557_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_557_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_557_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_557_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_585: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_587.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_605_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_605_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_605_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_605_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_628_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_628_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_628_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_628_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_656: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_658.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_676_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_676_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_676_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_676_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_534_weights_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}} }
    -   deallocate_queue: [e2e_layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0, e2e_layernorm_534_weights_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_layernorm_818_sub_0, e2e_layernorm_818_var.lc1_0]
    -   execute: {graph_name: fwd_5, queue_settings: {
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_699_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_699_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_699_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_699_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_727: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_729.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_747_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_747_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_747_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_747_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_770_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_770_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_770_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_770_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_798: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_800.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_818_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_818_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_layernorm_676_sub_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layernorm_676_recip_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}} }
    -   deallocate_queue: [e2e_layernorm_676_sub_0, e2e_layernorm_676_recip_s_brcst_m1_0_0.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_6, queue_settings: {
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_layernorm_818_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_818_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_841_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_841_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_841_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_841_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_layernorm_818_sub_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_818_var.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}} }
    -   deallocate_queue: [e2e_layernorm_818_var.lc1_0, e2e_layernorm_818_sub_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 512]
    - endloop

test-config:
  io-config:
    inputs: [hidden_states, attention_mask]
    outputs: [run_bert.output_layernorm_841_bias]

performance-check:
  fwd_0:
    graph-name: fwd_0
    program-name: run_fwd
    tensors-per-second:
      expected: 1510
      rtol: 0.10
  fwd_1:
    graph-name: fwd_1
    program-name: run_fwd
    tensors-per-second:
      expected: 1510
      rtol: 0.10
  fwd_2:
    graph-name: fwd_2
    program-name: run_fwd
    tensors-per-second:
      expected: 1510
      rtol: 0.10
  fwd_3:
    graph-name: fwd_3
    program-name: run_fwd
    tensors-per-second:
      expected: 1510
      rtol: 0.10
  fwd_4:
    graph-name: fwd_4
    program-name: run_fwd
    tensors-per-second:
      expected: 1510
      rtol: 0.10
  fwd_5:
    graph-name: fwd_5
    program-name: run_fwd
    tensors-per-second:
      expected: 1480
      rtol: 0.10
